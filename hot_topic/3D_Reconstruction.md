# üîç 3D_Reconstruction Papers ¬∑ 2025-03-31

[![Total Papers](https://img.shields.io/badge/Papers-19-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Reconstruction` `Nerf` `Gaussian`  
**Filter**: `2D`

---

## üìö Paper List

- **[Easi3R: Estimating Disentangled Motion from DUSt3R Without Training](http://arxiv.org/abs/2503.24391v1)**  `arXiv:2503.24391`  `cs.CV`  
  _Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen_
  <details open><summary>Abstract</summary>
  Recent advances in DUSt3R have enabled robust estimation of dense pointclouds and camera parameters of static scenes, leveraging Transformer networkarchitectures and direct supervision on large-scale 3D datasets. In contrast,the limited scale and diversity of available 4D datasets present a majorbottleneck for training a highly generalizable 4D model. This constraint hasdriven conventional 4D methods to fine-tune 3D models on scalable dynamic videodata with additional geometric priors such as optical flow and depths. In thiswork, we take an opposite path and introduce Easi3R, a simple yet efficienttraining-free method for 4D reconstruction. Our approach applies attentionadaptation during inference, eliminating the need for from-scratch pre-trainingor network fine-tuning. We find that the attention layers in DUSt3R inherentlyencode rich information about camera and object motion. By carefullydisentangling these attention maps, we achieve accurate dynamic regionsegmentation, camera pose estimation, and 4D dense point map reconstruction.Extensive experiments on real-world dynamic videos demonstrate that ourlightweight attention adaptation significantly outperforms previousstate-of-the-art methods that are trained or finetuned on extensive dynamicdatasets. Our code is publicly available for research purpose athttps://easi3r.github.io/
  </details>

- **[Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views](http://arxiv.org/abs/2503.24382v1)**  `arXiv:2503.24382`  `cs.CV`  
  _Chong Bao, Xiyu Zhang, Zehao Yu, Jiale Shi, Guofeng Zhang, Songyou Peng, et al._
  <details open><summary>Abstract</summary>
  Neural rendering has demonstrated remarkable success in high-quality 3Dneural reconstruction and novel view synthesis with dense input views andaccurate poses. However, applying it to extremely sparse, unposed views inunbounded 360{\deg} scenes remains a challenging problem. In this paper, wepropose a novel neural rendering framework to accomplish the unposed andextremely sparse-view 3D reconstruction in unbounded 360{\deg} scenes. Toresolve the spatial ambiguity inherent in unbounded scenes with sparse inputviews, we propose a layered Gaussian-based representation to effectively modelthe scene with distinct spatial layers. By employing a dense stereoreconstruction model to recover coarse geometry, we introduce a layer-specificbootstrap optimization to refine the noise and fill occluded regions in thereconstruction. Furthermore, we propose an iterative fusion of reconstructionand generation alongside an uncertainty-aware training approach to facilitatemutual conditioning and enhancement between these two processes. Comprehensiveexperiments show that our approach outperforms existing state-of-the-artmethods in terms of rendering quality and surface reconstruction accuracy.Project page: https://zju3dv.github.io/free360/
  </details>

- **[ERUPT: Efficient Rendering with Unposed Patch Transformer](http://arxiv.org/abs/2503.24374v1)**  `arXiv:2503.24374`  `cs.CV`  
  _Maxim V. Shugaev, Vincent Chen, Maxim Karrenbach, Kyle Ashley, Bridget Kennedy, Naresh P. Cuntoor_
  <details open><summary>Abstract</summary>
  This work addresses the problem of novel view synthesis in diverse scenesfrom small collections of RGB images. We propose ERUPT (Efficient Renderingwith Unposed Patch Transformer) a state-of-the-art scene reconstruction modelcapable of efficient scene rendering using unposed imagery. We introducepatch-based querying, in contrast to existing pixel-based queries, to reducethe compute required to render a target view. This makes our model highlyefficient both during training and at inference, capable of rendering at 600fps on commercial hardware. Notably, our model is designed to use a learnedlatent camera pose which allows for training using unposed targets in datasetswith sparse or inaccurate ground truth camera pose. We show that our approachcan generalize on large real-world data and introduce a new benchmark dataset(MSVS-1M) for latent view synthesis using street-view imagery collected fromMapillary. In contrast to NeRF and Gaussian Splatting, which require denseimagery and precise metadata, ERUPT can render novel views of arbitrary sceneswith as few as five unposed input images. ERUPT achieves better rendered imagequality than current state-of-the-art methods for unposed image synthesistasks, reduces labeled data requirements by ~95\% and decreases computationalrequirements by an order of magnitude, providing efficient novel view synthesisfor diverse real-world scenes.
  </details>

- **[Self-Supervised Pretraining for Aerial Road Extraction](http://arxiv.org/abs/2503.24326v1)**  `arXiv:2503.24326`  `cs.LG` `cs.CV`  
  _Rupert Polley, Sai Vignesh Abishek Deenadayalan, J. Marius Z√∂llner_
  <details open><summary>Abstract</summary>
  Deep neural networks for aerial image segmentation require large amounts oflabeled data, but high-quality aerial datasets with precise annotations arescarce and costly to produce. To address this limitation, we propose aself-supervised pretraining method that improves segmentation performance whilereducing reliance on labeled data. Our approach uses inpainting-basedpretraining, where the model learns to reconstruct missing regions in aerialimages, capturing their inherent structure before being fine-tuned for roadextraction. This method improves generalization, enhances robustness to domainshifts, and is invariant to model architecture and dataset choice. Experimentsshow that our pretraining significantly boosts segmentation accuracy,especially in low-data regimes, making it a scalable solution for aerial imageanalysis.
  </details>

- **[Point Tracking in Surgery--The 2024 Surgical Tattoos in Infrared (STIR) Challenge](http://arxiv.org/abs/2503.24306v1)**  `arXiv:2503.24306`  `cs.CV`  
  _Adam Schmidt, Mert Asim Karaoglu, Soham Sinha, Mingang Jang, Ho-Gun Ha, Kyungmin Jung, et al._
  <details open><summary>Abstract</summary>
  Understanding tissue motion in surgery is crucial to enable applications indownstream tasks such as segmentation, 3D reconstruction, virtual tissuelandmarking, autonomous probe-based scanning, and subtask autonomy. Labeleddata are essential to enabling algorithms in these downstream tasks since theyallow us to quantify and train algorithms. This paper introduces a pointtracking challenge to address this, wherein participants can submit theiralgorithms for quantification. The submitted algorithms are evaluated using adataset named surgical tattoos in infrared (STIR), with the challenge aptlynamed the STIR Challenge 2024. The STIR Challenge 2024 comprises twoquantitative components: accuracy and efficiency. The accuracy component teststhe accuracy of algorithms on in vivo and ex vivo sequences. The efficiencycomponent tests the latency of algorithm inference. The challenge was conductedas a part of MICCAI EndoVis 2024. In this challenge, we had 8 total teams, with4 teams submitting before and 4 submitting after challenge day. This paperdetails the STIR Challenge 2024, which serves to move the field towards moreaccurate and efficient algorithms for spatial understanding in surgery. In thispaper we summarize the design, submissions, and results from the challenge. Thechallenge dataset is available here: https://zenodo.org/records/14803158 , andthe code for baseline models and metric calculation is available here:https://github.com/athaddius/STIRMetrics
  </details>

- **[Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality](http://arxiv.org/abs/2503.24277v1)**  `arXiv:2503.24277`  `cs.LG` `cs.AI`  
  _Sewoong Lee, Adam Davies, Marc E. Canby, Julia Hockenmaier_
  <details open><summary>Abstract</summary>
  Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanisticinterpretability, but leading SAE approaches with top-$k$ style activationfunctions lack theoretical grounding for selecting the hyperparameter $k$. SAEsare based on the linear representation hypothesis (LRH), which assumes that therepresentations of large language models (LLMs) are linearly encoded, and thesuperposition hypothesis (SH), which states that there can be more features inthe model than its dimensionality. We show that, based on the formaldefinitions of the LRH and SH, the magnitude of sparse feature vectors (thelatent representations learned by SAEs of the dense embeddings of LLMs) can beapproximated using their corresponding dense vector with a closed-form errorbound. To visualize this, we propose the ZF plot, which reveals a previouslyunknown relationship between LLM hidden embeddings and SAE feature vectors,allowing us to make the first empirical measurement of the extent to whichfeature vectors of pre-trained SAEs are over- or under-activated for a giveninput. Correspondingly, we introduce Approximate Feature Activation (AFA),which approximates the magnitude of the ground-truth sparse feature vector, andpropose a new evaluation metric derived from AFA to assess the alignmentbetween inputs and activations. We also leverage AFA to introduce a novel SAEarchitecture, the top-AFA SAE, leading to SAEs that: (a) are more in line withtheoretical justifications; and (b) obviate the need to tune SAE sparsityhyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achievereconstruction loss comparable to that of state-of-the-art top-k SAEs, withoutrequiring the hyperparameter $k$ to be tuned. Our code is available at:https://github.com/SewoongLee/top-afa-sae.
  </details>

- **[Visual Acoustic Fields](http://arxiv.org/abs/2503.24270v1)**  `arXiv:2503.24270`  `cs.CV` `cs.AI`  
  _Yuelei Li, Hyunjin Kim, Fangneng Zhan, Ri-Zhao Qiu, Mazeyu Ji, Xiaojun Shan, et al._
  <details open><summary>Abstract</summary>
  Objects produce different sounds when hit, and humans can intuitively inferhow an object might sound based on its appearance and material properties.Inspired by this intuition, we propose Visual Acoustic Fields, a framework thatbridges hitting sounds and visual signals within a 3D space using 3D GaussianSplatting (3DGS). Our approach features two key modules: sound generation andsound localization. The sound generation module leverages a conditionaldiffusion model, which takes multiscale features rendered from afeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, thesound localization module enables querying the 3D scene, represented by thefeature-augmented 3DGS, to localize hitting positions based on the soundsources. To support this framework, we introduce a novel pipeline forcollecting scene-level visual-sound sample pairs, achieving alignment betweencaptured images, impact locations, and corresponding sounds. To the best of ourknowledge, this is the first dataset to connect visual and acoustic signals ina 3D context. Extensive experiments on our dataset demonstrate theeffectiveness of Visual Acoustic Fields in generating plausible impact soundsand accurately localizing impact sources. Our project page is athttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.
  </details>

- **[JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation](http://arxiv.org/abs/2503.23951v1)**  `arXiv:2503.23951`  `cs.CV`  
  _Fangda Chen, Shanshan Zhao, Chuanfu Xu, Long Lan_
  <details open><summary>Abstract</summary>
  Recent text-to-video advancements have enabled coherent video synthesis fromprompts and expanded to fine-grained control over appearance and motion.However, existing methods either suffer from concept interference due tofeature domain mismatch caused by naive decoupled optimizations or exhibitappearance contamination induced by spatial feature leakage resulting from theentanglement of motion and appearance in reference video reconstructions. Inthis paper, we propose JointTuner, a novel adaptive joint training framework,to alleviate these issues. Specifically, we develop Adaptive LoRA, whichincorporates a context-aware gating mechanism, and integrate the gated LoRAcomponents into the spatial and temporal Transformers within the diffusionmodel. These components enable simultaneous optimization of appearance andmotion, eliminating concept interference. In addition, we introduce theAppearance-independent Temporal Loss, which decouples motion patterns fromintrinsic appearance in reference video reconstructions through anappearance-agnostic noise prediction task. The key innovation lies in addingframe-wise offset noise to the ground-truth Gaussian noise, perturbing itsdistribution, thereby disrupting spatial attributes associated with frameswhile preserving temporal coherence. Furthermore, we construct a benchmarkcomprising 90 appearance-motion customized combinations and 10 multi-typeautomatic metrics across four dimensions, facilitating a more comprehensiveevaluation for this customization task. Extensive experiments demonstrate thesuperior performance of our method compared to current advanced approaches.
  </details>

- **[Communication-Efficient and Personalized Federated Foundation Model Fine-Tuning via Tri-Matrix Adaptation](http://arxiv.org/abs/2503.23869v1)**  `arXiv:2503.23869`  `cs.LG`  
  _Yongle Li, Bo Liu, Sheng Huang, ZHeng ZHang, Xiaotong Yuan, Richang Hong_
  <details open><summary>Abstract</summary>
  In federated learning, fine-tuning pre-trained foundation models posessignificant challenges, particularly regarding high communication cost andsuboptimal model performance due to data heterogeneity between the clients. Toaddress these issues, this paper introduces communication-efficient federatedLoRA adaption (CE-LoRA), a method that employs a tri-factorization low-rankadaptation approach with personalized model parameter aggregation. We firstpresents a novel LoRA parameter factorization by introducing a small-size densematrix, which can significantly reduce the communication cost and achievecomparable empirical performance than transferring the low-rank parametermatrix used by existing methods. Without violating data privacy, the serverconsiders the client similarity in both training dataset and model parameterspace, and learns personalized weights for model aggregation. Our experimentson various LLM and VLM fine-tuning tasks demonstrate that CE-LoRA not onlysignificantly reduces communication overhead but also improves performanceunder not independently and identically distributed data conditions. Inaddition, CE-LoRA improves data privacy protection, effectively mitigatinggradient-based data reconstruction attacks.
  </details>

- **[Learned Image Compression and Restoration for Digital Pathology](http://arxiv.org/abs/2503.23862v1)**  `arXiv:2503.23862`  `cs.CV` `cs.AI`  
  _SeonYeong Lee, EonSeung Seong, DongEon Lee, SiYeoul Lee, Yubin Cho, Chunsu Park, et al._
  <details open><summary>Abstract</summary>
  Digital pathology images play a crucial role in medical diagnostics, buttheir ultra-high resolution and large file sizes pose significant challengesfor storage, transmission, and real-time visualization. To address theseissues, we propose CLERIC, a novel deep learning-based image compressionframework designed specifically for whole slide images (WSIs). CLERICintegrates a learnable lifting scheme and advanced convolutional techniques toenhance compression efficiency while preserving critical pathological details.Our framework employs a lifting-scheme transform in the analysis stage todecompose images into low- and high-frequency components, enabling morestructured latent representations. These components are processed throughparallel encoders incorporating Deformable Residual Blocks (DRB) and RecurrentResidual Blocks (R2B) to improve feature extraction and spatial adaptability.The synthesis stage applies an inverse lifting transform for effective imagereconstruction, ensuring high-fidelity restoration of fine-grained tissuestructures. We evaluate CLERIC on a digital pathology image dataset and compareits performance against state-of-the-art learned image compression (LIC)models. Experimental results demonstrate that CLERIC achieves superiorrate-distortion (RD) performance, significantly reducing storage requirementswhile maintaining high diagnostic image quality. Our study highlights thepotential of deep learning-based compression in digital pathology, facilitatingefficient data management and long-term storage while ensuring seamlessintegration into clinical workflows and AI-assisted diagnostic systems. Codeand models are available at: https://github.com/pnu-amilab/CLERIC.
  </details>

- **[WaveFormer: A 3D Transformer with Wavelet-Driven Feature Representation for Efficient Medical Image Segmentation](http://arxiv.org/abs/2503.23764v1)**  `arXiv:2503.23764`  `cs.CV` `cs.AI`  
  _Md Mahfuz Al Hasan, Mahdi Zaman, Abdul Jawad, Alberto Santamaria-Pang, Ho Hin Lee, Ivan Tarapov, et al._
  <details open><summary>Abstract</summary>
  Transformer-based architectures have advanced medical image analysis byeffectively modeling long-range dependencies, yet they often struggle in 3Dsettings due to substantial memory overhead and insufficient capture offine-grained local features. We address these limi- tations with WaveFormer, anovel 3D-transformer that: i) leverages the fundamental frequency-domainproperties of features for contextual rep- resentation, and ii) is inspired bythe top-down mechanism of the human visual recognition system, making it abiologically motivated architec- ture. By employing discrete wavelettransformations (DWT) at multiple scales, WaveFormer preserves both globalcontext and high-frequency de- tails while replacing heavy upsampling layerswith efficient wavelet-based summarization and reconstruction. Thissignificantly reduces the number of parameters, which is critical forreal-world deployment where compu- tational resources and training times areconstrained. Furthermore, the model is generic and easily adaptable to diverseapplications. Evaluations on BraTS2023, FLARE2021, and KiTS2023 demonstrateperformance on par with state-of-the-art methods while offering substantiallylower computational complexity.
  </details>

- **[Detail-aware multi-view stereo network for depth estimation](http://arxiv.org/abs/2503.23684v1)**  `arXiv:2503.23684`  `cs.CV`  
  _Haitao Tian, Junyang Li, Chenxing Wang, Helong Jiang_
  <details open><summary>Abstract</summary>
  Multi-view stereo methods have achieved great success for depth estimationbased on the coarse-to-fine depth learning frameworks, however, the existingmethods perform poorly in recovering the depth of object boundaries and detailregions. To address these issues, we propose a detail-aware multi-view stereonetwork (DA-MVSNet) with a coarse-to-fine framework. The geometric depth clueshidden in the coarse stage are utilized to maintain the geometric structuralrelationships between object surfaces and enhance the expressive capability ofimage features. In addition, an image synthesis loss is employed to constrainthe gradient flow for detailed regions and further strengthen the supervisionof object boundaries and texture-rich areas. Finally, we propose an adaptivedepth interval adjustment strategy to improve the accuracy of objectreconstruction. Extensive experiments on the DTU and Tanks & Temples datasetsdemonstrate that our method achieves competitive results. The code is availableat https://github.com/wsmtht520-/DAMVSNet.
  </details>

- **[Learning Bijective Surface Parameterization for Inferring Signed Distance Functions from Sparse Point Clouds with Grid Deformation](http://arxiv.org/abs/2503.23670v1)**  `arXiv:2503.23670`  `cs.CV`  
  _Takeshi Noda, Chao Chen, Junsheng Zhou, Weiqi Zhang, Yu-Shen Liu, Zhizhong Han_
  <details open><summary>Abstract</summary>
  Inferring signed distance functions (SDFs) from sparse point clouds remains achallenge in surface reconstruction. The key lies in the lack of detailedgeometric information in sparse point clouds, which is essential for learning acontinuous field. To resolve this issue, we present a novel approach thatlearns a dynamic deformation network to predict SDFs in an end-to-end manner.To parameterize a continuous surface from sparse points, we propose a bijectivesurface parameterization (BSP) that learns the global shape from local patches.Specifically, we construct a bijective mapping for sparse points from theparametric domain to 3D local patches, integrating patches into the globalsurface. Meanwhile, we introduce grid deformation optimization (GDO) into thesurface approximation to optimize the deformation of grid points and furtherrefine the parametric surfaces. Experimental results on synthetic and realscanned datasets demonstrate that our method significantly outperforms thecurrent state-of-the-art methods. Project page:https://takeshie.github.io/Bijective-SDF
  </details>

- **[Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language Models](http://arxiv.org/abs/2503.18334v2)**  `arXiv:2503.18334`  `cs.CV`  
  _Haotian Zhai, Xinyu Chen, Can Zhang, Tianming Sha, Ruirui Li_
  <details open><summary>Abstract</summary>
  Test-time adaptation (TTA) of visual language models has recently attractedsignificant attention as a solution to the performance degradation caused bydistribution shifts in downstream tasks. However, existing cache-based TTAmethods have certain limitations. They mainly rely on the accuracy of cachedfeature labels, and the presence of noisy pseudo-labels can cause thesefeatures to deviate from their true distribution. This makes cache retrievalmethods based on similarity matching highly sensitive to outliers or extremesamples. Moreover, current methods lack effective mechanisms to model classdistributions, which limits their ability to fully exploit the potential ofcached information. To address these challenges, we introduce a comprehensiveand reliable caching mechanism and propose a novel zero-shot TTA method called"Cache, Residual, Gaussian" (CRG). This method not only employs learnableresidual parameters to better align positive and negative visual prototypeswith text prototypes, thereby optimizing the quality of cached features, butalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically modelintra-class feature distributions, further mitigating the impact of noisyfeatures. Experimental results on 13 benchmarks demonstrate that CRGoutperforms state-of-the-art TTA methods, showcasing exceptional robustness andadaptability.
  </details>

- **[Synthetic Prior for Few-Shot Drivable Head Avatar Inversion](http://arxiv.org/abs/2501.06903v3)**  `arXiv:2501.06903`  `cs.CV`  
  _Wojciech Zielonka, Stephan J. Garbin, Alexandros Lattas, George Kopanas, Paulo Gotardo, Thabo Beeler, et al._
  <details open><summary>Abstract</summary>
  We present SynShot, a novel method for the few-shot inversion of a drivablehead avatar based on a synthetic prior. We tackle three major challenges.First, training a controllable 3D generative network requires a large number ofdiverse sequences, for which pairs of images and high-quality tracked meshesare not always available. Second, the use of real data is strictly regulated(e.g., under the General Data Protection Regulation, which mandates frequentdeletion of models and data to accommodate a situation when a participant'sconsent is withdrawn). Synthetic data, free from these constraints, is anappealing alternative. Third, state-of-the-art monocular avatar models struggleto generalize to new views and expressions, lacking a strong prior and oftenoverfitting to a specific viewpoint distribution. Inspired by machine learningmodels trained solely on synthetic data, we propose a method that learns aprior model from a large dataset of synthetic heads with diverse identities,expressions, and viewpoints. With few input images, SynShot fine-tunes thepretrained synthetic prior to bridge the domain gap, modeling a photorealistichead avatar that generalizes to novel expressions and viewpoints. We model thehead avatar using 3D Gaussian splatting and a convolutional encoder-decoderthat outputs Gaussian parameters in UV texture space. To account for thedifferent modeling complexities over parts of the head (e.g., skin vs hair), weembed the prior with explicit control for upsampling the number of per-partprimitives. Compared to SOTA monocular and GAN-based methods, SynShotsignificantly improves novel view and expression synthesis.
  </details>

- **[DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models](http://arxiv.org/abs/2410.08207v2)**  `arXiv:2410.08207`  `cs.LG` `cs.CV`  
  _Xiaoxiao He, Ligong Han, Quan Dao, Song Wen, Minhao Bai, Di Liu, et al._
  <details open><summary>Abstract</summary>
  Discrete diffusion models have achieved success in tasks like imagegeneration and masked language modeling but face limitations in controlledcontent editing. We introduce DICE (Discrete Inversion for ControllableEditing), the first approach to enable precise inversion for discrete diffusionmodels, including multinomial diffusion and masked generative models. Byrecording noise sequences and masking patterns during the reverse diffusionprocess, DICE enables accurate reconstruction and flexible editing of discretedata without the need for predefined masks or attention manipulation. Wedemonstrate the effectiveness of DICE across both image and text domains,evaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our resultsshow that DICE preserves high data fidelity while enhancing editingcapabilities, offering new opportunities for fine-grained content manipulationin discrete spaces.
  </details>

- **[3D-GSW: 3D Gaussian Splatting for Robust Watermarking](http://arxiv.org/abs/2409.13222v4)**  `arXiv:2409.13222`  `cs.CV`  
  _Youngdong Jang, Hyunje Park, Feng Yang, Heeju Ko, Euijin Choo, Sangpil Kim_
  <details open><summary>Abstract</summary>
  As 3D Gaussian Splatting (3D-GS) gains significant attention and itscommercial usage increases, the need for watermarking technologies to preventunauthorized use of the 3D-GS models and rendered images has becomeincreasingly important. In this paper, we introduce a robust watermarkingmethod for 3D-GS that secures copyright of both the model and its renderedimages. Our proposed method remains robust against distortions in renderedimages and model attacks while maintaining high rendering quality. To achievethese objectives, we present Frequency-Guided Densification (FGD), whichremoves 3D Gaussians based on their contribution to rendering quality,enhancing real-time rendering and the robustness of the message. FGD utilizesDiscrete Fourier Transform to split 3D Gaussians in high-frequency areas,improving rendering quality. Furthermore, we employ a gradient mask for 3DGaussians and design a wavelet-subband loss to enhance rendering quality. Ourexperiments show that our method embeds the message in the rendered imagesinvisibly and robustly against various attacks, including model distortion. Ourmethod achieves superior performance in both rendering quality and watermarkrobustness while improving real-time rendering efficiency. Project page:https://kuai-lab.github.io/cvpr20253dgsw/
  </details>

- **[Gaussian Eigen Models for Human Heads](http://arxiv.org/abs/2407.04545v4)**  `arXiv:2407.04545`  `cs.CV`  
  _Wojciech Zielonka, Timo Bolkart, Thabo Beeler, Justus Thies_
  <details open><summary>Abstract</summary>
  Current personalized neural head avatars face a trade-off: lightweight modelslack detail and realism, while high-quality, animatable avatars requiresignificant computational resources, making them unsuitable for commoditydevices. To address this gap, we introduce Gaussian Eigen Models (GEM), whichprovide high-quality, lightweight, and easily controllable head avatars. GEMutilizes 3D Gaussian primitives for representing the appearance combined withGaussian splatting for rendering. Building on the success of mesh-based 3Dmorphable face models (3DMM), we define GEM as an ensemble of linear eigenbasesfor representing the head appearance of a specific subject. In particular, weconstruct linear bases to represent the position, scale, rotation, and opacityof the 3D Gaussians. This allows us to efficiently generate Gaussian primitivesof a specific head shape by a linear combination of the basis vectors, onlyrequiring a low-dimensional parameter vector that contains the respectivecoefficients. We propose to construct these linear bases (GEM) by distillinghigh-quality compute-intense CNN-based Gaussian avatar models that can generateexpression-dependent appearance changes like wrinkles. These high-qualitymodels are trained on multi-view videos of a subject and are distilled using aseries of principal component analyses. Once we have obtained the bases thatrepresent the animatable appearance space of a specific human, we learn aregressor that takes a single RGB image as input and predicts thelow-dimensional parameter vector that corresponds to the shown facialexpression. In a series of experiments, we compare GEM's self-reenactment andcross-person reenactment results to state-of-the-art 3D avatar methods,demonstrating GEM's higher visual quality and better generalization to newexpressions.
  </details>

- **[Gen3DSR: Generalizable 3D Scene Reconstruction via Divide and Conquer from a Single View](http://arxiv.org/abs/2404.03421v2)**  `arXiv:2404.03421`  `cs.CV`  
  _Andreea Ardelean, Mert √ñzer, Bernhard Egger_
  <details open><summary>Abstract</summary>
  Single-view 3D reconstruction is currently approached from two dominantperspectives: reconstruction of scenes with limited diversity using 3D datasupervision or reconstruction of diverse singular objects using large imagepriors. However, real-world scenarios are far more complex and exceed thecapabilities of these methods. We therefore propose a hybrid method following adivide-and-conquer strategy. We first process the scene holistically,extracting depth and semantic information, and then leverage an object-levelmethod for the detailed reconstruction of individual components. By splittingthe problem into simpler tasks, our system is able to generalize to varioustypes of scenes without retraining or fine-tuning. We purposely design ourpipeline to be highly modular with independent, self-contained modules, toavoid the need for end-to-end training of the whole system. This enables thepipeline to naturally improve as future methods can replace the individualmodules. We demonstrate the reconstruction performance of our approach on bothsynthetic and real-world scenes, comparing favorable against prior works.Project page: https://andreeadogaru.github.io/Gen3DSR
  </details>
