# üîç 3D_Reconstruction Papers ¬∑ 2025-03-28

[![Total Papers](https://img.shields.io/badge/Papers-21-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Reconstruction` `Nerf` `Gaussian`  
**Filter**: `2D`

---

## üìö Paper List

- **[Q-Insight: Understanding Image Quality via Visual Reinforcement Learning](http://arxiv.org/abs/2503.22679v1)**  `arXiv:2503.22679`  `cs.CV`  
  _Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, et al._
  <details open><summary>Abstract</summary>
  Image quality assessment (IQA) focuses on the perceptual visual quality ofimages, playing a crucial role in downstream tasks such as imagereconstruction, compression, and generation. The rapid advancement ofmulti-modal large language models (MLLMs) has significantly broadened the scopeof IQA, moving toward comprehensive image quality understanding thatincorporates content analysis, degradation perception, and comparison reasoningbeyond mere numerical scoring. Previous MLLM-based methods typically eithergenerate numerical scores lacking interpretability or heavily rely onsupervised fine-tuning (SFT) using large-scale annotated datasets to providedescriptive assessments, limiting their flexibility and applicability. In thispaper, we propose Q-Insight, a reinforcement learning-based model built upongroup relative policy optimization (GRPO), which demonstrates strong visualreasoning capability for image quality understanding while requiring only alimited amount of rating scores and degradation labels. By jointly optimizingscore regression and degradation perception tasks with carefully designedreward functions, our approach effectively exploits their mutual benefits forenhanced performance. Extensive experiments demonstrate that Q-Insightsubstantially outperforms existing state-of-the-art methods in both scoreregression and degradation perception tasks, while exhibiting impressivezero-shot generalization to comparison reasoning tasks. Code will be availableat https://github.com/lwq20020127/Q-Insight.
  </details>

- **[Generative Latent Neural PDE Solver using Flow Matching](http://arxiv.org/abs/2503.22600v1)**  `arXiv:2503.22600`  `cs.AI` `cs.LG`  
  _Zijie Li, Anthony Zhou, Amir Barati Farimani_
  <details open><summary>Abstract</summary>
  Autoregressive next-step prediction models have become the de-facto standardfor building data-driven neural solvers to forecast time-dependent partialdifferential equations (PDEs). Denoise training that is closely related todiffusion probabilistic model has been shown to enhance the temporal stabilityof neural solvers, while its stochastic inference mechanism enables ensemblepredictions and uncertainty quantification. In principle, such traininginvolves sampling a series of discretized diffusion timesteps during bothtraining and inference, inevitably increasing computational overhead. Inaddition, most diffusion models apply isotropic Gaussian noise on structured,uniform grids, limiting their adaptability to irregular domains. We propose alatent diffusion model for PDE simulation that embeds the PDE state in alower-dimensional latent space, which significantly reduces computationalcosts. Our framework uses an autoencoder to map different types of meshes ontoa unified structured latent grid, capturing complex geometries. By analyzingcommon diffusion paths, we propose to use a coarsely sampled noise schedulefrom flow matching for both training and testing. Numerical experiments showthat the proposed model outperforms several deterministic baselines in bothaccuracy and long-term stability, highlighting the potential of diffusion-basedapproaches for robust data-driven PDE learning.
  </details>

- **[LIM: Large Interpolator Model for Dynamic Reconstruction](http://arxiv.org/abs/2503.22537v1)**  `arXiv:2503.22537`  `cs.CV` `cs.AI`  
  _Remy Sabathier, Niloy J. Mitra, David Novotny_
  <details open><summary>Abstract</summary>
  Reconstructing dynamic assets from video data is central to many in computervision and graphics tasks. Existing 4D reconstruction approaches are limited bycategory-specific models or slow optimization-based methods. Inspired by therecent Large Reconstruction Model (LRM), we present the Large InterpolationModel (LIM), a transformer-based feed-forward solution, guided by a novelcausal consistency loss, for interpolating implicit 3D representations acrosstime. Given implicit 3D representations at times $t_0$ and $t_1$, LIM producesa deformed shape at any continuous time $t\in[t_0,t_1]$, deliveringhigh-quality interpolated frames in seconds. Furthermore, LIM allows explicitmesh tracking across time, producing a consistently uv-textured mesh sequenceready for integration into existing production pipelines. We also use LIM, inconjunction with a diffusion-based multiview generator, to produce dynamic 4Dreconstructions from monocular videos. We evaluate LIM on various dynamicdatasets, benchmarking against image-space interpolation methods (e.g., FiLM)and direct triplane linear interpolation, and demonstrate clear advantages. Insummary, LIM is the first feed-forward model capable of high-speed tracked 4Dasset reconstruction across diverse categories.
  </details>

- **[Collapse and Collision Aware Grasping for Cluttered Shelf Picking](http://arxiv.org/abs/2503.22427v1)**  `arXiv:2503.22427`  `cs.RO`  
  _Abhinav Pathak, Rajkumar Muthusamy_
  <details open><summary>Abstract</summary>
  Efficient and safe retrieval of stacked objects in warehouse environments isa significant challenge due to complex spatial dependencies and structuralinter-dependencies. Traditional vision-based methods excel at objectlocalization but often lack the physical reasoning required to predict theconsequences of extraction, leading to unintended collisions and collapses.This paper proposes a collapse and collision aware grasp planner thatintegrates dynamic physics simulations for robotic decision-making. Using asingle image and depth map, an approximate 3D representation of the scene isreconstructed in a simulation environment, enabling the robot to evaluatedifferent retrieval strategies before execution. Two approaches 1)heuristic-based and 2) physics-based are proposed for both single-boxextraction and shelf clearance tasks. Extensive real-world experiments onstructured and unstructured box stacks, along with validation using datasetsfrom existing databases, show that our physics-aware method significantlyimproves efficiency and success rates compared to baseline heuristics.
  </details>

- **[Modeling Multiple Normal Action Representations for Error Detection in Procedural Tasks](http://arxiv.org/abs/2503.22405v1)**  `arXiv:2503.22405`  `cs.CV`  
  _Wei-Jin Huang, Yuan-Ming Li, Zhi-Wei Xia, Yu-Ming Tang, Kun-Yu Lin, Jian-Fang Hu, et al._
  <details open><summary>Abstract</summary>
  Error detection in procedural activities is essential for consistent andcorrect outcomes in AR-assisted and robotic systems. Existing methods oftenfocus on temporal ordering errors or rely on static prototypes to representnormal actions. However, these approaches typically overlook the commonscenario where multiple, distinct actions are valid following a given sequenceof executed actions. This leads to two issues: (1) the model cannot effectivelydetect errors using static prototypes when the inference environment or actionexecution distribution differs from training; and (2) the model may also usethe wrong prototypes to detect errors if the ongoing action label is not thesame as the predicted one. To address this problem, we propose an AdaptiveMultiple Normal Action Representation (AMNAR) framework. AMNAR predicts allvalid next actions and reconstructs their corresponding normal actionrepresentations, which are compared against the ongoing action to detecterrors. Extensive experiments demonstrate that AMNAR achieves state-of-the-artperformance, highlighting the effectiveness of AMNAR and the importance ofmodeling multiple valid next actions in error detection. The code is availableat https://github.com/iSEE-Laboratory/AMNAR.
  </details>

- **[GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain](http://arxiv.org/abs/2503.22397v1)**  `arXiv:2503.22397`  `cs.CV`  
  _Vida Adeli, Soroush Mehraban, Majid Mirmehdi, Alan Whone, Benjamin Filtjens, Amirhossein Dadashzadeh, et al._
  <details open><summary>Abstract</summary>
  Gait analysis is crucial for the diagnosis and monitoring of movementdisorders like Parkinson's Disease. While computer vision models have shownpotential for objectively evaluating parkinsonian gait, their effectiveness islimited by scarce clinical datasets and the challenge of collecting large andwell-labelled data, impacting model accuracy and risk of bias. To address thesegaps, we propose GAITGen, a novel framework that generates realistic gaitsequences conditioned on specified pathology severity levels. GAITGen employs aConditional Residual Vector Quantized Variational Autoencoder to learndisentangled representations of motion dynamics and pathology-specific factors,coupled with Mask and Residual Transformers for conditioned sequencegeneration. GAITGen generates realistic, diverse gait sequences across severitylevels, enriching datasets and enabling large-scale model training inparkinsonian gait analysis. Experiments on our new PD-GaM (real) datasetdemonstrate that GAITGen outperforms adapted state-of-the-art models in bothreconstruction fidelity and generation quality, accurately capturing criticalpathology-specific gait features. A clinical user study confirms the realismand clinical relevance of our generated sequences. Moreover, incorporatingGAITGen-generated data into downstream tasks improves parkinsonian gaitseverity estimation, highlighting its potential for advancing clinical gaitanalysis.
  </details>

- **[GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion](http://arxiv.org/abs/2503.22349v1)**  `arXiv:2503.22349`  `cs.CV`  
  _Li-Heng Chen, Zi-Xin Zou, Chang Liu, Tianjiao Jing, Yan-Pei Cao, Shi-Sheng Huang, et al._
  <details open><summary>Abstract</summary>
  Accurate surface reconstruction from unposed images is crucial for efficient3D object or scene creation. However, it remains challenging, particularly forthe joint camera pose estimation. Previous approaches have achieved impressivepose-free surface reconstruction results in dense-view settings, but couldeasily fail for sparse-view scenarios without sufficient visual overlap. Inthis paper, we propose a new technique for pose-free surface reconstruction,which follows triplane-based signed distance field (SDF) learning butregularizes the learning by explicit points sampled from ray-based diffusion ofcamera pose estimation. Our key contribution is a novel Geometric ConsistentRay Diffusion model (GCRayDiffusion), where we represent camera poses as neuralbundle rays and regress the distribution of noisy rays via a diffusion model.More importantly, we further condition the denoising process of RGRayDiffusionusing the triplane-based SDF of the entire scene, which provides effective 3Dconsistent regularization to achieve multi-view consistent camera poseestimation. Finally, we incorporate RGRayDiffusion into the triplane-based SDFlearning by introducing on-surface geometric regularization from the samplingpoints of the neural bundle rays, which leads to highly accurate pose-freesurface reconstruction results even for sparse-view inputs. Extensiveevaluations on public datasets show that our GCRayDiffusion achieves moreaccurate camera pose estimation than previous approaches, with geometricallymore consistent surface reconstruction results, especially given sparse-viewinputs.
  </details>

- **[AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail Representation](http://arxiv.org/abs/2503.22324v1)**  `arXiv:2503.22324`  `cs.CV` `cs.AI`  
  _Chenyang Xu, XingGuo Deng, Rui Zhong_
  <details open><summary>Abstract</summary>
  The 3D Gaussian Splatting (3D-GS) is a novel method for scene representationand view synthesis. Although Scaffold-GS achieves higher quality real-timerendering compared to the original 3D-GS, its fine-grained rendering of thescene is extremely dependent on adequate viewing angles. The spectral bias ofneural network learning results in Scaffold-GS's poor ability to perceive andlearn high-frequency information in the scene. In this work, we proposeenhancing the manifold complexity of input features and using network-basedfeature map loss to improve the image reconstruction quality of 3D-GS models.We introduce AH-GS, which enables 3D Gaussians in structurally complex regionsto obtain higher-frequency encodings, allowing the model to more effectivelylearn the high-frequency information of the scene. Additionally, we incorporatehigh-frequency reinforce loss to further enhance the model's ability to capturedetailed frequency information. Our result demonstrates that our modelsignificantly improves rendering fidelity, and in specific scenarios (e.g.,MipNeRf360-garden), our method exceeds the rendering quality of Scaffold-GS injust 15K iterations.
  </details>

- **[FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation](http://arxiv.org/abs/2503.22249v1)**  `arXiv:2503.22249`  `cs.RO` `cs.LG`  
  _Xianqi Zhang, Hongliang Wei, Wenrui Wang, Xingtao Wang, Xiaopeng Fan, Debin Zhao_
  <details open><summary>Abstract</summary>
  Humanoid robots have attracted significant attention in recent years.Reinforcement Learning (RL) is one of the main ways to control the whole bodyof humanoid robots. RL enables agents to complete tasks by learning fromenvironment interactions, guided by task rewards. However, existing RL methodsrarely explicitly consider the impact of body stability on humanoid locomotionand manipulation. Achieving high performance in whole-body control remains achallenge for RL methods that rely solely on task rewards. In this paper, wepropose a Foundation model-based method for humanoid Locomotion AndManipulation (FLAM for short). FLAM integrates a stabilizing reward functionwith a basic policy. The stabilizing reward function is designed to encouragethe robot to learn stable postures, thereby accelerating the learning processand facilitating task completion. Specifically, the robot pose is first mappedto the 3D virtual human model. Then, the human pose is stabilized andreconstructed through a human motion reconstruction model. Finally, the posebefore and after reconstruction is used to compute the stabilizing reward. Bycombining this stabilizing reward with the task reward, FLAM effectively guidespolicy learning. Experimental results on a humanoid robot benchmark demonstratethat FLAM outperforms state-of-the-art RL methods, highlighting itseffectiveness in improving stability and overall performance.
  </details>

- **[Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance](http://arxiv.org/abs/2503.22225v1)**  `arXiv:2503.22225`  `cs.CV`  
  _Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang_
  <details open><summary>Abstract</summary>
  Pre-trained conditional diffusion models have demonstrated remarkablepotential in image editing. However, they often face challenges with temporalconsistency, particularly in the talking head domain, where continuous changesin facial expressions intensify the level of difficulty. These issues stem fromthe independent editing of individual images and the inherent loss of temporalcontinuity during the editing process. In this paper, we introduce Follow YourMotion (FYM), a generic framework for maintaining temporal consistency inportrait editing. Specifically, given portrait images rendered by a pre-trained3D Gaussian Splatting model, we first develop a diffusion model thatintuitively and inherently learns motion trajectory changes at different scalesand pixel coordinates, from the first frame to each subsequent frame. Thisapproach ensures that temporally inconsistent edited avatars inherit the motioninformation from the rendered avatars. Secondly, to maintain fine-grainedexpression temporal consistency in talking head editing, we propose a dynamicre-weighted attention mechanism. This mechanism assigns higher weightcoefficients to landmark points in space and dynamically updates these weightsbased on landmark loss, achieving more consistent and refined facialexpressions. Extensive experiments demonstrate that our method outperformsexisting approaches in terms of temporal consistency and can be used tooptimize and compensate for temporally inconsistent outputs in a range ofapplications, such as text-driven editing, relighting, and various otherapplications.
  </details>

- **[Interpretable Deep Learning Paradigm for Airborne Transient Electromagnetic Inversion](http://arxiv.org/abs/2503.22214v1)**  `arXiv:2503.22214`  `cs.LG`  
  _Shuang Wang, Xuben Wang, Fei Deng, Xiaodong Yu, Peifan Jiang, Lifeng Mao_
  <details open><summary>Abstract</summary>
  The extraction of geoelectric structural information from airborne transientelectromagnetic(ATEM)data primarily involves data processing and inversion.Conventional methods rely on empirical parameter selection, making it difficultto process complex field data with high noise levels. Additionally, inversioncomputations are time consuming and often suffer from multiple local minima.Existing deep learning-based approaches separate the data processing steps,where independently trained denoising networks struggle to ensure thereliability of subsequent inversions. Moreover, end to end networks lackinterpretability. To address these issues, we propose a unified andinterpretable deep learning inversion paradigm based on disentangledrepresentation learning. The network explicitly decomposes noisy data intonoise and signal factors, completing the entire data processing workflow basedon the signal factors while incorporating physical information for guidance.This approach enhances the network's reliability and interpretability. Theinversion results on field data demonstrate that our method can directly usenoisy data to accurately reconstruct the subsurface electrical structure.Furthermore, it effectively processes data severely affected by environmentalnoise, which traditional methods struggle with, yielding improved lateralstructural resolution.
  </details>

- **[Mitigating Trade-off: Stream and Query-guided Aggregation for Efficient and Effective 3D Occupancy Prediction](http://arxiv.org/abs/2503.22087v1)**  `arXiv:2503.22087`  `cs.CV`  
  _Seokha Moon, Janghyun Baek, Giseop Kim, Jinkyu Kim, Sunwook Choi_
  <details open><summary>Abstract</summary>
  3D occupancy prediction has emerged as a key perception task for autonomousdriving, as it reconstructs 3D environments to provide a comprehensive sceneunderstanding. Recent studies focus on integrating spatiotemporal informationobtained from past observations to improve prediction accuracy, using amulti-frame fusion approach that processes multiple past frames together.However, these methods struggle with a trade-off between efficiency andaccuracy, which significantly limits their practicality. To mitigate thistrade-off, we propose StreamOcc, a novel framework that aggregatesspatio-temporal information in a stream-based manner. StreamOcc consists of twokey components: (i) Stream-based Voxel Aggregation, which effectivelyaccumulates past observations while minimizing computational costs, and (ii)Query-guided Aggregation, which recurrently aggregates instance-level featuresof dynamic objects into corresponding voxel features, refining fine-graineddetails of dynamic objects. Experiments on the Occ3D-nuScenes dataset show thatStreamOcc achieves state-of-the-art performance in real-time settings, whilereducing memory usage by more than 50% compared to previous methods.
  </details>

- **[ReLU Networks as Random Functions: Their Distribution in Probability Space](http://arxiv.org/abs/2503.22082v1)**  `arXiv:2503.22082`  `cs.LG`  
  _Shreyas Chaudhari, Jos√© M. F. Moura_
  <details open><summary>Abstract</summary>
  This paper presents a novel framework for understanding trained ReLU networksas random, affine functions, where the randomness is induced by thedistribution over the inputs. By characterizing the probability distribution ofthe network's activation patterns, we derive the discrete probabilitydistribution over the affine functions realizable by the network. We extendthis analysis to describe the probability distribution of the network'soutputs. Our approach provides explicit, numerically tractable expressions forthese distributions in terms of Gaussian orthant probabilities. Additionally,we develop approximation techniques to identify the support of affine functionsa trained ReLU network can realize for a given distribution of inputs. Our workprovides a framework for understanding the behavior and performance of ReLUnetworks corresponding to stochastic inputs, paving the way for moreinterpretable and reliable models.
  </details>

- **[LandMarkSystem Technical Report](http://arxiv.org/abs/2503.21364v2)**  `arXiv:2503.21364`  `cs.CV`  
  _Zhenxiang Ma, Zhenyu Yang, Miao Tao, Yuanzhen Zhou, Zeyu He, Yuchang Zhang, et al._
  <details open><summary>Abstract</summary>
  3D reconstruction is vital for applications in autonomous driving, virtualreality, augmented reality, and the metaverse. Recent advancements such asNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformedthe field, yet traditional deep learning frameworks struggle to meet theincreasing demands for scene quality and scale. This paper introducesLandMarkSystem, a novel computing framework designed to enhance multi-scalescene reconstruction and rendering. By leveraging a componentized modeladaptation layer, LandMarkSystem supports various NeRF and 3DGS structureswhile optimizing computational efficiency through distributed parallelcomputing and model parameter offloading. Our system addresses the limitationsof existing frameworks, providing dedicated operators for complex 3D sparsecomputations, thus facilitating efficient training and rapid inference overextensive scenes. Key contributions include a modular architecture, a dynamicloading strategy for limited resources, and proven capabilities across multiplerepresentative algorithms.This comprehensive solution aims to advance theefficiency and effectiveness of 3D reconstruction tasks.To facilitate furtherresearch and collaboration, the source code and documentation for theLandMarkSystem project are publicly available in an open-source repository,accessing the repository at: https://github.com/InternLandMark/LandMarkSystem.
  </details>

- **[Omni-AD: Learning to Reconstruct Global and Local Features for Multi-class Anomaly Detection](http://arxiv.org/abs/2503.21125v2)**  `arXiv:2503.21125`  `cs.CV`  
  _Jiajie Quan, Ao Tong, Yuxuan Cai, Xinwei He, Yulong Wang, Yang Zhou_
  <details open><summary>Abstract</summary>
  In multi-class unsupervised anomaly detection(MUAD), reconstruction-basedmethods learn to map input images to normal patterns to identify anomalouspixels. However, this strategy easily falls into the well-known "learningshortcut" issue when decoders fail to capture normal patterns and reconstructboth normal and abnormal samples naively. To address that, we propose to learnthe input features in global and local manners, forcing the network to memorizethe normal patterns more comprehensively. Specifically, we design a two-branchdecoder block, named Omni-block. One branch corresponds to global featurelearning, where we serialize two self-attention blocks but replace the queryand (key, value) with learnable tokens, respectively, thus capturing globalfeatures of normal patterns concisely and thoroughly. The local branchcomprises depth-separable convolutions, whose locality enables effective andefficient learning of local features for normal patterns. By stackingOmni-blocks, we build a framework, Omni-AD, to learn normal patterns ofdifferent granularity and reconstruct them progressively. Comprehensiveexperiments on public anomaly detection benchmarks show that our methodoutperforms state-of-the-art approaches in MUAD. Code is available athttps://github.com/easyoo/Omni-AD.git
  </details>

- **[VidTwin: Video VAE with Decoupled Structure and Dynamics](http://arxiv.org/abs/2412.17726v2)**  `arXiv:2412.17726`  `cs.CV` `cs.AI` `cs.LG`  
  _Yuchi Wang, Junliang Guo, Xinyi Xie, Tianyu He, Xu Sun, Jiang Bian_
  <details open><summary>Abstract</summary>
  Recent advancements in video autoencoders (Video AEs) have significantlyimproved the quality and efficiency of video generation. In this paper, wepropose a novel and compact video autoencoder, VidTwin, that decouples videointo two distinct latent spaces: Structure latent vectors, which captureoverall content and global movement, and Dynamics latent vectors, whichrepresent fine-grained details and rapid movements. Specifically, our approachleverages an Encoder-Decoder backbone, augmented with two submodules forextracting these latent spaces, respectively. The first submodule employs aQ-Former to extract low-frequency motion trends, followed by downsamplingblocks to remove redundant content details. The second averages the latentvectors along the spatial dimension to capture rapid motion. Extensiveexperiments show that VidTwin achieves a high compression rate of 0.20% withhigh reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), andperforms efficiently and effectively in downstream generative tasks. Moreover,our model demonstrates explainability and scalability, paving the way forfuture research in video latent representation and generation. Check ourproject page for more details: https://vidtwin.github.io/.
  </details>

- **[Combating Semantic Contamination in Learning with Label Noise](http://arxiv.org/abs/2412.11620v3)**  `arXiv:2412.11620`  `cs.CV` `cs.AI`  
  _Wenxiao Fan, Kan Li_
  <details open><summary>Abstract</summary>
  Noisy labels can negatively impact the performance of deep neural networks.One common solution is label refurbishment, which involves reconstructing noisylabels through predictions and distributions. However, these methods mayintroduce problematic semantic associations, a phenomenon that we identify asSemantic Contamination. Through an analysis of Robust LR, a representativelabel refurbishment method, we found that utilizing the logits of views forrefurbishment does not adequately balance the semantic information ofindividual classes. Conversely, using the logits of models fails to maintainconsistent semantic relationships across models, which explains why labelrefurbishment methods frequently encounter issues related to SemanticContamination. To address this issue, we propose a novel method calledCollaborative Cross Learning, which utilizes semi-supervised learning onrefurbished labels to extract appropriate semantic associations from embeddingsacross views and models. Experimental results show that our method outperformsexisting approaches on both synthetic and real-world noisy datasets,effectively mitigating the impact of label noise and Semantic Contamination.
  </details>

- **[Continuous-Time State Estimation Methods in Robotics: A Survey](http://arxiv.org/abs/2411.03951v2)**  `arXiv:2411.03951`  `cs.RO`  
  _William Talbot, Julian Nubert, Turcan Tuna, Cesar Cadena, Frederike D√ºmbgen, Jesus Tordesillas, et al._
  <details open><summary>Abstract</summary>
  Accurate, efficient, and robust state estimation is more important than everin robotics as the variety of platforms and complexity of tasks continue togrow. Historically, discrete-time filters and smoothers have been the dominantapproach, in which the estimated variables are states at discrete sample times.The paradigm of continuous-time state estimation proposes an alternativestrategy by estimating variables that express the state as a continuousfunction of time, which can be evaluated at any query time. Not only can thisbenefit downstream tasks such as planning and control, but it alsosignificantly increases estimator performance and flexibility, as well asreduces sensor preprocessing and interfacing complexity. Despite this,continuous-time methods remain underutilized, potentially because they are lesswell-known within robotics. To remedy this, this work presents a unifyingformulation of these methods and the most exhaustive literature review to date,systematically categorizing prior work by methodology, application, statevariables, historical context, and theoretical contribution to the field. Bysurveying splines and Gaussian processes together and contextualizing worksfrom other research domains, this work identifies and analyzes open problems incontinuous-time state estimation and suggests new research directions.
  </details>

- **[MM-GTUNets: Unified Multi-Modal Graph Deep Learning for Brain Disorders Prediction](http://arxiv.org/abs/2406.14455v3)**  `arXiv:2406.14455`  `cs.CV`  
  _Luhui Cai, Weiming Zeng, Hongyu Chen, Hua Zhang, Yueyang Li, Yu Feng, et al._
  <details open><summary>Abstract</summary>
  Graph deep learning (GDL) has demonstrated impressive performance inpredicting population-based brain disorders (BDs) through the integration ofboth imaging and non-imaging data. However, the effectiveness of GDL basedmethods heavily depends on the quality of modeling the multi-modal populationgraphs and tends to degrade as the graph scale increases. Furthermore, thesemethods often constrain interactions between imaging and non-imaging data tonode-edge interactions within the graph, overlooking complex inter-modalcorrelations, leading to suboptimal outcomes. To overcome these challenges, wepropose MM-GTUNets, an end-to-end graph transformer based multi-modal graphdeep learning (MMGDL) framework designed for brain disorders prediction atlarge scale. Specifically, to effectively leverage rich multi-modal informationrelated to diseases, we introduce Modality Reward Representation Learning(MRRL) which adaptively constructs population graphs using a reward system.Additionally, we employ variational autoencoder to reconstruct latentrepresentations of non-imaging features aligned with imaging features. Based onthis, we propose Adaptive Cross-Modal Graph Learning (ACMGL), which capturescritical modality-specific and modality-shared features through a unifiedGTUNet encoder taking advantages of Graph UNet and Graph Transformer, andfeature fusion module. We validated our method on two public multi-modaldatasets ABIDE and ADHD-200, demonstrating its superior performance indiagnosing BDs. Our code is available at https://github.com/NZWANG/MM-GTUNets.
  </details>

- **[MixRT: Mixed Neural Representations For Real-Time NeRF Rendering](http://arxiv.org/abs/2312.11841v5)**  `arXiv:2312.11841`  `cs.CV`  
  _Chaojian Li, Bichen Wu, Peter Vajda, Yingyan Celine Lin_
  <details open><summary>Abstract</summary>
  Neural Radiance Field (NeRF) has emerged as a leading technique for novelview synthesis, owing to its impressive photorealistic reconstruction andrendering capability. Nevertheless, achieving real-time NeRF rendering inlarge-scale scenes has presented challenges, often leading to the adoption ofeither intricate baked mesh representations with a substantial number oftriangles or resource-intensive ray marching in baked representations. Wechallenge these conventions, observing that high-quality geometry, representedby meshes with substantial triangles, is not necessary for achievingphotorealistic rendering quality. Consequently, we propose MixRT, a novel NeRFrepresentation that includes a low-quality mesh, a view-dependent displacementmap, and a compressed NeRF model. This design effectively harnesses thecapabilities of existing graphics hardware, thus enabling real-time NeRFrendering on edge devices. Leveraging a highly-optimized WebGL-based renderingframework, our proposed MixRT attains real-time rendering speeds on edgedevices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop),better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360datasets), and a smaller storage size (less than 80% compared tostate-of-the-art methods).
  </details>

- **[INGeo: Accelerating Instant Neural Scene Reconstruction with Noisy Geometry Priors](http://arxiv.org/abs/2212.01959v2)**  `arXiv:2212.01959`  `cs.CV`  
  _Chaojian Li, Bichen Wu, Albert Pumarola, Peizhao Zhang, Yingyan Celine Lin, Peter Vajda_
  <details open><summary>Abstract</summary>
  We present a method that accelerates reconstruction of 3D scenes and objects,aiming to enable instant reconstruction on edge devices such as mobile phonesand AR/VR headsets. While recent works have accelerated scene reconstructiontraining to minute/second-level on high-end GPUs, there is still a large gap tothe goal of instant training on edge devices which is yet highly desired inmany emerging applications such as immersive AR/VR. To this end, this work aimsto further accelerate training by leveraging geometry priors of the targetscene. Our method proposes strategies to alleviate the noise of the imperfectgeometry priors to accelerate the training speed on top of the highly optimizedInstant-NGP. On the NeRF Synthetic dataset, our work uses half of the trainingiterations to reach an average test PSNR of >30.
  </details>
