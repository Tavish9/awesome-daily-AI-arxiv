# üîç 3D_Reconstruction Papers ¬∑ 2025-03-27

[![Total Papers](https://img.shields.io/badge/Papers-38-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Reconstruction` `Nerf` `Gaussian`  
**Filter**: `2D`

---

## üìö Paper List

- **[X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction](http://arxiv.org/abs/2503.21779v1)**  `arXiv:2503.21779`  `cs.CV`  
  _Weihao Yu, Yuanhao Cai, Ruyi Zha, Zhiwen Fan, Chenxin Li, Yixuan Yuan_
  <details open><summary>Abstract</summary>
  Four-dimensional computed tomography (4D CT) reconstruction is crucial forcapturing dynamic anatomical changes but faces inherent limitations fromconventional phase-binning workflows. Current methods discretize temporalresolution into fixed phases with respiratory gating devices, introducingmotion misalignment and restricting clinical practicality. In this paper, Wepropose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CTreconstruction by integrating dynamic radiative Gaussian splatting withself-supervised respiratory motion learning. Our approach models anatomicaldynamics through a spatiotemporal encoder-decoder architecture that predictstime-varying Gaussian deformations, eliminating phase discretization. To removedependency on external gating devices, we introduce a physiology-drivenperiodic consistency loss that learns patient-specific breathing cyclesdirectly from projections via differentiable optimization. Extensiveexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNRgain over traditional methods and 2.25 dB improvement against prior Gaussiansplatting techniques. By unifying continuous motion modeling with hardware-freeperiod learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction fordynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.
  </details>

- **[HS-SLAM: Hybrid Representation with Structural Supervision for Improved Dense SLAM](http://arxiv.org/abs/2503.21778v1)**  `arXiv:2503.21778`  `cs.CV`  
  _Ziren Gong, Fabio Tosi, Youmin Zhang, Stefano Mattoccia, Matteo Poggi_
  <details open><summary>Abstract</summary>
  NeRF-based SLAM has recently achieved promising results in tracking andreconstruction. However, existing methods face challenges in providingsufficient scene representation, capturing structural information, andmaintaining global consistency in scenes emerging significant movement or beingforgotten. To this end, we present HS-SLAM to tackle these problems. To enhancescene representation capacity, we propose a hybrid encoding network thatcombines the complementary strengths of hash-grid, tri-planes, and one-blob,improving the completeness and smoothness of reconstruction. Additionally, weintroduce structural supervision by sampling patches of non-local pixels ratherthan individual rays to better capture the scene structure. To ensure globalconsistency, we implement an active global bundle adjustment (BA) to eliminatecamera drifts and mitigate accumulative errors. Experimental resultsdemonstrate that HS-SLAM outperforms the baselines in tracking andreconstruction accuracy while maintaining the efficiency required for robotics.
  </details>

- **[Test-Time Visual In-Context Tuning](http://arxiv.org/abs/2503.21777v1)**  `arXiv:2503.21777`  `cs.CV` `cs.LG`  
  _Jiahao Xie, Alessio Tonioni, Nathalie Rauschmayr, Federico Tombari, Bernt Schiele_
  <details open><summary>Abstract</summary>
  Visual in-context learning (VICL), as a new paradigm in computer vision,allows the model to rapidly adapt to various tasks with only a handful ofprompts and examples. While effective, the existing VICL paradigm exhibits poorgeneralizability under distribution shifts. In this work, we propose test-timeVisual In-Context Tuning (VICT), a method that can adapt VICL models on the flywith a single test sample. Specifically, we flip the role between the taskprompts and the test sample and use a cycle consistency loss to reconstruct theoriginal task prompt output. Our key insight is that a model should be aware ofa new test distribution if it can successfully recover the original taskprompts. Extensive experiments on six representative vision tasks ranging fromhigh-level visual understanding to low-level image processing, with 15 commoncorruptions, demonstrate that our VICT can improve the generalizability of VICLto unseen new domains. In addition, we show the potential of applying VICT forunseen tasks at test time. Code: https://github.com/Jiahao000/VICT.
  </details>

- **[Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video](http://arxiv.org/abs/2503.21761v1)**  `arXiv:2503.21761`  `cs.AI` `cs.CV` `cs.LG`  
  _David Yifan Yao, Albert J. Zhai, Shenlong Wang_
  <details open><summary>Abstract</summary>
  This paper presents a unified approach to understanding dynamic scenes fromcasual videos. Large pretrained vision foundation models, such asvision-language, video depth prediction, motion tracking, and segmentationmodels, offer promising capabilities. However, training a single model forcomprehensive 4D understanding remains challenging. We introduce Uni4D, amulti-stage optimization framework that harnesses multiple pretrained models toadvance dynamic 3D modeling, including static/dynamic reconstruction, camerapose estimation, and dense 3D motion tracking. Our results showstate-of-the-art performance in dynamic 4D modeling with superior visualquality. Notably, Uni4D requires no retraining or fine-tuning, highlighting theeffectiveness of repurposing visual foundation models for 4D understanding.
  </details>

- **[Reconstructing Humans with a Biomechanically Accurate Skeleton](http://arxiv.org/abs/2503.21751v1)**  `arXiv:2503.21751`  `cs.CV`  
  _Yan Xia, Xiaowei Zhou, Etienne Vouga, Qixing Huang, Georgios Pavlakos_
  <details open><summary>Abstract</summary>
  In this paper, we introduce a method for reconstructing 3D humans from asingle image using a biomechanically accurate skeleton model. To achieve this,we train a transformer that takes an image as input and estimates theparameters of the model. Due to the lack of training data for this task, webuild a pipeline to produce pseudo ground truth model parameters for singleimages and implement a training procedure that iteratively refines these pseudolabels. Compared to state-of-the-art methods for 3D human mesh recovery, ourmodel achieves competitive performance on standard benchmarks, while itsignificantly outperforms them in settings with extreme 3D poses andviewpoints. Additionally, we show that previous reconstruction methodsfrequently violate joint angle limits, leading to unnatural rotations. Incontrast, our approach leverages the biomechanically plausible degrees offreedom making more realistic joint rotation estimates. We validate ourapproach across multiple human pose estimation benchmarks. We make the code,models and data available at: https://isshikihugh.github.io/HSMR/
  </details>

- **[SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling](http://arxiv.org/abs/2503.21732v1)**  `arXiv:2503.21732`  `cs.CV`  
  _Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, et al._
  <details open><summary>Abstract</summary>
  Creating high-fidelity 3D meshes with arbitrary topology, including opensurfaces and complex interiors, remains a significant challenge. Existingimplicit field methods often require costly and detail-degrading watertightconversion, while other approaches struggle with high resolutions. This paperintroduces SparseFlex, a novel sparse-structured isosurface representation thatenables differentiable mesh reconstruction at resolutions up to $1024^3$directly from rendering losses. SparseFlex combines the accuracy of Flexicubeswith a sparse voxel structure, focusing computation on surface-adjacent regionsand efficiently handling open surfaces. Crucially, we introduce a frustum-awaresectional voxel training strategy that activates only relevant voxels duringrendering, dramatically reducing memory consumption and enablinghigh-resolution training. This also allows, for the first time, thereconstruction of mesh interiors using only rendering supervision. Buildingupon this, we demonstrate a complete shape modeling pipeline by training avariational autoencoder (VAE) and a rectified flow transformer for high-quality3D shape generation. Our experiments show state-of-the-art reconstructionaccuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase inF-score compared to previous methods, and demonstrate the generation ofhigh-resolution, detailed 3D shapes with arbitrary topology. By enablinghigh-resolution, differentiable mesh reconstruction and generation withrendering losses, SparseFlex significantly advances the state-of-the-art in 3Dshape representation and modeling.
  </details>

- **[ICG-MVSNet: Learning Intra-view and Cross-view Relationships for Guidance in Multi-View Stereo](http://arxiv.org/abs/2503.21525v1)**  `arXiv:2503.21525`  `cs.CV`  
  _Yuxi Hu, Jun Zhang, Zhe Zhang, Rafael Weilharter, Yuchen Rao, Kuangyi Chen, et al._
  <details open><summary>Abstract</summary>
  Multi-view Stereo (MVS) aims to estimate depth and reconstruct 3D pointclouds from a series of overlapping images. Recent learning-based MVSframeworks overlook the geometric information embedded in features andcorrelations, leading to weak cost matching. In this paper, we proposeICG-MVSNet, which explicitly integrates intra-view and cross-view relationshipsfor depth estimation. Specifically, we develop an intra-view feature fusionmodule that leverages the feature coordinate correlations within a single imageto enhance robust cost matching. Additionally, we introduce a lightweightcross-view aggregation module that efficiently utilizes the contextualinformation from volume correlations to guide regularization. Our method isevaluated on the DTU dataset and Tanks and Temples benchmark, consistentlyachieving competitive performance against state-of-the-art works, whilerequiring lower computational resources.
  </details>

- **[F-INR: Functional Tensor Decomposition for Implicit Neural Representations](http://arxiv.org/abs/2503.21507v1)**  `arXiv:2503.21507`  `cs.LG`  
  _Sai Karthikeya Vemuri, Tim B√ºchner, Joachim Denzler_
  <details open><summary>Abstract</summary>
  Implicit Neural Representation (INR) has emerged as a powerful tool forencoding discrete signals into continuous, differentiable functions usingneural networks. However, these models often have an unfortunate reliance onmonolithic architectures to represent high-dimensional data, leading toprohibitive computational costs as dimensionality grows. We propose F-INR, aframework that reformulates INR learning through functional tensordecomposition, breaking down high-dimensional tasks into lightweight,axis-specific sub-networks. Each sub-network learns a low-dimensional datacomponent (e.g., spatial or temporal). Then, we combine these components viatensor operations, reducing forward pass complexity while improving accuracythrough specialized learning. F-INR is modular and, therefore,architecture-agnostic, compatible with MLPs, SIREN, WIRE, or otherstate-of-the-art INR architecture. It is also decomposition-agnostic,supporting CP, TT, and Tucker modes with user-defined rank for speed-accuracycontrol. In our experiments, F-INR trains $100\times$ faster than existingapproaches on video tasks while achieving higher fidelity (+3.4 dB PSNR).Similar gains hold for image compression, physics simulations, and 3D geometryreconstruction. Through this, F-INR offers a new scalable, flexible solutionfor high-dimensional signal modeling.
  </details>

- **[STAMICS: Splat, Track And Map with Integrated Consistency and Semantics for Dense RGB-D SLAM](http://arxiv.org/abs/2503.21425v1)**  `arXiv:2503.21425`  `cs.RO` `cs.CV`  
  _Yongxu Wang, Xu Cao, Weiyun Yi, Zhaoxin Fan_
  <details open><summary>Abstract</summary>
  Simultaneous Localization and Mapping (SLAM) is a critical task in robotics,enabling systems to autonomously navigate and understand complex environments.Current SLAM approaches predominantly rely on geometric cues for mapping andlocalization, but they often fail to ensure semantic consistency, particularlyin dynamic or densely populated scenes. To address this limitation, weintroduce STAMICS, a novel method that integrates semantic information with 3DGaussian representations to enhance both localization and mapping accuracy.STAMICS consists of three key components: a 3D Gaussian-based scenerepresentation for high-fidelity reconstruction, a graph-based clusteringtechnique that enforces temporal semantic consistency, and an open-vocabularysystem that allows for the classification of unseen objects. Extensiveexperiments show that STAMICS significantly improves camera pose estimation andmap quality, outperforming state-of-the-art methods while reducingreconstruction errors. Code will be public available.
  </details>

- **[Diffusion Image Prior](http://arxiv.org/abs/2503.21410v1)**  `arXiv:2503.21410`  `cs.CV`  
  _Hamadi Chihaoui, Paolo Favaro_
  <details open><summary>Abstract</summary>
  Zero-shot image restoration (IR) methods based on pretrained diffusion modelshave recently achieved significant success. These methods typically require atleast a parametric form of the degradation model. However, in real-worldscenarios, the degradation may be too complex to define explicitly. To handlethis general case, we introduce the Diffusion Image Prior (DIIP). We takeinspiration from the Deep Image Prior (DIP)[16], since it can be used to removeartifacts without the need for an explicit degradation model. However, incontrast to DIP, we find that pretrained diffusion models offer a much strongerprior, despite being trained without knowledge from corrupted data. We showthat, the optimization process in DIIP first reconstructs a clean version ofthe image before eventually overfitting to the degraded input, but it does sofor a broader range of degradations than DIP. In light of this result, wepropose a blind image restoration (IR) method based on early stopping, whichdoes not require prior knowledge of the degradation model. We validate DIIP onvarious degradation-blind IR tasks, including JPEG artifact removal, waterdropremoval, denoising and super-resolution with state-of-the-art results.
  </details>

- **[VALLR: Visual ASR Language Model for Lip Reading](http://arxiv.org/abs/2503.21408v1)**  `arXiv:2503.21408`  `cs.CV`  
  _Marshall Thomas, Edward Fish, Richard Bowden_
  <details open><summary>Abstract</summary>
  Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complextask requiring the interpretation of spoken language exclusively from visualcues, primarily lip movements and facial expressions. This task is especiallychallenging due to the absence of auditory information and the inherentambiguity when visually distinguishing phonemes that have overlapping visemeswhere different phonemes appear identical on the lips. Current methodstypically attempt to predict words or characters directly from these visualcues, but this approach frequently encounters high error rates due tocoarticulation effects and viseme ambiguity. We propose a novel two-stage,phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) thataddresses these longstanding challenges. First, our model predicts a compactsequence of phonemes from visual inputs using a Video Transformer with a CTChead, thereby reducing the task complexity and achieving robust speakerinvariance. This phoneme output then serves as the input to a fine-tuned LargeLanguage Model (LLM), which reconstructs coherent words and sentences byleveraging broader linguistic context. Unlike existing methods that eitherpredict words directly-often faltering on visually similar phonemes-or rely onlarge-scale multimodal pre-training, our approach explicitly encodesintermediate linguistic structure while remaining highly data efficient. Wedemonstrate state-of-the-art performance on two challenging datasets, LRS2 andLRS3, where our method achieves significant reductions in Word Error Rate (WER)achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled datathan the next best approach.
  </details>

- **[Unsupervised Real-World Denoising: Sparsity is All You Need](http://arxiv.org/abs/2503.21377v1)**  `arXiv:2503.21377`  `cs.CV`  
  _Hamadi Chihaoui, Paolo Favaro_
  <details open><summary>Abstract</summary>
  Supervised training for real-world denoising presents challenges due to thedifficulty of collecting large datasets of paired noisy and clean images.Recent methods have attempted to address this by utilizing unpaired datasets ofclean and noisy images. Some approaches leverage such unpaired data to traindenoisers in a supervised manner by generating synthetic clean-noisy pairs.However, these methods often fall short due to the distribution gap betweensynthetic and real noisy images. To mitigate this issue, we propose a solutionbased on input sparsification, specifically using random input masking. Ourmethod, which we refer to as Mask, Inpaint and Denoise (MID), trains a denoiserto simultaneously denoise and inpaint synthetic clean-noisy pairs. On one hand,input sparsification reduces the gap between synthetic and real noisy images.On the other hand, an inpainter trained in a supervised manner can stillaccurately reconstruct sparse inputs by predicting missing clean pixels usingthe remaining unmasked pixels. Our approach begins with a synthetic Gaussiannoise sampler and iteratively refines it using a noise dataset derived from thedenoiser's predictions. The noise dataset is created by subtracting predictedpseudo-clean images from real noisy images at each iteration. The coreintuition is that improving the denoiser results in a more accurate noisedataset and, consequently, a better noise sampler. We validate our methodthrough extensive experiments on real-world noisy image datasets, demonstratingcompetitive performance compared to existing unsupervised denoising methods.
  </details>

- **[LandMarkSystem Technical Report](http://arxiv.org/abs/2503.21364v1)**  `arXiv:2503.21364`  `cs.CV`  
  _Zhenxiang Ma, Zhenyu Yang, Miao Tao, Yuanzhen Zhou, Zeyu He, Yuchang Zhang, et al._
  <details open><summary>Abstract</summary>
  3D reconstruction is vital for applications in autonomous driving, virtualreality, augmented reality, and the metaverse. Recent advancements such asNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformedthe field, yet traditional deep learning frameworks struggle to meet theincreasing demands for scene quality and scale. This paper introducesLandMarkSystem, a novel computing framework designed to enhance multi-scalescene reconstruction and rendering. By leveraging a componentized modeladaptation layer, LandMarkSystem supports various NeRF and 3DGS structureswhile optimizing computational efficiency through distributed parallelcomputing and model parameter offloading. Our system addresses the limitationsof existing frameworks, providing dedicated operators for complex 3D sparsecomputations, thus facilitating efficient training and rapid inference overextensive scenes. Key contributions include a modular architecture, a dynamicloading strategy for limited resources, and proven capabilities across multiplerepresentative algorithms.This comprehensive solution aims to advance theefficiency and effectiveness of 3D reconstruction tasks.To facilitate furtherresearch and collaboration, the source code and documentation for theLandMarkSystem project are publicly available in an open-source repository,accessing the repository at: https://github.com/InternLandMark/LandMarkSystem.
  </details>

- **[UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based on Uncertainty-Guided NeRF Augmentation](http://arxiv.org/abs/2503.21338v1)**  `arXiv:2503.21338`  `cs.RO` `cs.CV`  
  _Yehui Shen, Lei Zhang, Qingqiu Li, Xiongwei Zhao, Yue Wang, Huimin Lu, et al._
  <details open><summary>Abstract</summary>
  Visual place recognition (VPR) is crucial for robots to identify previouslyvisited locations, playing an important role in autonomous navigation in bothindoor and outdoor environments. However, most existing VPR datasets arelimited to single-viewpoint scenarios, leading to reduced recognition accuracy,particularly in multi-directional driving or feature-sparse scenes. Moreover,obtaining additional data to mitigate these limitations is often expensive.This paper introduces a novel training paradigm to improve the performance ofexisting VPR networks by enhancing multi-view diversity within current datasetsthrough uncertainty estimation and NeRF-based data augmentation. Specifically,we initially train NeRF using the existing VPR dataset. Then, our devisedself-supervised uncertainty estimation network identifies places with highuncertainty. The poses of these uncertain places are input into NeRF togenerate new synthetic observations for further training of VPR networks.Additionally, we propose an improved storage method for efficient organizationof augmented and original training data. We conducted extensive experiments onthree datasets and tested three different VPR backbone networks. The resultsdemonstrate that our proposed training paradigm significantly improves VPRperformance by fully utilizing existing data, outperforming other trainingapproaches. We further validated the effectiveness of our approach onself-recorded indoor and outdoor datasets, consistently demonstrating superiorresults. Our dataset and code have been released at\href{https://github.com/nubot-nudt/UGNA-VPR}{https://github.com/nubot-nudt/UGNA-VPR}.
  </details>

- **[HORT: Monocular Hand-held Objects Reconstruction with Transformers](http://arxiv.org/abs/2503.21313v1)**  `arXiv:2503.21313`  `cs.CV`  
  _Zerui Chen, Rolandos Alexandros Potamias, Shizhe Chen, Cordelia Schmid_
  <details open><summary>Abstract</summary>
  Reconstructing hand-held objects in 3D from monocular images remains asignificant challenge in computer vision. Most existing approaches rely onimplicit 3D representations, which produce overly smooth reconstructions andare time-consuming to generate explicit 3D shapes. While more recent methodsdirectly reconstruct point clouds with diffusion models, the multi-stepdenoising makes high-resolution reconstruction inefficient. To address theselimitations, we propose a transformer-based model to efficiently reconstructdense 3D point clouds of hand-held objects. Our method follows a coarse-to-finestrategy, first generating a sparse point cloud from the image andprogressively refining it into a dense representation using pixel-aligned imagefeatures. To enhance reconstruction accuracy, we integrate image features with3D hand geometry to jointly predict the object point cloud and its poserelative to the hand. Our model is trained end-to-end for optimal performance.Experimental results on both synthetic and real datasets demonstrate that ourmethod achieves state-of-the-art accuracy with much faster inference speed,while generalizing well to in-the-wild images.
  </details>

- **[ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate](http://arxiv.org/abs/2503.21268v1)**  `arXiv:2503.21268`  `cs.CV`  
  _Ming Yan, Xincheng Lin, Yuhua Luo, Shuqi Fan, Yudi Dai, Qixin Zhong, et al._
  <details open><summary>Abstract</summary>
  Human Motion Recovery (HMR) research mainly focuses on ground-based motionssuch as running. The study on capturing climbing motion, an off-ground motion,is sparse. This is partly due to the limited availability of climbing motiondatasets, especially large-scale and challenging 3D labeled datasets. Toaddress the insufficiency of climbing motion datasets, we collect AscendMotion,a large-scale well-annotated, and challenging climbing motion dataset. Itconsists of 412k RGB, LiDAR frames, and IMU measurements, including thechallenging climbing motions of 22 skilled climbing coaches across 12 differentrock walls. Capturing the climbing motions is challenging as it requiresprecise recovery of not only the complex pose but also the global position ofclimbers. Although multiple global HMR methods have been proposed, they cannotfaithfully capture climbing motions. To address the limitations of HMR methodsfor climbing, we propose ClimbingCap, a motion recovery method thatreconstructs continuous 3D human climbing motion in a global coordinate system.One key insight is to use the RGB and LiDAR modalities to separatelyreconstruct motions in camera coordinates and global coordinates and tooptimize them jointly. We demonstrate the quality of the AscendMotion datasetand present promising results from ClimbingCap. The AscendMotion dataset andsource code release publicly at \href{thislink}{http://www.lidarhumanmotion.net/climbingcap/}
  </details>

- **[Frequency-Aware Gaussian Splatting Decomposition](http://arxiv.org/abs/2503.21226v1)**  `arXiv:2503.21226`  `cs.CV`  
  _Yishai Lavi, Leo Segre, Shai Avidan_
  <details open><summary>Abstract</summary>
  3D Gaussian Splatting (3D-GS) has revolutionized novel view synthesis withits efficient, explicit representation. However, it lacks frequencyinterpretability, making it difficult to separate low-frequency structures fromfine details. We introduce a frequency-decomposed 3D-GS framework that groups3D Gaussians that correspond to subbands in the Laplacian Pyrmaids of the inputimages. Our approach enforces coherence within each subband (i.e., group of 3DGaussians) through dedicated regularization, ensuring well-separated frequencycomponents. We extend color values to both positive and negative ranges,allowing higher-frequency layers to add or subtract residual details. Tostabilize optimization, we employ a progressive training scheme that refinesdetails in a coarse-to-fine manner. Beyond interpretability, thisfrequency-aware design unlocks a range of practical benefits. Explicitfrequency separation enables advanced 3D editing and stylization, allowingprecise manipulation of specific frequency bands. It also supports dynamiclevel-of-detail control for progressive rendering, streaming, foveatedrendering and fast geometry interaction. Through extensive experiments, wedemonstrate that our method provides improved control and flexibility foremerging applications in scene editing and interactive rendering. Our code willbe made publicly available.
  </details>

- **[GenFusion: Closing the Loop between Reconstruction and Generation via Videos](http://arxiv.org/abs/2503.21219v1)**  `arXiv:2503.21219`  `cs.AI` `cs.CV`  
  _Sibo Wu, Congrong Xu, Binbin Huang, Andreas Geiger, Anpei Chen_
  <details open><summary>Abstract</summary>
  Recently, 3D reconstruction and generation have demonstrated impressive novelview synthesis results, achieving high fidelity and efficiency. However, anotable conditioning gap can be observed between these two fields, e.g.,scalable 3D scene reconstruction often requires densely captured views, whereas3D generation typically relies on a single or no input view, whichsignificantly limits their applications. We found that the source of thisphenomenon lies in the misalignment between 3D constraints and generativepriors. To address this problem, we propose a reconstruction-driven videodiffusion model that learns to condition video frames on artifact-prone RGB-Drenderings. Moreover, we propose a cyclical fusion pipeline that iterativelyadds restoration frames from the generative model to the training set, enablingprogressive expansion and addressing the viewpoint saturation limitations seenin previous reconstruction and generation pipelines. Our evaluation, includingview synthesis from sparse view and masked input, validates the effectivenessof our approach.
  </details>

- **[VADMamba: Exploring State Space Models for Fast Video Anomaly Detection](http://arxiv.org/abs/2503.21169v1)**  `arXiv:2503.21169`  `cs.CV`  
  _Jiahao Lyu, Minghua Zhao, Jing Hu, Xuewen Huang, Yifei Chen, Shuangli Du_
  <details open><summary>Abstract</summary>
  Video anomaly detection (VAD) methods are mostly CNN-based orTransformer-based, achieving impressive results, but the focus on detectionaccuracy often comes at the expense of inference speed. The emergence of statespace models in computer vision, exemplified by the Mamba model, demonstratesimproved computational efficiency through selective scans and showcases thegreat potential for long-range modeling. Our study pioneers the application ofMamba to VAD, dubbed VADMamba, which is based on multi-task learning for frameprediction and optical flow reconstruction. Specifically, we propose theVQ-Mamba Unet (VQ-MaU) framework, which incorporates a Vector Quantization (VQ)layer and Mamba-based Non-negative Visual State Space (NVSS) block.Furthermore, two individual VQ-MaU networks separately predict frames andreconstruct corresponding optical flows, further boosting accuracy through aclip-level fusion evaluation strategy. Experimental results validate theefficacy of the proposed VADMamba across three benchmark datasets,demonstrating superior performance in inference speed compared to previouswork. Code is available at https://github.com/jLooo/VADMamba.
  </details>

- **[Omni-AD: Learning to Reconstruct Global and Local Features for Multi-class Anomaly Detection](http://arxiv.org/abs/2503.21125v1)**  `arXiv:2503.21125`  `cs.CV`  
  _Jiajie Quan, Ao Tong, Yuxuan Cai, Xinwei He, Yulong Wang, Yang Zhou_
  <details open><summary>Abstract</summary>
  In multi-class unsupervised anomaly detection(MUAD), reconstruction-basedmethods learn to map input images to normal patterns to identify anomalouspixels. However, this strategy easily falls into the well-known "learningshortcut" issue when decoders fail to capture normal patterns and reconstructboth normal and abnormal samples naively. To address that, we propose to learnthe input features in global and local manners, forcing the network to memorizethe normal patterns more comprehensively. Specifically, we design a two-branchdecoder block, named Omni-block. One branch corresponds to global featurelearning, where we serialize two self-attention blocks but replace the queryand (key, value) with learnable tokens, respectively, thus capturing globalfeatures of normal patterns concisely and thoroughly. The local branchcomprises depth-separable convolutions, whose locality enables effective andefficient learning of local features for normal patterns. By stackingOmni-blocks, we build a framework, Omni-AD, to learn normal patterns ofdifferent granularity and reconstruct them progressively. Comprehensiveexperiments on public anomaly detection benchmarks show that our methodoutperforms state-of-the-art approaches in MUAD. Code is available athttps://github.com/easyoo/Omni-AD.git.
  </details>

- **[StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency](http://arxiv.org/abs/2503.21104v1)**  `arXiv:2503.21104`  `cs.CV`  
  _Yuyin Chen, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Yifei Zhan, et al._
  <details open><summary>Abstract</summary>
  Urban scene reconstruction requires modeling both static infrastructure anddynamic elements while supporting diverse environmental conditions. We present\textbf{StyledStreets}, a multi-style street simulator that achievesinstruction-driven scene editing with guaranteed spatial and temporalconsistency. Building on a state-of-the-art Gaussian Splatting framework forstreet scenarios enhanced by our proposed pose optimization and multi-viewtraining, our method enables photorealistic style transfers across seasons,weather conditions, and camera setups through three key innovations: First, ahybrid embedding scheme disentangles persistent scene geometry from transientstyle attributes, allowing realistic environmental edits while preservingstructural integrity. Second, uncertainty-aware rendering mitigates supervisionnoise from diffusion priors, enabling robust training across extreme stylevariations. Third, a unified parametric model prevents geometric drift throughregularized updates, maintaining multi-view consistency across sevenvehicle-mounted cameras.  Our framework preserves the original scene's motion patterns and geometricrelationships. Qualitative results demonstrate plausible transitions betweendiverse conditions (snow, sandstorm, night), while quantitative evaluationsshow state-of-the-art geometric accuracy under style transfers. The approachestablishes new capabilities for urban simulation, with applications inautonomous vehicle testing and augmented reality systems requiring reliableenvironmental consistency. Codes will be publicly available upon publication.
  </details>

- **[Can Video Diffusion Model Reconstruct 4D Geometry?](http://arxiv.org/abs/2503.21082v1)**  `arXiv:2503.21082`  `cs.CV`  
  _Jinjie Mai, Wenxuan Zhu, Haozhe Liu, Bing Li, Cheng Zheng, J√ºrgen Schmidhuber, et al._
  <details open><summary>Abstract</summary>
  Reconstructing dynamic 3D scenes (i.e., 4D geometry) from monocular video isan important yet challenging problem. Conventional multiview geometry-basedapproaches often struggle with dynamic motion, whereas recent learning-basedmethods either require specialized 4D representation or sophisticatedoptimization. In this paper, we present Sora3R, a novel framework that tapsinto the rich spatiotemporal priors of large-scale video diffusion models todirectly infer 4D pointmaps from casual videos. Sora3R follows a two-stagepipeline: (1) we adapt a pointmap VAE from a pretrained video VAE, ensuringcompatibility between the geometry and video latent spaces; (2) we finetune adiffusion backbone in combined video and pointmap latent space to generatecoherent 4D pointmaps for every frame. Sora3R operates in a fully feedforwardmanner, requiring no external modules (e.g., depth, optical flow, orsegmentation) or iterative global alignment. Extensive experiments demonstratethat Sora3R reliably recovers both camera poses and detailed scene geometry,achieving performance on par with state-of-the-art methods for dynamic 4Dreconstruction across diverse scenarios.
  </details>

- **[Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning](http://arxiv.org/abs/2503.20752v2)**  `arXiv:2503.20752`  `cs.AI` `cs.CV`  
  _Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, et al._
  <details open><summary>Abstract</summary>
  Visual reasoning abilities play a crucial role in understanding complexmultimodal data, advancing both domain-specific applications and artificialgeneral intelligence (AGI). Existing methods improve VLM reasoning viaChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotatedtraining data to enhance visual reasoning capabilities. However, this trainingparadigm may lead to overfitting and cognitive rigidity, restricting themodel's ability to transfer visual reasoning skills across domains and limitingits real-world applicability. To address these limitations, we proposeReason-RFT, a novel reinforcement fine-tuning framework that significantlyenhances generalization capabilities in visual reasoning tasks. Reason-RFTintroduces a two-phase training framework for visual reasoning: (1) SupervisedFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates thereasoning potential of Vision-Language Models (VLMs), followed by (2) GroupRelative Policy Optimization (GRPO)-based reinforcement learning that generatesmultiple reasoning-response pairs, significantly enhancing generalization invisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,we reconstructed a comprehensive dataset spanning visual counting, structureperception, and spatial transformation. Experimental results demonstrateReasoning-RFT's three key advantages: (1) Performance Enhancement: achievingstate-of-the-art results across multiple tasks, outperforming most mainstreamopen-source and proprietary models; (2) Generalization Superiority:consistently maintaining robust performance across diverse tasks and domains,outperforming alternative training paradigms; (3) Data Efficiency: excelling infew-shot learning scenarios while surpassing full-dataset SFT baselines.Project website: https://tanhuajie.github.io/ReasonRFT
  </details>

- **[EventMamba: Enhancing Spatio-Temporal Locality with State Space Models for Event-Based Video Reconstruction](http://arxiv.org/abs/2503.19721v2)**  `arXiv:2503.19721`  `cs.CV`  
  _Chengjie Ge, Xueyang Fu, Peng He, Kunyu Wang, Chengzhi Cao, Zheng-Jun Zha_
  <details open><summary>Abstract</summary>
  Leveraging its robust linear global modeling capability, Mamba has notablyexcelled in computer vision. Despite its success, existing Mamba-based visionmodels have overlooked the nuances of event-driven tasks, especially in videoreconstruction. Event-based video reconstruction (EBVR) demands spatialtranslation invariance and close attention to local event relationships in thespatio-temporal domain. Unfortunately, conventional Mamba algorithms applystatic window partitions and standard reshape scanning methods, leading tosignificant losses in local connectivity. To overcome these limitations, weintroduce EventMamba--a specialized model designed for EBVR tasks. EventMambainnovates by incorporating random window offset (RWO) in the spatial domain,moving away from the restrictive fixed partitioning. Additionally, it featuresa new consistent traversal serialization approach in the spatio-temporaldomain, which maintains the proximity of adjacent events both spatially andtemporally. These enhancements enable EventMamba to retain Mamba's robustmodeling capabilities while significantly preserving the spatio-temporallocality of event data. Comprehensive testing on multiple datasets shows thatEventMamba markedly enhances video reconstruction, drastically improvingcomputation speed while delivering superior visual quality compared toTransformer-based methods.
  </details>

- **[Rethinking Video Tokenization: A Conditioned Diffusion-based Approach](http://arxiv.org/abs/2503.03708v3)**  `arXiv:2503.03708`  `cs.AI` `cs.CV`  
  _Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, et al._
  <details open><summary>Abstract</summary>
  Existing video tokenizers typically use the traditional VariationalAutoencoder (VAE) architecture for video compression and reconstruction.However, to achieve good performance, its training process often relies oncomplex multi-stage training tricks that go beyond basic reconstruction lossand KL regularization. Among these tricks, the most challenging is the precisetuning of adversarial training with additional Generative Adversarial Networks(GANs) in the final stage, which can hinder stable convergence. In contrast toGANs, diffusion models offer more stable training processes and can generatehigher-quality results. Inspired by these advantages, we propose CDT, a novelConditioned Diffusion-based video Tokenizer, that replaces the GAN-baseddecoder with a conditional causal diffusion model. The encoder compressesspatio-temporal information into compact latents, while the decoderreconstructs videos through a reverse diffusion process conditioned on theselatents. During inference, we incorporate a feature cache mechanism to generatevideos of arbitrary length while maintaining temporal continuity and adoptsampling acceleration technique to enhance efficiency. Trained using only abasic MSE diffusion loss for reconstruction, along with KL term and LPIPSperceptual loss from scratch, extensive experiments demonstrate that CDTachieves state-of-the-art performance in video reconstruction tasks with just asingle-step sampling. Even a scaled-down version of CDT (3$\times$ inferencespeedup) still performs comparably with top baselines. Moreover, the latentvideo generation model trained with CDT also exhibits superior performance. Thesource code and pretrained weights are available athttps://github.com/ali-vilab/CDT.
  </details>

- **[VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors](http://arxiv.org/abs/2503.01107v2)**  `arXiv:2503.01107`  `cs.CV`  
  _Juil Koo, Paul Guerrero, Chun-Hao Paul Huang, Duygu Ceylan, Minhyuk Sung_
  <details open><summary>Abstract</summary>
  Generative methods for image and video editing use generative models aspriors to perform edits despite incomplete information, such as changing thecomposition of 3D objects shown in a single image. Recent methods have shownpromising composition editing results in the image setting, but in the videosetting, editing methods have focused on editing object's appearance andmotion, or camera motion, and as a result, methods to edit object compositionin videos are still missing. We propose \name as a method for editing 3D objectcompositions in videos of static scenes with camera motion. Our approach allowsediting the 3D position of a 3D object across all frames of a video in atemporally consistent manner. This is achieved by lifting intermediate featuresof a generative model to a 3D reconstruction that is shared between all frames,editing the reconstruction, and projecting the features on the editedreconstruction back to each frame. To the best of our knowledge, this is thefirst generative approach to edit object compositions in videos. Our approachis simple and training-free, while outperforming state-of-the-art image editingbaselines.
  </details>

- **[Enhancing LLM Character-Level Manipulation via Divide and Conquer](http://arxiv.org/abs/2502.08180v2)**  `arXiv:2502.08180`  `cs.AI` `cs.CL`  
  _Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Zhecheng Li, Yiwei Wang_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have demonstrated strong generalizationcapabilities across a wide range of natural language processing (NLP) tasks.However, they exhibit notable weaknesses in character-level stringmanipulation, struggling with fundamental operations such as characterdeletion, insertion, and substitution. These challenges stem primarily fromtokenization constraints, despite the critical role of such operations in datapreprocessing and code generation. Through systematic analysis, we derive twokey insights: (1) LLMs face significant difficulties in leveraging intrinsictoken knowledge for character-level reasoning, and (2) atomized word structurescan substantially enhance LLMs' ability to process token-level structuralinformation. Building on these insights, we propose Character-LevelManipulation via Divide and Conquer, a novel approach designed to bridge thegap between token-level processing and character-level manipulation. Our methoddecomposes complex operations into explicit character-level subtasks coupledwith controlled token reconstruction phases, leading to significantimprovements in accuracy. Without additional training, our method significantlyimproves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and$\texttt{Substitution}$ tasks. To support further research, we open-source ourimplementation and benchmarks.
  </details>

- **[TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models](http://arxiv.org/abs/2502.06608v3)**  `arXiv:2502.06608`  `cs.AI` `cs.CV`  
  _Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in diffusion techniques have propelled image and videogeneration to unprecedented levels of quality, significantly accelerating thedeployment and application of generative AI. However, 3D shape generationtechnology has so far lagged behind, constrained by limitations in 3D datascale, complexity of 3D data processing, and insufficient exploration ofadvanced techniques in the 3D domain. Current approaches to 3D shape generationface substantial challenges in terms of output quality, generalizationcapability, and alignment with input conditions. We present TripoSG, a newstreamlined shape diffusion paradigm capable of generating high-fidelity 3Dmeshes with precise correspondence to input images. Specifically, we propose:1) A large-scale rectified flow transformer for 3D shape generation, achievingstate-of-the-art fidelity through training on extensive, high-quality data. 2)A hybrid supervised training strategy combining SDF, normal, and eikonal lossesfor 3D VAE, achieving high-quality 3D reconstruction performance. 3) A dataprocessing pipeline to generate 2 million high-quality 3D samples, highlightingthe crucial rules for data quality and quantity in training 3D generativemodels. Through comprehensive experiments, we have validated the effectivenessof each component in our new framework. The seamless integration of these partshas enabled TripoSG to achieve state-of-the-art performance in 3D shapegeneration. The resulting 3D shapes exhibit enhanced detail due tohigh-resolution capabilities and demonstrate exceptional fidelity to inputimages. Moreover, TripoSG demonstrates improved versatility in generating 3Dmodels from diverse image styles and contents, showcasing strong generalizationcapabilities. To foster progress and innovation in the field of 3D generation,we will make our model publicly available.
  </details>

- **[Survey on Monocular Metric Depth Estimation](http://arxiv.org/abs/2501.11841v2)**  `arXiv:2501.11841`  `cs.CV`  
  _Jiuling Zhang_
  <details open><summary>Abstract</summary>
  Monocular Depth Estimation (MDE) is fundamental to computer vision, enablingspatial understanding, 3D reconstruction, and autonomous driving. Deeplearning-based MDE predicts relative depth from a single image, but the lack ofmetric scale introduces inconsistencies, limiting applicability in tasks suchas visual SLAM, 3D reconstruction, and novel view synthesis. Monocular MetricDepth Estimation (MMDE) overcomes this limitation by enabling precisescene-scale inference, improving depth consistency, enhancing stability insequential tasks, and streamlining integration into practical systems. Thispaper systematically reviews the evolution of depth estimation, fromtraditional geometric methods to deep learning breakthroughs, emphasizingscale-agnostic approaches in zero-shot generalization which is crucial foradvancing MMDE. Recent progress in zero-shot MMDE is examined, focusing onchallenges such as model generalization and boundary detail loss. To addressthese issues, researchers have explored unlabeled data augmentation, imagepatching, architectural optimization, and generative techniques. This reviewanalyzes these developments, assessing their impact and limitations. Keyfindings are synthesized, unresolved challenges outlined, and future researchdirection proposal. By providing a clear technical roadmap and insight intoemerging trends, this work aims to drive innovation and expand the real-worldapplications of MMDE.
  </details>

- **[OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional Images with Editable Capabilities](http://arxiv.org/abs/2412.16604v2)**  `arXiv:2412.16604`  `cs.CV`  
  _Suyoung Lee, Jaeyoung Chung, Kihoon Kim, Jaeyoo Huh, Gunhee Lee, Minsoo Lee, et al._
  <details open><summary>Abstract</summary>
  Feed-forward 3D Gaussian splatting (3DGS) models have gained significantpopularity due to their ability to generate scenes immediately without needingper-scene optimization. Although omnidirectional images are becoming morepopular since they reduce the computation required for image stitching tocomposite a holistic scene, existing feed-forward models are only designed forperspective images. The unique optical properties of omnidirectional imagesmake it difficult for feature encoders to correctly understand the context ofthe image and make the Gaussian non-uniform in space, which hinders the imagequality synthesized from novel views. We propose OmniSplat, a training-freefast feed-forward 3DGS generation framework for omnidirectional images. Weadopt a Yin-Yang grid and decompose images based on it to reduce the domain gapbetween omnidirectional and perspective images. The Yin-Yang grid can use theexisting CNN structure as it is, but its quasi-uniform characteristic allowsthe decomposed image to be similar to a perspective image, so it can exploitthe strong prior knowledge of the learned feed-forward network. OmniSplatdemonstrates higher reconstruction accuracy than existing feed-forward networkstrained on perspective images. Our project page is available on:https://robot0321.github.io/omnisplat/index.html.
  </details>

- **[RatBodyFormer: Rat Body Surface from Keypoints](http://arxiv.org/abs/2412.09599v3)**  `arXiv:2412.09599`  `cs.CV`  
  _Ayaka Higami, Karin Oshima, Tomoyo Isoguchi Shiramatsu, Hirokazu Takahashi, Shohei Nobuhara, Ko Nishino_
  <details open><summary>Abstract</summary>
  Analyzing rat behavior lies at the heart of many scientific studies. Pastmethods for automated rodent modeling have focused on 3D pose estimation fromkeypoints, e.g., face and appendages. The pose, however, does not capture therich body surface movement encoding the subtle rat behaviors like curling andstretching. The body surface lacks features that can be visually defined,evading these established keypoint-based methods. In this paper, we introducethe first method for reconstructing the rat body surface as a dense set ofpoints by learning to predict it from the sparse keypoints that can be detectedwith past methods. Our method consists of two key contributions. The first isRatDome, a novel multi-camera system for rat behavior capture, and alarge-scale dataset captured with it that consists of pairs of 3D keypoints and3D body surface points. The second is RatBodyFormer, a novel network totransform detected keypoints to 3D body surface points. RatBodyFormer isagnostic to the exact locations of the 3D body surface points in the trainingdata and is trained with masked-learning. We experimentally validate ourframework with a number of real-world experiments. Our results collectivelyserve as a novel foundation for automated rat behavior analysis.
  </details>

- **[ReCap: Better Gaussian Relighting with Cross-Environment Captures](http://arxiv.org/abs/2412.07534v3)**  `arXiv:2412.07534`  `cs.CV`  
  _Jingzhi Li, Zongwei Wu, Eduard Zamfir, Radu Timofte_
  <details open><summary>Abstract</summary>
  Accurate 3D objects relighting in diverse unseen environments is crucial forrealistic virtual object placement. Due to the albedo-lighting ambiguity,existing methods often fall short in producing faithful relights. Withoutproper constraints, observed training views can be explained by numerouscombinations of lighting and material attributes, lacking physicalcorrespondence with the actual environment maps used for relighting. In thiswork, we present ReCap, treating cross-environment captures as multi-tasktarget to provide the missing supervision that cuts through the entanglement.Specifically, ReCap jointly optimizes multiple lighting representations thatshare a common set of material attributes. This naturally harmonizes a coherentset of lighting representations around the mutual material attributes,exploiting commonalities and differences across varied object appearances. Suchcoherence enables physically sound lighting reconstruction and robust materialestimation - both essential for accurate relighting. Together with astreamlined shading function and effective post-processing, ReCap outperformsall leading competitors on an expanded relighting benchmark.
  </details>

- **[Event-boosted Deformable 3D Gaussians for Dynamic Scene Reconstruction](http://arxiv.org/abs/2411.16180v2)**  `arXiv:2411.16180`  `cs.CV`  
  _Wenhao Xu, Wenming Weng, Yueyi Zhang, Ruikang Xu, Zhiwei Xiong_
  <details open><summary>Abstract</summary>
  Deformable 3D Gaussian Splatting (3D-GS) is limited by missing intermediatemotion information due to the low temporal resolution of RGB cameras. Toaddress this, we introduce the first approach combining event cameras, whichcapture high-temporal-resolution, continuous motion data, with deformable 3D-GSfor dynamic scene reconstruction. We observe that threshold modeling for eventsplays a crucial role in achieving high-quality reconstruction. Therefore, wepropose a GS-Threshold Joint Modeling strategy, creating a mutually reinforcingprocess that greatly improves both 3D reconstruction and threshold modeling.Moreover, we introduce a Dynamic-Static Decomposition strategy that firstidentifies dynamic areas by exploiting the inability of static Gaussians torepresent motions, then applies a buffer-based soft decomposition to separatedynamic and static areas. This strategy accelerates rendering by avoidingunnecessary deformation in static areas, and focuses on dynamic areas toenhance fidelity. Additionally, we contribute the first event-inclusive 4Dbenchmark with synthetic and real-world dynamic scenes, on which our methodachieves state-of-the-art performance.
  </details>

- **[Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly 4D Reconstruction](http://arxiv.org/abs/2411.14847v2)**  `arXiv:2411.14847`  `cs.AI` `cs.CV`  
  _Zhening Liu, Yingdong Hu, Xinjie Zhang, Rui Song, Jiawei Shao, Zehong Lin, et al._
  <details open><summary>Abstract</summary>
  The recent development of 3D Gaussian Splatting (3DGS) has led to greatinterest in 4D dynamic spatial reconstruction. Existing approaches mainly relyon full-length multi-view videos, while there has been limited exploration ofonline reconstruction methods that enable on-the-fly training and per-timestepstreaming. Current 3DGS-based streaming methods treat the Gaussian primitivesuniformly and constantly renew the densified Gaussians, thereby overlooking thedifference between dynamic and static features as well as neglecting thetemporal continuity in the scene. To address these limitations, we propose anovel three-stage pipeline for iterative streamable 4D dynamic spatialreconstruction. Our pipeline comprises a selective inheritance stage topreserve temporal continuity, a dynamics-aware shift stage to distinguishdynamic and static primitives and optimize their movements, and an error-guideddensification stage to accommodate emerging objects. Our method achievesstate-of-the-art performance in online 4D reconstruction, demonstrating thefastest on-the-fly training, superior representation quality, and real-timerendering capability. Project page: https://www.liuzhening.top/DASS
  </details>

- **[GI-GS: Global Illumination Decomposition on Gaussian Splatting for Inverse Rendering](http://arxiv.org/abs/2410.02619v2)**  `arXiv:2410.02619`  `cs.CV`  
  _Hongze Chen, Zehong Lin, Jun Zhang_
  <details open><summary>Abstract</summary>
  We present GI-GS, a novel inverse rendering framework that leverages 3DGaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novelview synthesis and relighting. In inverse rendering, accurately modeling theshading processes of objects is essential for achieving high-fidelity results.Therefore, it is critical to incorporate global illumination to account forindirect lighting that reaches an object after multiple bounces across thescene. Previous 3DGS-based methods have attempted to model indirect lighting bycharacterizing indirect illumination as learnable lighting volumes oradditional attributes of each Gaussian, while using baked occlusion torepresent shadow effects. These methods, however, fail to accurately model thecomplex physical interactions between light and objects, making it impossibleto construct realistic indirect illumination during relighting. To address thislimitation, we propose to calculate indirect lighting using efficient pathtracing with deferred shading. In our framework, we first render a G-buffer tocapture the detailed geometry and material properties of the scene. Then, weperform physically-based rendering (PBR) only for direct lighting. With theG-buffer and previous rendering results, the indirect lighting can becalculated through a lightweight path tracing. Our method effectively modelsindirect lighting under any given lighting conditions, thereby achieving betternovel view synthesis and competitive relighting. Quantitative and qualitativeresults show that our GI-GS outperforms existing baselines in both renderingquality and efficiency.
  </details>

- **[MESA: Effective Matching Redundancy Reduction by Semantic Area Segmentation](http://arxiv.org/abs/2408.00279v2)**  `arXiv:2408.00279`  `cs.CV`  
  _Yesheng Zhang, Shuhan Shen, Xu Zhao_
  <details open><summary>Abstract</summary>
  We propose MESA and DMESA as novel feature matching methods, which utilizeSegment Anything Model (SAM) to effectively mitigate matching redundancy. Thekey insight of our methods is to establish implicit-semantic area matchingprior to point matching, based on advanced image understanding of SAM. Then,informative area matches with consistent internal semantic are able to undergodense feature comparison, facilitating precise inside-area point matching.Specifically, MESA adopts a sparse matching framework and first obtainscandidate areas from SAM results through a novel Area Graph (AG). Then, areamatching among the candidates is formulated as graph energy minimization andsolved by graphical models derived from AG. To address the efficiency issue ofMESA, we further propose DMESA as its dense counterpart, applying a densematching framework. After candidate areas are identified by AG, DMESAestablishes area matches through generating dense matching distributions. Thedistributions are produced from off-the-shelf patch matching utilizing theGaussian Mixture Model and refined via the Expectation Maximization. With lessrepetitive computation, DMESA showcases a speed improvement of nearly fivetimes compared to MESA, while maintaining competitive accuracy. Our methods areextensively evaluated on five datasets encompassing indoor and outdoor scenes.The results illustrate consistent performance improvements from our methods forfive distinct point matching baselines across all datasets. Furthermore, ourmethods exhibit promise generalization and improved robustness against imageresolution variations. The code is publicly available athttps://github.com/Easonyesheng/A2PM-MESA.
  </details>

- **[How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey](http://arxiv.org/abs/2402.13255v3)**  `arXiv:2402.13255`  `cs.RO` `cs.CV`  
  _Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandstr√∂m, Stefano Mattoccia, Martin R. Oswald, et al._
  <details open><summary>Abstract</summary>
  Over the past two decades, research in the field of Simultaneous Localizationand Mapping (SLAM) has undergone a significant evolution, highlighting itscritical role in enabling autonomous exploration of unknown environments. Thisevolution ranges from hand-crafted methods, through the era of deep learning,to more recent developments focused on Neural Radiance Fields (NeRFs) and 3DGaussian Splatting (3DGS) representations. Recognizing the growing body ofresearch and the absence of a comprehensive survey on the topic, this paperaims to provide the first comprehensive overview of SLAM progress through thelens of the latest advancements in radiance fields. It sheds light on thebackground, evolutionary path, inherent strengths and limitations, and servesas a fundamental reference to highlight the dynamic progress and specificchallenges.
  </details>

- **[GMTalker: Gaussian Mixture-based Audio-Driven Emotional Talking Video Portraits](http://arxiv.org/abs/2312.07669v3)**  `arXiv:2312.07669`  `cs.CV`  
  _Yibo Xia, Lizhen Wang, Xiang Deng, Xiaoyan Luo, Yunhong Wang, Yebin Liu_
  <details open><summary>Abstract</summary>
  Synthesizing high-fidelity and emotion-controllable talking video portraits,with audio-lip sync, vivid expressions, realistic head poses, and eye blinks,has been an important and challenging task in recent years. Most existingmethods suffer in achieving personalized and precise emotion control, smoothtransitions between different emotion states, and the generation of diversemotions. To tackle these challenges, we present GMTalker, a Gaussianmixture-based emotional talking portraits generation framework. Specifically,we propose a Gaussian mixture-based expression generator that can construct acontinuous and disentangled latent space, achieving more flexible emotionmanipulation. Furthermore, we introduce a normalizing flow-based motiongenerator pretrained on a large dataset with a wide-range motion to generatediverse head poses, blinks, and eyeball movements. Finally, we propose apersonalized emotion-guided head generator with an emotion mapping network thatcan synthesize high-fidelity and faithful emotional video portraits. Bothquantitative and qualitative experiments demonstrate our method outperformsprevious methods in image quality, photo-realism, emotion accuracy, and motiondiversity.
  </details>
