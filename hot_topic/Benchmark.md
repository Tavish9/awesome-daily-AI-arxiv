# üîç Benchmark Papers ¬∑ 2025-03-27

[![Total Papers](https://img.shields.io/badge/Papers-88-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Benchmark`  
**Filter**: `None`

---

## üìö Paper List

- **[Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2503.21780v1)**  `arXiv:2503.21780`  `cs.CV`  
  _Reza Qorbani, Gianluca Villani, Theodoros Panagiotakopoulos, Marc Botet Colomer, Linus H√§renstam-Nielsen, Mattia Segu, et al._
  <details open><summary>Abstract</summary>
  Open-vocabulary semantic segmentation models associate vision and text tolabel pixels from an undefined set of classes using textual queries, providingversatile performance on novel datasets. However, large shifts between trainingand test domains degrade their performance, requiring fine-tuning for effectivereal-world applications. We introduce Semantic Library Adaptation (SemLA), anovel framework for training-free, test-time domain adaptation. SemLA leveragesa library of LoRA-based adapters indexed with CLIP embeddings, dynamicallymerging the most relevant adapters based on proximity to the target domain inthe embedding space. This approach constructs an ad-hoc model tailored to eachspecific input without additional training. Our method scales efficiently,enhances explainability by tracking adapter contributions, and inherentlyprotects data privacy, making it ideal for sensitive applications.Comprehensive experiments on a 20-domain benchmark built over 10 standarddatasets demonstrate SemLA's superior adaptability and performance acrossdiverse settings, establishing a new standard in domain adaptation foropen-vocabulary semantic segmentation.
  </details>

- **[Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model](http://arxiv.org/abs/2503.21782v1)**  `arXiv:2503.21782`  `cs.CV`  
  _Abdelrahman Shaker, Muhammad Maaz, Chenhui Gou, Hamid Rezatofighi, Salman Khan, Fahad Shahbaz Khan_
  <details open><summary>Abstract</summary>
  Video understanding models often struggle with high computationalrequirements, extensive parameter counts, and slow inference speed, making theminefficient for practical use. To tackle these challenges, we proposeMobile-VideoGPT, an efficient multimodal framework designed to operate withfewer than a billion parameters. Unlike traditional video large multimodalmodels (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders,efficient projectors, and a small language model (SLM), enabling real-timethroughput. To further improve efficiency, we present an Attention-Based FrameScoring mechanism to select the key-frames, along with an efficient tokenprojector that prunes redundant visual tokens and preserves essentialcontextual cues. We evaluate our model across well-established six videounderstanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest).Our results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens persecond while outperforming existing state-of-the-art 0.5B-parameter models by 6points on average with 40% fewer parameters and more than 2x higher throughput.Our code and models are publicly available at:https://github.com/Amshaker/Mobile-VideoGPT.
  </details>

- **[Video-R1: Reinforcing Video Reasoning in MLLMs](http://arxiv.org/abs/2503.21776v1)**  `arXiv:2503.21776`  `cs.CV`  
  _Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, et al._
  <details open><summary>Abstract</summary>
  Inspired by DeepSeek-R1's success in eliciting reasoning abilities throughrule-based reinforcement learning (RL), we introduce Video-R1 as the firstattempt to systematically explore the R1 paradigm for eliciting video reasoningwithin multimodal large language models (MLLMs). However, directly applying RLtraining with the GRPO algorithm to video reasoning presents two primarychallenges: (i) a lack of temporal modeling for video reasoning, and (ii) thescarcity of high-quality video-reasoning data. To address these issues, wefirst propose the T-GRPO algorithm, which encourages models to utilize temporalinformation in videos for reasoning. Additionally, instead of relying solely onvideo data, we incorporate high-quality image-reasoning data into the trainingprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold startand Video-R1-260k for RL training, both comprising image and video data.Experimental results demonstrate that Video-R1 achieves significantimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, aswell as on general video benchmarks including MVBench and TempCompass, etc.Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoningbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. Allcodes, models, data are released.
  </details>

- **[LOCORE: Image Re-ranking with Long-Context Sequence Modeling](http://arxiv.org/abs/2503.21772v1)**  `arXiv:2503.21772`  `cs.CV`  
  _Zilin Xiao, Pavel Suma, Ayush Sachdeva, Hao-Jen Wang, Giorgos Kordopatis-Zilos, Giorgos Tolias, et al._
  <details open><summary>Abstract</summary>
  We introduce LOCORE, Long-Context Re-ranker, a model that takes as inputlocal descriptors corresponding to an image query and a list of gallery imagesand outputs similarity scores between the query and each gallery image. Thismodel is used for image retrieval, where typically a first ranking is performedwith an efficient similarity measure, and then a shortlist of top-ranked imagesis re-ranked based on a more fine-grained similarity measure. Compared toexisting methods that perform pair-wise similarity estimation with localdescriptors or list-wise re-ranking with global descriptors, LOCORE is thefirst method to perform list-wise re-ranking with local descriptors. To achievethis, we leverage efficient long-context sequence models to effectively capturethe dependencies between query and gallery images at the local-descriptorlevel. During testing, we process long shortlists with a sliding windowstrategy that is tailored to overcome the context size limitations of sequencemodels. Our approach achieves superior performance compared with otherre-rankers on established image retrieval benchmarks of landmarks (ROxf andRPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200)while having comparable latency to the pair-wise local descriptor re-rankers.
  </details>

- **[Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying](http://arxiv.org/abs/2503.21767v1)**  `arXiv:2503.21767`  `cs.CV`  
  _Hairong Yin, Huangying Zhan, Yi Xu, Raymond A. Yeh_
  <details open><summary>Abstract</summary>
  Open-vocabulary querying in 3D Gaussian Splatting aims to identifysemantically relevant regions within a 3D Gaussian representation based on agiven text query. Prior work, such as LangSplat, addressed this task byretrieving these regions in the form of segmentation masks on 2D renderings.More recently, OpenGaussian introduced point-level querying, which directlyselects a subset of 3D Gaussians. In this work, we propose a point-levelquerying method that builds upon LangSplat's framework. Our approach improvesthe framework in two key ways: (a) we leverage masklets from the SegmentAnything Model 2 (SAM2) to establish semantic consistent ground-truth fordistilling the language Gaussians; (b) we introduces a novel two-step queryingapproach that first retrieves the distilled ground-truth and subsequently usesthe ground-truth to query the individual Gaussians. Experimental evaluations onthree benchmark datasets demonstrate that the proposed method achieves betterperformance compared to state-of-the-art approaches. For instance, our methodachieves an mIoU improvement of +20.42 on the 3D-OVS dataset.
  </details>

- **[Exploring the Evolution of Physics Cognition in Video Generation: A Survey](http://arxiv.org/abs/2503.21765v1)**  `arXiv:2503.21765`  `cs.CV`  
  _Minghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, Pengxiang Ding, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in video generation have witnessed significant progress,especially with the rapid advancement of diffusion models. Despite this, theirdeficiencies in physical cognition have gradually received widespread attention- generated content often violates the fundamental laws of physics, fallinginto the dilemma of ''visual realism but physical absurdity". Researchers beganto increasingly recognize the importance of physical fidelity in videogeneration and attempted to integrate heuristic physical cognition such asmotion representations and physical knowledge into generative systems tosimulate real-world dynamic scenarios. Considering the lack of a systematicoverview in this field, this survey aims to provide a comprehensive summary ofarchitecture designs and their applications to fill this gap. Specifically, wediscuss and organize the evolutionary process of physical cognition in videogeneration from a cognitive science perspective, while proposing a three-tiertaxonomy: 1) basic schema perception for generation, 2) passive cognition ofphysical knowledge for generation, and 3) active cognition for worldsimulation, encompassing state-of-the-art methods, classical paradigms, andbenchmarks. Subsequently, we emphasize the inherent key challenges in thisdomain and delineate potential pathways for future research, contributing toadvancing the frontiers of discussion in both academia and industry. Throughstructured review and interdisciplinary analysis, this survey aims to providedirectional guidance for developing interpretable, controllable, and physicallyconsistent video generation paradigms, thereby propelling generative modelsfrom the stage of ''visual mimicry'' towards a new phase of ''human-likephysical comprehension''.
  </details>

- **[Lumina-Image 2.0: A Unified and Efficient Image Generative Framework](http://arxiv.org/abs/2503.21758v1)**  `arXiv:2503.21758`  `cs.CV`  
  _Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, et al._
  <details open><summary>Abstract</summary>
  We introduce Lumina-Image 2.0, an advanced text-to-image generation frameworkthat achieves significant progress compared to previous work, Lumina-Next.Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adoptsa unified architecture (Unified Next-DiT) that treats text and image tokens asa joint sequence, enabling natural cross-modal interactions and allowingseamless task expansion. Besides, since high-quality captioners can providesemantically well-aligned text-image training pairs, we introduce a unifiedcaptioning system, Unified Captioner (UniCap), specifically designed for T2Igeneration tasks. UniCap excels at generating comprehensive and accuratecaptions, accelerating convergence and enhancing prompt adherence. (2)Efficiency - to improve the efficiency of our proposed model, we developmulti-stage progressive training strategies and introduce inferenceacceleration techniques without compromising image quality. Extensiveevaluations on academic benchmarks and public text-to-image arenas show thatLumina-Image 2.0 delivers strong performances even with only 2.6B parameters,highlighting its scalability and design efficiency. We have released ourtraining details, code, and models athttps://github.com/Alpha-VLLM/Lumina-Image-2.0.
  </details>

- **[VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness](http://arxiv.org/abs/2503.21755v1)**  `arXiv:2503.21755`  `cs.CV`  
  _Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, et al._
  <details open><summary>Abstract</summary>
  Video generation has advanced significantly, evolving from producingunrealistic outputs to generating videos that appear visually convincing andtemporally coherent. To evaluate these video generative models, benchmarks suchas VBench have been developed to assess their faithfulness, measuring factorslike per-frame aesthetics, temporal consistency, and basic prompt adherence.However, these aspects mainly represent superficial faithfulness, which focuson whether the video appears visually convincing rather than whether it adheresto real-world principles. While recent models perform increasingly well onthese metrics, they still struggle to generate videos that are not justvisually plausible but fundamentally realistic. To achieve real "world models"through video generation, the next frontier lies in intrinsic faithfulness toensure that generated videos adhere to physical laws, commonsense reasoning,anatomical correctness, and compositional integrity. Achieving this level ofrealism is essential for applications such as AI-assisted filmmaking andsimulated world modeling. To bridge this gap, we introduce VBench-2.0, anext-generation benchmark designed to automatically evaluate video generativemodels for their intrinsic faithfulness. VBench-2.0 assesses five keydimensions: Human Fidelity, Controllability, Creativity, Physics, andCommonsense, each further broken down into fine-grained capabilities. Tailoredfor individual dimensions, our evaluation framework integrates generalists suchas state-of-the-art VLMs and LLMs, and specialists, including anomaly detectionmethods proposed for video generation. We conduct extensive annotations toensure alignment with human judgment. By pushing beyond superficialfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a newstandard for the next generation of video generative models in pursuit ofintrinsic faithfulness.
  </details>

- **[Reconstructing Humans with a Biomechanically Accurate Skeleton](http://arxiv.org/abs/2503.21751v1)**  `arXiv:2503.21751`  `cs.CV`  
  _Yan Xia, Xiaowei Zhou, Etienne Vouga, Qixing Huang, Georgios Pavlakos_
  <details open><summary>Abstract</summary>
  In this paper, we introduce a method for reconstructing 3D humans from asingle image using a biomechanically accurate skeleton model. To achieve this,we train a transformer that takes an image as input and estimates theparameters of the model. Due to the lack of training data for this task, webuild a pipeline to produce pseudo ground truth model parameters for singleimages and implement a training procedure that iteratively refines these pseudolabels. Compared to state-of-the-art methods for 3D human mesh recovery, ourmodel achieves competitive performance on standard benchmarks, while itsignificantly outperforms them in settings with extreme 3D poses andviewpoints. Additionally, we show that previous reconstruction methodsfrequently violate joint angle limits, leading to unnatural rotations. Incontrast, our approach leverages the biomechanically plausible degrees offreedom making more realistic joint rotation estimates. We validate ourapproach across multiple human pose estimation benchmarks. We make the code,models and data available at: https://isshikihugh.github.io/HSMR/
  </details>

- **[LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis](http://arxiv.org/abs/2503.21749v1)**  `arXiv:2503.21749`  `cs.CV`  
  _Shitian Zhao, Qilong Wu, Xinyue Li, Bo Zhang, Ming Li, Qi Qin, et al._
  <details open><summary>Abstract</summary>
  We introduce LeX-Art, a comprehensive suite for high-quality text-imagesynthesis that systematically bridges the gap between prompt expressiveness andtext rendering fidelity. Our approach follows a data-centric paradigm,constructing a high-quality data synthesis pipeline based on Deepseek-R1 tocurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined1024$\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer,a robust prompt enrichment model, and train two text-to-image models, LeX-FLUXand LeX-Lumina, achieving state-of-the-art text rendering performance. Tosystematically evaluate visual text generation, we introduce LeX-Bench, abenchmark that assesses fidelity, aesthetics, and alignment, complemented byPairwise Normalized Edit Distance (PNED), a novel metric for robust textaccuracy evaluation. Experiments demonstrate significant improvements, withLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUXoutperforming baselines in color (+3.18%), positional (+4.45%), and fontaccuracy (+3.81%). Our codes, models, datasets, and demo are publiclyavailable.
  </details>

- **[3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models](http://arxiv.org/abs/2503.21745v1)**  `arXiv:2503.21745`  `cs.CV`  
  _Yuhan Zhang, Mengchen Zhang, Tong Wu, Tengfei Wang, Gordon Wetzstein, Dahua Lin, et al._
  <details open><summary>Abstract</summary>
  3D generation is experiencing rapid advancements, while the development of 3Devaluation has not kept pace. How to keep automatic evaluation equitablyaligned with human perception has become a well-recognized challenge. Recentadvances in the field of language and image generation have explored humanpreferences and showcased respectable fitting ability. However, the 3D domainstill lacks such a comprehensive preference dataset over generative models. Tomitigate this absence, we develop 3DGen-Arena, an integrated platform in abattle manner. Then, we carefully design diverse text and image prompts andleverage the arena platform to gather human preferences from both public usersand expert annotators, resulting in a large-scale multi-dimension humanpreference dataset 3DGen-Bench. Using this dataset, we further train aCLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator,3DGen-Eval. These two models innovatively unify the quality evaluation oftext-to-3D and image-to-3D generation, and jointly form our automatedevaluation system with their respective strengths. Extensive experimentsdemonstrate the efficacy of our scoring model in predicting human preferences,exhibiting a superior correlation with human ranks compared to existingmetrics. We believe that our 3DGen-Bench dataset and automated evaluationsystem will foster a more equitable evaluation in the field of 3D generation,further promoting the development of 3D generative models and their downstreamapplications.
  </details>

- **[Evaluating Text-to-Image Synthesis with a Conditional Fr√©chet Distance](http://arxiv.org/abs/2503.21721v1)**  `arXiv:2503.21721`  `cs.CV`  
  _Jaywon Koo, Jefferson Hernandez, Moayed Haji-Ali, Ziyan Yang, Vicente Ordonez_
  <details open><summary>Abstract</summary>
  Evaluating text-to-image synthesis is challenging due to misalignment betweenestablished metrics and human preferences. We propose cFreD, a metric based onthe notion of Conditional Fr\'echet Distance that explicitly accounts for bothvisual fidelity and text-prompt alignment. Existing metrics such as InceptionScore (IS), Fr\'echet Inception Distance (FID) and CLIPScore assess eitherimage quality or image-text alignment but not both which limits theircorrelation with human preferences. Scoring models explicitly trained toreplicate human preferences require constant updates and may not generalize tonovel generation techniques or out-of-domain inputs. Through extensiveexperiments across multiple recently proposed text-to-image models and diverseprompt datasets, we demonstrate that cFreD exhibits a higher correlation withhuman judgments compared to statistical metrics, including metrics trained withhuman preferences. Our findings validate cFreD as a robust, future-proof metricfor the systematic evaluation of text-to-image models, standardizingbenchmarking in this rapidly evolving field. We release our evaluation toolkitand benchmark in the appendix.
  </details>

- **[CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?](http://arxiv.org/abs/2503.21717v1)**  `arXiv:2503.21717`  `cs.CL`  
  _Jiefu Ou, William Gantt Walden, Kate Sanders, Zhengping Jiang, Kaiser Sun, Jeffrey Cheng, et al._
  <details open><summary>Abstract</summary>
  A core part of scientific peer review involves providing expert critiquesthat directly assess the scientific claims a paper makes. While it is nowpossible to automatically generate plausible (if generic) reviews, ensuringthat these reviews are sound and grounded in the papers' claims remainschallenging. To facilitate LLM benchmarking on these challenges, we introduceCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions andreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts forweakness statements in the reviews and the paper claims that they dispute, aswell as fine-grained labels of the validity, objectivity, and type of theidentified weaknesses. We benchmark several LLMs on three claim-centric taskssupported by CLAIMCHECK, requiring models to (1) associate weaknesses with theclaims they dispute, (2) predict fine-grained labels for weaknesses and rewritethe weaknesses to enhance their specificity, and (3) verify a paper's claimswith grounded reasoning. Our experiments reveal that cutting-edge LLMs, whilecapable of predicting weakness labels in (2), continue to underperform relativeto human experts on all other tasks.
  </details>

- **[Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI](http://arxiv.org/abs/2503.21668v1)**  `arXiv:2503.21668`  `cs.AI` `cs.LG` `cs.CV`  
  _Danaja Rutar, Alva Markelius, Konstantinos Voudouris, Jos√© Hern√°ndez-Orallo, Lucy Cheke_
  <details open><summary>Abstract</summary>
  One of the core components of our world models is 'intuitive physics' - anunderstanding of objects, space, and causality. This capability enables us topredict events, plan action and navigate environments, all of which rely on acomposite sense of objecthood. Despite its importance, there is no single,unified account of objecthood, though multiple theoretical frameworks provideinsights. In the first part of this paper, we present a comprehensive overviewof the main theoretical frameworks in objecthood research - Gestalt psychology,enactive cognition, and developmental psychology - and identify the corecapabilities each framework attributes to object understanding, as well as whatfunctional roles they play in shaping world models in biological agents. Giventhe foundational role of objecthood in world modelling, understandingobjecthood is also essential in AI. In the second part of the paper, weevaluate how current AI paradigms approach and test objecthood capabilitiescompared to those in cognitive science. We define an AI paradigm as acombination of how objecthood is conceptualised, the methods used for studyingobjecthood, the data utilised, and the evaluation techniques. We find that,whilst benchmarks can detect that AI systems model isolated aspects ofobjecthood, the benchmarks cannot detect when AI systems lack functionalintegration across these capabilities, not solving the objecthood challengefully. Finally, we explore novel evaluation approaches that align with theintegrated vision of objecthood outlined in this paper. These methods arepromising candidates for advancing from isolated object capabilities towardgeneral-purpose AI with genuine object understanding in real-world contexts.
  </details>

- **[InteractionMap: Improving Online Vectorized HDMap Construction with Interaction](http://arxiv.org/abs/2503.21659v1)**  `arXiv:2503.21659`  `cs.CV`  
  _Kuang Wu, Chuan Yang, Zhanbin Li_
  <details open><summary>Abstract</summary>
  Vectorized high-definition (HD) maps are essential for an autonomous drivingsystem. Recently, state-of-the-art map vectorization methods are mainly basedon DETR-like framework to generate HD maps in an end-to-end manner. In thispaper, we propose InteractionMap, which improves previous map vectorizationmethods by fully leveraging local-to-global information interaction in bothtime and space. Firstly, we explore enhancing DETR-like detectors by explicitposition relation prior from point-level to instance-level, since map elementscontain strong shape priors. Secondly, we propose a key-frame-basedhierarchical temporal fusion module, which interacts temporal information fromlocal to global. Lastly, the separate classification branch and regressionbranch lead to the problem of misalignment in the output distribution. Weinteract semantic information with geometric information by introducing a novelgeometric-aware classification loss in optimization and a geometric-awarematching cost in label assignment. InteractionMap achieves state-of-the-artperformance on both nuScenes and Argoverse2 benchmarks.
  </details>

- **[The MVTec AD 2 Dataset: Advanced Scenarios for Unsupervised Anomaly Detection](http://arxiv.org/abs/2503.21622v1)**  `arXiv:2503.21622`  `cs.CV`  
  _Lars Heckler-Kram, Jan-Hendrik Neudeck, Ulla Scheler, Rebecca K√∂nig, Carsten Steger_
  <details open><summary>Abstract</summary>
  In recent years, performance on existing anomaly detection benchmarks likeMVTec AD and VisA has started to saturate in terms of segmentation AU-PRO, withstate-of-the-art models often competing in the range of less than onepercentage point. This lack of discriminatory power prevents a meaningfulcomparison of models and thus hinders progress of the field, especially whenconsidering the inherent stochastic nature of machine learning results. Wepresent MVTec AD 2, a collection of eight anomaly detection scenarios with morethan 8000 high-resolution images. It comprises challenging and highly relevantindustrial inspection use cases that have not been considered in previousdatasets, including transparent and overlapping objects, dark-field and backlight illumination, objects with high variance in the normal data, andextremely small defects. We provide comprehensive evaluations ofstate-of-the-art methods and show that their performance remains below 60%average AU-PRO. Additionally, our dataset provides test scenarios with lightingcondition changes to assess the robustness of methods under real-worlddistribution shifts. We host a publicly accessible evaluation server that holdsthe pixel-precise ground truth of the test set (https://benchmark.mvtec.com/).All image data is available athttps://www.mvtec.com/company/research/datasets/mvtec-ad-2.
  </details>

- **[UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning](http://arxiv.org/abs/2503.21620v1)**  `arXiv:2503.21620`  `cs.AI`  
  _Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, et al._
  <details open><summary>Abstract</summary>
  The recent DeepSeek-R1 has showcased the emergence of reasoning capabilitiesin LLMs through reinforcement learning (RL) with rule-based rewards. Buildingon this idea, we are the first to explore how rule-based RL can enhance thereasoning capabilities of multimodal large language models (MLLMs) for graphicuser interface (GUI) action prediction tasks. To this end, we curate a smallyet high-quality dataset of 136 challenging tasks, encompassing five commonaction types on mobile devices. We also introduce a unified rule-based actionreward, enabling model optimization via policy-based algorithms such as GroupRelative Policy Optimization (GRPO). Experimental results demonstrate that ourproposed data-efficient model, UI-R1-3B, achieves substantial improvements onboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the IDbenchmark AndroidControl, the action type accuracy improves by 15%, whilegrounding accuracy increases by 10.3%, compared with the base model (i.e.Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our modelsurpasses the base model by 6.0% and achieves competitive performance withlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning(SFT) on 76K data. These results underscore the potential of rule-basedreinforcement learning to advance GUI understanding and control, paving the wayfor future research in this domain.
  </details>

- **[ICG-MVSNet: Learning Intra-view and Cross-view Relationships for Guidance in Multi-View Stereo](http://arxiv.org/abs/2503.21525v1)**  `arXiv:2503.21525`  `cs.CV`  
  _Yuxi Hu, Jun Zhang, Zhe Zhang, Rafael Weilharter, Yuchen Rao, Kuangyi Chen, et al._
  <details open><summary>Abstract</summary>
  Multi-view Stereo (MVS) aims to estimate depth and reconstruct 3D pointclouds from a series of overlapping images. Recent learning-based MVSframeworks overlook the geometric information embedded in features andcorrelations, leading to weak cost matching. In this paper, we proposeICG-MVSNet, which explicitly integrates intra-view and cross-view relationshipsfor depth estimation. Specifically, we develop an intra-view feature fusionmodule that leverages the feature coordinate correlations within a single imageto enhance robust cost matching. Additionally, we introduce a lightweightcross-view aggregation module that efficiently utilizes the contextualinformation from volume correlations to guide regularization. Our method isevaluated on the DTU dataset and Tanks and Temples benchmark, consistentlyachieving competitive performance against state-of-the-art works, whilerequiring lower computational resources.
  </details>

- **[Uncertainty-aware Bayesian machine learning modelling of land cover classification](http://arxiv.org/abs/2503.21510v1)**  `arXiv:2503.21510`  `cs.LG` `cs.CV`  
  _Samuel Bilson, Anna Pustogvar_
  <details open><summary>Abstract</summary>
  Land cover classification involves the production of land cover maps, whichdetermine the type of land through remote sensing imagery. Over recent years,such classification is being performed by machine learning classificationmodels, which can give highly accurate predictions on land cover per pixelusing large quantities of input training data. However, such models do notcurrently take account of input measurement uncertainty, which is vital fortraceability in metrology. In this work we propose a Bayesian classificationframework using generative modelling to take account of input measurementuncertainty. We take the specific case of Bayesian quadratic discriminantanalysis, and apply it to land cover datasets from Copernicus Sentinel-2 in2020 and 2021. We benchmark the performance of the model against more popularclassification models used in land cover maps such as random forests and neuralnetworks. We find that such Bayesian models are more trustworthy, in the sensethat they are more interpretable, explicitly model the input measurementuncertainty, and maintain predictive performance of class probability outputsacross datasets of different years and sizes, whilst also being computationallyefficient.
  </details>

- **[Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving](http://arxiv.org/abs/2503.21505v1)**  `arXiv:2503.21505`  `cs.CL` `cs.CV`  
  _Yue Li, Meng Tian, Zhenyu Lin, Jiangtong Zhu, Dechang Zhu, Haiqiang Liu, et al._
  <details open><summary>Abstract</summary>
  Existing benchmarks for Vision-Language Model (VLM) on autonomous driving(AD) primarily assess interpretability through open-form visual questionanswering (QA) within coarse-grained tasks, which remain insufficient to assesscapabilities in complex driving scenarios. To this end, we introduce$\textbf{VLADBench}$, a challenging and fine-grained dataset featuringclose-form QAs that progress from static foundational knowledge and elements toadvanced reasoning for dynamic on-road situations. The elaborate$\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,General Element Recognition, Traffic Graph Generation, Target AttributeComprehension, and Ego Decision-Making and Planning. These domains are furtherbroken down into 11 secondary aspects and 29 tertiary tasks for a granularevaluation. A thorough assessment of general and domain-specific (DS) VLMs onthis benchmark reveals both their strengths and critical limitations in ADcontexts. To further exploit the cognitive and reasoning interactions among the5 domains for AD understanding, we start from a small-scale VLM and train theDS models on individual domain datasets (collected from 1.4M DS QAs acrosspublic sources). The experimental results demonstrate that the proposedbenchmark provides a crucial step toward a more comprehensive assessment ofVLMs in AD, paving the way for the development of more cognitivelysophisticated and reasoning-capable AD systems.
  </details>

- **[OpenHuEval: Evaluating Large Language Model on Hungarian Specifics](http://arxiv.org/abs/2503.21500v1)**  `arXiv:2503.21500`  `cs.CL`  
  _Haote Yang, Xingjian Wei, Jiang Wu, No√©mi Ligeti-Nagy, Jiaxing Sun, Yinfan Wang, et al._
  <details open><summary>Abstract</summary>
  We introduce OpenHuEval, the first benchmark for LLMs focusing on theHungarian language and specifics. OpenHuEval is constructed from a vastcollection of Hungarian-specific materials sourced from multiple origins. Inthe construction, we incorporated the latest design principles for evaluatingLLMs, such as using real user queries from the internet, emphasizing theassessment of LLMs' generative capabilities, and employing LLM-as-judge toenhance the multidimensionality and accuracy of evaluations. Ultimately,OpenHuEval encompasses eight Hungarian-specific dimensions, featuring fivetasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive,in-depth, and scientifically accurate assessment of LLM performance in thecontext of the Hungarian language and its specifics. We evaluated currentmainstream LLMs, including both traditional LLMs and recently developed LargeReasoning Models. The results demonstrate the significant necessity forevaluation and model optimization tailored to the Hungarian language andspecifics. We also established the framework for analyzing the thinkingprocesses of LRMs with OpenHuEval, revealing intrinsic patterns and mechanismsof these models in non-English languages, with Hungarian serving as arepresentative example. We will release OpenHuEval athttps://github.com/opendatalab/OpenHuEval .
  </details>

- **[BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding](http://arxiv.org/abs/2503.21483v1)**  `arXiv:2503.21483`  `cs.CV`  
  _Shuming Liu, Chen Zhao, Tianqi Xu, Bernard Ghanem_
  <details open><summary>Abstract</summary>
  Large video-language models (VLMs) have demonstrated promising progress invarious video understanding tasks. However, their effectiveness in long-formvideo analysis is constrained by limited context windows. Traditionalapproaches, such as uniform frame sampling, often inevitably allocate resourcesto irrelevant content, diminishing their effectiveness in real-world scenarios.In this paper, we introduce BOLT, a method to BOost Large VLMs withoutadditional Training through a comprehensive study of frame selectionstrategies. First, to enable a more realistic evaluation of VLMs in long-formvideo understanding, we propose a multi-source retrieval evaluation setting.Our findings reveal that uniform sampling performs poorly in noisy contexts,underscoring the importance of selecting the right frames. Second, we exploreseveral frame selection strategies based on query-frame similarity and analyzetheir effectiveness at inference time. Our results show that inverse transformsampling yields the most significant performance improvement, increasingaccuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from58.9% to 63.4%. Our code is available at https://github.com/sming256/BOLT.
  </details>

- **[OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs](http://arxiv.org/abs/2503.21480v1)**  `arXiv:2503.21480`  `cs.CL`  
  _John Murzaku, Owen Rambow_
  <details open><summary>Abstract</summary>
  The use of omni-LLMs (large language models that accept any modality asinput), particularly for multimodal cognitive state tasks involving speech, isunderstudied. We present OmniVox, the first systematic evaluation of fouromni-LLMs on the zero-shot emotion recognition task. We evaluate on two widelyused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shotomni-LLMs outperform or are competitive with fine-tuned audio models. Alongsideour audio-only evaluation, we also evaluate omni-LLMs on text only and text andaudio. We present acoustic prompting, an audio-specific prompting strategy foromni-LLMs which focuses on acoustic feature analysis, conversation contextanalysis, and step-by-step reasoning. We compare our acoustic prompting tominimal prompting and full chain-of-thought prompting techniques. We perform acontext window analysis on IEMOCAP and MELD, and find that using context helps,especially on IEMOCAP. We conclude with an error analysis on the generatedacoustic reasoning outputs from the omni-LLMs.
  </details>

- **[The Procedural Content Generation Benchmark: An Open-source Testbed for Generative Challenges in Games](http://arxiv.org/abs/2503.21474v1)**  `arXiv:2503.21474`  `cs.AI` `cs.LG`  
  _Ahmed Khalifa, Roberto Gallotta, Matthew Barthet, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis_
  <details open><summary>Abstract</summary>
  This paper introduces the Procedural Content Generation Benchmark forevaluating generative algorithms on different game content creation tasks. Thebenchmark comes with 12 game-related problems with multiple variants on eachproblem. Problems vary from creating levels of different kinds to creating rulesets for simple arcade games. Each problem has its own content representation,control parameters, and evaluation metrics for quality, diversity, andcontrollability. This benchmark is intended as a first step towards astandardized way of comparing generative algorithms. We use the benchmark toscore three baseline algorithms: a random generator, an evolution strategy, anda genetic algorithm. Results show that some problems are easier to solve thanothers, as well as the impact the chosen objective has on quality, diversity,and controllability of the generated artifacts.
  </details>

- **[RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives](http://arxiv.org/abs/2503.21459v1)**  `arXiv:2503.21459`  `cs.CV`  
  _Chirag Parikh, Deepti Rawat, Rakshitha R. T., Tathagata Ghosh, Ravi Kiran Sarvadevabhatla_
  <details open><summary>Abstract</summary>
  We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored forgeneric road event understanding from social media narratives. Unlike existingdatasets limited by regional bias, viewpoint bias and expert-drivenannotations, RoadSocial captures the global complexity of road events withvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich socialdiscourse. Our scalable semi-automatic annotation framework leverages Text LLMsand Video LLMs to generate comprehensive question-answer pairs across 12challenging QA tasks, pushing the boundaries of road event understanding.RoadSocial is derived from social media videos spanning 14M frames and 414Ksocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260Khigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,driving-specific and general-purpose) on our road event understandingbenchmark. We also demonstrate RoadSocial's utility in improving road eventunderstanding capabilities of general-purpose Video LLMs.
  </details>

- **[FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs](http://arxiv.org/abs/2503.21457v1)**  `arXiv:2503.21457`  `cs.CV`  
  _Xiaoqin Wang, Xusen Ma, Xianxu Hou, Meidan Ding, Yudong Li, Junliang Chen, et al._
  <details open><summary>Abstract</summary>
  Multimodal large language models (MLLMs) have demonstrated remarkablecapabilities in various tasks. However, effectively evaluating these MLLMs onface perception remains largely unexplored. To address this gap, we introduceFaceBench, a dataset featuring hierarchical multi-view and multi-levelattributes specifically designed to assess the comprehensive face perceptionabilities of MLLMs. Initially, we construct a hierarchical facial attributestructure, which encompasses five views with up to three levels of attributes,totaling over 210 attributes and 700 attribute values. Based on the structure,the proposed FaceBench consists of 49,919 visual question-answering (VQA) pairsfor evaluation and 23,841 pairs for fine-tuning. Moreover, we further develop arobust face perception MLLM baseline, Face-LLaVA, by training with our proposedface VQA data. Extensive experiments on various mainstream MLLMs and Face-LLaVAare conducted to test their face perception ability, with results also comparedagainst human performance. The results reveal that, the existing MLLMs are farfrom satisfactory in understanding the fine-grained facial attributes, whileour Face-LLaVA significantly outperforms existing open-source models with asmall amount of training data and is comparable to commercial ones like GPT-4oand Gemini. The dataset will be released athttps://github.com/CVI-SZU/FaceBench.
  </details>

- **[Stochastic Engrams for Efficient Continual Learning with Binarized Neural Networks](http://arxiv.org/abs/2503.21436v1)**  `arXiv:2503.21436`  `cs.LG`  
  _Isabelle Aguilar, Luis Fernando Herbozo Contreras, Omid Kavehei_
  <details open><summary>Abstract</summary>
  The ability to learn continuously in artificial neural networks (ANNs) isoften limited by catastrophic forgetting, a phenomenon in which new knowledgebecomes dominant. By taking mechanisms of memory encoding in neuroscience (aka.engrams) as inspiration, we propose a novel approach that integratesstochastically-activated engrams as a gating mechanism for metaplasticbinarized neural networks (mBNNs). This method leverages the computationalefficiency of mBNNs combined with the robustness of probabilistic memory tracesto mitigate forgetting and maintain the model's reliability. Previouslyvalidated metaplastic optimization techniques have been incorporated to enhancesynaptic stability further. Compared to baseline binarized models and benchmarkfully connected continual learning approaches, our method is the only strategycapable of reaching average accuracies over 20% in class-incremental scenariosand achieving comparable domain-incremental results to full precisionstate-of-the-art methods. Furthermore, we achieve a significant reduction inpeak GPU and RAM usage, under 5% and 20%, respectively. Our findingsdemonstrate (A) an improved stability vs. plasticity trade-off, (B) a reducedmemory intensiveness, and (C) an enhanced performance in binarizedarchitectures. By uniting principles of neuroscience and efficient computing,we offer new insights into the design of scalable and robust deep learningsystems.
  </details>

- **[Graph-to-Vision: Multi-graph Understanding and Reasoning using Vision-Language Models](http://arxiv.org/abs/2503.21435v1)**  `arXiv:2503.21435`  `cs.AI`  
  _Ruizhou Li, Haiyun Jiang_
  <details open><summary>Abstract</summary>
  Graph Neural Networks (GNNs), as the dominant paradigm for graph-structuredlearning, have long faced dual challenges of exponentially escalatingcomputational complexity and inadequate cross-scenario generalizationcapability. With the rapid advancement of multimodal learning, Vision-LanguageModels (VLMs) have demonstrated exceptional cross-modal relational reasoningcapabilities and generalization capacities, thereby opening up novel pathwaysfor overcoming the inherent limitations of conventional graph learningparadigms. However, current research predominantly concentrates oninvestigating the single-graph reasoning capabilities of VLMs, whichfundamentally fails to address the critical requirement for coordinatedreasoning across multiple heterogeneous graph data in real-world applicationscenarios. To address these limitations, we propose the first multi-graph jointreasoning benchmark for VLMs. Our benchmark encompasses four graph categories:knowledge graphs, flowcharts, mind maps, and route maps,with each graph groupaccompanied by three progressively challenging instruction-response pairs.Leveraging this benchmark, we conducted comprehensive capability assessments ofstate-of-the-art VLMs and performed fine-tuning on open-source models. Thisstudy not only addresses the underexplored evaluation gap in multi-graphreasoning for VLMs but also empirically validates their generalizationsuperiority in graph-structured learning.
  </details>

- **[Controlling Large Language Model with Latent Actions](http://arxiv.org/abs/2503.21383v1)**  `arXiv:2503.21383`  `cs.CL` `cs.LG`  
  _Chengxing Jia, Ziniu Li, Pengyuan Wang, Yi-Chen Li, Zhenyu Hou, Yuxiao Dong, et al._
  <details open><summary>Abstract</summary>
  Adapting Large Language Models (LLMs) to downstream tasks using ReinforcementLearning (RL) has proven to be an effective approach. However, LLMs do notinherently define the structure of an agent for RL training, particularly interms of defining the action space. This paper studies learning a compactlatent action space to enhance the controllability and exploration of RL forLLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),a framework that integrates a latent action space into pre-trained LLMs. Weapply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,compared to RL with token-level actions, CoLA's latent action enables greatersemantic diversity in text generation. For enhancing downstream tasks, we showthat CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassingthe baseline score of 38.2, and reaches 68.2 when augmented with a Monte CarloTree Search variant. Furthermore, CoLA with RL consistently improvesperformance on agent-based tasks without degrading the pre-trained LLM'scapabilities, unlike the baseline. Finally, CoLA reduces computation time byhalf in tasks involving enhanced thinking prompts for LLMs by RL. These resultshighlight CoLA's potential to advance RL-based adaptation of LLMs fordownstream applications.
  </details>

- **[Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models](http://arxiv.org/abs/2503.21380v1)**  `arXiv:2503.21380`  `cs.CL`  
  _Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, et al._
  <details open><summary>Abstract</summary>
  In recent years, the rapid development of large reasoning models has resultedin the saturation of existing benchmarks for evaluating mathematical reasoning,highlighting the urgent need for more challenging and rigorous evaluationframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-levelmathematical benchmark, designed to rigorously test the complex reasoningcapabilities of LLMs. OlymMATH features 200 meticulously curated problems, eachmanually verified and available in parallel English and Chinese versions. Theproblems are systematically organized into two distinct difficulty tiers: (1)AIME-level problems (easy) that establish a baseline for mathematical reasoningassessment, and (2) significantly more challenging problems (hard) designed topush the boundaries of current state-of-the-art models. In our benchmark, theseproblems span four core mathematical fields, each including a verifiablenumerical solution to enable objective, rule-based evaluation. Empiricalresults underscore the significant challenge presented by OlymMATH, withstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-minidemonstrating notably limited accuracy on the hard subset. Furthermore, thebenchmark facilitates comprehensive bilingual assessment of mathematicalreasoning abilities-a critical dimension that remains largely unaddressed inmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmarkat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.
  </details>

- **[FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for Composed Image Retrieval](http://arxiv.org/abs/2503.21309v1)**  `arXiv:2503.21309`  `cs.AI` `cs.CV`  
  _Zixu Li, Zhiheng Fu, Yupeng Hu, Zhiwei Chen, Haokun Wen, Liqiang Nie_
  <details open><summary>Abstract</summary>
  Composed Image Retrieval (CIR) facilitates image retrieval through amultimodal query consisting of a reference image and modification text. Thereference image defines the retrieval context, while the modification textspecifies desired alterations. However, existing CIR datasets predominantlyemploy coarse-grained modification text (CoarseMT), which inadequately capturesfine-grained retrieval intents. This limitation introduces two key challenges:(1) ignoring detailed differences leads to imprecise positive samples, and (2)greater ambiguity arises when retrieving visually similar images. These issuesdegrade retrieval accuracy, necessitating manual result filtering or repeatedqueries. To address these limitations, we develop a robust fine-grained CIRdata annotation pipeline that minimizes imprecise positive samples and enhancesCIR systems' ability to discern modification intents accurately. Using thispipeline, we refine the FashionIQ and CIRR datasets to create two fine-grainedCIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR,the first CIR framework explicitly designed to parse the modification text.FineCIR effectively captures fine-grained modification semantics and alignsthem with ambiguous visual entities, enhancing retrieval precision. Extensiveexperiments demonstrate that FineCIR consistently outperforms state-of-the-artCIR baselines on both fine-grained and traditional CIR benchmark datasets. OurFineCIR code and fine-grained CIR datasets are available athttps://github.com/SDU-L/FineCIR.git.
  </details>

- **[InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression](http://arxiv.org/abs/2503.21307v1)**  `arXiv:2503.21307`  `cs.AI` `cs.CV`  
  _Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, et al._
  <details open><summary>Abstract</summary>
  Most multimodal large language models (MLLMs) treat visual tokens as "asequence of text", integrating them with text tokens into a large languagemodel (LLM). However, a great quantity of visual tokens significantly increasesthe demand for computational resources and time. In this paper, we proposeInternVL-X, which outperforms the InternVL model in both performance andefficiency by incorporating three visual token compression methods. First, wepropose a novel vision-language projector, PVTC. This component integratesadjacent visual embeddings to form a local query and utilizes the transformedCLS token as a global query, then performs point-to-region cross-attentionthrough these local and global queries to more effectively convert visualfeatures. Second, we present a layer-wise visual token compression module,LVTC, which compresses tokens in the LLM shallow layers and then expands themthrough upsampling and residual connections in the deeper layers. Thissignificantly enhances the model computational efficiency. Futhermore, wepropose an efficient high resolution slicing method, RVTC, which dynamicallyadjusts the number of visual tokens based on image area or length filtering.RVTC greatly enhances training efficiency with only a slight reduction inperformance. By utilizing 20% or fewer visual tokens, InternVL-X achievesstate-of-the-art performance on 7 public MLLM benchmarks, and improves theaverage metric by 2.34% across 12 tasks.
  </details>

- **[An analysis of higher-order kinematics formalisms for an innovative surgical parallel robot](http://arxiv.org/abs/2503.21291v1)**  `arXiv:2503.21291`  `cs.RO`  
  _Calin Vaida, Iosif Birlescu, Bogdan Gherman, Daniel Condurache, Damien Chablat, Doina Pisla_
  <details open><summary>Abstract</summary>
  The paper presents a novel modular hybrid parallel robot for pancreaticsurgery and its higher-order kinematics derived based on various formalisms.The classical vector, homogeneous transformation matrices and dual quaternionapproaches are studied for the kinematic functions using both classicaldifferentiation and multidual algebra. The algorithms for inverse kinematicsfor all three studied formalisms are presented for both differentiation andmultidual algebra approaches. Furthermore, these algorithms are compared basedon numerical stability, execution times and number and type of mathematicalfunctions and operators contained in each algorithm. A statistical analysisshows that there is significant improvement in execution time for thealgorithms implemented using multidual algebra, while the numerical stabilityis appropriate for all algorithms derived based on differentiation andmultidual algebra. While the implementation of the kinematic algorithms usingmultidual algebra shows positive results when benchmarked on a standard PC,further work is required to evaluate the multidual algorithms onhardware/software used for the modular parallel robot command and control.
  </details>

- **[Delving Deep into Semantic Relation Distillation](http://arxiv.org/abs/2503.21269v1)**  `arXiv:2503.21269`  `cs.CV`  
  _Zhaoyi Yan, Kangjun Liu, Qixiang Ye_
  <details open><summary>Abstract</summary>
  Knowledge distillation has become a cornerstone technique in deep learning,facilitating the transfer of knowledge from complex models to lightweightcounterparts. Traditional distillation approaches focus on transferringknowledge at the instance level, but fail to capture nuanced semanticrelationships within the data. In response, this paper introduces a novelmethodology, Semantics-based Relation Knowledge Distillation (SeRKD), whichreimagines knowledge distillation through a semantics-relation lens among eachsample. By leveraging semantic components, \ie, superpixels, SeRKD enables amore comprehensive and context-aware transfer of knowledge, which skillfullyintegrates superpixel-based semantic extraction with relation-based knowledgedistillation for a sophisticated model compression and distillation.Particularly, the proposed method is naturally relevant in the domain of VisionTransformers (ViTs), where visual tokens serve as fundamental units ofrepresentation. Experimental evaluations on benchmark datasets demonstrate thesuperiority of SeRKD over existing methods, underscoring its efficacy inenhancing model performance and generalization capabilities.
  </details>

- **[Cultivating Game Sense for Yourself: Making VLMs Gaming Experts](http://arxiv.org/abs/2503.21263v1)**  `arXiv:2503.21263`  `cs.CL`  
  _Wenxuan Lu, Jiangyang He, Zhanqiu Zhang, Yiwen Guo, Tianning Zang_
  <details open><summary>Abstract</summary>
  Developing agents capable of fluid gameplay in first/third-person gameswithout API access remains a critical challenge in Artificial GeneralIntelligence (AGI). Recent efforts leverage Vision Language Models (VLMs) asdirect controllers, frequently pausing the game to analyze screens and planaction through language reasoning. However, this inefficient paradigmfundamentally restricts agents to basic and non-fluent interactions: relying onisolated VLM reasoning for each action makes it impossible to handle tasksrequiring high reactivity (e.g., FPS shooting) or dynamic adaptability (e.g.,ACT combat). To handle this, we propose a paradigm shift in gameplay agentdesign: instead of directly controlling gameplay, VLM develops specializedexecution modules tailored for tasks like shooting and combat. These moduleshandle real-time game interactions, elevating VLM to a high-level developer.Building upon this paradigm, we introduce GameSense, a gameplay agent frameworkwhere VLM develops task-specific game sense modules by observing task executionand leveraging vision tools and neural network training pipelines. Thesemodules encapsulate action-feedback logic, ranging from direct action rules toneural network-based decisions. Experiments demonstrate that our framework isthe first to achieve fluent gameplay in diverse genres, including ACT, FPS, andFlappy Bird, setting a new benchmark for game-playing agents.
  </details>

- **[Reducing CT Metal Artifacts by Learning Latent Space Alignment with Gemstone Spectral Imaging Data](http://arxiv.org/abs/2503.21259v1)**  `arXiv:2503.21259`  `cs.CV`  
  _Wencheng Han, Dongqian Guo, Xiao Chen, Pang Lyu, Yi Jin, Jianbing Shen_
  <details open><summary>Abstract</summary>
  Metal artifacts in CT slices have long posed challenges in medicaldiagnostics. These artifacts degrade image quality, resulting in suboptimalvisualization and complicating the accurate interpretation of tissues adjacentto metal implants. To address these issues, we introduce the Latent GemstoneSpectral Imaging (GSI) Alignment Framework, which effectively reduces metalartifacts while avoiding the introduction of noise information. Our work isbased on a key finding that even artifact-affected ordinary CT sequencescontain sufficient information to discern detailed structures. The challengelies in the inability to clearly represent this information. To address thisissue, we developed an Alignment Framework that adjusts the representation ofordinary CT images to match GSI CT sequences. GSI is an advanced imagingtechnique using multiple energy levels to mitigate artifacts caused by metalimplants. By aligning the representation to GSI data, we can effectivelysuppress metal artifacts while clearly revealing detailed structure, withoutintroducing extraneous information into CT sequences. To facilitate theapplication, we propose a new dataset, Artifacts-GSI, captured from realpatients with metal implants, and establish a new benchmark based on thisdataset. Experimental results show that our method significantly reduces metalartifacts and greatly enhances the readability of CT slices. All our code anddata are available at: https://um-lab.github.io/GSI-MAR/
  </details>

- **[DynamiCtrl: Rethinking the Basic Structure and the Role of Text for High-quality Human Image Animation](http://arxiv.org/abs/2503.21246v1)**  `arXiv:2503.21246`  `cs.CV`  
  _Haoyu Zhao, Zhongang Qi, Cong Wang, Qingping Zheng, Guansong Lu, Fei Chen, et al._
  <details open><summary>Abstract</summary>
  Human image animation has recently gained significant attention due toadvancements in generative models. However, existing methods still face twomajor challenges: (1) architectural limitations, most models rely on U-Net,which underperforms compared to the MM-DiT; and (2) the neglect of textualinformation, which can enhance controllability. In this work, we introduceDynamiCtrl, a novel framework that not only explores different pose-guidedcontrol structures in MM-DiT, but also reemphasizes the crucial role of text inthis task. Specifically, we employ a Shared VAE encoder for both referenceimages and driving pose videos, eliminating the need for an additional poseencoder and simplifying the overall framework. To incorporate pose featuresinto the full attention blocks, we propose Pose-adaptive Layer Norm (PadaLN),which utilizes adaptive layer normalization to encode sparse pose features. Theencoded features are directly added to the visual input, preserving thespatiotemporal consistency of the backbone while effectively introducing posecontrol into MM-DiT. Furthermore, within the full attention mechanism, we aligntextual and visual features to enhance controllability. By leveraging text, wenot only enable fine-grained control over the generated content, but also, forthe first time, achieve simultaneous control over both background and motion.Experimental results verify the superiority of DynamiCtrl on benchmarkdatasets, demonstrating its strong identity preservation, heterogeneouscharacter driving, background controllability, and high-quality synthesis. Theproject page is available at https://gulucaptain.github.io/DynamiCtrl/.
  </details>

- **[LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models](http://arxiv.org/abs/2503.21227v1)**  `arXiv:2503.21227`  `cs.CL`  
  _Hengyuan Zhao, Ziqin Wang, Qixin Sun, Kaiyou Song, Yilin Li, Xiaolin Hu, et al._
  <details open><summary>Abstract</summary>
  Although applying Mixture of Experts to large language models for learningnew tasks is widely regarded as an effective strategy for continuous learning,there still remain two major challenges: (1) As the number of tasks grows,simple parameter expansion strategies can lead to excessively large models. (2)Modifying the parameters of the existing router results in the erosion ofpreviously acquired knowledge. In this paper, we present an innovativeframework named LLaVA-CMoE, which is a continuous Mixture of Experts (MoE)architecture without any replay data. Specifically, we have developed a methodcalled Probe-Guided Knowledge Extension (PGKE), which employs probe experts toassess whether additional knowledge is required for a specific layer. Thisapproach enables the model to adaptively expand its network parameters based ontask distribution, thereby significantly improving the efficiency of parameterexpansion. Additionally, we introduce a hierarchical routing algorithm calledProbabilistic Task Locator (PTL), where high-level routing captures inter-taskinformation and low-level routing focuses on intra-task details, ensuring thatnew task experts do not interfere with existing ones. Our experiments showsthat our efficient architecture has substantially improved model performance onthe Coin benchmark while maintaining a reasonable parameter count.
  </details>

- **[Learning Generalizable Skills from Offline Multi-Task Data for Multi-Agent Cooperation](http://arxiv.org/abs/2503.21200v1)**  `arXiv:2503.21200`  `cs.MA` `cs.LG`  
  _Sicong Liu, Yang Shu, Chenjuan Guo, Bin Yang_
  <details open><summary>Abstract</summary>
  Learning cooperative multi-agent policy from offline multi-task data that cangeneralize to unseen tasks with varying numbers of agents and targets is anattractive problem in many scenarios. Although aggregating general behaviorpatterns among multiple tasks as skills to improve policy transfer is apromising approach, two primary challenges hinder the further advancement ofskill learning in offline multi-task MARL. Firstly, extracting generalcooperative behaviors from various action sequences as common skills lacksbringing cooperative temporal knowledge into them. Secondly, existing worksonly involve common skills and can not adaptively choose independent knowledgeas task-specific skills in each task for fine-grained action execution. Totackle these challenges, we propose Hierarchical and Separate Skill Discovery(HiSSD), a novel approach for generalizable offline multi-task MARL throughskill learning. HiSSD leverages a hierarchical framework that jointly learnscommon and task-specific skills. The common skills learn cooperative temporalknowledge and enable in-sample exploitation for offline multi-task MARL. Thetask-specific skills represent the priors of each task and achieve atask-guided fine-grained action execution. To verify the advancement of ourmethod, we conduct experiments on multi-agent MuJoCo and SMAC benchmarks. Aftertraining the policy using HiSSD on offline multi-task data, the empiricalresults show that HiSSD assigns effective cooperative behaviors and obtainssuperior performance in unseen tasks.
  </details>

- **[Leveraging LLMs with Iterative Loop Structure for Enhanced Social Intelligence in Video Question Answering](http://arxiv.org/abs/2503.21190v1)**  `arXiv:2503.21190`  `cs.CV`  
  _Erika Mori, Yue Qiu, Hirokatsu Kataoka, Yoshimitsu Aoki_
  <details open><summary>Abstract</summary>
  Social intelligence, the ability to interpret emotions, intentions, andbehaviors, is essential for effective communication and adaptive responses. Asrobots and AI systems become more prevalent in caregiving, healthcare, andeducation, the demand for AI that can interact naturally with humans grows.However, creating AI that seamlessly integrates multiple modalities, such asvision and speech, remains a challenge. Current video-based methods for socialintelligence rely on general video recognition or emotion recognitiontechniques, often overlook the unique elements inherent in human interactions.To address this, we propose the Looped Video Debating (LVD) framework, whichintegrates Large Language Models (LLMs) with visual information, such as facialexpressions and body movements, to enhance the transparency and reliability ofquestion-answering tasks involving human interaction videos. Our results on theSocial-IQ 2.0 benchmark show that LVD achieves state-of-the-art performancewithout fine-tuning. Furthermore, supplementary human annotations on existingdatasets provide insights into the model's accuracy, guiding futureimprovements in AI-driven social intelligence.
  </details>

- **[VADMamba: Exploring State Space Models for Fast Video Anomaly Detection](http://arxiv.org/abs/2503.21169v1)**  `arXiv:2503.21169`  `cs.CV`  
  _Jiahao Lyu, Minghua Zhao, Jing Hu, Xuewen Huang, Yifei Chen, Shuangli Du_
  <details open><summary>Abstract</summary>
  Video anomaly detection (VAD) methods are mostly CNN-based orTransformer-based, achieving impressive results, but the focus on detectionaccuracy often comes at the expense of inference speed. The emergence of statespace models in computer vision, exemplified by the Mamba model, demonstratesimproved computational efficiency through selective scans and showcases thegreat potential for long-range modeling. Our study pioneers the application ofMamba to VAD, dubbed VADMamba, which is based on multi-task learning for frameprediction and optical flow reconstruction. Specifically, we propose theVQ-Mamba Unet (VQ-MaU) framework, which incorporates a Vector Quantization (VQ)layer and Mamba-based Non-negative Visual State Space (NVSS) block.Furthermore, two individual VQ-MaU networks separately predict frames andreconstruct corresponding optical flows, further boosting accuracy through aclip-level fusion evaluation strategy. Experimental results validate theefficacy of the proposed VADMamba across three benchmark datasets,demonstrating superior performance in inference speed compared to previouswork. Code is available at https://github.com/jLooo/VADMamba.
  </details>

- **[Unveiling the Potential of Superexpressive Networks in Implicit Neural Representations](http://arxiv.org/abs/2503.21166v1)**  `arXiv:2503.21166`  `cs.LG`  
  _Uvini Balasuriya Mudiyanselage, Woojin Cho, Minju Jo, Noseong Park, Kookjin Lee_
  <details open><summary>Abstract</summary>
  In this study, we examine the potential of one of the ``superexpressive''networks in the context of learning neural functions for representing complexsignals and performing machine learning downstream tasks. Our focus is onevaluating their performance on computer vision and scientific machine learningtasks including signal representation/inverse problems and solutions of partialdifferential equations. Through an empirical investigation in various benchmarktasks, we demonstrate that superexpressive networks, as proposed by [Zhang etal. NeurIPS, 2022], which employ a specialized network structure characterizedby having an additional dimension, namely width, depth, and ``height'', cansurpass recent implicit neural representations that use highly-specializednonlinear activation functions.
  </details>

- **[A Data Balancing and Ensemble Learning Approach for Credit Card Fraud Detection](http://arxiv.org/abs/2503.21160v1)**  `arXiv:2503.21160`  `cs.LG`  
  _Yuhan Wang_
  <details open><summary>Abstract</summary>
  This research introduces an innovative method for identifying credit cardfraud by combining the SMOTE-KMEANS technique with an ensemble machine learningmodel. The proposed model was benchmarked against traditional models such aslogistic regression, decision trees, random forests, and support vectormachines. Performance was evaluated using metrics, including accuracy, recall,and area under the curve (AUC). The results demonstrated that the proposedmodel achieved superior performance, with an AUC of 0.96 when combined with theSMOTE-KMEANS algorithm. This indicates a significant improvement in detectingfraudulent transactions while maintaining high precision and recall. The studyalso explores the application of different oversampling techniques to enhancethe performance of various classifiers. The findings suggest that the proposedmethod is robust and effective for classification tasks on balanced datasets.Future research directions include further optimization of the SMOTE-KMEANSapproach and its integration into existing fraud detection systems to enhancefinancial security and consumer protection.
  </details>

- **[Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?](http://arxiv.org/abs/2503.21157v1)**  `arXiv:2503.21157`  `cs.LG`  
  _Ashish Sardana_
  <details open><summary>Abstract</summary>
  This article surveys Evaluation models to automatically detect hallucinationsin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmarkof their performance across six RAG applications. Methods included in our studyinclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination EvaluationModel (HHEM), and the Trustworthy Language Model (TLM). These approaches areall reference-free, requiring no ground-truth answers/labels to catch incorrectLLM responses. Our study reveals that, across diverse RAG applications, some ofthese approaches consistently detect incorrect RAG responses with highprecision/recall.
  </details>

- **[Omni-AD: Learning to Reconstruct Global and Local Features for Multi-class Anomaly Detection](http://arxiv.org/abs/2503.21125v1)**  `arXiv:2503.21125`  `cs.CV`  
  _Jiajie Quan, Ao Tong, Yuxuan Cai, Xinwei He, Yulong Wang, Yang Zhou_
  <details open><summary>Abstract</summary>
  In multi-class unsupervised anomaly detection(MUAD), reconstruction-basedmethods learn to map input images to normal patterns to identify anomalouspixels. However, this strategy easily falls into the well-known "learningshortcut" issue when decoders fail to capture normal patterns and reconstructboth normal and abnormal samples naively. To address that, we propose to learnthe input features in global and local manners, forcing the network to memorizethe normal patterns more comprehensively. Specifically, we design a two-branchdecoder block, named Omni-block. One branch corresponds to global featurelearning, where we serialize two self-attention blocks but replace the queryand (key, value) with learnable tokens, respectively, thus capturing globalfeatures of normal patterns concisely and thoroughly. The local branchcomprises depth-separable convolutions, whose locality enables effective andefficient learning of local features for normal patterns. By stackingOmni-blocks, we build a framework, Omni-AD, to learn normal patterns ofdifferent granularity and reconstruct them progressively. Comprehensiveexperiments on public anomaly detection benchmarks show that our methodoutperforms state-of-the-art approaches in MUAD. Code is available athttps://github.com/easyoo/Omni-AD.git.
  </details>

- **[AdaMHF: Adaptive Multimodal Hierarchical Fusion for Survival Prediction](http://arxiv.org/abs/2503.21124v1)**  `arXiv:2503.21124`  `cs.CV`  
  _Shuaiyu Zhang, Xun Lin, Rongxiang Zhang, Yu Bai, Yong Xu, Tao Tan, et al._
  <details open><summary>Abstract</summary>
  The integration of pathologic images and genomic data for survival analysishas gained increasing attention with advances in multimodal learning. However,current methods often ignore biological characteristics, such as heterogeneityand sparsity, both within and across modalities, ultimately limiting theiradaptability to clinical practice. To address these challenges, we proposeAdaMHF: Adaptive Multimodal Hierarchical Fusion, a framework designed forefficient, comprehensive, and tailored feature extraction and fusion. AdaMHF isspecifically adapted to the uniqueness of medical data, enabling accuratepredictions with minimal resource consumption, even under challenging scenarioswith missing modalities. Initially, AdaMHF employs an experts expansion andresidual structure to activate specialized experts for extracting heterogeneousand sparse features. Extracted tokens undergo refinement via selection andaggregation, reducing the weight of non-dominant features while preservingcomprehensive information. Subsequently, the encoded features arehierarchically fused, allowing multi-grained interactions across modalities tobe captured. Furthermore, we introduce a survival prediction benchmark designedto resolve scenarios with missing modalities, mirroring real-world clinicalconditions. Extensive experiments on TCGA datasets demonstrate that AdaMHFsurpasses current state-of-the-art (SOTA) methods, showcasing exceptionalperformance in both complete and incomplete modality settings.
  </details>

- **[KAC: Kolmogorov-Arnold Classifier for Continual Learning](http://arxiv.org/abs/2503.21076v1)**  `arXiv:2503.21076`  `cs.LG` `cs.CV`  
  _Yusong Hu, Zichen Liang, Fei Yang, Qibin Hou, Xialei Liu, Ming-Ming Cheng_
  <details open><summary>Abstract</summary>
  Continual learning requires models to train continuously across consecutivetasks without forgetting. Most existing methods utilize linear classifiers,which struggle to maintain a stable classification space while learning newtasks. Inspired by the success of Kolmogorov-Arnold Networks (KAN) inpreserving learning stability during simple continual regression tasks, we setout to explore their potential in more complex continual learning scenarios. Inthis paper, we introduce the Kolmogorov-Arnold Classifier (KAC), a novelclassifier developed for continual learning based on the KAN structure. Wedelve into the impact of KAN's spline functions and introduce Radial BasisFunctions (RBF) for improved compatibility with continual learning. We replacelinear classifiers with KAC in several recent approaches and conductexperiments across various continual learning benchmarks, all of whichdemonstrate performance improvements, highlighting the effectiveness androbustness of KAC in continual learning. The code is available athttps://github.com/Ethanhuhuhu/KAC.
  </details>

- **[Efficient Multi-Instance Generation with Janus-Pro-Dirven Prompt Parsing](http://arxiv.org/abs/2503.21069v1)**  `arXiv:2503.21069`  `cs.CV`  
  _Fan Qi, Yu Duan, Changsheng Xu_
  <details open><summary>Abstract</summary>
  Recent advances in text-guided diffusion models have revolutionizedconditional image generation, yet they struggle to synthesize complex sceneswith multiple objects due to imprecise spatial grounding and limitedscalability. We address these challenges through two key modules: 1)Janus-Pro-driven Prompt Parsing, a prompt-layout parsing module that bridgestext understanding and layout generation via a compact 1B-parameterarchitecture, and 2) MIGLoRA, a parameter-efficient plug-in integratingLow-Rank Adaptation (LoRA) into UNet (SD1.5) and DiT (SD3) backbones. MIGLoRAis capable of preserving the base model's parameters and ensuring plug-and-playadaptability, minimizing architectural intrusion while enabling efficientfine-tuning. To support a comprehensive evaluation, we create DescripBox andDescripBox-1024, benchmarks that span diverse scenes and resolutions. Theproposed method achieves state-of-the-art performance on COCO and LVISbenchmarks while maintaining parameter efficiency, demonstrating superiorlayout fidelity and scalability for open-world synthesis.
  </details>

- **[Neural Architecture Search by Learning a Hierarchical Search Space](http://arxiv.org/abs/2503.21061v1)**  `arXiv:2503.21061`  `cs.CV`  
  _Mehraveh Javan Roshtkhari, Matthew Toews, Marco Pedersoli_
  <details open><summary>Abstract</summary>
  Monte-Carlo Tree Search (MCTS) is a powerful tool for many non-differentiablesearch related problems such as adversarial games. However, the performance ofsuch approach highly depends on the order of the nodes that are considered ateach branching of the tree. If the first branches cannot distinguish betweenpromising and deceiving configurations for the final task, the efficiency ofthe search is exponentially reduced. In Neural Architecture Search (NAS), asonly the final architecture matters, the visiting order of the branching can beoptimized to improve learning. In this paper, we study the application of MCTSto NAS for image classification. We analyze several sampling methods andbranching alternatives for MCTS and propose to learn the branching byhierarchical clustering of architectures based on their similarity. Thesimilarity is measured by the pairwise distance of output vectors ofarchitectures. Extensive experiments on two challenging benchmarks on CIFAR10and ImageNet show that MCTS, if provided with a good branching hierarchy, canyield promising solutions more efficiently than other approaches for NASproblems.
  </details>

- **[Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs](http://arxiv.org/abs/2503.20749v2)**  `arXiv:2503.20749`  `cs.CL`  
  _Yuxuan Lu, Jing Huang, Yan Han, Bennet Bei, Yaochen Xie, Dakuo Wang, et al._
  <details open><summary>Abstract</summary>
  Recent research shows that LLMs can simulate ``believable'' human behaviorsto power LLM agents via prompt-only methods. In this work, we focus onevaluating and improving LLM's objective ``accuracy'' rather than thesubjective ``believability'' in the web action generation task, leveraging alarge-scale, real-world dataset collected from online shopping human actions.We present the first comprehensive quantitative evaluation of state-of-the-artLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web actiongeneration. Our results show that fine-tuning LLMs on real-world behavioraldata substantially improves their ability to generate actions compared toprompt-only methods. Furthermore, incorporating synthesized reasoning tracesinto model training leads to additional performance gains, demonstrating thevalue of explicit rationale in behavior modeling. This work establishes a newbenchmark for evaluating LLMs in behavior simulation and offers actionableinsights into how real-world action data and reasoning augmentation can enhancethe fidelity of LLM agents.
  </details>

- **[RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models](http://arxiv.org/abs/2503.19654v2)**  `arXiv:2503.19654`  `cs.AI` `cs.LG` `cs.CV`  
  _Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen_
  <details open><summary>Abstract</summary>
  We introduce RGB-Th-Bench, the first benchmark designed to evaluate theability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.While VLMs have demonstrated remarkable progress in visual reasoning andmultimodal understanding, their evaluation has been predominantly limited toRGB-based benchmarks, leaving a critical gap in assessing their capabilities ininfrared vision tasks. Existing visible-infrared datasets are eithertask-specific or lack high-quality annotations necessary for rigorous modelevaluation. To address these limitations, RGB-Th-Bench provides a comprehensiveevaluation framework covering 14 distinct skill dimensions, with a total of1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracymetrics: a standard question-level accuracy and a stricter skill-levelaccuracy, which evaluates model robustness across multiple questions withineach skill dimension. This design ensures a thorough assessment of modelperformance, including resilience to adversarial and hallucinated responses. Weconduct extensive evaluations on 19 state-of-the-art VLMs, revealingsignificant performance gaps in RGB-Thermal understanding. Our results showthat even the strongest models struggle with thermal image comprehension, withperformance heavily constrained by their RGB-based capabilities. Additionally,the lack of large-scale application-specific and expert-annotatedthermal-caption-pair datasets in pre-training is an important reason of theobserved performance gap. RGB-Th-Bench highlights the urgent need for furtheradvancements in multimodal learning to bridge the gap between visible andthermal image understanding. The dataset is available through this link, andthe evaluation code will also be made publicly available.
  </details>

- **[ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning](http://arxiv.org/abs/2503.19470v2)**  `arXiv:2503.19470`  `cs.AI` `cs.CL`  
  _Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, et al._
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have shown remarkable capabilities in reasoning,exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integratingreasoning with external search processes remains challenging, especially forcomplex multi-hop questions requiring multiple retrieval steps. We proposeReSearch, a novel framework that trains LLMs to Reason with Search viareinforcement learning without using any supervised data on reasoning steps.Our approach treats search operations as integral components of the reasoningchain, where when and how to perform searches is guided by text-based thinking,and search results subsequently influence further reasoning. We train ReSearchon Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conductextensive experiments. Despite being trained on only one dataset, our modelsdemonstrate strong generalizability across various benchmarks. Analysis revealsthat ReSearch naturally elicits advanced reasoning capabilities such asreflection and self-correction during the reinforcement learning process.
  </details>

- **[SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding](http://arxiv.org/abs/2503.18943v2)**  `arXiv:2503.18943`  `cs.CV`  
  _Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, et al._
  <details open><summary>Abstract</summary>
  We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family ofvideo large language models (LLMs) offering a token-efficient solution forlong-form video understanding. We incorporate the two-stream SlowFast mechanisminto a streamlined training pipeline, and perform joint video-image training ona carefully curated data mixture of only publicly available datasets. Ourprimary focus is on highly efficient model scales (1B and 3B), demonstratingthat even relatively small Video LLMs can achieve state-of-the-art performanceon video understanding, meeting the demand for mobile-friendly models.Experimental results demonstrate that SF-LLaVA-1.5 achieves superiorperformance on a wide range of video and image tasks, with robust results atall model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achievesstate-of-the-art results in long-form video understanding (e.g., LongVideoBenchand MLVU) and excels at small scales across various video benchmarks.
  </details>

- **[WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for Efficient LLM Inference](http://arxiv.org/abs/2503.17922v2)**  `arXiv:2503.17922`  `cs.CL`  
  _Youhui Zuo, Sibo Wei, Chen Zhang, Zhuorui Liu, Wenpeng Lu, Dawei Song_
  <details open><summary>Abstract</summary>
  With the advancements in long-context inference capabilities of largelanguage models (LLMs), the KV cache has become one of the foundationalcomponents. However, its substantial GPU memory consumption makes KV cachecompression a key technique for enabling efficient LLM inference in industrialscenarios. While recent studies have focused on optimizing the memory occupiedby the KV cache, they overlook two critical factors: preserving semanticcoherence and considering task-specific characteristic during compression. Toaddress these limitations, we propose a novel task-adaptive KV cache windowselection method, WindowKV. WindowKV dynamically selects local semantic windowsconsisting of consecutive tokens, according to task-specific characteristics,ensuring the retained KV cache captures continuous, essential context.Additionally, we introduce an intra-group layer KV cache indices sharingstrategy to reduce computational overhead, achieving a balance betweenperformance and efficiency. We rigorously evaluate WindowKV on the LongBenchbenchmark, and the results demonstrate that it maintains a performancecomparable to full KV cache retention while using only 12% of the original KVcache, significantly reducing memory requirements. Furthermore, our method alsoachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,highlighting its effectiveness and robustness.
  </details>

- **[ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos](http://arxiv.org/abs/2503.16400v2)**  `arXiv:2503.16400`  `cs.LG`  
  _Haolin Yang, Feilong Tang, Ming Hu, Yulong Li, Yexin Liu, Zelin Peng, et al._
  <details open><summary>Abstract</summary>
  Video diffusion models (VDMs) facilitate the generation of high-qualityvideos, with current research predominantly concentrated on scaling effortsduring training through improvements in data quality, computational resources,and model complexity. However, inference-time scaling has received lessattention, with most approaches restricting models to a single generationattempt. Recent studies have uncovered the existence of "golden noises" thatcan enhance video quality during generation. Building on this, we find thatguiding the scaling inference-time search of VDMs to identify better noisecandidates not only evaluates the quality of the frames generated in thecurrent step but also preserves the high-level object features by referencingthe anchor frame from previous multi-chunks, thereby delivering long-termvalue. Our analysis reveals that diffusion models inherently possess flexibleadjustments of computation by varying denoising steps, and even a one-stepdenoising approach, when guided by a reward signal, yields significantlong-term benefits. Based on the observation, we proposeScalingNoise, aplug-and-play inference-time search strategy that identifies golden initialnoises for the diffusion sampling process to improve global content consistencyand visual diversity. Specifically, we perform one-step denoising to convertinitial noises into a clip and subsequently evaluate its long-term value,leveraging a reward model anchored by previously generated content. Moreover,to preserve diversity, we sample candidates from a tilted noise distributionthat up-weights promising noises. In this way, ScalingNoise significantlyreduces noise-induced errors, ensuring more coherent and spatiotemporallyconsistent video generation. Extensive experiments on benchmark datasetsdemonstrate that the proposed ScalingNoise effectively improves long videogeneration.
  </details>

- **[Bias Evaluation and Mitigation in Retrieval-Augmented Medical Question-Answering Systems](http://arxiv.org/abs/2503.15454v3)**  `arXiv:2503.15454`  `cs.CL`  
  _Yuelyu Ji, Hang Zhang, Yanshan Wang_
  <details open><summary>Abstract</summary>
  Medical Question Answering systems based on Retrieval Augmented Generation ispromising for clinical decision support because they can integrate externalknowledge, thus reducing inaccuracies inherent in standalone large languagemodels (LLMs). However, these systems may unintentionally propagate or amplifybiases associated with sensitive demographic attributes like race, gender, andsocioeconomic factors. This study systematically evaluates demographic biaseswithin medical RAG pipelines across multiple QA benchmarks, including MedQA,MedMCQA, MMLU, and EquityMedQA. We quantify disparities in retrievalconsistency and answer correctness by generating and analyzing queriessensitive to demographic variations. We further implement and compare severalbias mitigation strategies to address identified biases, including Chain ofThought reasoning, Counterfactual filtering, Adversarial prompt refinement, andMajority Vote aggregation. Experimental results reveal significant demographicdisparities, highlighting that Majority Vote aggregation notably improvesaccuracy and fairness metrics. Our findings underscore the critical need forexplicitly fairness-aware retrieval methods and prompt engineering strategiesto develop truly equitable medical QA systems.
  </details>

- **[GR00T N1: An Open Foundation Model for Generalist Humanoid Robots](http://arxiv.org/abs/2503.14734v2)**  `arXiv:2503.14734`  `cs.AI` `cs.RO` `cs.LG`  
  _NVIDIA, :, Johan Bjorck, Fernando Casta√±eda, Nikita Cherniadev, Xingye Da, et al._
  <details open><summary>Abstract</summary>
  General-purpose robots need a versatile body and an intelligent mind. Recentadvancements in humanoid robots have shown great promise as a hardware platformfor building generalist autonomy in the human world. A robot foundation model,trained on massive and diverse data sources, is essential for enabling therobots to reason about novel situations, robustly handle real-worldvariability, and rapidly learn new tasks. To this end, we introduce GR00T N1,an open foundation model for humanoid robots. GR00T N1 is aVision-Language-Action (VLA) model with a dual-system architecture. Thevision-language module (System 2) interprets the environment through vision andlanguage instructions. The subsequent diffusion transformer module (System 1)generates fluid motor actions in real time. Both modules are tightly coupledand jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixtureof real-robot trajectories, human videos, and synthetically generated datasets.We show that our generalist robot model GR00T N1 outperforms thestate-of-the-art imitation learning baselines on standard simulation benchmarksacross multiple robot embodiments. Furthermore, we deploy our model on theFourier GR-1 humanoid robot for language-conditioned bimanual manipulationtasks, achieving strong performance with high data efficiency.
  </details>

- **[TLUE: A Tibetan Language Understanding Evaluation Benchmark](http://arxiv.org/abs/2503.12051v2)**  `arXiv:2503.12051`  `cs.CL`  
  _Fan Gao, Cheng Huang, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) have made tremendous progress in recent years,but low-resource languages, such as Tibetan, remain significantlyunderrepresented in their evaluation. Despite Tibetan being spoken by overseven million people, it has largely been neglected in the development andassessment of LLMs. To address this gap, we present TLUE (A Tibetan LanguageUnderstanding Evaluation Benchmark), the first large-scale benchmark forassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:(1) a comprehensive multi-task understanding benchmark spanning 5 domains and67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate adiverse set of state-of-the-art LLMs. Experimental results demonstrate thatmost LLMs perform below the random baseline, highlighting the considerablechallenges LLMs face in processing Tibetan, a low-resource language. TLUEprovides an essential foundation for driving future research and progress inTibetan language understanding and underscores the need for greater inclusivityin LLM development.
  </details>

- **[Cognitive-Mental-LLM: Evaluating Reasoning in Large Language Models for Mental Health Prediction via Online Text](http://arxiv.org/abs/2503.10095v2)**  `arXiv:2503.10095`  `cs.AI` `cs.CL`  
  _Avinash Patil, Amardeep Kour Gedhu_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have demonstrated potential in predicting mentalhealth outcomes from online text, yet traditional classification methods oftenlack interpretability and robustness. This study evaluates structured reasoningtechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), andTree-of-Thought (ToT)-to improve classification accuracy across multiple mentalhealth datasets sourced from Reddit. We analyze reasoning-driven promptingstrategies, including Zero-shot CoT and Few-shot CoT, using key performancemetrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Ourfindings indicate that reasoning-enhanced techniques improve classificationperformance over direct prediction, particularly in complex cases. Compared tobaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trainedtransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMssuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notablegains on datasets like Dreaddit (+0.52\% over M-LLM, +0.82\% over BERT) andSDCNL (+4.67\% over M-LLM, +2.17\% over BERT). However, performance declines inDepression Severity, and CSSRS predictions suggest dataset-specificlimitations, likely due to our using a more extensive test set. Among promptingstrategies, Few-shot CoT consistently outperforms others, reinforcing theeffectiveness of reasoning-driven LLMs. Nonetheless, dataset variabilityhighlights challenges in model reliability and interpretability. This studyprovides a comprehensive benchmark of reasoning-based LLM techniques for mentalhealth text classification. It offers insights into their potential forscalable clinical applications while identifying key challenges for futureimprovements.
  </details>

- **[Deep Cut-informed Graph Embedding and Clustering](http://arxiv.org/abs/2503.06635v2)**  `arXiv:2503.06635`  `cs.AI` `cs.LG`  
  _Zhiyuan Ning, Zaitian Wang, Ran Zhang, Ping Xu, Kunpeng Liu, Pengyang Wang, et al._
  <details open><summary>Abstract</summary>
  Graph clustering aims to divide the graph into different clusters. Therecently emerging deep graph clustering approaches are largely built on graphneural networks (GNN). However, GNN is designed for general graph encoding andthere is a common issue of representation collapse in existing GNN-based deepgraph clustering algorithms. We attribute two main reasons for such issues: (i)the inductive bias of GNN models: GNNs tend to generate similar representationsfor proximal nodes. Since graphs often contain a non-negligible amount ofinter-cluster links, the bias results in error message passing and leads tobiased clustering; (ii) the clustering guided loss function: most traditionalapproaches strive to make all samples closer to pre-learned cluster centers,which causes a degenerate solution assigning all data points to a single labelthus make all samples and less discriminative. To address these challenges, weinvestigate graph clustering from a graph cut perspective and propose aninnovative and non-GNN-based Deep Cut-informed Graph embedding and Clusteringframework, namely DCGC. This framework includes two modules: (i) cut-informedgraph encoding; (ii) self-supervised graph clustering via optimal transport.For the encoding module, we derive a cut-informed graph embedding objective tofuse graph structure and attributes by minimizing their joint normalized cut.For the clustering module, we utilize the optimal transport theory to obtainthe clustering assignments, which can balance the guidance of "proximity to thepre-learned cluster center". With the above two tailored designs, DCGC is moresuitable for the graph clustering task, which can effectively alleviate theproblem of representation collapse and achieve better performance. We conductextensive experiments to demonstrate that our method is simple but effectivecompared with benchmarks.
  </details>

- **[GNNMerge: Merging of GNN Models Without Accessing Training Data](http://arxiv.org/abs/2503.03384v2)**  `arXiv:2503.03384`  `cs.LG`  
  _Vipul Garg, Ishita Thakre, Sayan Ranu_
  <details open><summary>Abstract</summary>
  Model merging has gained prominence in machine learning as a method tointegrate multiple trained models into a single model without accessing theoriginal training data. While existing approaches have demonstrated success indomains such as computer vision and NLP, their application to Graph NeuralNetworks (GNNs) remains unexplored. These methods often rely on the assumptionof shared initialization, which is seldom applicable to GNNs. In this work, weundertake the first benchmarking study of model merging algorithms for GNNs,revealing their limited effectiveness in this context. To address thesechallenges, we propose GNNMerge, which utilizes a task-agnostic node embeddingalignment strategy to merge GNNs. Furthermore, we establish that under a mildrelaxation, the proposed optimization objective admits direct analyticalsolutions for widely used GNN architectures, significantly enhancing itscomputational efficiency. Empirical evaluations across diverse datasets, tasks,and architectures establish GNNMerge to be up to 24% more accurate thanexisting methods while delivering over 2 orders of magnitude speed-up comparedto training from scratch.
  </details>

- **[Starjob: Dataset for LLM-Driven Job Shop Scheduling](http://arxiv.org/abs/2503.01877v2)**  `arXiv:2503.01877`  `cs.AI` `cs.LG`  
  _Henrik Abgaryan, Tristan Cazenave, Ararat Harutyunyan_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have shown remarkable capabilities acrossvarious domains, but their potential for solving combinatorial optimizationproblems remains largely unexplored. In this paper, we investigate theapplicability of LLMs to the Job Shop Scheduling Problem (JSSP), a classicchallenge in combinatorial optimization that requires efficient job allocationto machines to minimize makespan. To this end, we introduce Starjob, the firstsupervised dataset for JSSP, comprising 130k instances specifically designedfor training LLMs. Leveraging this dataset, we fine-tune the LLaMA 8B 4-bitquantized model with the LoRA method to develop an end-to-end schedulingapproach. Our evaluation on standard benchmarks demonstrates that the proposedLLM-based method not only surpasses traditional Priority Dispatching Rules(PDRs) but also achieves notable improvements over state-of-the-art neuralapproaches like L2D, with an average improvement of 15.36% on DMU and 7.85% onTaillard benchmarks. These results highlight the untapped potential of LLMs intackling combinatorial optimization problems, paving the way for futureadvancements in this area.
  </details>

- **[ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval](http://arxiv.org/abs/2502.15682v2)**  `arXiv:2502.15682`  `cs.CV`  
  _Guanqi Zhan, Yuanpei Liu, Kai Han, Weidi Xie, Andrew Zisserman_
  <details open><summary>Abstract</summary>
  The objective in this paper is to improve the performance of text-to-imageretrieval. To this end, we introduce a new framework that can boost theperformance of large-scale pre-trained vision-language models, so that they canbe used for text-to-image re-ranking. The approach, Enhanced Language-ImagePre-training (ELIP), uses the text query, via a simple MLP mapping network, topredict a set of visual prompts to condition the ViT image encoding. ELIP caneasily be applied to the commonly used CLIP, SigLIP and BLIP-2 networks. Totrain the architecture with limited computing resources, we develop a 'studentfriendly' best practice, involving global hard sample mining, and curation of alarge-scale dataset. On the evaluation side, we set up two newout-of-distribution (OOD) benchmarks, Occluded COCO and ImageNet-R, to assessthe zero-shot generalisation of the models to different domains. The resultsdemonstrate that ELIP significantly boosts CLIP/SigLIP/SigLIP-2 text-to-imageretrieval performance and outperforms BLIP-2 on several benchmarks, as well asproviding an easy means to adapt to OOD datasets.
  </details>

- **[Enhancing LLM Character-Level Manipulation via Divide and Conquer](http://arxiv.org/abs/2502.08180v2)**  `arXiv:2502.08180`  `cs.AI` `cs.CL`  
  _Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Zhecheng Li, Yiwei Wang_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have demonstrated strong generalizationcapabilities across a wide range of natural language processing (NLP) tasks.However, they exhibit notable weaknesses in character-level stringmanipulation, struggling with fundamental operations such as characterdeletion, insertion, and substitution. These challenges stem primarily fromtokenization constraints, despite the critical role of such operations in datapreprocessing and code generation. Through systematic analysis, we derive twokey insights: (1) LLMs face significant difficulties in leveraging intrinsictoken knowledge for character-level reasoning, and (2) atomized word structurescan substantially enhance LLMs' ability to process token-level structuralinformation. Building on these insights, we propose Character-LevelManipulation via Divide and Conquer, a novel approach designed to bridge thegap between token-level processing and character-level manipulation. Our methoddecomposes complex operations into explicit character-level subtasks coupledwith controlled token reconstruction phases, leading to significantimprovements in accuracy. Without additional training, our method significantlyimproves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and$\texttt{Substitution}$ tasks. To support further research, we open-source ourimplementation and benchmarks.
  </details>

- **[AnyEdit: Edit Any Knowledge Encoded in Language Models](http://arxiv.org/abs/2502.05628v2)**  `arXiv:2502.05628`  `cs.CL`  
  _Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Guojun Ma, Mingyang Wan, Xiang Wang, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) often produce incorrect or outdated information,necessitating efficient and precise knowledge updates. Current model editingmethods, however, struggle with long-form knowledge in diverse formats, such aspoetry, code snippets, and mathematical derivations. These limitations arisefrom their reliance on editing a single token's hidden state, a limitation weterm "efficacy barrier". To solve this, we propose AnyEdit, a newautoregressive editing paradigm. It decomposes long-form knowledge intosequential chunks and iteratively edits the key token in each chunk, ensuringconsistent and accurate outputs. Theoretically, we ground AnyEdit in the ChainRule of Mutual Information, showing its ability to update any knowledge withinLLMs. Empirically, it outperforms strong baselines by 21.5% on benchmarksincluding UnKEBench, AKEW, and our new EditEverything dataset for long-formdiverse-formatted knowledge. Additionally, AnyEdit serves as a plug-and-playframework, enabling current editing methods to update knowledge with arbitrarylength and format, significantly advancing the scope and practicality of LLMknowledge editing.
  </details>

- **[Group Reasoning Emission Estimation Networks](http://arxiv.org/abs/2502.06874v2)**  `arXiv:2502.06874`  `cs.AI` `cs.CL` `cs.LG`  
  _Yanming Guo, Xiao Qian, Kevin Credit, Jin Ma_
  <details open><summary>Abstract</summary>
  Accurate greenhouse gas (GHG) emission reporting is critical for governments,businesses, and investors. However, adoption remains limited particularly amongsmall and medium enterprises due to high implementation costs, fragmentedemission factor databases, and a lack of robust sector classification methods.To address these challenges, we introduce Group Reasoning Emission EstimationNetworks (GREEN), an AI-driven carbon accounting framework that standardizesenterprise-level emission estimation, constructs a large-scale benchmarkdataset, and leverages a novel reasoning approach with large language models(LLMs). Specifically, we compile textual descriptions for 20,850 companies withvalidated North American Industry Classification System (NAICS) labels andalign these with an economic model of carbon intensity factors. By reframingsector classification as an information retrieval task, we fine-tuneSentence-BERT models using a contrastive learning loss. To overcome thelimitations of single-stage models in handling thousands of hierarchicalcategories, we propose a Group Reasoning method that ensembles LLM classifiersbased on the natural NAICS ontology, decomposing the task into multiplesub-classification steps. We theoretically prove that this approach reducesclassification uncertainty and computational complexity. Experiments on 1,114NAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%Top-10 accuracy), and case studies on 20 companies report a mean absolutepercentage error (MAPE) of 45.88%. The project is available at:https://huggingface.co/datasets/Yvnminc/ExioNAICS.
  </details>

- **[Rethinking the Global Knowledge of CLIP in Training-Free Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2502.06818v2)**  `arXiv:2502.06818`  `cs.LG`  
  _Jingyun Wang, Cilin Yan, Guoliang Kang_
  <details open><summary>Abstract</summary>
  Recent works modify CLIP to perform open-vocabulary semantic segmentation ina training-free manner (TF-OVSS). In vanilla CLIP, patch-wise imagerepresentations mainly encode homogeneous image-level properties, which hindersthe application of CLIP to the dense prediction task. Previous TF-OVSS workssacrifice globality to enhance the locality of CLIP features, by making eachpatch mainly attend to itself or its neighboring patches within a narrow localwindow. With their modifications,the ability of CLIP to aggregate globalcontext information is largely weakened. Differently, in this paper, we rethinkthe global knowledge encoded by CLIP and propose GCLIP to answer how to extractand utilize beneficial global knowledge of CLIP for TF-OVSS. As therepresentation of each patch is finally determined by the attention weights andthe Value embeddings, we propose to reshape the last-block attention and Valueembeddings to aggregate useful global context into final features. Firstly, weaim to equip the last-block attention with image-level properties while notintroducing homogeneous attention patterns across patches. To realize the goal,we fuse the attention from the global-token emerging blocks with theQuery-Query attention. Secondly, we aim to make Value embeddings of thelast-block attention module more semantically correlated. To realize this, wedesign a novel channel suppression strategy.Extensive experiments on fivestandard benchmarks demonstrate that our method consistently outperformsprevious state-of-the-arts.
  </details>

- **[OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?](http://arxiv.org/abs/2501.05510v2)**  `arXiv:2501.05510`  `cs.AI` `cs.CV`  
  _Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, et al._
  <details open><summary>Abstract</summary>
  Temporal Awareness, the ability to reason dynamically based on the timestampwhen a question is raised, is the key distinction between offline and onlinevideo LLMs. Unlike offline models, which rely on complete videos for static,post hoc analysis, online models process video streams incrementally anddynamically adapt their responses based on the timestamp at which the questionis posed. Despite its significance, temporal awareness has not been adequatelyevaluated in existing benchmarks. To fill this gap, we present OVO-Bench(Online-VideO-Benchmark), a novel video benchmark that emphasizes theimportance of timestamps for advanced online video understanding capabilitybenchmarking. OVO-Bench evaluates the ability of video LLMs to reason andrespond to events occurring at specific timestamps under three distinctscenarios: (1) Backward tracing: trace back to past events to answer thequestion. (2) Real-time understanding: understand and respond to events as theyunfold at the current timestamp. (3) Forward active responding: delay theresponse until sufficient future information becomes available to answer thequestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videosand approximately human-curated 2,800 fine-grained meta-annotations withprecise timestamps. We combine automated generation pipelines with humancuration. With these high-quality samples, we further developed an evaluationpipeline to systematically query video LLMs along the video timeline.Evaluations of nine Video-LLMs reveal that, despite advancements on traditionalbenchmarks, current models struggle with online video understanding, showing asignificant gap compared to human agents. We hope OVO-Bench will drive progressin video LLMs and inspire future research in online video reasoning. Ourbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.
  </details>

- **[LongViTU: Instruction Tuning for Long-Form Video Understanding](http://arxiv.org/abs/2501.05037v2)**  `arXiv:2501.05037`  `cs.LG` `cs.CV`  
  _Rujie Wu, Xiaojian Ma, Hai Ci, Yue Fan, Yuxuan Wang, Haozhe Zhao, et al._
  <details open><summary>Abstract</summary>
  This paper introduces LongViTU, a large-scale (~121k QA pairs, ~900h videos),automatically generated dataset for long-form video understanding. We propose asystematic approach that organizes videos into a hierarchical tree structurefor QA generation and incorporates self-revision mechanisms to ensurehigh-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context(average certificate length of 4.6 minutes); 2) rich knowledge and condensedreasoning (commonsense, causality, planning, etc.)). We also offer explicittimestamp annotations of relevant events for each QA pair. We have conductedextensive human studies on LongViTU, and the results prove the quality of ourdataset. To better evaluate the challenges posed by LongViTU's emphasis onlong-term context and condensed reasoning, we manually curate a subset ofLongViTU into a benchmark. Evaluations using a state-of-the-art open-sourcemodel (LongVU), a proprietary model (Gemini-1.5-Pro), and human annotatorsyield GPT-4 scores of 49.9, 52.3, and 81.0, respectively, underscoring thesubstantial difficulty presented by LongViTU questions. Performing supervisedfine-tuning (SFT) of LongVU and LLaVA-Video on LongViTU data results in averageperformance gains of 2.5% and 3.7%, respectively, across a suite of long videounderstanding benchmarks (EgoSchema, VideoMME-Long, MLVU, LVBench).
  </details>

- **[TREAD: Token Routing for Efficient Architecture-agnostic Diffusion Training](http://arxiv.org/abs/2501.04765v2)**  `arXiv:2501.04765`  `cs.AI` `cs.CV`  
  _Felix Krause, Timy Phan, Ming Gui, Stefan Andreas Baumann, Vincent Tao Hu, Bj√∂rn Ommer_
  <details open><summary>Abstract</summary>
  Diffusion models have emerged as the mainstream approach for visualgeneration. However, these models typically suffer from sample inefficiency andhigh training costs. Consequently, methods for efficient finetuning, inferenceand personalization were quickly adopted by the community. However, trainingthese models in the first place remains very costly. While several recentapproaches - including masking, distillation, and architectural modifications -have been proposed to improve training efficiency, each of these methods comeswith a tradeoff: they achieve enhanced performance at the expense of increasedcomputational cost or vice versa. In contrast, this work aims to improvetraining efficiency as well as generative performance at the same time throughroutes that act as a transport mechanism for randomly selected tokens fromearly layers to deeper layers of the model. Our method is not limited to thecommon transformer-based model - it can also be applied to state-space modelsand achieves this without architectural modifications or additional parameters.Finally, we show that TREAD reduces computational cost and simultaneouslyboosts model performance on the standard ImageNet-256 benchmark inclass-conditional synthesis. Both of these benefits multiply to a convergencespeedup of 14x at 400K training iterations compared to DiT and 37x compared tothe best benchmark performance of DiT at 7M training iterations. Furthermore,we achieve a competitive FID of 2.09 in a guided and 3.93 in an unguidedsetting, which improves upon the DiT, without architectural changes.
  </details>

- **[Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models](http://arxiv.org/abs/2412.18609v2)**  `arXiv:2412.18609`  `cs.CV`  
  _Jinhui Yi, Syed Talal Wasim, Yanan Luo, Muzammal Naseer, Juergen Gall_
  <details open><summary>Abstract</summary>
  We present an efficient encoder-free approach for video-languageunderstanding that achieves competitive performance while significantlyreducing computational overhead. Current video-language models typically relyon heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4Bparameters), creating a substantial computational burden when processingmulti-frame videos. Our method introduces a novel Spatio-Temporal AlignmentBlock (STAB) that directly processes video inputs without requiring pre-trainedencoders while using only 45M parameters for visual processing - at least a6.5$\times$ reduction compared to traditional approaches. The STAB architecturecombines Local Spatio-Temporal Encoding for fine-grained feature extraction,efficient spatial downsampling through learned attention and separatemechanisms for modeling frame-level and video-level relationships. Our modelachieves comparable or superior performance to encoder-based approaches foropen-ended video question answering on standard benchmarks. The fine-grainedvideo question-answering evaluation demonstrates our model's effectiveness,outperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in keyaspects like correctness and temporal understanding. Extensive ablation studiesvalidate our architectural choices and demonstrate the effectiveness of ourspatio-temporal modeling approach while achieving 3-4$\times$ faster processingspeeds than previous methods. Code is available athttps://jh-yi.github.io/Video-Panda.
  </details>

- **[GNN-Transformer Cooperative Architecture for Trustworthy Graph Contrastive Learning](http://arxiv.org/abs/2412.16218v4)**  `arXiv:2412.16218`  `cs.LG`  
  _Jianqing Liang, Xinkai Wei, Min Chen, Zhiqiang Wang, Jiye Liang_
  <details open><summary>Abstract</summary>
  Graph contrastive learning (GCL) has become a hot topic in the field of graphrepresentation learning. In contrast to traditional supervised learning relyingon a large number of labels, GCL exploits augmentation strategies to generatemultiple views and positive/negative pairs, both of which greatly influence theperformance. Unfortunately, commonly used random augmentations may disturb theunderlying semantics of graphs. Moreover, traditional GNNs, a type of widelyemployed encoders in GCL, are inevitably confronted with over-smoothing andover-squashing problems. To address these issues, we propose GNN-TransformerCooperative Architecture for Trustworthy Graph Contrastive Learning (GTCA),which inherits the advantages of both GNN and Transformer, incorporating graphtopology to obtain comprehensive graph representations. Theoretical analysisverifies the trustworthiness of the proposed method. Extensive experiments onbenchmark datasets demonstrate state-of-the-art empirical performance.
  </details>

- **[Do Multimodal Large Language Models See Like Humans?](http://arxiv.org/abs/2412.09603v2)**  `arXiv:2412.09603`  `cs.CV`  
  _Jiaying Lin, Shuquan Ye, Rynson W. H. Lau_
  <details open><summary>Abstract</summary>
  Multimodal Large Language Models (MLLMs) have achieved impressive results onvarious vision tasks, leveraging recent advancements in large language models.However, a critical question remains unaddressed: do MLLMs perceive visualinformation similarly to humans? Current benchmarks lack the ability toevaluate MLLMs from this perspective. To address this challenge, we introduceHVSBench, a large-scale benchmark designed to assess the alignment betweenMLLMs and the human visual system (HVS) on fundamental vision tasks that mirrorhuman vision. HVSBench curated over 85K multimodal samples, spanning 13categories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,Free-Viewing, and Searching. Extensive experiments demonstrate theeffectiveness of our benchmark in providing a comprehensive evaluation ofMLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best modelsshow significant room for improvement, with most achieving only moderateresults. Our experiments reveal that HVSBench presents a new and significantchallenge for cutting-edge MLLMs. Diverse human participants attained strongperformance, significantly outperforming MLLMs, which further underscores thebenchmark's high quality. We believe that HVSBench will facilitate research onhuman-aligned and explainable MLLMs, marking a key step in understanding howMLLMs perceive and process visual information.
  </details>

- **[ReCap: Better Gaussian Relighting with Cross-Environment Captures](http://arxiv.org/abs/2412.07534v3)**  `arXiv:2412.07534`  `cs.CV`  
  _Jingzhi Li, Zongwei Wu, Eduard Zamfir, Radu Timofte_
  <details open><summary>Abstract</summary>
  Accurate 3D objects relighting in diverse unseen environments is crucial forrealistic virtual object placement. Due to the albedo-lighting ambiguity,existing methods often fall short in producing faithful relights. Withoutproper constraints, observed training views can be explained by numerouscombinations of lighting and material attributes, lacking physicalcorrespondence with the actual environment maps used for relighting. In thiswork, we present ReCap, treating cross-environment captures as multi-tasktarget to provide the missing supervision that cuts through the entanglement.Specifically, ReCap jointly optimizes multiple lighting representations thatshare a common set of material attributes. This naturally harmonizes a coherentset of lighting representations around the mutual material attributes,exploiting commonalities and differences across varied object appearances. Suchcoherence enables physically sound lighting reconstruction and robust materialestimation - both essential for accurate relighting. Together with astreamlined shading function and effective post-processing, ReCap outperformsall leading competitors on an expanded relighting benchmark.
  </details>

- **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v2)**  `arXiv:2412.01095`  `cs.AI` `cs.LG` `cs.CV`  
  _Muchao Ye, Weiyang Liu, Pan He_
  <details open><summary>Abstract</summary>
  The rapid advancement of vision-language models (VLMs) has established a newparadigm in video anomaly detection (VAD): leveraging VLMs to simultaneouslydetect anomalies and provide comprehendible explanations for the decisions.Existing work in this direction often assumes the complex reasoning requiredfor VAD exceeds the capabilities of pretrained VLMs. Consequently, theseapproaches either incorporate specialized reasoning modules during inference orrely on instruction tuning datasets through additional training to adapt VLMsfor VAD. However, such strategies often incur substantial computational costsor data annotation overhead. To address these challenges in explainable VAD, weintroduce a verbalized learning framework named VERA that enables VLMs toperform VAD without model parameter modifications. Specifically, VERAautomatically decomposes the complex reasoning required for VAD intoreflections on simpler, more focused guiding questions capturing distinctabnormal patterns. It treats these reflective questions as learnable parametersand optimizes them through data-driven verbal interactions between learner andoptimizer VLMs, using coarsely labeled training data. During inference, VERAembeds the learned questions into model prompts to guide VLMs in generatingsegment-level anomaly scores, which are then refined into frame-level scoresvia the fusion of scene and temporal contexts. Experimental results onchallenging benchmarks demonstrate that the learned questions of VERA arehighly adaptable, significantly improving both detection performance andexplainability of VLMs for VAD.
  </details>

- **[Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding](http://arxiv.org/abs/2412.00493v2)**  `arXiv:2412.00493`  `cs.CL` `cs.CV`  
  _Duo Zheng, Shijia Huang, Liwei Wang_
  <details open><summary>Abstract</summary>
  The rapid advancement of Multimodal Large Language Models (MLLMs) hassignificantly impacted various multimodal tasks. However, these models facechallenges in tasks that require spatial understanding within 3D environments.Efforts to enhance MLLMs, such as incorporating point cloud features, have beenmade, yet a considerable gap remains between the models' learnedrepresentations and the inherent complexity of 3D scenes. This discrepancylargely stems from the training of MLLMs on predominantly 2D data, whichrestricts their effectiveness in comprehending 3D spaces. To address thisissue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,for 3D scene understanding. By treating 3D scenes as dynamic videos andincorporating 3D position encoding into these representations, our Video-3D LLMaligns video representations with real-world spatial contexts more accurately.In addition, we have implemented a maximum coverage sampling technique tooptimize the trade-off between computational cost and performance. Extensiveexperiments demonstrate that our model achieves state-of-the-art performance onseveral 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,Scan2Cap, ScanQA, and SQA3D.
  </details>

- **[Event-boosted Deformable 3D Gaussians for Dynamic Scene Reconstruction](http://arxiv.org/abs/2411.16180v2)**  `arXiv:2411.16180`  `cs.CV`  
  _Wenhao Xu, Wenming Weng, Yueyi Zhang, Ruikang Xu, Zhiwei Xiong_
  <details open><summary>Abstract</summary>
  Deformable 3D Gaussian Splatting (3D-GS) is limited by missing intermediatemotion information due to the low temporal resolution of RGB cameras. Toaddress this, we introduce the first approach combining event cameras, whichcapture high-temporal-resolution, continuous motion data, with deformable 3D-GSfor dynamic scene reconstruction. We observe that threshold modeling for eventsplays a crucial role in achieving high-quality reconstruction. Therefore, wepropose a GS-Threshold Joint Modeling strategy, creating a mutually reinforcingprocess that greatly improves both 3D reconstruction and threshold modeling.Moreover, we introduce a Dynamic-Static Decomposition strategy that firstidentifies dynamic areas by exploiting the inability of static Gaussians torepresent motions, then applies a buffer-based soft decomposition to separatedynamic and static areas. This strategy accelerates rendering by avoidingunnecessary deformation in static areas, and focuses on dynamic areas toenhance fidelity. Additionally, we contribute the first event-inclusive 4Dbenchmark with synthetic and real-world dynamic scenes, on which our methodachieves state-of-the-art performance.
  </details>

- **[Not Just Object, But State: Compositional Incremental Learning without Forgetting](http://arxiv.org/abs/2411.01739v3)**  `arXiv:2411.01739`  `cs.CV`  
  _Yanyi Zhang, Binglin Qiu, Qi Jia, Yu Liu, Ran He_
  <details open><summary>Abstract</summary>
  Most incremental learners excessively prioritize coarse classes of objectswhile neglecting various kinds of states (e.g. color and material) attached tothe objects. As a result, they are limited in the ability to reasonfine-grained compositionality of state-object pairs. To remedy this limitation,we propose a novel task called Compositional Incremental Learning(composition-IL), enabling the model to recognize state-object compositions asa whole in an incremental learning fashion. Since the lack of suitablebenchmarks, we re-organize two existing datasets and make them tailored forcomposition-IL. Then, we propose a prompt-based Composition Incremental Learner(CompILer), to overcome the ambiguous composition boundary problem whichchallenges composition-IL largely. Specifically, we exploit multi-pool promptlearning, which is regularized by inter-pool prompt discrepancy and intra-poolprompt diversity. Besides, we devise object-injected state prompting by usingobject prompts to guide the selection of state prompts. Furthermore, we fusethe selected prompts by a generalized-mean strategy, to eliminate irrelevantinformation learned in the prompts. Extensive experiments on two datasetsexhibit state-of-the-art performance achieved by CompILer.
  </details>

- **[ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom](http://arxiv.org/abs/2410.14138v2)**  `arXiv:2410.14138`  `cs.AI` `cs.CV`  
  _Jingqi Zhou, Sheng Wang, Jingwei Dong, Lei Li, Jiahui Gao, Jiyue Jiang, et al._
  <details open><summary>Abstract</summary>
  Large vision-language models (LVLMs) have witnessed significant progress onvisual understanding tasks. However, they often prioritize language knowledgeover image information on visual reasoning tasks, incurring performancedegradation. To tackle this issue, we first identify the drawbacks of existingsolutions (i.e., insufficient and irrelevant visual descriptions, and limitedmulti-modal capacities). We then decompose visual reasoning process into twostages: visual perception (i.e., eyesight) and textual reasoning (i.e.,wisdom), and introduce a novel visual reasoning framework named ProReason. Thisframework features multi-run proactive perception and decoupledvision-reasoning capabilities. Briefly, given a multi-modal question, ProReasoniterates proactive information collection and reasoning until the answer can beconcluded with necessary and sufficient visual descriptions. Notably, thedisassociation of capabilities allows seamless integration of existing largelanguage models (LLMs) to compensate for the reasoning deficits of LVLMs. Ourextensive experiments demonstrate that ProReason outperforms both existingmulti-step reasoning frameworks and passive peer methods on a wide range ofbenchmarks for both open-source and closed-source models. In addition, with theassistance of LLMs, ProReason achieves a performance improvement of up to 15%on MMMU benchmark. Our insights into existing solutions and the decoupledperspective for feasible integration of LLMs illuminate future research onvisual reasoning techniques, especially LLM-assisted ones.
  </details>

- **[Evaluating the effects of Data Sparsity on the Link-level Bicycling Volume Estimation: A Graph Convolutional Neural Network Approach](http://arxiv.org/abs/2410.08522v2)**  `arXiv:2410.08522`  `cs.LG`  
  _Mohit Gupta, Debjit Bhowmick, Meead Saberi, Shirui Pan, Ben Beck_
  <details open><summary>Abstract</summary>
  Accurate bicycling volume estimation is crucial for making informed decisionsand planning about future investments in bicycling infrastructure. However,traditional link-level volume estimation models are effective for motorizedtraffic but face significant challenges when applied to the bicycling contextbecause of sparse data and the intricate nature of bicycling mobility patterns.To the best of our knowledge, we present the first study to utilize a GraphConvolutional Network (GCN) architecture to model link-level bicycling volumesand systematically investigate the impact of varying levels of data sparsity(0%--99%) on model performance, simulating real-world scenarios. We haveleveraged Strava Metro data as the primary source of bicycling counts across15,933 road segments/links in the City of Melbourne, Australia. To evaluate theeffectiveness of the GCN model, we benchmark it against traditional machinelearning models, such as linear regression, support vector machines, and randomforest. Our results show that the GCN model outperforms these traditionalmodels in predicting Annual Average Daily Bicycle (AADB) counts, demonstratingits ability to capture the spatial dependencies inherent in bicycle trafficnetworks. While GCN remains robust up to 80% sparsity, its performance declinessharply beyond this threshold, highlighting the challenges of extreme datasparsity. These findings underscore the potential of GCNs in enhancingbicycling volume estimation, while also emphasizing the need for furtherresearch on methods to improve model resilience under high-sparsity conditions.Our findings offer valuable insights for city planners aiming to improvebicycling infrastructure and promote sustainable transportation.
  </details>

- **[OmniBench: Towards The Future of Universal Omni-Language Models](http://arxiv.org/abs/2409.15272v4)**  `arXiv:2409.15272`  `cs.AI` `cs.CL` `cs.CV`  
  _Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in multimodal large language models (MLLMs) have focusedon integrating multiple modalities, yet their ability to simultaneously processand reason across different inputs remains underexplored. We introduceOmniBench, a novel benchmark designed to evaluate models' ability to recognize,interpret, and reason across visual, acoustic, and textual inputssimultaneously. We define language models capable of such tri-modal processingas omni-language models (OLMs). OmniBench features high-quality humanannotations that require integrated understanding across all modalities. Ourevaluation reveals that: i) open-source OLMs show significant limitations ininstruction-following and reasoning in tri-modal contexts; and ii) mostbaseline models perform poorly (around 50% accuracy) even with textualalternatives to image/audio inputs. To address these limitations, we developOmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. Weadvocate for developing more robust tri-modal integration techniques andtraining strategies to enhance OLM performance. Codes and data could be foundat our repo (https://github.com/multimodal-art-projection/OmniBench).
  </details>

- **[StableMamba: Distillation-free Scaling of Large SSMs for Images and Videos](http://arxiv.org/abs/2409.11867v2)**  `arXiv:2409.11867`  `cs.CV`  
  _Hamid Suleman, Syed Talal Wasim, Muzammal Naseer, Juergen Gall_
  <details open><summary>Abstract</summary>
  State-space models (SSMs), exemplified by S4, have introduced a novel contextmodeling method by integrating state-space techniques into deep learning.However, they struggle with global context modeling due to theirdata-independent matrices. The Mamba model addressed this with data-dependentvariants via the S6 selective-scan algorithm, enhancing context modeling,especially for long sequences. However, Mamba-based architectures are difficultto scale with respect to the number of parameters, which is a major limitationfor vision applications. This paper addresses the scalability issue of largeSSMs for image classification and action recognition without requiringadditional techniques like knowledge distillation. We analyze the distinctcharacteristics of Mamba-based and Attention-based models, proposing aMamba-Attention interleaved architecture that enhances scalability, robustness,and performance. We demonstrate that the stable and efficient interleavedarchitecture resolves the scalability issue of Mamba-based architectures forimages and videos and increases robustness to common artifacts like JPEGcompression. Our thorough evaluation on the ImageNet-1K, Kinetics-400 andSomething-Something-v2 benchmarks demonstrates that our approach improves theaccuracy of state-of-the-art Mamba-based architectures by up to $+1.7$.
  </details>

- **[On the Viability of Semi-Supervised Segmentation Methods for Statistical Shape Modeling](http://arxiv.org/abs/2407.15260v2)**  `arXiv:2407.15260`  `cs.CV`  
  _Asma Khan, Tushar Kataria, Janmesh Ukey, Shireen Y. Elhabian_
  <details open><summary>Abstract</summary>
  Statistical Shape Models (SSMs) excel at identifying population levelanatomical variations, which is at the core of various clinical and biomedicalapplications, including morphology-based diagnostics and surgical planning.However, the effectiveness of SSM is often constrained by the necessity forexpert-driven manual segmentation, a process that is both time-intensive andexpensive, thereby restricting their broader application and utility. Recentdeep learning approaches enable the direct estimation of Statistical ShapeModels (SSMs) from unsegmented images. While these models can predict SSMswithout segmentation during deployment, they do not address the challenge ofacquiring the manual annotations needed for training, particularly inresource-limited settings. Semi-supervised models for anatomy segmentation canmitigate the annotation burden. Yet, despite the abundance of availableapproaches, there are no established guidelines to inform end-users on theireffectiveness for the downstream task of constructing SSMs. In this study, wesystematically evaluate the potential of semi-supervised methods as viablealternatives to manual segmentations for building SSMs. We establish a newperformance benchmark by employing various semi-supervised methods for anatomysegmentation under low annotation settings, utilizing the predictedsegmentations for the task of SSM. Our results indicate that some methodsproduce noisy segmentation, which is very unfavorable for SSM tasks, whileothers can capture the correct modes of variations in the population cohortwith 60-80% reduction in required manual annotation
  </details>

- **[Vision language models are blind: Failing to translate detailed visual features into words](http://arxiv.org/abs/2407.06581v6)**  `arXiv:2407.06581`  `cs.AI` `cs.CV`  
  _Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen_
  <details open><summary>Abstract</summary>
  While large language models with vision capabilities (VLMs), e.g., GPT-4o andGemini 1.5 Pro, score high on many vision-understanding benchmarks, they arestill struggling with low-level vision tasks that are easy to humans.Specifically, on BlindTest, our suite of 7 very simple tasks, includingidentifying (a) whether two circles overlap; (b) how many times two linesintersect; (c) which letter is being circled in a word; and (d) the number ofcircles in an Olympic-like logo, four state-of-the-art VLMs are only 58.07%accurate on average. Claude 3.5 Sonnet performs the best at 77.84% accuracy,far from the human expected accuracy of 100%. Across different imageresolutions and line widths, VLMs including slow-thinking models consistentlystruggle with those tasks that require precise spatial information whengeometric primitives overlap or are close. Yet, VLMs perform at near-100%accuracy when much more space is added to separate shapes and letters. Linearprobing experiments show that vision encoders contain sufficient visualinformation to solve BlindTest and that language models fail to decode thisinformation into correct answers. Code and data are at:https://vlmsareblind.github.io
  </details>

- **[What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models](http://arxiv.org/abs/2405.15668v4)**  `arXiv:2405.15668`  `cs.CV`  
  _Abdelrahman Abdelhamed, Mahmoud Afifi, Alec Go_
  <details open><summary>Abstract</summary>
  Large language models (LLMs) have been effectively used for many computervision tasks, including image classification. In this paper, we present asimple yet effective approach for zero-shot image classification usingmultimodal LLMs. Using multimodal LLMs, we generate comprehensive textualrepresentations from input images. These textual representations are thenutilized to generate fixed-dimensional features in a cross-modal embeddingspace. Subsequently, these features are fused together to perform zero-shotclassification using a linear classifier. Our method does not require promptengineering for each dataset; instead, we use a single, straightforward set ofprompts across all datasets. We evaluated our method on several datasets andour results demonstrate its remarkable effectiveness, surpassing benchmarkaccuracy on multiple datasets. On average, for ten benchmarks, our methodachieved an accuracy gain of 6.2 percentage points, with an increase of 6.8percentage points on the ImageNet dataset, compared to prior methodsre-evaluated with the same setup. Our findings highlight the potential ofmultimodal LLMs to enhance computer vision tasks such as zero-shot imageclassification, offering a significant improvement over traditional methods.
  </details>

- **[Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems](http://arxiv.org/abs/2404.14963v5)**  `arXiv:2404.14963`  `cs.AI` `cs.CL`  
  _Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du_
  <details open><summary>Abstract</summary>
  Chain-of-Thought (CoT) prompting has enhanced the performance of LargeLanguage Models (LLMs) across various reasoning tasks. However, CoT still fallsshort in dealing with complex math word problems, as it usually suffers fromthree pitfalls: semantic misunderstanding errors, calculation errors, andstep-missing errors. Prior studies involve addressing the calculation errorsand step-missing errors, but neglect the semantic misunderstanding errors,which is the major factor limiting the reasoning performance of LLMs. To thisend, we propose a simple-yet-effective method, namely Deeply Understanding theProblems (DUP), to improve the LLMs' math problem-solving ability by addressingsemantic misunderstanding errors. The core of our method is to encourage theLLMs to deeply understand the problems and extract the key problem-solvinginformation used for better reasoning. Extensive experiments on 10 diversereasoning benchmarks show that our DUP method consistently outperforms theother counterparts by a large margin. More encouragingly, DUP achieves a newSOTA result on the GSM8K benchmark, with an accuracy of 97.1% under thezero-shot setting.
  </details>

- **[MoReVQA: Exploring Modular Reasoning Models for Video Question Answering](http://arxiv.org/abs/2404.06511v2)**  `arXiv:2404.06511`  `cs.AI` `cs.LG` `cs.CV`  
  _Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid_
  <details open><summary>Abstract</summary>
  This paper addresses the task of video question answering (videoQA) via adecomposed multi-stage, modular reasoning framework. Previous modular methodshave shown promise with a single planning stage ungrounded in visual content.However, through a simple and effective baseline, we find that such systems canlead to brittle behavior in practice for challenging videoQA settings. Thus,unlike traditional single-stage planning methods, we propose a multi-stagesystem consisting of an event parser, a grounding stage, and a final reasoningstage in conjunction with an external memory. All stages are training-free, andperformed using few-shot prompting of large models, creating interpretableintermediate outputs at each stage. By decomposing the underlying planning andtask complexity, our method, MoReVQA, improves over prior work on standardvideoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) withstate-of-the-art results, and extensions to related tasks (grounded videoQA,paragraph captioning).
  </details>

- **[Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics](http://arxiv.org/abs/2402.11317v2)**  `arXiv:2402.11317`  `cs.AI` `cs.LG`  
  _Xinyu Zhang, Wenjie Qiu, Yi-Chen Li, Lei Yuan, Chengxing Jia, Zongzhang Zhang, et al._
  <details open><summary>Abstract</summary>
  Developing policies that can adjust to non-stationary environments isessential for real-world reinforcement learning applications. However, learningsuch adaptable policies in offline settings, with only a limited set ofpre-collected trajectories, presents significant challenges. A key difficultyarises because the limited offline data makes it hard for the context encoderto differentiate between changes in the environment dynamics and shifts in thebehavior policy, often leading to context misassociations. To address thisissue, we introduce a novel approach called Debiased Offline Representation forfast online Adaptation (DORA). DORA incorporates an information bottleneckprinciple that maximizes mutual information between the dynamics encoding andthe environmental data, while minimizing mutual information between thedynamics encoding and the actions of the behavior policy. We present apractical implementation of DORA, leveraging tractable bounds of theinformation bottleneck principle. Our experimental evaluation across sixbenchmark MuJoCo tasks with variable parameters demonstrates that DORA not onlyachieves a more precise dynamics encoding but also significantly outperformsexisting baselines in terms of performance.
  </details>
