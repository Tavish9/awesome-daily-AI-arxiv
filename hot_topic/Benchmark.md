# üîç Benchmark Papers ¬∑ 2025-03-28

[![Total Papers](https://img.shields.io/badge/Papers-60-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Benchmark`  
**Filter**: `None`

---

## üìö Paper List

- **[Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions](http://arxiv.org/abs/2503.22678v1)**  `arXiv:2503.22678`  `cs.CL`  
  _Mohammad Almansoori, Komal Kumar, Hisham Cholakkal_
  <details open><summary>Abstract</summary>
  In this work, we introduce MedAgentSim, an open-source simulated clinicalenvironment with doctor, patient, and measurement agents designed to evaluateand enhance LLM performance in dynamic diagnostic settings. Unlike priorapproaches, our framework requires doctor agents to actively engage withpatients through multi-turn conversations, requesting relevant medicalexaminations (e.g., temperature, blood pressure, ECG) and imaging results(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnosticprocess. Additionally, we incorporate self improvement mechanisms that allowmodels to iteratively refine their diagnostic strategies. We enhance LLMperformance in our simulated setting by integrating multi-agent discussions,chain-of-thought reasoning, and experience-based knowledge retrieval,facilitating progressive learning as doctor agents interact with more patients.We also introduce an evaluation benchmark for assessing the LLM's ability toengage in dynamic, context-aware diagnostic interactions. While MedAgentSim isfully automated, it also supports a user-controlled mode, enabling humaninteraction with either the doctor or patient agent. Comprehensive evaluationsin various simulated diagnostic scenarios demonstrate the effectiveness of ourapproach. Our code, simulation tool, and benchmark are available at\href{https://medagentsim.netlify.app/}.
  </details>

- **[QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?](http://arxiv.org/abs/2503.22674v1)**  `arXiv:2503.22674`  `cs.CL` `cs.AI` `cs.LG`  
  _Belinda Z. Li, Been Kim, Zi Wang_
  <details open><summary>Abstract</summary>
  Recently, a large amount of work has focused on improving large languagemodels' (LLMs') performance on reasoning benchmarks such as math and logic.However, past work has largely assumed that tasks are well-defined. In the realworld, queries to LLMs are often underspecified, only solvable throughacquiring missing information. We formalize this as a constraint satisfactionproblem (CSP) with missing variable assignments. Using a special case of thisformalism where only one necessary variable assignment is missing, we canrigorously evaluate an LLM's ability to identify the minimal necessary questionto ask and quantify axes of difficulty levels for each problem. We presentQuestBench, a set of underspecified reasoning tasks solvable by asking at mostone question, which includes: (1) Logic-Q: Logical reasoning tasks with onemissing proposition, (2) Planning-Q: PDDL planning problems with initial statesthat are partially-observed, (3) GSM-Q: Human-annotated grade school mathproblems with one missing variable assignment, and (4) GSME-Q: a version ofGSM-Q where word problems are translated into equations by human annotators.The LLM is tasked with selecting the correct clarification question(s) from alist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, theiraccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates thatthe ability to solve well-specified reasoning problems may not be sufficientfor success on our benchmark: models have difficulty identifying the rightquestion to ask, even when they can solve the fully specified version of theproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, evenwhen explicitly presented with the option to predict ``not sure.'' Thishighlights the need for deeper investigation into models' informationacquisition capabilities.
  </details>

- **[ActionStudio: A Lightweight Framework for Data and Training of Action Models](http://arxiv.org/abs/2503.22673v1)**  `arXiv:2503.22673`  `cs.CL` `cs.AI`  
  _Jianguo Zhang, Thai Hoang, Ming Zhu, Zuxin Liu, Shiyu Wang, Tulika Awalgaonkar, et al._
  <details open><summary>Abstract</summary>
  Action models are essential for enabling autonomous agents to perform complextasks. However, training large action models remains challenging due to thediversity of agent environments and the complexity of agentic data. Despitegrowing interest, existing infrastructure provides limited support forscalable, agent-specific fine-tuning. We present ActionStudio, a lightweightand extensible data and training framework designed for action models.ActionStudio unifies heterogeneous agent trajectories through a standardizedformat, supports diverse training paradigms including LoRA, full fine-tuning,and distributed setups, and integrates robust preprocessing and verificationtools. We validate its effectiveness across both public and realistic industrybenchmarks, demonstrating strong performance and practical scalability. Weopen-sourced code and data at https://github.com/SalesforceAIResearch/xLAM tofacilitate research in the community.
  </details>

- **[Understanding Co-speech Gestures in-the-wild](http://arxiv.org/abs/2503.22668v1)**  `arXiv:2503.22668`  `cs.CV`  
  _Sindhu B Hegde, K R Prajwal, Taein Kwon, Andrew Zisserman_
  <details open><summary>Abstract</summary>
  Co-speech gestures play a vital role in non-verbal communication. In thispaper, we introduce a new framework for co-speech gesture understanding in thewild. Specifically, we propose three new tasks and benchmarks to evaluate amodel's capability to comprehend gesture-text-speech associations: (i)gesture-based retrieval, (ii) gestured word spotting, and (iii) active speakerdetection using gestures. We present a new approach that learns a tri-modalspeech-text-video-gesture representation to solve these tasks. By leveraging acombination of global phrase contrastive loss and local gesture-word couplingloss, we demonstrate that a strong gesture representation can be learned in aweakly supervised manner from videos in the wild. Our learned representationsoutperform previous methods, including large vision-language models (VLMs),across all three tasks. Further analysis reveals that speech and textmodalities capture distinct gesture-related signals, underscoring theadvantages of learning a shared tri-modal embedding space. The dataset, model,and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal
  </details>

- **[SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles](http://arxiv.org/abs/2503.22541v1)**  `arXiv:2503.22541`  `cs.RO` `cs.AI`  
  _Haicheng Liao, Hanlin Kong, Bin Rao, Bonan Wang, Chengyue Wang, Guyang Yu, et al._
  <details open><summary>Abstract</summary>
  Accurate motion forecasting is essential for the safety and reliability ofautonomous driving (AD) systems. While existing methods have made significantprogress, they often overlook explicit safety constraints and struggle tocapture the complex interactions among traffic agents, environmental factors,and motion dynamics. To address these challenges, we present SafeCast, arisk-responsive motion forecasting model that integrates safety-awaredecision-making with uncertainty-aware adaptability. SafeCast is the first toincorporate the Responsibility-Sensitive Safety (RSS) framework into motionforecasting, encoding interpretable safety rules--such as safe distances andcollision avoidance--based on traffic norms and physical principles. To furtherenhance robustness, we introduce the Graph Uncertainty Feature (GUF), agraph-based module that injects learnable noise into Graph Attention Networks,capturing real-world uncertainties and enhancing generalization across diversescenarios. We evaluate SafeCast on four real-world benchmark datasets--NextGeneration Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and theMacao Connected Autonomous Driving (MoCAD)--covering highway, urban, andmixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA)accuracy while maintaining a lightweight architecture and low inferencelatency, underscoring its potential for real-time deployment in safety-criticalAD systems.
  </details>

- **[LIM: Large Interpolator Model for Dynamic Reconstruction](http://arxiv.org/abs/2503.22537v1)**  `arXiv:2503.22537`  `cs.CV` `cs.AI`  
  _Remy Sabathier, Niloy J. Mitra, David Novotny_
  <details open><summary>Abstract</summary>
  Reconstructing dynamic assets from video data is central to many in computervision and graphics tasks. Existing 4D reconstruction approaches are limited bycategory-specific models or slow optimization-based methods. Inspired by therecent Large Reconstruction Model (LRM), we present the Large InterpolationModel (LIM), a transformer-based feed-forward solution, guided by a novelcausal consistency loss, for interpolating implicit 3D representations acrosstime. Given implicit 3D representations at times $t_0$ and $t_1$, LIM producesa deformed shape at any continuous time $t\in[t_0,t_1]$, deliveringhigh-quality interpolated frames in seconds. Furthermore, LIM allows explicitmesh tracking across time, producing a consistently uv-textured mesh sequenceready for integration into existing production pipelines. We also use LIM, inconjunction with a diffusion-based multiview generator, to produce dynamic 4Dreconstructions from monocular videos. We evaluate LIM on various dynamicdatasets, benchmarking against image-space interpolation methods (e.g., FiLM)and direct triplane linear interpolation, and demonstrate clear advantages. Insummary, LIM is the first feed-forward model capable of high-speed tracked 4Dasset reconstruction across diverse categories.
  </details>

- **[Robust Offline Imitation Learning Through State-level Trajectory Stitching](http://arxiv.org/abs/2503.22524v1)**  `arXiv:2503.22524`  `cs.RO` `cs.AI`  
  _Shuze Wang, Yunpeng Mei, Hongjie Cao, Yetian Yuan, Gang Wang, Jian Sun, et al._
  <details open><summary>Abstract</summary>
  Imitation learning (IL) has proven effective for enabling robots to acquirevisuomotor skills through expert demonstrations. However, traditional ILmethods are limited by their reliance on high-quality, often scarce, expertdata, and suffer from covariate shift. To address these challenges, recentadvances in offline IL have incorporated suboptimal, unlabeled datasets intothe training. In this paper, we propose a novel approach to enhance policylearning from mixed-quality offline datasets by leveraging task-relevanttrajectory fragments and rich environmental dynamics. Specifically, weintroduce a state-based search framework that stitches state-action pairs fromimperfect demonstrations, generating more diverse and informative trainingtrajectories. Experimental results on standard IL benchmarks and real-worldrobotic tasks showcase that our proposed method significantly improves bothgeneralization and performance.
  </details>

- **[Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery](http://arxiv.org/abs/2503.22516v1)**  `arXiv:2503.22516`  `cs.LG`  
  _Samira Alkaee Taleghan, Morteza Karimzadeh, Andrew P. Barrett, Walter N. Meier, Farnoush Banaei-Kashani_
  <details open><summary>Abstract</summary>
  Accurate segmentation of sea ice types is essential for mapping andoperational forecasting of sea ice conditions for safe navigation and resourceextraction in ice-covered waters, as well as for understanding polar climateprocesses. While deep learning methods have shown promise in automating sea icesegmentation, they often rely on extensive labeled datasets which requireexpert knowledge and are time-consuming to create. Recently, foundation models(FMs) have shown excellent results for segmenting remote sensing images byutilizing pre-training on large datasets using self-supervised techniques.However, their effectiveness for sea ice segmentation remains unexplored,especially given sea ice's complex structures, seasonal changes, and uniquespectral signatures, as well as peculiar Synthetic Aperture Radar (SAR) imagerycharacteristics including banding and scalloping noise, and varying icebackscatter characteristics, which are often missing in standard remote sensingpre-training datasets. In particular, SAR images over polar regions areacquired using different modes than used to capture the images at lowerlatitudes by the same sensors that form training datasets for FMs. This studyevaluates ten remote sensing FMs for sea ice type segmentation using Sentinel-1SAR imagery, focusing on their seasonal and spatial generalization. Among theselected models, Prithvi-600M outperforms the baseline models, while CROMAachieves a very similar performance in F1-score. Our contributions includeoffering a systematic methodology for selecting FMs for sea ice data analysis,a comprehensive benchmarking study on performances of FMs for sea icesegmentation with tailored performance metrics, and insights into existing gapsand future directions for improving domain-specific models in polarapplications using SAR data.
  </details>

- **[WorkTeam: Constructing Workflows from Natural Language with Multi-Agents](http://arxiv.org/abs/2503.22473v1)**  `arXiv:2503.22473`  `cs.CL`  
  _Hanchao Liu, Rongjun Li, Weimin Xiong, Ziyu Zhou, Wei Peng_
  <details open><summary>Abstract</summary>
  Workflows play a crucial role in enhancing enterprise efficiency byorchestrating complex processes with multiple tools or components. However,hand-crafted workflow construction requires expert knowledge, presentingsignificant technical barriers. Recent advancements in Large Language Models(LLMs) have improved the generation of workflows from natural languageinstructions (aka NL2Workflow), yet existing single LLM agent-based methodsface performance degradation on complex tasks due to the need for specializedknowledge and the strain of task-switching. To tackle these challenges, wepropose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor,orchestrator, and filler agent, each with distinct roles that collaborativelyenhance the conversion process. As there are currently no publicly availableNL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, whichincludes 3,695 real-world business samples for training and evaluation.Experimental results show that our approach significantly increases the successrate of workflow construction, providing a novel and effective solution forenterprise NL2Workflow services.
  </details>

- **[NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous Driving](http://arxiv.org/abs/2503.22436v1)**  `arXiv:2503.22436`  `cs.CV`  
  _Fuhao Li, Huan Jin, Bin Gao, Liaoyuan Fan, Lihui Jiang, Long Zeng_
  <details open><summary>Abstract</summary>
  Multi-view 3D visual grounding is critical for autonomous driving vehicles tointerpret natural languages and localize target objects in complexenvironments. However, existing datasets and methods suffer from coarse-grainedlanguage instructions, and inadequate integration of 3D geometric reasoningwith linguistic comprehension. To this end, we introduce NuGrounding, the firstlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.We present a Hierarchy of Grounding (HoG) method to construct NuGrounding togenerate hierarchical multi-level instructions, ensuring comprehensive coverageof human instruction patterns. To tackle this challenging dataset, we propose anovel paradigm that seamlessly combines instruction comprehension abilities ofmulti-modal LLMs (MLLMs) with precise localization abilities of specialistdetection models. Our approach introduces two decoupled task tokens and acontext query to aggregate 3D geometric information and semantic instructions,followed by a fusion decoder to refine spatial-semantic feature fusion forprecise localization. Extensive experiments demonstrate that our methodsignificantly outperforms the baselines adapted from representative 3D sceneunderstanding methods by a significant margin and achieves 0.59 in precisionand 0.64 in recall, with improvements of 50.8% and 54.7%.
  </details>

- **[MVSAnywhere: Zero-Shot Multi-View Stereo](http://arxiv.org/abs/2503.22430v1)**  `arXiv:2503.22430`  `cs.CV`  
  _Sergio Izquierdo, Mohamed Sayed, Michael Firman, Guillermo Garcia-Hernando, Daniyar Turmukhambetov, Javier Civera, et al._
  <details open><summary>Abstract</summary>
  Computing accurate depth from multiple views is a fundamental andlongstanding challenge in computer vision. However, most existing approaches donot generalize well across different domains and scene types (e.g. indoor vs.outdoor). Training a general-purpose multi-view stereo model is challenging andraises several questions, e.g. how to best make use of transformer-basedarchitectures, how to incorporate additional metadata when there is a variablenumber of input views, and how to estimate the range of valid depths which canvary considerably across different scenes and is typically not known a priori?To address these issues, we introduce MVSA, a novel and versatile Multi-ViewStereo architecture that aims to work Anywhere by generalizing across diversedomains and depth ranges. MVSA combines monocular and multi-view cues with anadaptive cost volume to deal with scale-related issues. We demonstratestate-of-the-art zero-shot depth estimation on the Robust Multi-View DepthBenchmark, surpassing existing multi-view stereo and monocular baselines.
  </details>

- **[Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis](http://arxiv.org/abs/2503.22420v1)**  `arXiv:2503.22420`  `cs.CV`  
  _Jiangyong Huang, Baoxiong Jia, Yan Wang, Ziyu Zhu, Xiongkun Linghu, Qing Li, et al._
  <details open><summary>Abstract</summary>
  Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VLmodels, creating a "mist" that obscures rigorous insights into modelcapabilities and 3D-VL tasks. This mist persists due to three key limitations.First, flawed test data, like ambiguous referential text in the grounding task,can yield incorrect and unreliable test results. Second, oversimplified metricssuch as simply averaging accuracy per question answering (QA) pair, cannotreveal true model capability due to their vulnerability to language variations.Third, existing benchmarks isolate the grounding and QA tasks, disregarding theunderlying coherence that QA should be based on solid grounding capabilities.To unveil the "mist", we propose Beacon3D, a benchmark for 3D-VL grounding andQA tasks, delivering a perspective shift in the evaluation of 3D-VLunderstanding. Beacon3D features (i) high-quality test data with precise andnatural language, (ii) object-centric evaluation with multiple tests per objectto ensure robustness, and (iii) a novel chain-of-analysis paradigm to addresslanguage robustness and model performance coherence across grounding and QA.Our evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)object-centric evaluation elicits true model performance and particularly weakgeneralization in QA; (ii) grounding-QA coherence remains fragile in current3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VLmodels, though as a prevalent practice, hinders grounding capabilities and hasyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysiscould benefit the 3D-VL community towards faithful developments.
  </details>

- **[DF-Net: The Digital Forensics Network for Image Forgery Detection](http://arxiv.org/abs/2503.22398v1)**  `arXiv:2503.22398`  `cs.CV`  
  _David Fischinger, Martin Boyer_
  <details open><summary>Abstract</summary>
  The orchestrated manipulation of public opinion, particularly throughmanipulated images, often spread via online social networks (OSN), has become aserious threat to society. In this paper we introduce the Digital Forensics Net(DF-Net), a deep neural network for pixel-wise image forgery detection. Thereleased model outperforms several state-of-the-art methods on four establishedbenchmark datasets. Most notably, DF-Net's detection is robust against lossyimage operations (e.g resizing, compression) as they are automaticallyperformed by social networks.
  </details>

- **[MASCOTS: Model-Agnostic Symbolic COunterfactual explanations for Time Series](http://arxiv.org/abs/2503.22389v1)**  `arXiv:2503.22389`  `cs.LG`  
  _Dawid P≈Çudowski, Francesco Spinnato, Piotr Wilczy≈Ñski, Krzysztof Kotowski, Evridiki Vasileia Ntagiou, Riccardo Guidotti, et al._
  <details open><summary>Abstract</summary>
  Counterfactual explanations provide an intuitive way to understand modeldecisions by identifying minimal changes required to alter an outcome. However,applying counterfactual methods to time series models remains challenging dueto temporal dependencies, high dimensionality, and the lack of an intuitivehuman-interpretable representation. We introduce MASCOTS, a method thatleverages the Bag-of-Receptive-Fields representation alongside symbolictransformations inspired by Symbolic Aggregate Approximation. By operating in asymbolic feature space, it enhances interpretability while preserving fidelityto the original data and model. Unlike existing approaches that either dependon model structure or autoencoder-based sampling, MASCOTS directly generatesmeaningful and diverse counterfactual observations in a model-agnostic manner,operating on both univariate and multivariate data. We evaluate MASCOTS onunivariate and multivariate benchmark datasets, demonstrating comparablevalidity, proximity, and plausibility to state-of-the-art methods, whilesignificantly improving interpretability and sparsity. Its symbolic natureallows for explanations that can be expressed visually, in natural language, orthrough semantic representations, making counterfactual reasoning moreaccessible and actionable.
  </details>

- **[Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors](http://arxiv.org/abs/2503.22388v1)**  `arXiv:2503.22388`  `cs.CL`  
  _Zhiyu Yang, Shuo Wang, Yukun Yan, Yang Deng_
  <details open><summary>Abstract</summary>
  LLMs are transforming software development, yet current code generation andcode repair benchmarks mainly assess syntactic and functional correctness insimple, single-error cases. LLMs' capabilities to autonomously find and fixruntime logical errors in complex data science code remain largely unexplored.To address this gap, we introduce DSDBench: the Data Science DebuggingBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hoperror tracing and multi-bug detection in data science code debugging. DSDBenchadapts datasets from existing data science task benchmarks, such as DABench andMatPlotBench, featuring realistic data science debugging tasks withautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes1,117 annotated samples with 741 cause-effect error pairs and runtime errormessages. Evaluations of state-of-the-art LLMs on DSDBench show significantperformance gaps, highlighting challenges in debugging logical runtime errorsin data science code. DSDBench offers a crucial resource to evaluate andimprove LLMs' debugging and reasoning capabilities, enabling more reliableAI-assisted data science in the future.DSDBench is publicly available athttps://github.com/KevinCL16/DSDBench.
  </details>

- **[ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch Recognition and Generation](http://arxiv.org/abs/2503.22374v1)**  `arXiv:2503.22374`  `cs.CV` `cs.AI`  
  _Giulio Federico, Giuseppe Amato, Fabio Carrara, Claudio Gennaro, Marco Di Benedetto_
  <details open><summary>Abstract</summary>
  Understanding the nature of human sketches is challenging because of the widevariation in how they are created. Recognizing complex structural patternsimproves both the accuracy in recognizing sketches and the fidelity of thegenerated sketches. In this work, we introduce ViSketch-GPT, a novel algorithmdesigned to address these challenges through a multi-scale context extractionapproach. The model captures intricate details at multiple scales and combinesthem using an ensemble-like mechanism, where the extracted features workcollaboratively to enhance the recognition and generation of key detailscrucial for classification and generation tasks.  The effectiveness of ViSketch-GPT is validated through extensive experimentson the QuickDraw dataset. Our model establishes a new benchmark, significantlyoutperforming existing methods in both classification and generation tasks,with substantial improvements in accuracy and the fidelity of generatedsketches.  The proposed algorithm offers a robust framework for understanding complexstructures by extracting features that collaborate to recognize intricatedetails, enhancing the understanding of structures like sketches and making ita versatile tool for various applications in computer vision and machinelearning.
  </details>

- **[Mitigating Knowledge Discrepancies among Multiple Datasets for Task-agnostic Unified Face Alignment](http://arxiv.org/abs/2503.22359v1)**  `arXiv:2503.22359`  `cs.CV`  
  _Jiahao Xia, Min Xu, Wenjian Huang, Jianguo Zhang, Haimin Zhang, Chunxia Xiao_
  <details open><summary>Abstract</summary>
  Despite the similar structures of human faces, existing face alignmentmethods cannot learn unified knowledge from multiple datasets with differentlandmark annotations. The limited training samples in a single dataset commonlyresult in fragile robustness in this field. To mitigate knowledge discrepanciesamong different datasets and train a task-agnostic unified face alignment(TUFA) framework, this paper presents a strategy to unify knowledge frommultiple datasets. Specifically, we calculate a mean face shape for eachdataset. To explicitly align these mean shapes on an interpretable plane basedon their semantics, each shape is then incorporated with a group of semanticalignment embeddings. The 2D coordinates of these aligned shapes can be viewedas the anchors of the plane. By encoding them into structure prompts andfurther regressing the corresponding facial landmarks using image features, amapping from the plane to the target faces is finally established, whichunifies the learning target of different datasets. Consequently, multipledatasets can be utilized to boost the generalization ability of the model. Thesuccessful mitigation of discrepancies also enhances the efficiency ofknowledge transferring to a novel dataset, significantly boosts the performanceof few-shot face alignment. Additionally, the interpretable plane endows TUFAwith a task-agnostic characteristic, enabling it to locate landmarks unseenduring training in a zero-shot manner. Extensive experiments are carried onseven benchmarks and the results demonstrate an impressive improvement in facealignment brought by knowledge discrepancies mitigation.
  </details>

- **[Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions](http://arxiv.org/abs/2503.22353v1)**  `arXiv:2503.22353`  `cs.CL` `cs.AI`  
  _Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have shown remarkable capabilities acrossvarious tasks, but their deployment in high-stake domains requires consistentperformance across multiple interaction rounds. This paper introduces acomprehensive framework for evaluating and improving LLM response consistency,making three key contributions. First, we propose a novel Position-WeightedConsistency (PWC) score that captures both the importance of early-stagestability and recovery patterns in multi-turn interactions. Second, we presenta carefully curated benchmark dataset spanning diverse domains and difficultylevels, specifically designed to evaluate LLM consistency under variouschallenging follow-up scenarios. Third, we introduce Confidence-Aware ResponseGeneration (CARG), a framework that significantly improves response stabilityby incorporating model confidence signals into the generation process.Empirical results demonstrate that CARG significantly improves responsestability without sacrificing accuracy, underscoring its potential for reliableLLM deployment in critical applications.
  </details>

- **[Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization](http://arxiv.org/abs/2503.22352v1)**  `arXiv:2503.22352`  `cs.CV`  
  _Barƒ±≈ü Batuhan Topal, Umut √ñzyurt, Zafer Doƒüan Budak, Ramazan Gokberk Cinbis_
  <details open><summary>Abstract</summary>
  Recent advancements in text-to-image generative models, particularly latentdiffusion models (LDMs), have demonstrated remarkable capabilities insynthesizing high-quality images from textual prompts. However, achievingidentity personalization-ensuring that a model consistently generatessubject-specific outputs from limited reference images-remains a fundamentalchallenge. To address this, we introduce Meta-Low-Rank Adaptation (Meta-LoRA),a novel framework that leverages meta-learning to encode domain-specific priorsinto LoRA-based identity personalization. Our method introduces a structuredthree-layer LoRA architecture that separates identity-agnostic knowledge fromidentity-specific adaptation. In the first stage, the LoRA Meta-Down layers aremeta-trained across multiple subjects, learning a shared manifold that capturesgeneral identity-related features. In the second stage, only the LoRA-Mid andLoRA-Up layers are optimized to specialize on a given subject, significantlyreducing adaptation time while improving identity fidelity. To evaluate ourapproach, we introduce Meta-PHD, a new benchmark dataset for identitypersonalization, and compare Meta-LoRA against state-of-the-art methods. Ourresults demonstrate that Meta-LoRA achieves superior identity retention,computational efficiency, and adaptability across diverse identity conditions.The code, model weights, and dataset will be released publicly upon acceptance.
  </details>

- **[VisTa: Visual-contextual and Text-augmented Zero-shot Object-level OOD Detection](http://arxiv.org/abs/2503.22291v1)**  `arXiv:2503.22291`  `cs.CV`  
  _Bin Zhang, Xiaoyang Qu, Guokuan Li, Jiguang Wan, Jianzong Wang_
  <details open><summary>Abstract</summary>
  As object detectors are increasingly deployed as black-box cloud services orpre-trained models with restricted access to the original training data, thechallenge of zero-shot object-level out-of-distribution (OOD) detection arises.This task becomes crucial in ensuring the reliability of detectors inopen-world settings. While existing methods have demonstrated success inimage-level OOD detection using pre-trained vision-language models like CLIP,directly applying such models to object-level OOD detection presents challengesdue to the loss of contextual information and reliance on image-levelalignment. To tackle these challenges, we introduce a new method that leveragesvisual prompts and text-augmented in-distribution (ID) space construction toadapt CLIP for zero-shot object-level OOD detection. Our method preservescritical contextual information and improves the ability to differentiatebetween ID and OOD objects, achieving competitive performance across differentbenchmarks.
  </details>

- **[Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion](http://arxiv.org/abs/2503.22262v1)**  `arXiv:2503.22262`  `cs.CV`  
  _Songsong Yu, Yuxin Chen, Zhongang Qi, Zeke Xie, Yifan Wang, Lijun Wang, et al._
  <details open><summary>Abstract</summary>
  With the rapid proliferation of 3D devices and the shortage of 3D content,stereo conversion is attracting increasing attention. Recent works introducepretrained Diffusion Models (DMs) into this task. However, due to the scarcityof large-scale training data and comprehensive benchmarks, the optimalmethodologies for employing DMs in stereo conversion and the accurateevaluation of stereo effects remain largely unexplored. In this work, weintroduce the Mono2Stereo dataset, providing high-quality training data andbenchmark to support in-depth exploration of stereo conversion. With thisdataset, we conduct an empirical study that yields two primary findings. 1) Thedifferences between the left and right views are subtle, yet existing metricsconsider overall pixels, failing to concentrate on regions critical to stereoeffects. 2) Mainstream methods adopt either one-stage left-to-right generationor warp-and-inpaint pipeline, facing challenges of degraded stereo effect andimage distortion respectively. Based on these findings, we introduce a newevaluation metric, Stereo Intersection-over-Union, which prioritizes disparityand achieves a high correlation with human judgments on stereo effect.Moreover, we propose a strong baseline model, harmonizing the stereo effect andimage quality simultaneously, and notably surpassing current mainstreammethods. Our code and data will be open-sourced to promote further research instereo conversion. Our models are available at mono2stereo-bench.github.io.
  </details>

- **[FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation](http://arxiv.org/abs/2503.22249v1)**  `arXiv:2503.22249`  `cs.RO` `cs.LG`  
  _Xianqi Zhang, Hongliang Wei, Wenrui Wang, Xingtao Wang, Xiaopeng Fan, Debin Zhao_
  <details open><summary>Abstract</summary>
  Humanoid robots have attracted significant attention in recent years.Reinforcement Learning (RL) is one of the main ways to control the whole bodyof humanoid robots. RL enables agents to complete tasks by learning fromenvironment interactions, guided by task rewards. However, existing RL methodsrarely explicitly consider the impact of body stability on humanoid locomotionand manipulation. Achieving high performance in whole-body control remains achallenge for RL methods that rely solely on task rewards. In this paper, wepropose a Foundation model-based method for humanoid Locomotion AndManipulation (FLAM for short). FLAM integrates a stabilizing reward functionwith a basic policy. The stabilizing reward function is designed to encouragethe robot to learn stable postures, thereby accelerating the learning processand facilitating task completion. Specifically, the robot pose is first mappedto the 3D virtual human model. Then, the human pose is stabilized andreconstructed through a human motion reconstruction model. Finally, the posebefore and after reconstruction is used to compute the stabilizing reward. Bycombining this stabilizing reward with the task reward, FLAM effectively guidespolicy learning. Experimental results on a humanoid robot benchmark demonstratethat FLAM outperforms state-of-the-art RL methods, highlighting itseffectiveness in improving stability and overall performance.
  </details>

- **[Learning to Instruct for Visual Instruction Tuning](http://arxiv.org/abs/2503.22215v1)**  `arXiv:2503.22215`  `cs.CL` `cs.CV` `cs.AI` `cs.LG`  
  _Zhihan Zhou, Feng Hong, Jiaan Luo, Jiangchao Yao, Dongsheng Li, Bo Han, et al._
  <details open><summary>Abstract</summary>
  We propose LIT, an advancement of visual instruction tuning (VIT). While VITequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, thecurrent design choices for VIT often result in overfitting and shortcutlearning, potentially degrading performance. This gap arises from anoveremphasis on instruction-following abilities, while neglecting the proactiveunderstanding of visual information. Inspired by this, LIT adopts a simple yeteffective approach by incorporating the loss function into both the instructionand response sequences. It seamlessly expands the training data, andregularizes the MLLMs from overly relying on language priors. Based on thismerit, LIT achieves a significant relative improvement of up to 9% oncomprehensive multimodal benchmarks, requiring no additional training data andincurring negligible computational overhead. Surprisingly, LIT attainsexceptional fundamental visual capabilities, yielding up to an 18% improvementin captioning performance, while simultaneously alleviating hallucination inMLLMs.
  </details>

- **[Fuzzy Cluster-Aware Contrastive Clustering for Time Series](http://arxiv.org/abs/2503.22211v1)**  `arXiv:2503.22211`  `cs.LG`  
  _Congyu Wang, Mingjing Du, Xiang Jiang, Yongquan Dong_
  <details open><summary>Abstract</summary>
  The rapid growth of unlabeled time series data, driven by the Internet ofThings (IoT), poses significant challenges in uncovering underlying patterns.Traditional unsupervised clustering methods often fail to capture the complexnature of time series data. Recent deep learning-based clustering approaches,while effective, struggle with insufficient representation learning and theintegration of clustering objectives. To address these issues, we propose afuzzy cluster-aware contrastive clustering framework (FCACC) that jointlyoptimizes representation learning and clustering.  Our approach introduces a novel three-view data augmentation strategy toenhance feature extraction by leveraging various characteristics of time seriesdata. Additionally, we propose a cluster-aware hard negative sample generationmechanism that dynamically constructs high-quality negative samples usingclustering structure information, thereby improving the model's discriminativeability.  By leveraging fuzzy clustering, FCACC dynamically generates clusterstructures to guide the contrastive learning process, resulting in moreaccurate clustering. Extensive experiments on 40 benchmark datasets show thatFCACC outperforms the selected baseline methods (eight in total), providing aneffective solution for unsupervised time series learning.
  </details>

- **[EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge Devices](http://arxiv.org/abs/2503.22196v1)**  `arXiv:2503.22196`  `cs.CL`  
  _Jiyu Chen, Shuang Peng, Daxiong Luo, Fan Yang, Renshou Wu, Fangyuan Li, et al._
  <details open><summary>Abstract</summary>
  Transformer-based large language models (LLMs) encounter challenges inprocessing long sequences on edge devices due to the quadratic complexity ofattention mechanisms and growing memory demands from Key-Value (KV) cache.Existing KV cache optimizations struggle with irreversible token eviction inlong-output tasks, while alternative sequence modeling architectures provecostly to adopt within established Transformer infrastructure. We presentEdgeInfinite, a memory-efficient solution for infinite contexts that integratescompressed memory into Transformer-based LLMs through a trainable memory-gatingmodule. This approach maintains full compatibility with standard Transformerarchitectures, requiring fine-tuning only a small part of parameters, andenables selective activation of the memory-gating module for long and shortcontext task routing. The experimental result shows that EdgeInfinite achievescomparable performance to baseline Transformer-based LLM on long contextbenchmarks while optimizing memory consumption and time to first token.
  </details>

- **[Knowledge Rectification for Camouflaged Object Detection: Unlocking Insights from Low-Quality Data](http://arxiv.org/abs/2503.22180v1)**  `arXiv:2503.22180`  `cs.CV`  
  _Juwei Guan, Xiaolin Fang, Donghyun Kim, Haotian Gong, Tongxin Zhu, Zhen Ling, et al._
  <details open><summary>Abstract</summary>
  Low-quality data often suffer from insufficient image details, introducing anextra implicit aspect of camouflage that complicates camouflaged objectdetection (COD). Existing COD methods focus primarily on high-quality data,overlooking the challenges posed by low-quality data, which leads tosignificant performance degradation. Therefore, we propose KRNet, the firstframework explicitly designed for COD on low-quality data. KRNet presents aLeader-Follower framework where the Leader extracts dual gold-standarddistributions: conditional and hybrid, from high-quality data to drive theFollower in rectifying knowledge learned from low-quality data. The frameworkfurther benefits from a cross-consistency strategy that improves therectification of these distributions and a time-dependent conditional encoderthat enriches the distribution diversity. Extensive experiments on benchmarkdatasets demonstrate that KRNet outperforms state-of-the-art COD methods andsuper-resolution-assisted COD approaches, proving its effectiveness in tacklingthe challenges of low-quality data in COD.
  </details>

- **[Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis](http://arxiv.org/abs/2503.22168v1)**  `arXiv:2503.22168`  `cs.CV`  
  _Woojung Han, Yeonkyung Lee, Chanyoung Kim, Kwanghyun Park, Seong Jae Hwang_
  <details open><summary>Abstract</summary>
  Diffusion-based text-to-image (T2I) models have recently excelled inhigh-quality image generation, particularly in a training-free manner, enablingcost-effective adaptability and generalization across diverse tasks. However,while the existing methods have been continuously focusing on severalchallenges, such as "missing objects" and "mismatched attributes," anothercritical issue of "mislocated objects" remains where generated spatialpositions fail to align with text prompts. Surprisingly, ensuring suchseemingly basic functionality remains challenging in popular T2I models due tothe inherent difficulty of imposing explicit spatial guidance via text forms.To address this, we propose STORM (Spatial Transport Optimization byRepositioning Attention Map), a novel training-free approach for spatiallycoherent T2I synthesis. STORM employs Spatial Transport Optimization (STO),rooted in optimal transport theory, to dynamically adjust object attention mapsfor precise spatial adherence, supported by a Spatial Transport (ST) Costfunction that enhances spatial understanding. Our analysis shows thatintegrating spatial awareness is most effective in the early denoising stages,while later phases refine details. Extensive experiments demonstrate that STORMsurpasses existing methods, effectively mitigating mislocated objects whileimproving missing and mismatched attributes, setting a new benchmark forspatial alignment in T2I synthesis.
  </details>

- **[Permutation-Invariant and Orientation-Aware Dataset Distillation for 3D Point Clouds](http://arxiv.org/abs/2503.22154v1)**  `arXiv:2503.22154`  `cs.CV`  
  _Jae-Young Yim, Dongwook Kim, Jae-Young Sim_
  <details open><summary>Abstract</summary>
  We should collect large amount of data to train deep neural networks forvarious applications. Recently, the dataset distillation for images and textshas been attracting a lot of attention, that reduces the original dataset to asynthetic dataset while preserving essential task-relevant information.However, 3D point clouds distillation is almost unexplored due to thechallenges of unordered structures of points. In this paper, we propose a noveldistribution matching-based dataset distillation method for 3D point cloudsthat jointly optimizes the geometric structures of synthetic dataset as well asthe orientations of synthetic models. To ensure the consistent featurealignment between different 3D point cloud models, we devise a permutationinvariant distribution matching loss with the sorted feature vectors. We alsoemploy learnable rotation angles to transform each syntheic model according tothe optimal orientation best representing the original feature distribution.Extensive experimental results on widely used four benchmark datasets,including ModelNet10, ModelNet40, ShapeNet, and ScanObjectNN, demonstrate thatthe proposed method consistently outperforms the existing methods.
  </details>

- **[EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos](http://arxiv.org/abs/2503.22152v1)**  `arXiv:2503.22152`  `cs.CV` `cs.AI`  
  _Yuxuan Li, Vijay Veerabadran, Michael L. Iuzzolino, Brett D. Roads, Asli Celikyilmaz, Karl Ridgeway_
  <details open><summary>Abstract</summary>
  We introduce EgoToM, a new video question-answering benchmark that extendsTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToMmodel, we generate multi-choice video QA instances for the Ego4D dataset tobenchmark the ability to predict a camera wearer's goals, beliefs, and nextactions. We study the performance of both humans and state of the artmultimodal large language models (MLLMs) on these three interconnectedinference problems. Our evaluation shows that MLLMs achieve close tohuman-level accuracy on inferring goals from egocentric videos. However, MLLMs(including the largest ones we tested with over 100B parameters) fall short ofhuman performance when inferring the camera wearers' in-the-moment beliefstates and future actions that are most consistent with the unseen videofuture. We believe that our results will shape the future design of animportant class of egocentric digital assistants which are equipped with areasonable model of the user's internal mental states.
  </details>

- **[REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation](http://arxiv.org/abs/2503.22122v1)**  `arXiv:2503.22122`  `cs.CL` `cs.CV` `cs.RO` `cs.AI`  
  _Puzhen Yuan, Angyuan Ma, Yunchao Yao, Huaxiu Yao, Masayoshi Tomizuka, Mingyu Ding_
  <details open><summary>Abstract</summary>
  Vision-language models (VLMs) have demonstrated remarkable capabilities inrobotic planning, particularly for long-horizon tasks that require a holisticunderstanding of the environment for task decomposition. Existing methodstypically rely on prior environmental knowledge or carefully designedtask-specific prompts, making them struggle with dynamic scene changes orunexpected task conditions, e.g., a robot attempting to put a carrot in themicrowave but finds the door was closed. Such challenges underscore twocritical issues: adaptability and efficiency. To address them, in this work, wepropose an adaptive multi-agent planning framework, termed REMAC, that enablesefficient, scene-agnostic multi-robot long-horizon task planning and executionthrough continuous reflection and self-evolution. REMAC incorporates two keymodules: a self-reflection module performing pre-condition and post-conditionchecks in the loop to evaluate progress and refine plans, and a self-evolvementmodule dynamically adapting plans based on scene-specific reasoning. It offersseveral appealing benefits: 1) Robots can initially explore and reason aboutthe environment without complex prompt design. 2) Robots can keep reflecting onpotential planning errors and adapting the plan based on task-specificinsights. 3) After iterations, a robot can call another one to coordinate tasksin parallel, maximizing the task execution efficiency. To validate REMAC'seffectiveness, we build a multi-agent environment for long-horizon robotmanipulation and navigation based on RoboCasa, featuring 4 task categories with27 task styles and 50+ different objects. Based on it, we further benchmarkstate-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, andGrok3, demonstrating REMAC's superiority by boosting average success rates by40% and execution efficiency by 52.7% over the single robot baseline.
  </details>

- **[Detecting Localized Deepfake Manipulations Using Action Unit-Guided Video Representations](http://arxiv.org/abs/2503.22121v1)**  `arXiv:2503.22121`  `cs.CV`  
  _Tharun Anand, Siva Sankar, Pravin Nair_
  <details open><summary>Abstract</summary>
  With rapid advancements in generative modeling, deepfake techniques areincreasingly narrowing the gap between real and synthetic videos, raisingserious privacy and security concerns. Beyond traditional face swapping andreenactment, an emerging trend in recent state-of-the-art deepfake generationmethods involves localized edits such as subtle manipulations of specificfacial features like raising eyebrows, altering eye shapes, or modifying mouthexpressions. These fine-grained manipulations pose a significant challenge forexisting detection models, which struggle to capture such localized variations.To the best of our knowledge, this work presents the first detection approachexplicitly designed to generalize to localized edits in deepfake videos byleveraging spatiotemporal representations guided by facial action units. Ourmethod leverages a cross-attention-based fusion of representations learned frompretext tasks like random masking and action unit detection, to create anembedding that effectively encodes subtle, localized changes. Comprehensiveevaluations across multiple deepfake generation methods demonstrate that ourapproach, despite being trained solely on the traditional FF+ dataset, sets anew benchmark in detecting recent deepfake-generated videos with fine-grainedlocal edits, achieving a $20\%$ improvement in accuracy over currentstate-of-the-art detection methods. Additionally, our method deliverscompetitive performance on standard datasets, highlighting its robustness andgeneralization across diverse types of local and global forgeries.
  </details>

- **[How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark](http://arxiv.org/abs/2503.22093v1)**  `arXiv:2503.22093`  `cs.CV` `cs.AI`  
  _Ximing Wen, Mallika Mainali, Anik Sen_
  <details open><summary>Abstract</summary>
  Vision Language Models (VLMs) have demonstrated strong reasoning capabilitiesin Visual Question Answering (VQA) tasks; However, their ability to performTheory of Mind (ToM) tasks such as accurately inferring human intentions,beliefs, and other mental states remains underexplored. In this work, wepropose an open-ended question framework to comprehensively evaluate VLMs'performance across diverse categories of ToM tasks. We curated and annotated abenchmark dataset composed of 30 images. We then assessed the performance offour VLMs of varying sizes on this dataset. Our experimental results show thatthe GPT-4 model outperformed all others, with only one smaller model,GPT-4o-mini, achieving comparable performance. Additionally, we observed thatVLMs often struggle to accurately infer intentions in complex scenarios such asbullying or cheating. Moreover, our findings also reveal that smaller modelscan sometimes infer correct intentions despite relying on incorrect visualcues.
  </details>

- **[A Semantic-Enhanced Heterogeneous Graph Learning Method for Flexible Objects Recognition](http://arxiv.org/abs/2503.22079v1)**  `arXiv:2503.22079`  `cs.CV`  
  _Kunshan Yang, Wenwei Luo, Yuguo Hu, Jiafu Yan, Mengmeng Jing, Lin Zuo_
  <details open><summary>Abstract</summary>
  Flexible objects recognition remains a significant challenge due to itsinherently diverse shapes and sizes, translucent attributes, and subtleinter-class differences. Graph-based models, such as graph convolution networksand graph vision models, are promising in flexible objects recognition due totheir ability of capturing variable relations within the flexible objects.These methods, however, often focus on global visual relationships or fail toalign semantic and visual information. To alleviate these limitations, wepropose a semantic-enhanced heterogeneous graph learning method. First, anadaptive scanning module is employed to extract discriminative semanticcontext, facilitating the matching of flexible objects with varying shapes andsizes while aligning semantic and visual nodes to enhance cross-modal featurecorrelation. Second, a heterogeneous graph generation module aggregates globalvisual and local semantic node features, improving the recognition of flexibleobjects. Additionally, We introduce the FSCW, a large-scale flexible datasetcurated from existing sources. We validate our method through extensiveexperiments on flexible datasets (FDA and FSCW), and challenge benchmarks(CIFAR-100 and ImageNet-Hard), demonstrating competitive performance.
  </details>

- **[Deep Depth Estimation from Thermal Image: Dataset, Benchmark, and Challenges](http://arxiv.org/abs/2503.22060v1)**  `arXiv:2503.22060`  `cs.RO` `cs.CV`  
  _Ukcheol Shin, Jinsun Park_
  <details open><summary>Abstract</summary>
  Achieving robust and accurate spatial perception under adverse weather andlighting conditions is crucial for the high-level autonomy of self-drivingvehicles and robots. However, existing perception algorithms relying on thevisible spectrum are highly affected by weather and lighting conditions. Along-wave infrared camera (i.e., thermal imaging camera) can be a potentialsolution to achieve high-level robustness. However, the absence of large-scaledatasets and standardized benchmarks remains a significant bottleneck toprogress in active research for robust visual perception from thermal images.To this end, this manuscript provides a large-scale Multi-Spectral Stereo(MS$^2$) dataset that consists of stereo RGB, stereo NIR, stereo thermal,stereo LiDAR data, and GNSS/IMU information along with semi-dense depth groundtruth. MS$^2$ dataset includes 162K synchronized multi-modal data pairscaptured across diverse locations (e.g., urban city, residential area, campus,and high-way road) at different times (e.g., morning, daytime, and nighttime)and under various weather conditions (e.g., clear-sky, cloudy, and rainy).Secondly, we conduct a thorough evaluation of monocular and stereo depthestimation networks across RGB, NIR, and thermal modalities to establishstandardized benchmark results on MS$^2$ depth test sets (e.g., day, night, andrainy). Lastly, we provide in-depth analyses and discuss the challengesrevealed by the benchmark results, such as the performance variability for eachmodality under adverse conditions, domain shift between different sensormodalities, and potential research direction for thermal perception. Ourdataset and source code are publicly available athttps://sites.google.com/view/multi-spectral-stereo-dataset andhttps://github.com/UkcheolShin/SupDepth4Thermal.
  </details>

- **[OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs](http://arxiv.org/abs/2503.21480v2)**  `arXiv:2503.21480`  `cs.CL`  
  _John Murzaku, Owen Rambow_
  <details open><summary>Abstract</summary>
  The use of omni-LLMs (large language models that accept any modality asinput), particularly for multimodal cognitive state tasks involving speech, isunderstudied. We present OmniVox, the first systematic evaluation of fouromni-LLMs on the zero-shot emotion recognition task. We evaluate on two widelyused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shotomni-LLMs outperform or are competitive with fine-tuned audio models. Alongsideour audio-only evaluation, we also evaluate omni-LLMs on text only and text andaudio. We present acoustic prompting, an audio-specific prompting strategy foromni-LLMs which focuses on acoustic feature analysis, conversation contextanalysis, and step-by-step reasoning. We compare our acoustic prompting tominimal prompting and full chain-of-thought prompting techniques. We perform acontext window analysis on IEMOCAP and MELD, and find that using context helps,especially on IEMOCAP. We conclude with an error analysis on the generatedacoustic reasoning outputs from the omni-LLMs.
  </details>

- **[The Procedural Content Generation Benchmark: An Open-source Testbed for Generative Challenges in Games](http://arxiv.org/abs/2503.21474v2)**  `arXiv:2503.21474`  `cs.AI` `cs.LG`  
  _Ahmed Khalifa, Roberto Gallotta, Matthew Barthet, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis_
  <details open><summary>Abstract</summary>
  This paper introduces the Procedural Content Generation Benchmark forevaluating generative algorithms on different game content creation tasks. Thebenchmark comes with 12 game-related problems with multiple variants on eachproblem. Problems vary from creating levels of different kinds to creating rulesets for simple arcade games. Each problem has its own content representation,control parameters, and evaluation metrics for quality, diversity, andcontrollability. This benchmark is intended as a first step towards astandardized way of comparing generative algorithms. We use the benchmark toscore three baseline algorithms: a random generator, an evolution strategy, anda genetic algorithm. Results show that some problems are easier to solve thanothers, as well as the impact the chosen objective has on quality, diversity,and controllability of the generated artifacts.
  </details>

- **[Omni-AD: Learning to Reconstruct Global and Local Features for Multi-class Anomaly Detection](http://arxiv.org/abs/2503.21125v2)**  `arXiv:2503.21125`  `cs.CV`  
  _Jiajie Quan, Ao Tong, Yuxuan Cai, Xinwei He, Yulong Wang, Yang Zhou_
  <details open><summary>Abstract</summary>
  In multi-class unsupervised anomaly detection(MUAD), reconstruction-basedmethods learn to map input images to normal patterns to identify anomalouspixels. However, this strategy easily falls into the well-known "learningshortcut" issue when decoders fail to capture normal patterns and reconstructboth normal and abnormal samples naively. To address that, we propose to learnthe input features in global and local manners, forcing the network to memorizethe normal patterns more comprehensively. Specifically, we design a two-branchdecoder block, named Omni-block. One branch corresponds to global featurelearning, where we serialize two self-attention blocks but replace the queryand (key, value) with learnable tokens, respectively, thus capturing globalfeatures of normal patterns concisely and thoroughly. The local branchcomprises depth-separable convolutions, whose locality enables effective andefficient learning of local features for normal patterns. By stackingOmni-blocks, we build a framework, Omni-AD, to learn normal patterns ofdifferent granularity and reconstruct them progressively. Comprehensiveexperiments on public anomaly detection benchmarks show that our methodoutperforms state-of-the-art approaches in MUAD. Code is available athttps://github.com/easyoo/Omni-AD.git
  </details>

- **[VinaBench: Benchmark for Faithful and Consistent Visual Narratives](http://arxiv.org/abs/2503.20871v2)**  `arXiv:2503.20871`  `cs.CL` `cs.CV` `cs.AI`  
  _Silin Gao, Sheryl Mathew, Li Mi, Sepideh Mamooler, Mengjie Zhao, Hiromi Wakaki, et al._
  <details open><summary>Abstract</summary>
  Visual narrative generation transforms textual narratives into sequences ofimages illustrating the content of the text. However, generating visualnarratives that are faithful to the input text and self-consistent acrossgenerated images remains an open challenge, due to the lack of knowledgeconstraints used for planning the stories. In this work, we propose a newbenchmark, VinaBench, to address this challenge. Our benchmark annotates theunderlying commonsense and discourse constraints in visual narrative samples,offering systematic scaffolds for learning the implicit strategies of visualstorytelling. Based on the incorporated narrative constraints, we furtherpropose novel metrics to closely evaluate the consistency of generatednarrative images and the alignment of generations with the input textualnarrative. Our results across three generative vision models demonstrate thatlearning with VinaBench's knowledge constraints effectively improves thefaithfulness and cohesion of generated visual narratives.
  </details>

- **[Adaptive Weighted Parameter Fusion with CLIP for Class-Incremental Learning](http://arxiv.org/abs/2503.19503v2)**  `arXiv:2503.19503`  `cs.CV`  
  _Juncen Guo, Xiaoguang Zhu, Liangyu Teng, Hao Yang, Jing Liu, Yang Liu, et al._
  <details open><summary>Abstract</summary>
  Class-incremental Learning (CIL) enables the model to incrementally absorbknowledge from new classes and build a generic classifier across all previouslyencountered classes. When the model optimizes with new classes, the knowledgeof previous classes is inevitably erased, leading to catastrophic forgetting.Addressing this challenge requires making a trade-off between retaining oldknowledge and accommodating new information. However, this balancing processoften requires sacrificing some information, which can lead to a partial lossin the model's ability to discriminate between classes. To tackle this issue,we design the adaptive weighted parameter fusion with ContrastiveLanguage-Image Pre-training (CLIP), which not only takes into account thevariability of the data distribution of different tasks, but also retains allthe effective information of the parameter matrix to the greatest extent. Inaddition, we introduce a balance factor that can balance the data distributionalignment and distinguishability of adjacent tasks. Experimental results onseveral traditional benchmarks validate the superiority of the proposed method.
  </details>

- **[DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts](http://arxiv.org/abs/2503.19498v2)**  `arXiv:2503.19498`  `cs.CL`  
  _Ling Zhong, Yujing Lu, Jing Yang, Weiming Li, Peng Wei, Yongheng Wang, et al._
  <details open><summary>Abstract</summary>
  Chart Question Answering (CQA) benchmarks are essential for evaluating thecapability of Multimodal Large Language Models (MLLMs) to interpret visualdata. However, current benchmarks focus primarily on the evaluation ofgeneral-purpose CQA but fail to adequately capture domain-specific challenges.We introduce DomainCQA, a systematic methodology for constructingdomain-specific CQA benchmarks, and demonstrate its effectiveness by developingAstroChart, a CQA benchmark in the field of astronomy. Our evaluation showsthat chart reasoning and combining chart information with domain knowledge fordeeper analysis and summarization, rather than domain-specific knowledge, posethe primary challenge for existing MLLMs, highlighting a critical gap incurrent benchmarks. By providing a scalable and rigorous framework, DomainCQAenables more precise assessment and improvement of MLLMs for domain-specificapplications.
  </details>

- **[GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting](http://arxiv.org/abs/2503.19458v2)**  `arXiv:2503.19458`  `cs.CV`  
  _Shujuan Li, Yu-Shen Liu, Zhizhong Han_
  <details open><summary>Abstract</summary>
  Reconstructing open surfaces from multi-view images is vital in digitalizingcomplex objects in daily life. A widely used strategy is to learn unsigneddistance functions (UDFs) by checking if their appearance conforms to the imageobservations through neural rendering. However, it is still hard to learncontinuous and implicit UDF representations through 3D Gaussians splatting(3DGS) due to the discrete and explicit scene representation, i.e., 3DGaussians. To resolve this issue, we propose a novel approach to bridge the gapbetween 3D Gaussians and UDFs. Our key idea is to overfit thin and flat 2DGaussian planes on surfaces, and then, leverage the self-supervision andgradient-based inference to supervise unsigned distances in both near and fararea to surfaces. To this end, we introduce novel constraints and strategies toconstrain the learning of 2D Gaussians to pursue more stable optimization andmore reliable self-supervision, addressing the challenges brought bycomplicated gradient field on or near the zero level set of UDFs. We reportnumerical and visual comparisons with the state-of-the-art on widely usedbenchmarks and real data to show our advantages in terms of accuracy,efficiency, completeness, and sharpness of reconstructed open surfaces withboundaries.
  </details>

- **[Overtrained Language Models Are Harder to Fine-Tune](http://arxiv.org/abs/2503.19206v2)**  `arXiv:2503.19206`  `cs.CL` `cs.AI`  
  _Jacob Mitchell Springer, Sachin Goyal, Kaiyue Wen, Tanishq Kumar, Xiang Yue, Sadhika Malladi, et al._
  <details open><summary>Abstract</summary>
  Large language models are pre-trained on ever-growing token budgets under theassumption that better pre-training performance translates to improveddownstream models. In this work, we challenge this assumption and show thatextended pre-training can make models harder to fine-tune, leading to degradedfinal performance. We term this phenomenon catastrophic overtraining. Forexample, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads toover 2% worse performance on multiple standard LLM benchmarks than its 2.3Ttoken counterpart. Through controlled experiments and theoretical analysis, weshow that catastrophic overtraining arises from a systematic increase in thebroad sensitivity of pre-trained parameters to modifications, including but notlimited to fine-tuning. Our findings call for a critical reassessment ofpre-training design that considers the downstream adaptability of the model.
  </details>

- **[Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2503.18352v2)**  `arXiv:2503.18352`  `cs.CV`  
  _Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, Di Huang_
  <details open><summary>Abstract</summary>
  In this paper, we present Diffusion-4K, a novel framework for directultra-high-resolution image synthesis using text-to-image diffusion models. Thecore advancements include: (1) Aesthetic-4K Benchmark: addressing the absenceof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,a comprehensive benchmark for ultra-high-resolution image generation. Wecurated a high-quality 4K dataset with carefully selected images and captionsgenerated by GPT-4o. Additionally, we introduce GLCM Score and CompressionRatio metrics to evaluate fine details, combined with holistic measures such asFID, Aesthetics and CLIPScore for a comprehensive assessment ofultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose awavelet-based fine-tuning approach for direct training with photorealistic 4Kimages, applicable to various latent diffusion models, demonstrating itseffectiveness in synthesizing highly detailed 4K images. Consequently,Diffusion-4K achieves impressive performance in high-quality image synthesisand text prompt adherence, especially when powered by modern large-scalediffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental resultsfrom our benchmark demonstrate the superiority of Diffusion-4K inultra-high-resolution image synthesis.
  </details>

- **[Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering](http://arxiv.org/abs/2503.18172v2)**  `arXiv:2503.18172`  `cs.CL` `cs.AI`  
  _Zixin Chen, Sicheng Song, Kashun Shum, Yanna Lin, Rui Sheng, Huamin Qu_
  <details open><summary>Abstract</summary>
  Misleading chart visualizations, which intentionally manipulate datarepresentations to support specific claims, can distort perceptions and lead toincorrect conclusions. Despite decades of research, misleading visualizationsremain a widespread and pressing issue. Recent advances in multimodal largelanguage models (MLLMs) have demonstrated strong chart comprehensioncapabilities, yet no existing work has systematically evaluated their abilityto detect and interpret misleading charts. This paper introduces the MisleadingChart Question Answering (Misleading ChartQA) Benchmark, a large-scalemultimodal dataset designed to assess MLLMs in identifying and reasoning aboutmisleading charts. It contains over 3,000 curated examples, covering 21 typesof misleaders and 10 chart types. Each example includes standardized chartcode, CSV data, and multiple-choice questions with labeled explanations,validated through multi-round MLLM checks and exhausted expert human review. Webenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitationsin identifying visually deceptive practices. We also propose a novel pipelinethat detects and localizes misleaders, enhancing MLLMs' accuracy in misleadingchart interpretation. Our work establishes a foundation for advancingMLLM-driven misleading chart comprehension. We publicly release the sampledataset to support further research in this critical area.
  </details>

- **[Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](http://arxiv.org/abs/2503.12496v2)**  `arXiv:2503.12496`  `cs.CV`  
  _Tianyuan Qu, Longxiang Tang, Bohao Peng, Senqiao Yang, Bei Yu, Jiaya Jia_
  <details open><summary>Abstract</summary>
  The rise of Large Vision-Language Models (LVLMs) has significantly advancedvideo understanding. However, efficiently processing long videos remains achallenge due to the ``Sampling Dilemma'': low-density sampling risks missingcritical information, while high-density sampling introduces redundancy. Toaddress this issue, we introduce LSDBench, the first benchmark designed toevaluate LVLMs on long-video tasks by constructing high Necessary SamplingDensity (NSD) questions, where NSD represents the minimum sampling densityrequired to accurately answer a given question. LSDBench focuses on dense,short-duration actions to rigorously assess the sampling strategies employed byLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novelReasoning-Driven Hierarchical Sampling (RHS) framework, which combines globallocalization of question-relevant cues with local dense sampling for preciseinference. Additionally, we develop a lightweight Semantic-Guided FrameSelector to prioritize informative frames, enabling RHS to achieve comparableor superior performance with significantly fewer sampled frames. Together, ourLSDBench and RHS framework address the unique challenges of high-NSD long-videotasks, setting a new standard for evaluating and improving LVLMs in thisdomain. Our benchmark and evaluation codes has been released at:https://github.com/dvlab-research/LSDBench
  </details>

- **[StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through Event-Gated Cognition](http://arxiv.org/abs/2503.06220v2)**  `arXiv:2503.06220`  `cs.CV` `cs.LG`  
  _Xin Ding, Hao Wu, Yifan Yang, Shiqi Jiang, Donglin Bai, Zhibo Chen, et al._
  <details open><summary>Abstract</summary>
  With the rise of real-world human-AI interaction applications, such as AIassistants, the need for Streaming Video Dialogue is critical. To address thisneed, we introduce StreamMind, a video LLM framework that achieves ultra-FPSstreaming video processing (100 fps on a single A100) and enables proactive,always-on responses in real time, without explicit user intervention.  To solve the key challenge of the contradiction between linear videostreaming speed and quadratic transformer computation cost, we propose a novelperception-cognition interleaving paradigm named ''event-gated LLMinvocation'', in contrast to the existing per-time-step LLM invocation. Byintroducing a Cognition Gate network between the video encoder and the LLM, LLMis only invoked when relevant events occur. To realize the event featureextraction with constant cost, we propose Event-Preserving Feature Extractor(EPFE) based on state-space method, generating a single perception token forspatiotemporal features. These techniques enable the video LLM with full-FPSperception and real-time cognition response.  Experiments on Ego4D and SoccerNet streaming tasks, as well as standardoffline benchmarks, demonstrate state-of-the-art performance in both modelcapability and real-time efficiency, paving the way for ultra-high-FPSapplications, such as Game AI and interactive media. The code and data isavailable at https://aka.ms/StreamMind.
  </details>

- **[Improving SAM for Camouflaged Object Detection via Dual Stream Adapters](http://arxiv.org/abs/2503.06042v2)**  `arXiv:2503.06042`  `cs.CV`  
  _Jiaming Liu, Linghe Kong, Guihai Chen_
  <details open><summary>Abstract</summary>
  Segment anything model (SAM) has shown impressive general-purposesegmentation performance on natural images, but its performance on camouflagedobject detection (COD) is unsatisfactory. In this paper, we propose SAM-CODthat performs camouflaged object detection for RGB-D inputs. While keeping theSAM architecture intact, dual stream adapters are expanded on the image encoderto learn potential complementary information from RGB images and depth images,and fine-tune the mask decoder and its depth replica to perform dual-streammask prediction. In practice, the dual stream adapters are embedded into theattention block of the image encoder in a parallel manner to facilitate therefinement and correction of the two types of image embeddings. To mitigatechannel discrepancies arising from dual stream embeddings that do not directlyinteract with each other, we augment the association of dual stream embeddingsusing bidirectional knowledge distillation including a model distiller and amodal distiller. In addition, to predict the masks for RGB and depth attentionmaps, we hybridize the two types of image embeddings which are jointly learnedwith the prompt embeddings to update the initial prompt, and then feed theminto the mask decoders to synchronize the consistency of image embeddings andprompt embeddings. Experimental results on four COD benchmarks show that ourSAM-COD achieves excellent detection performance gains over SAM and achievesstate-of-the-art results with a given fine-tuning paradigm.
  </details>

- **[Solving Instance Detection from an Open-World Perspective](http://arxiv.org/abs/2503.00359v2)**  `arXiv:2503.00359`  `cs.CV`  
  _Qianqian Shen, Yunhan Zhao, Nahyun Kwon, Jeeeun Kim, Yanan Li, Shu Kong_
  <details open><summary>Abstract</summary>
  Instance detection (InsDet) aims to localize specific object instances withina novel scene imagery based on given visual references. Technically, itrequires proposal detection to identify all possible object instances, followedby instance-level matching to pinpoint the ones of interest. Its open-worldnature supports its broad applications from robotics to AR/VR but also presentssignificant challenges: methods must generalize to unknown testing datadistributions because (1) the testing scene imagery is unseen during training,and (2) there are domain gaps between visual references and detected proposals.Existing methods tackle these challenges by synthesizing diverse trainingexamples or utilizing off-the-shelf foundation models (FMs). However, they onlypartially capitalize the available open-world information. In contrast, weapproach InsDet from an Open-World perspective, introducing our method IDOW. Wefind that, while pretrained FMs yield high recall in instance detection, theyare not specifically optimized for instance-level feature matching. Therefore,we adapt pretrained FMs for improved instance-level matching using open-worlddata. Our approach incorporates metric learning along with novel dataaugmentations, which sample distractors as negative examples and synthesizenovel-view instances to enrich the visual references. Extensive experimentsdemonstrate that our method significantly outperforms prior works, achieving>10 AP over previous results on two recently released challenging benchmarkdatasets in both conventional and novel instance detection settings.
  </details>

- **[Retrieval Backward Attention without Additional Training: Enhance Embeddings of Large Language Models via Repetition](http://arxiv.org/abs/2502.20726v2)**  `arXiv:2502.20726`  `cs.CL` `cs.LG`  
  _Yifei Duan, Raphael Shang, Deng Liang, Yongqiang Cai_
  <details open><summary>Abstract</summary>
  Language models can be viewed as functions that embed text into Euclideanspace, where the quality of the embedding vectors directly determines modelperformance, training such neural networks involves various uncertainties. Thispaper focuses on improving the performance of pre-trained language models inzero-shot settings through a simple and easily implementable method. We proposea novel backward attention mechanism to enhance contextual informationencoding. Evaluated on the Chinese Massive Text Embedding Benchmark (C-MTEB),our approach achieves significant improvements across multiple tasks, providingvaluable insights for advancing zero-shot learning capabilities.
  </details>

- **[Foot-In-The-Door: A Multi-turn Jailbreak for LLMs](http://arxiv.org/abs/2502.19820v3)**  `arXiv:2502.19820`  `cs.CL` `cs.AI`  
  _Zixuan Weng, Xiaolong Jin, Jinyuan Jia, Xiangyu Zhang_
  <details open><summary>Abstract</summary>
  Ensuring AI safety is crucial as large language models become increasinglyintegrated into real-world applications. A key challenge is jailbreak, whereadversarial prompts bypass built-in safeguards to elicit harmful disallowedoutputs. Inspired by psychological foot-in-the-door principles, we introduceFITD,a novel multi-turn jailbreak method that leverages the phenomenon whereminor initial commitments lower resistance to more significant or moreunethical transgressions. Our approach progressively escalates the maliciousintent of user queries through intermediate bridge prompts and aligns themodel's response by itself to induce toxic responses. Extensive experimentalresults on two jailbreak benchmarks demonstrate that FITD achieves an averageattack success rate of 94% across seven widely used models, outperformingexisting state-of-the-art methods. Additionally, we provide an in-depthanalysis of LLM self-corruption, highlighting vulnerabilities in currentalignment strategies and emphasizing the risks inherent in multi-turninteractions. The code is available athttps://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak.
  </details>

- **[SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines](http://arxiv.org/abs/2502.14739v4)**  `arXiv:2502.14739`  `cs.CL`  
  _M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) have demonstrated remarkable proficiency inmainstream academic disciplines such as mathematics, physics, and computerscience. However, human knowledge encompasses over 200 specialized disciplines,far exceeding the scope of existing benchmarks. The capabilities of LLMs inmany of these specialized fields-particularly in light industry, agriculture,and service-oriented disciplines-remain inadequately evaluated. To address thisgap, we present SuperGPQA, a comprehensive benchmark that evaluatesgraduate-level knowledge and reasoning capabilities across 285 disciplines. Ourbenchmark employs a novel Human-LLM collaborative filtering mechanism toeliminate trivial or ambiguous questions through iterative refinement based onboth LLM responses and expert feedback. Our experimental results revealsignificant room for improvement in the performance of current state-of-the-artLLMs across diverse knowledge domains (e.g., the reasoning-focused modelDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlightingthe considerable gap between current model capabilities and artificial generalintelligence. Additionally, we present comprehensive insights from ourmanagement of a large-scale annotation process, involving over 80 expertannotators and an interactive Human-LLM collaborative system, offering valuablemethodological guidance for future research initiatives of comparable scope.
  </details>

- **[Visual Agentic AI for Spatial Reasoning with a Dynamic API](http://arxiv.org/abs/2502.06787v2)**  `arXiv:2502.06787`  `cs.CV`  
  _Damiano Marsili, Rohun Agrawal, Yisong Yue, Georgia Gkioxari_
  <details open><summary>Abstract</summary>
  Visual reasoning -- the ability to interpret the visual world -- is crucialfor embodied agents that operate within three-dimensional scenes. Progress inAI has led to vision and language models capable of answering questions fromimages. However, their performance declines when tasked with 3D spatialreasoning. To tackle the complexity of such reasoning problems, we introduce anagentic program synthesis approach where LLM agents collaboratively generate aPythonic API with new functions to solve common subproblems. Our methodovercomes limitations of prior approaches that rely on a static, human-definedAPI, allowing it to handle a wider range of queries. To assess AI capabilitiesfor 3D understanding, we introduce a new benchmark of queries involvingmultiple steps of grounding and inference. We show that our method outperformsprior zero-shot models for visual reasoning in 3D and empirically validate theeffectiveness of our agentic framework for 3D spatial reasoning tasks. Projectwebsite: https://glab-caltech.github.io/vadar/
  </details>

- **[TADFormer : Task-Adaptive Dynamic Transformer for Efficient Multi-Task Learning](http://arxiv.org/abs/2501.04293v2)**  `arXiv:2501.04293`  `cs.CV`  
  _Seungmin Baek, Soyul Lee, Hayeon Jo, Hyesong Choi, Dongbo Min_
  <details open><summary>Abstract</summary>
  Transfer learning paradigm has driven substantial advancements in variousvision tasks. However, as state-of-the-art models continue to grow, classicalfull fine-tuning often becomes computationally impractical, particularly inmulti-task learning (MTL) setup where training complexity increasesproportional to the number of tasks. Consequently, recent studies have exploredParameter-Efficient Fine-Tuning (PEFT) for MTL architectures. Despite someprogress, these approaches still exhibit limitations in capturing fine-grained,task-specific features that are crucial to MTL. In this paper, we introduceTask-Adaptive Dynamic transFormer, termed TADFormer, a novel PEFT frameworkthat performs task-aware feature adaptation in the fine-grained manner bydynamically considering task-specific input contexts. TADFormer proposes theparameter-efficient prompting for task adaptation and the Dynamic Task Filter(DTF) to capture task information conditioned on input contexts. Experiments onthe PASCAL-Context benchmark demonstrate that the proposed method achieveshigher accuracy in dense scene understanding tasks, while reducing the numberof trainable parameters by up to 8.4 times when compared to full fine-tuning ofMTL models. TADFormer also demonstrates superior parameter efficiency andaccuracy compared to recent PEFT methods.
  </details>

- **[Find Any Part in 3D](http://arxiv.org/abs/2411.13550v2)**  `arXiv:2411.13550`  `cs.CV`  
  _Ziqi Ma, Yisong Yue, Georgia Gkioxari_
  <details open><summary>Abstract</summary>
  Why don't we have foundation models in 3D yet? A key limitation is datascarcity. For 3D object part segmentation, existing datasets are small in sizeand lack diversity. We show that it is possible to break this data barrier bybuilding a data engine powered by 2D foundation models. Our data engineautomatically annotates any number of object parts: 1755x more unique parttypes than existing datasets combined. By training on our annotated data with asimple contrastive objective, we obtain an open-world model that generalizes toany part in any object based on any text query. Even when evaluated zero-shot,we outperform existing methods on the datasets they train on. We achieve 260%improvement in mIoU and boost speed by 6x to 300x. Our scaling analysisconfirms that this generalization stems from the data scale, which underscoresthe impact of our data engine. Finally, to advance general-category open-world3D part segmentation, we release a benchmark covering a wide range of objectsand parts. Project website: https://ziqi-ma.github.io/find3dsite/
  </details>

- **[Stacking Brick by Brick: Aligned Feature Isolation for Incremental Face Forgery Detection](http://arxiv.org/abs/2411.11396v3)**  `arXiv:2411.11396`  `cs.CV`  
  _Jikang Cheng, Zhiyuan Yan, Ying Zhang, Li Hao, Jiaxin Ai, Qin Zou, et al._
  <details open><summary>Abstract</summary>
  The rapid advancement of face forgery techniques has introduced a growingvariety of forgeries. Incremental Face Forgery Detection (IFFD), involvinggradually adding new forgery data to fine-tune the previously trained model,has been introduced as a promising strategy to deal with evolving forgerymethods. However, a naively trained IFFD model is prone to catastrophicforgetting when new forgeries are integrated, as treating all forgeries as asingle ''Fake" class in the Real/Fake classification can cause differentforgery types overriding one another, thereby resulting in the forgetting ofunique characteristics from earlier tasks and limiting the model'seffectiveness in learning forgery specificity and generality. In this paper, wepropose to stack the latent feature distributions of previous and new tasksbrick by brick, $\textit{i.e.}$, achieving $\textbf{aligned featureisolation}$. In this manner, we aim to preserve learned forgery information andaccumulate new knowledge by minimizing distribution overriding, therebymitigating catastrophic forgetting. To achieve this, we first introduce SparseUniform Replay (SUR) to obtain the representative subsets that could be treatedas the uniformly sparse versions of the previous global distributions. We thenpropose a Latent-space Incremental Detector (LID) that leverages SUR data toisolate and align distributions. For evaluation, we construct a more advancedand comprehensive benchmark tailored for IFFD. The leading experimental resultsvalidate the superiority of our method.
  </details>

- **[Do LLMs estimate uncertainty well in instruction-following?](http://arxiv.org/abs/2410.14582v4)**  `arXiv:2410.14582`  `cs.CL` `cs.AI`  
  _Juyeon Heo, Miao Xiong, Christina Heinze-Deml, Jaya Narain_
  <details open><summary>Abstract</summary>
  Large language models (LLMs) could be valuable personal AI agents acrossvarious domains, provided they can precisely follow user instructions. However,recent studies have shown significant limitations in LLMs'instruction-following capabilities, raising concerns about their reliability inhigh-stakes applications. Accurately estimating LLMs' uncertainty in adheringto instructions is critical to mitigating deployment risks. We present, to ourknowledge, the first systematic evaluation of the uncertainty estimationabilities of LLMs in the context of instruction-following. Our study identifieskey challenges with existing instruction-following benchmarks, where multiplefactors are entangled with uncertainty stems from instruction-following,complicating the isolation and comparison across methods and models. To addressthese issues, we introduce a controlled evaluation setup with two benchmarkversions of data, enabling a comprehensive comparison of uncertainty estimationmethods under various conditions. Our findings show that existing uncertaintymethods struggle, particularly when models make subtle errors in instructionfollowing. While internal model states provide some improvement, they remaininadequate in more complex scenarios. The insights from our controlledevaluation setups provide a crucial understanding of LLMs' limitations andpotential for uncertainty estimation in instruction-following tasks, paving theway for more trustworthy AI agents.
  </details>

- **[Frame-Voyager: Learning to Query Frames for Video Large Language Models](http://arxiv.org/abs/2410.03226v4)**  `arXiv:2410.03226`  `cs.CL` `cs.CV`  
  _Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, et al._
  <details open><summary>Abstract</summary>
  Video Large Language Models (Video-LLMs) have made remarkable progress invideo understanding tasks. However, they are constrained by the maximum lengthof input tokens, making it impractical to input entire videos. Existing frameselection approaches, such as uniform frame sampling and text-frame retrieval,fail to account for the information density variations in the videos or thecomplex instructions in the tasks, leading to sub-optimal performance. In thispaper, we propose Frame-Voyager that learns to query informative framecombinations, based on the given textual queries in the task. To trainFrame-Voyager, we introduce a new data collection and labeling pipeline, byranking frame combinations using a pre-trained Video-LLM. Given a video of Mframes, we traverse its T-frame combinations, feed them into a Video-LLM, andrank them based on Video-LLM's prediction losses. Using this ranking assupervision, we train Frame-Voyager to query the frame combinations with lowerlosses. In experiments, we evaluate Frame-Voyager on four Video QuestionAnswering benchmarks by plugging it into two different Video-LLMs. Theexperimental results demonstrate that Frame-Voyager achieves impressive resultsin all settings, highlighting its potential as a plug-and-play solution forVideo-LLMs.
  </details>

- **[DRExplainer: Quantifiable Interpretability in Drug Response Prediction with Directed Graph Convolutional Network](http://arxiv.org/abs/2408.12139v2)**  `arXiv:2408.12139`  `cs.AI` `cs.LG`  
  _Haoyuan Shi, Tao Xu, Xiaodi Li, Qian Gao, Zhiwei Xiong, Junfeng Xia, et al._
  <details open><summary>Abstract</summary>
  Predicting the response of a cancer cell line to a therapeutic drug ispivotal for personalized medicine. Despite numerous deep learning methods thathave been developed for drug response prediction, integrating diverseinformation about biological entities and predicting the directional responseremain major challenges. Here, we propose a novel interpretable predictivemodel, DRExplainer, which leverages a directed graph convolutional network toenhance the prediction in a directed bipartite network framework. DRExplainerconstructs a directed bipartite network integrating multi-omics profiles ofcell lines, the chemical structure of drugs and known drug response to achievedirected prediction. Then, DRExplainer identifies the most relevant subgraph toeach prediction in this directed bipartite network by learning a mask,facilitating critical medical decision-making. Additionally, we introduce aquantifiable method for model interpretability that leverages a ground truthbenchmark dataset curated from biological features. In computationalexperiments, DRExplainer outperforms state-of-the-art predictive methods andanother graph-based explanation method under the same experimental setting.Finally, the case studies further validate the interpretability and theeffectiveness of DRExplainer in predictive novel drug response. Our code isavailable at: https://github.com/vshy-dream/DRExplainer.
  </details>

- **[A Comprehensive Review of Few-shot Action Recognition](http://arxiv.org/abs/2407.14744v2)**  `arXiv:2407.14744`  `cs.CV`  
  _Yuyang Wanyan, Xiaoshan Yang, Weiming Dong, Changsheng Xu_
  <details open><summary>Abstract</summary>
  Few-shot action recognition aims to address the high cost and impracticalityof manually labeling complex and variable video data in action recognition. Itrequires accurately classifying human actions in videos using only a fewlabeled examples per class. Compared to few-shot learning in image scenarios,few-shot action recognition is more challenging due to the intrinsic complexityof video data. Numerous approaches have driven significant advancements infew-shot action recognition, which underscores the need for a comprehensivesurvey. Unlike early surveys that focus on few-shot image or textclassification, we deeply consider the unique challenges of few-shot actionrecognition. In this survey, we provide a comprehensive review of recentmethods and introduce a novel and systematic taxonomy of existing approaches,accompanied by a detailed analysis. We categorize the methods intogenerative-based and meta-learning frameworks, and further elaborate on themethods within the meta-learning framework, covering aspects: video instancerepresentation, category prototype learning, and generalized video alignment.Additionally, the survey presents the commonly used benchmarks and discussesrelevant advanced topics and promising future directions. We hope this surveycan serve as a valuable resource for researchers, offering essential guidanceto newcomers and stimulating seasoned researchers with fresh insights.
  </details>

- **[Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models](http://arxiv.org/abs/2405.12523v3)**  `arXiv:2405.12523`  `cs.CV` `cs.AI`  
  _Jiaqi Li, Qianshan Wei, Chuanyi Zhang, Guilin Qi, Miaozeng Du, Yongrui Chen, et al._
  <details open><summary>Abstract</summary>
  Machine unlearning empowers individuals with the `right to be forgotten' byremoving their private or sensitive information encoded in machine learningmodels. However, it remains uncertain whether MU can be effectively applied toMultimodal Large Language Models (MLLMs), particularly in scenarios offorgetting the leaked visual data of concepts. To overcome the challenge, wepropose an efficient method, Single Image Unlearning (SIU), to unlearn thevisual recognition of a concept by fine-tuning a single associated image forfew steps. SIU consists of two key aspects: (i) Constructing Multifacetedfine-tuning data. We introduce four targets, based on which we constructfine-tuning data for the concepts to be forgotten; (ii) Jointly training loss.To synchronously forget the visual recognition of concepts and preserve theutility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergenceLoss combined with Cross Entropy loss. Alongside our method, we establishMMUBench, a new benchmark for MU in MLLMs and introduce a collection of metricsfor its evaluation. Experimental results on MMUBench show that SIU completelysurpasses the performance of existing methods. Furthermore, we surprisinglyfind that SIU can avoid invasive membership inference attacks and jailbreakattacks. To the best of our knowledge, we are the first to explore MU in MLLMs.We will release the code and benchmark in the near future.
  </details>
