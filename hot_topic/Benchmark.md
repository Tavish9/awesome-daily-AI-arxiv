# üîç Benchmark Papers ¬∑ 2025-03-31

[![Total Papers](https://img.shields.io/badge/Papers-60-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Benchmark`  
**Filter**: `None`

---

## üìö Paper List

- **[UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving](http://arxiv.org/abs/2503.24381v1)**  `arXiv:2503.24381`  `cs.LG` `cs.RO` `cs.CV` `cs.AI` `cs.MA`  
  _Yuping Wang, Xiangyu Huang, Xiaokang Sun, Mingxuan Yan, Shuo Xing, Zhengzhong Tu, et al._
  <details open><summary>Abstract</summary>
  We introduce UniOcc, a comprehensive, unified benchmark for occupancyforecasting (i.e., predicting future occupancies based on historicalinformation) and current-frame occupancy prediction from camera images. UniOccunifies data from multiple real-world datasets (i.e., nuScenes, Waymo) andhigh-fidelity driving simulators (i.e., CARLA, OpenCOOD), which provides 2D/3Doccupancy labels with per-voxel flow annotations and support for cooperativeautonomous driving. In terms of evaluation, unlike existing studies that relyon suboptimal pseudo labels for evaluation, UniOcc incorporates novel metricsthat do not depend on ground-truth occupancy, enabling robust assessment ofadditional aspects of occupancy quality. Through extensive experiments onstate-of-the-art models, we demonstrate that large-scale, diverse training dataand explicit flow information significantly enhance occupancy prediction andforecasting performance.
  </details>

- **[Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](http://arxiv.org/abs/2503.24376v1)**  `arXiv:2503.24376`  `cs.LG` `cs.CV` `cs.CL` `cs.AI`  
  _Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in Chain of Thought (COT) generation have significantlyimproved the reasoning capabilities of Large Language Models (LLMs), withreinforcement learning (RL) emerging as an effective post-training approach.Multimodal Large Language Models (MLLMs) inherit this reasoning potential butremain underexplored in tasks requiring both perception and logical reasoning.To address this, we introduce SEED-Bench-R1, a benchmark designed tosystematically evaluate post-training methods for MLLMs in video understanding.It includes intricate real-world videos and complex everyday planning tasks inthe format of multiple-choice questions, requiring sophisticated perception andreasoning. SEED-Bench-R1 assesses generalization through a three-levelhierarchy: in-distribution, cross-environment, and cross-environment-taskscenarios, equipped with a large-scale training dataset with easily verifiableground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RLwith supervised fine-tuning (SFT), demonstrating RL's data efficiency andsuperior performance on both in-distribution and out-of-distribution tasks,even outperforming SFT on general video understanding benchmarks likeLongVideoBench. Our detailed analysis reveals that RL enhances visualperception but often produces less logically coherent reasoning chains. Weidentify key limitations such as inconsistent reasoning and overlooked visualcues, and suggest future improvements in base model reasoning, reward modeling,and RL robustness against noisy signals.
  </details>

- **[ERUPT: Efficient Rendering with Unposed Patch Transformer](http://arxiv.org/abs/2503.24374v1)**  `arXiv:2503.24374`  `cs.CV`  
  _Maxim V. Shugaev, Vincent Chen, Maxim Karrenbach, Kyle Ashley, Bridget Kennedy, Naresh P. Cuntoor_
  <details open><summary>Abstract</summary>
  This work addresses the problem of novel view synthesis in diverse scenesfrom small collections of RGB images. We propose ERUPT (Efficient Renderingwith Unposed Patch Transformer) a state-of-the-art scene reconstruction modelcapable of efficient scene rendering using unposed imagery. We introducepatch-based querying, in contrast to existing pixel-based queries, to reducethe compute required to render a target view. This makes our model highlyefficient both during training and at inference, capable of rendering at 600fps on commercial hardware. Notably, our model is designed to use a learnedlatent camera pose which allows for training using unposed targets in datasetswith sparse or inaccurate ground truth camera pose. We show that our approachcan generalize on large real-world data and introduce a new benchmark dataset(MSVS-1M) for latent view synthesis using street-view imagery collected fromMapillary. In contrast to NeRF and Gaussian Splatting, which require denseimagery and precise metadata, ERUPT can render novel views of arbitrary sceneswith as few as five unposed input images. ERUPT achieves better rendered imagequality than current state-of-the-art methods for unposed image synthesistasks, reduces labeled data requirements by ~95\% and decreases computationalrequirements by an order of magnitude, providing efficient novel view synthesisfor diverse real-world scenes.
  </details>

- **[Evaluating machine learning models for predicting pesticides toxicity to honey bees](http://arxiv.org/abs/2503.24305v1)**  `arXiv:2503.24305`  `cs.LG` `cs.AI`  
  _Jakub Adamczyk, Jakub Poziemski, Pawel Siedlecki_
  <details open><summary>Abstract</summary>
  Small molecules play a critical role in the biomedical, environmental, andagrochemical domains, each with distinct physicochemical requirements andsuccess criteria. Although biomedical research benefits from extensive datasetsand established benchmarks, agrochemical data remain scarce, particularly withrespect to species-specific toxicity. This work focuses on ApisTox, the mostcomprehensive dataset of experimentally validated chemical toxicity to thehoney bee (\textit{Apis mellifera}), an ecologically vital pollinator. Weevaluate ApisTox using a diverse suite of machine learning approaches,including molecular fingerprints, graph kernels, and graph neural networks, aswell as pretrained models. Comparative analysis with medicinal datasets fromthe MoleculeNet benchmark reveals that ApisTox represents a distinct chemicalspace. Performance degradation on non-medicinal datasets, such as ApisTox,demonstrates their limited generalizability of current state-of-the-artalgorithms trained solely on biomedical data. Our study highlights the need formore diverse datasets and for targeted model development geared toward theagrochemical domain.
  </details>

- **[Order Matters: On Parameter-Efficient Image-to-Video Probing for Recognizing Nearly Symmetric Actions](http://arxiv.org/abs/2503.24298v1)**  `arXiv:2503.24298`  `cs.CV`  
  _Thinesh Thiyakesan Ponbagavathi, Alina Roitberg_
  <details open><summary>Abstract</summary>
  We study parameter-efficient image-to-video probing for the unaddressedchallenge of recognizing nearly symmetric actions - visually similar actionsthat unfold in opposite temporal order (e.g., opening vs. closing a bottle).Existing probing mechanisms for image-pretrained models, such as DinoV2 andCLIP, rely on attention mechanism for temporal modeling but are inherentlypermutation-invariant, leading to identical predictions regardless of frameorder. To address this, we introduce Self-attentive Temporal Embedding Probing(STEP), a simple yet effective approach designed to enforce temporalsensitivity in parameter-efficient image-to-video transfer. STEP enhancesself-attentive probing with three key modifications: (1) a learnable frame-wisepositional encoding, explicitly encoding temporal order; (2) a single globalCLS token, for sequence coherence; and (3) a simplified attention mechanism toimprove parameter efficiency. STEP outperforms existing image-to-video probingmechanisms by 3-15% across four activity recognition benchmarks with only 1/3of the learnable parameters. On two datasets, it surpasses all publishedmethods, including fully fine-tuned models. STEP shows a distinct advantage inrecognizing nearly symmetric actions, surpassing other probing mechanisms by9-19%. and parameter-heavier PEFT-based transfer methods by 5-15%. Code andmodels will be made publicly available.
  </details>

- **[Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model](http://arxiv.org/abs/2503.24290v1)**  `arXiv:2503.24290`  `cs.LG` `cs.CL`  
  _Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, Heung-Yeung Shum_
  <details open><summary>Abstract</summary>
  We introduce Open-Reasoner-Zero, the first open source implementation oflarge-scale reasoning-oriented RL training focusing on scalability, simplicityand accessibility. Through extensive experiments, we demonstrate that aminimalist approach, vanilla PPO with GAE ($\lambda=1$, $\gamma=1$) andstraightforward rule-based rewards, without any KL regularization, issufficient to scale up both response length and benchmark performance, similarto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model asDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance onAIME2024, MATH500, and the GPQA Diamond benchmark while demonstratingremarkable efficiency -- requiring only a tenth of the training steps, comparedto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release oursource code, parameter settings, training data, and model weights acrossvarious sizes.
  </details>

- **[PAARS: Persona Aligned Agentic Retail Shoppers](http://arxiv.org/abs/2503.24228v1)**  `arXiv:2503.24228`  `cs.MA` `cs.CL` `cs.AI`  
  _Saab Mansour, Leonardo Perelli, Lorenzo Mainetti, George Davidson, Stefano D'Amato_
  <details open><summary>Abstract</summary>
  In e-commerce, behavioral data is collected for decision making which can becostly and slow. Simulation with LLM powered agents is emerging as a promisingalternative for representing human population behavior. However, LLMs are knownto exhibit certain biases, such as brand bias, review rating bias and limitedrepresentation of certain groups in the population, hence they need to becarefully benchmarked and aligned to user behavior. Ultimately, our goal is tosynthesise an agent population and verify that it collectively approximates areal sample of humans. To this end, we propose a framework that: (i) createssynthetic shopping agents by automatically mining personas from anonymisedhistorical shopping data, (ii) equips agents with retail-specific tools tosynthesise shopping sessions and (iii) introduces a novel alignment suitemeasuring distributional differences between humans and shopping agents at thegroup (i.e. population) level rather than the traditional "individual" level.Experimental results demonstrate that using personas improves performance onthe alignment suite, though a gap remains to human behaviour. We showcase aninitial application of our framework for automated agentic A/B testing andcompare the findings to human results. Finally, we discuss applications,limitations and challenges setting the stage for impactful future work.
  </details>

- **[CIBR: Cross-modal Information Bottleneck Regularization for Robust CLIP Generalization](http://arxiv.org/abs/2503.24182v1)**  `arXiv:2503.24182`  `cs.CV`  
  _Yingrui Ji, Xi Xiao, Gaofei Chen, Hao Xu, Chenrui Ma, Lijing Zhu, et al._
  <details open><summary>Abstract</summary>
  Contrastive Language-Image Pretraining (CLIP) has achieved remarkable successin cross-modal tasks such as zero-shot image classification and text-imageretrieval by effectively aligning visual and textual representations. However,the theoretical foundations underlying CLIP's strong generalization remainunclear. In this work, we address this gap by proposing the Cross-modalInformation Bottleneck (CIB) framework. CIB offers a principled interpretationof CLIP's contrastive learning objective as an implicit Information Bottleneckoptimization. Under this view, the model maximizes shared cross-modalinformation while discarding modality-specific redundancies, thereby preservingessential semantic alignment across modalities. Building on this insight, weintroduce a Cross-modal Information Bottleneck Regularization (CIBR) methodthat explicitly enforces these IB principles during training. CIBR introduces apenalty term to discourage modality-specific redundancy, thereby enhancingsemantic alignment between image and text features. We validate CIBR onextensive vision-language benchmarks, including zero-shot classification acrossseven diverse image datasets and text-image retrieval on MSCOCO and Flickr30K.The results show consistent performance gains over standard CLIP. Thesefindings provide the first theoretical understanding of CLIP's generalizationthrough the IB lens. They also demonstrate practical improvements, offeringguidance for future cross-modal representation learning.
  </details>

- **[CTSketch: Compositional Tensor Sketching for Scalable Neurosymbolic Learning](http://arxiv.org/abs/2503.24123v1)**  `arXiv:2503.24123`  `cs.LG`  
  _Seewon Choi, Alaia Solko-Breslin, Rajeev Alur, Eric Wong_
  <details open><summary>Abstract</summary>
  Many computational tasks benefit from being formulated as the composition ofneural networks followed by a discrete symbolic program. The goal ofneurosymbolic learning is to train the neural networks using only end-to-endinput-output labels of the composite. We introduce CTSketch, a novel, scalableneurosymbolic learning algorithm. CTSketch uses two techniques to improve thescalability of neurosymbolic inference: decompose the symbolic program intosub-programs and summarize each sub-program with a sketched tensor. Thisstrategy allows us to approximate the output distribution of the program withsimple tensor operations over the input distributions and summaries. We providetheoretical insight into the maximum error of the approximation. Furthermore,we evaluate CTSketch on many benchmarks from the neurosymbolic literature,including some designed for evaluating scalability. Our results show thatCTSketch pushes neurosymbolic learning to new scales that have previously beenunattainable by obtaining high accuracy on tasks involving over one thousandinputs.
  </details>

- **[PolypSegTrack: Unified Foundation Model for Colonoscopy Video Analysis](http://arxiv.org/abs/2503.24108v1)**  `arXiv:2503.24108`  `cs.CV` `cs.AI`  
  _Anwesa Choudhuri, Zhongpai Gao, Meng Zheng, Benjamin Planche, Terrence Chen, Ziyan Wu_
  <details open><summary>Abstract</summary>
  Early detection, accurate segmentation, classification and tracking of polypsduring colonoscopy are critical for preventing colorectal cancer. Many existingdeep-learning-based methods for analyzing colonoscopic videos either requiretask-specific fine-tuning, lack tracking capabilities, or rely ondomain-specific pre-training. In this paper, we introduce\textit{PolypSegTrack}, a novel foundation model that jointly addresses polypdetection, segmentation, classification and unsupervised tracking incolonoscopic videos. Our approach leverages a novel conditional mask loss,enabling flexible training across datasets with either pixel-level segmentationmasks or bounding box annotations, allowing us to bypass task-specificfine-tuning. Our unsupervised tracking module reliably associates polypinstances across frames using object queries, without relying on anyheuristics. We leverage a robust vision foundation model backbone that ispre-trained unsupervisedly on natural images, thereby removing the need fordomain-specific pre-training. Extensive experiments on multiple polypbenchmarks demonstrate that our method significantly outperforms existingstate-of-the-art approaches in detection, segmentation, classification, andtracking.
  </details>

- **[Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?](http://arxiv.org/abs/2503.24102v1)**  `arXiv:2503.24102`  `cs.CL`  
  _Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, et al._
  <details open><summary>Abstract</summary>
  Low-Resource Languages (LRLs) present significant challenges in naturallanguage processing due to their limited linguistic resources andunderrepresentation in standard datasets. While recent advancements in LargeLanguage Models (LLMs) and Neural Machine Translation (NMT) have substantiallyimproved translation capabilities for high-resource languages, performancedisparities persist for LRLs, particularly impacting privacy-sensitive andresource-constrained scenarios. This paper systematically evaluates thelimitations of current LLMs across 200 languages using benchmarks such asFLORES-200. We also explore alternative data sources, including news articlesand bilingual dictionaries, and demonstrate how knowledge distillation fromlarge pre-trained models can significantly improve smaller LRL translations.Additionally, we investigate various fine-tuning strategies, revealing thatincremental enhancements markedly reduce performance gaps on smaller LLMs.
  </details>

- **[COSMO: Combination of Selective Memorization for Low-cost Vision-and-Language Navigation](http://arxiv.org/abs/2503.24065v1)**  `arXiv:2503.24065`  `cs.CV` `cs.RO`  
  _Siqi Zhang, Yanyuan Qiao, Qunbo Wang, Zike Yan, Qi Wu, Zhihua Wei, et al._
  <details open><summary>Abstract</summary>
  Vision-and-Language Navigation (VLN) tasks have gained prominence withinartificial intelligence research due to their potential application in fieldslike home assistants. Many contemporary VLN approaches, while based ontransformer architectures, have increasingly incorporated additional componentssuch as external knowledge bases or map information to enhance performance.These additions, while boosting performance, also lead to larger models andincreased computational costs. In this paper, to achieve both high performanceand low computational costs, we propose a novel architecture with theCOmbination of Selective MemOrization (COSMO). Specifically, COSMO integratesstate-space modules and transformer modules, and incorporates twoVLN-customized selective state space modules: the Round Selective Scan (RSS)and the Cross-modal Selective State Space Module (CS3). RSS facilitatescomprehensive inter-modal interactions within a single scan, while the CS3module adapts the selective state space module into a dual-stream architecture,thereby enhancing the acquisition of cross-modal interactions. Experimentalvalidations on three mainstream VLN benchmarks, REVERIE, R2R, and R2R-CE, notonly demonstrate competitive navigation performance of our model but also showa significant reduction in computational costs.
  </details>

- **[Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents](http://arxiv.org/abs/2503.24047v1)**  `arXiv:2503.24047`  `cs.MA` `cs.AI`  
  _Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, Jiajun Zhang_
  <details open><summary>Abstract</summary>
  As scientific research becomes increasingly complex, innovative tools areneeded to manage vast data, facilitate interdisciplinary collaboration, andaccelerate discovery. Large language models (LLMs) are now evolving intoLLM-based scientific agents that automate critical tasks, ranging fromhypothesis generation and experiment design to data analysis and simulation.Unlike general-purpose LLMs, these specialized agents integrate domain-specificknowledge, advanced tool sets, and robust validation mechanisms, enabling themto handle complex data types, ensure reproducibility, and drive scientificbreakthroughs. This survey provides a focused review of the architectures,design, benchmarks, applications, and ethical considerations surroundingLLM-based scientific agents. We highlight why they differ from general agentsand the ways in which they advance research across various scientific fields.By examining their development and challenges, this survey offers acomprehensive roadmap for researchers and practitioners to harness these agentsfor more efficient, reliable, and ethically sound scientific discovery.
  </details>

- **[Pay More Attention to the Robustness of Prompt for Instruction Data Mining](http://arxiv.org/abs/2503.24028v1)**  `arXiv:2503.24028`  `cs.AI`  
  _Qiang Wang, Dawei Feng, Xu Zhang, Ao Shen, Yang Xu, Bo Ding, et al._
  <details open><summary>Abstract</summary>
  Instruction tuning has emerged as a paramount method for tailoring thebehaviors of LLMs. Recent work has unveiled the potential for LLMs to achievehigh performance through fine-tuning with a limited quantity of high-qualityinstruction data. Building upon this approach, we further explore the impact ofprompt's robustness on the selection of high-quality instruction data. Thispaper proposes a pioneering framework of high-quality online instruction datamining for instruction tuning, focusing on the impact of prompt's robustness onthe data mining process. Our notable innovation, is to generate the adversarialinstruction data by conducting the attack for the prompt of online instructiondata. Then, we introduce an Adversarial Instruction-Following Difficulty metricto measure how much help the adversarial instruction data can provide to thegeneration of the corresponding response. Apart from it, we propose a novelAdversarial Instruction Output Embedding Consistency approach to selecthigh-quality online instruction data. We conduct extensive experiments on twobenchmark datasets to assess the performance. The experimental results serve tounderscore the effectiveness of our proposed two methods. Moreover, the resultsunderscore the critical practical significance of considering prompt'srobustness.
  </details>

- **[H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding](http://arxiv.org/abs/2503.24008v1)**  `arXiv:2503.24008`  `cs.CV` `cs.AI`  
  _Qi Wu, Quanlong Zheng, Yanhao Zhang, Junlin Xie, Jinguo Luo, Kuo Wang, et al._
  <details open><summary>Abstract</summary>
  With the rapid development of multimodal models, the demand for assessingvideo understanding capabilities has been steadily increasing. However,existing benchmarks for evaluating video understanding exhibit significantlimitations in coverage, task diversity, and scene adaptability. Theseshortcomings hinder the accurate assessment of models' comprehensive videounderstanding capabilities. To tackle this challenge, we propose a hierarchicaland holistic video understanding (H2VU) benchmark designed to evaluate bothgeneral video and online streaming video comprehension. This benchmarkcontributes three key features:  Extended video duration: Spanning videos from brief 3-second clips tocomprehensive 1.5-hour recordings, thereby bridging the temporal gaps found incurrent benchmarks. Comprehensive assessment tasks: Beyond traditionalperceptual and reasoning tasks, we have introduced modules forcountercommonsense comprehension and trajectory state tracking. These additionstest the models' deep understanding capabilities beyond mere prior knowledge.Enriched video data: To keep pace with the rapid evolution of current AIagents, we have expanded first-person streaming video datasets. This expansionallows for the exploration of multimodal models' performance in understandingstreaming videos from a first-person perspective. Extensive results from H2VUreveal that existing multimodal large language models (MLLMs) possesssubstantial potential for improvement in our newly proposed evaluation tasks.We expect that H2VU will facilitate advancements in video understandingresearch by offering a comprehensive and in-depth analysis of MLLMs.
  </details>

- **[Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving](http://arxiv.org/abs/2503.24000v1)**  `arXiv:2503.24000`  `cs.LG` `cs.AI`  
  _Wei Gao, Xinyu Zhou, Peng Sun, Tianwei Zhang, Yonggang Wen_
  <details open><summary>Abstract</summary>
  Key-Value cache (\texttt{KV} \texttt{cache}) compression has emerged as apromising technique to optimize Large Language Model (LLM) serving. Itprimarily decreases the memory consumption of \texttt{KV} \texttt{cache} toreduce the computation cost. Despite the development of many compressionalgorithms, their applications in production environments are still notprevalent. In this paper, we revisit mainstream \texttt{KV} \texttt{cache}compression solutions from a practical perspective. Our contributions arethree-fold. First, we comprehensively review existing algorithmic designs andbenchmark studies for \texttt{KV} \texttt{cache} compression and identifymissing pieces in their performance measurement, which could hinder theiradoption in practice. Second, we empirically evaluate representative\texttt{KV} \texttt{cache} compression methods to uncover two key issues thataffect the computational efficiency: (1) while compressing \texttt{KV}\texttt{cache} can reduce memory consumption, current implementations (e.g.,FlashAttention, PagedAttention) do not optimize for production-level LLMserving, resulting in suboptimal throughput performance; (2) compressing\texttt{KV} \texttt{cache} may lead to longer outputs, resulting in increasedend-to-end latency. We further investigate the accuracy performance ofindividual samples rather than the overall performance, revealing the intrinsiclimitations in \texttt{KV} \texttt{cache} compression when handling specificLLM tasks. Third, we provide tools to shed light on future \texttt{KV}\texttt{cache} compression studies and facilitate their practical deployment inproduction. They are open-sourced in\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.
  </details>

- **[BeMERC: Behavior-Aware MLLM-based Framework for Multimodal Emotion Recognition in Conversation](http://arxiv.org/abs/2503.23990v1)**  `arXiv:2503.23990`  `cs.CL`  
  _Yumeng Fu, Junjie Wu, Zhongjie Wang, Meishan Zhang, Yulin Wu, Bingquan Liu_
  <details open><summary>Abstract</summary>
  Multimodal emotion recognition in conversation (MERC), the task ofidentifying the emotion label for each utterance in a conversation, is vitalfor developing empathetic machines. Current MLLM-based MERC studies focusmainly on capturing the speaker's textual or vocal characteristics, but ignorethe significance of video-derived behavior information. Different from text andaudio inputs, learning videos with rich facial expression, body language andposture, provides emotion trigger signals to the models for more accurateemotion predictions. In this paper, we propose a novel behavior-awareMLLM-based framework (BeMERC) to incorporate speaker's behaviors, includingsubtle facial micro-expression, body language and posture, into a vanillaMLLM-based MERC model, thereby facilitating the modeling of emotional dynamicsduring a conversation. Furthermore, BeMERC adopts a two-stage instructiontuning strategy to extend the model to the conversations scenario forend-to-end training of a MERC predictor. Experiments demonstrate that BeMERCachieves superior performance than the state-of-the-art methods on twobenchmark datasets, and also provides a detailed discussion on the significanceof video-derived behavior information in MERC.
  </details>

- **[A Reactive Framework for Whole-Body Motion Planning of Mobile Manipulators Combining Reinforcement Learning and SDF-Constrained Quadratic Programmi](http://arxiv.org/abs/2503.23975v1)**  `arXiv:2503.23975`  `cs.RO`  
  _Chenyu Zhang, Shiying Sun, Kuan Liu, Chuanbao Zhou, Xiaoguang Zhao, Min Tan, et al._
  <details open><summary>Abstract</summary>
  As an important branch of embodied artificial intelligence, mobilemanipulators are increasingly applied in intelligent services, but theirredundant degrees of freedom also limit efficient motion planning in clutteredenvironments. To address this issue, this paper proposes a hybrid learning andoptimization framework for reactive whole-body motion planning of mobilemanipulators. We develop the Bayesian distributional soft actor-critic(Bayes-DSAC) algorithm to improve the quality of value estimation and theconvergence performance of the learning. Additionally, we introduce a quadraticprogramming method constrained by the signed distance field to enhance thesafety of the obstacle avoidance motion. We conduct experiments and makecomparison with standard benchmark. The experimental results verify that ourproposed framework significantly improves the efficiency of reactive whole-bodymotion planning, reduces the planning time, and improves the success rate ofmotion planning. Additionally, the proposed reinforcement learning methodensures a rapid learning process in the whole-body planning task. The novelframework allows mobile manipulators to adapt to complex environments moresafely and efficiently.
  </details>

- **[A Benchmark for Vision-Centric HD Mapping by V2I Systems](http://arxiv.org/abs/2503.23963v1)**  `arXiv:2503.23963`  `cs.CV` `cs.RO`  
  _Miao Fan, Shanshan Yu, Shengtong Xu, Kun Jiang, Haoyi Xiong, Xiangzeng Liu_
  <details open><summary>Abstract</summary>
  Autonomous driving faces safety challenges due to a lack of globalperspective and the semantic information of vectorized high-definition (HD)maps. Information from roadside cameras can greatly expand the map perceptionrange through vehicle-to-infrastructure (V2I) communications. However, there isstill no dataset from the real world available for the study on mapvectorization onboard under the scenario of vehicle-infrastructure cooperation.To prosper the research on online HD mapping for Vehicle-InfrastructureCooperative Autonomous Driving (VICAD), we release a real-world dataset, whichcontains collaborative camera frames from both vehicles and roadsideinfrastructures, and provides human annotations of HD map elements. We alsopresent an end-to-end neural framework (i.e., V2I-HD) leveraging vision-centricV2I systems to construct vectorized maps. To reduce computation costs andfurther deploy V2I-HD on autonomous vehicles, we introduce a directionallydecoupled self-attention mechanism to V2I-HD. Extensive experiments show thatV2I-HD has superior performance in real-time inference speed, as tested by ourreal-world dataset. Abundant qualitative results also demonstrate stable androbust map construction quality with low cost in complex and various drivingscenes. As a benchmark, both source codes and the dataset have been released atOneDrive for the purpose of further study.
  </details>

- **[AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference](http://arxiv.org/abs/2503.23956v1)**  `arXiv:2503.23956`  `cs.CV` `cs.AI`  
  _Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang_
  <details open><summary>Abstract</summary>
  Recent advancements in Large Visual Language Models (LVLMs) have gainedsignificant attention due to their remarkable reasoning capabilities andproficiency in generalization. However, processing a large number of visualtokens and generating long-context outputs impose substantial computationaloverhead, leading to excessive demands for key-value (KV) cache. To addressthis critical bottleneck, we propose AirCache, a novel KV cache compressionmethod aimed at accelerating LVLMs inference. This work systematicallyinvestigates the correlations between visual and textual tokens within theattention mechanisms of LVLMs. Our empirical analysis reveals considerableredundancy in cached visual tokens, wherein strategically eliminating thesetokens preserves model performance while significantly accelerating contextgeneration. Inspired by these findings, we introduce an elite observationwindow for assessing the importance of visual components in the KV cache,focusing on stable inter-modal relevancy modeling with enhancedmulti-perspective consistency. Additionally, we develop an adaptive layer-wisebudget allocation strategy that capitalizes on the strength and skewness oftoken importance distribution, showcasing superior efficiency compared touniform allocation. Comprehensive evaluations across multiple LVLMs andbenchmarks demonstrate that our method achieves comparable performance to thefull cache while retaining only 10% of visual KV cache, thereby reducingdecoding latency by 29% to 66% across various batch size and prompt length ofinputs. Notably, as cache retention rates decrease, our method exhibitsincreasing performance advantages over existing approaches.
  </details>

- **[JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation](http://arxiv.org/abs/2503.23951v1)**  `arXiv:2503.23951`  `cs.CV`  
  _Fangda Chen, Shanshan Zhao, Chuanfu Xu, Long Lan_
  <details open><summary>Abstract</summary>
  Recent text-to-video advancements have enabled coherent video synthesis fromprompts and expanded to fine-grained control over appearance and motion.However, existing methods either suffer from concept interference due tofeature domain mismatch caused by naive decoupled optimizations or exhibitappearance contamination induced by spatial feature leakage resulting from theentanglement of motion and appearance in reference video reconstructions. Inthis paper, we propose JointTuner, a novel adaptive joint training framework,to alleviate these issues. Specifically, we develop Adaptive LoRA, whichincorporates a context-aware gating mechanism, and integrate the gated LoRAcomponents into the spatial and temporal Transformers within the diffusionmodel. These components enable simultaneous optimization of appearance andmotion, eliminating concept interference. In addition, we introduce theAppearance-independent Temporal Loss, which decouples motion patterns fromintrinsic appearance in reference video reconstructions through anappearance-agnostic noise prediction task. The key innovation lies in addingframe-wise offset noise to the ground-truth Gaussian noise, perturbing itsdistribution, thereby disrupting spatial attributes associated with frameswhile preserving temporal coherence. Furthermore, we construct a benchmarkcomprising 90 appearance-motion customized combinations and 10 multi-typeautomatic metrics across four dimensions, facilitating a more comprehensiveevaluation for this customization task. Extensive experiments demonstrate thesuperior performance of our method compared to current advanced approaches.
  </details>

- **[Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in Discriminative and Generative AI Operations](http://arxiv.org/abs/2503.23934v1)**  `arXiv:2503.23934`  `cs.LG` `cs.AI`  
  _Adri√°n S√°nchez-Momp√≥, Ioannis Mavromatis, Peizheng Li, Konstantinos Katsaros, Aftab Khan_
  <details open><summary>Abstract</summary>
  This study presents an empirical investigation into the energy consumption ofDiscriminative and Generative AI models within real-world MLOps pipelines. ForDiscriminative models, we examine various architectures and hyperparametersduring training and inference and identify energy-efficient practices. ForGenerative AI, Large Language Models (LLMs) are assessed, focusing primarily onenergy consumption across different model sizes and varying service requests.Our study employs software-based power measurements, ensuring ease ofreplication across diverse configurations, models, and datasets. We analysemultiple models and hardware setups to uncover correlations among variousmetrics, identifying key contributors to energy consumption. The resultsindicate that for Discriminative models, optimising architectures,hyperparameters, and hardware can significantly reduce energy consumptionwithout sacrificing performance. For LLMs, energy efficiency depends onbalancing model size, reasoning complexity, and request-handling capacity, aslarger models do not necessarily consume more energy when utilisation remainslow. This analysis provides practical guidelines for designing green andsustainable ML operations, emphasising energy consumption and carbon footprintreductions while maintaining performance. This paper can serve as a benchmarkfor accurately estimating total energy use across different types of AI models.
  </details>

- **[CoMatch: Dynamic Covisibility-Aware Transformer for Bilateral Subpixel-Level Semi-Dense Image Matching](http://arxiv.org/abs/2503.23925v1)**  `arXiv:2503.23925`  `cs.CV`  
  _Zizhuo Li, Yifan Lu, Linfeng Tang, Shihua Zhang, Jiayi Ma_
  <details open><summary>Abstract</summary>
  This prospective study proposes CoMatch, a novel semi-dense image matcherwith dynamic covisibility awareness and bilateral subpixel accuracy. Firstly,observing that modeling context interaction over the entire coarse feature mapelicits highly redundant computation due to the neighboring representationsimilarity of tokens, a covisibility-guided token condenser is introduced toadaptively aggregate tokens in light of their covisibility scores that aredynamically estimated, thereby ensuring computational efficiency whileimproving the representational capacity of aggregated tokens simultaneously.Secondly, considering that feature interaction with massive non-covisible areasis distracting, which may degrade feature distinctiveness, acovisibility-assisted attention mechanism is deployed to selectively suppressirrelevant message broadcast from non-covisible reduced tokens, resulting inrobust and compact attention to relevant rather than all ones. Thirdly, we findthat at the fine-level stage, current methods adjust only the target view'skeypoints to subpixel level, while those in the source view remain restrictedat the coarse level and thus not informative enough, detrimental to keypointlocation-sensitive usages. A simple yet potent fine correlation module isdeveloped to refine the matching candidates in both source and target views tosubpixel level, attaining attractive performance improvement. Thoroughexperimentation across an array of public benchmarks affirms CoMatch'spromising accuracy, efficiency, and generalizability.
  </details>

- **[Entropy-Based Adaptive Weighting for Self-Training](http://arxiv.org/abs/2503.23913v1)**  `arXiv:2503.23913`  `cs.CL`  
  _Xiaoxuan Wang, Yihe Deng, Mingyu Derek Ma, Wei Wang_
  <details open><summary>Abstract</summary>
  The mathematical problem-solving capabilities of large language models havebecome a focal point of research, with growing interests in leveragingself-generated reasoning paths as a promising way to refine and enhance thesemodels. These paths capture step-by-step logical processes while requiring onlythe correct answer for supervision. The self-training method has been shown tobe effective in reasoning tasks while eliminating the need for external modelsand manual annotations. However, optimizing the use of self-generated data formodel training remains an open challenge. In this work, we proposeEntropy-Based Adaptive Weighting for Self-Training (EAST), an adaptiveweighting strategy designed to prioritize uncertain data during self-training.Specifically, EAST employs a mapping function with a tunable parameter thatcontrols the sharpness of the weighting, assigning higher weights to data wherethe model exhibits greater uncertainty. This approach guides the model to focuson more informative and challenging examples, thereby enhancing its reasoningability. We evaluate our approach on GSM8K and MATH benchmarks. Empiricalresults show that, while the vanilla method yields virtually no improvement(0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K,EAST attains a further 1-2% performance boost compared to the vanilla method.
  </details>

- **[Conformal uncertainty quantification to evaluate predictive fairness of foundation AI model for skin lesion classes across patient demographics](http://arxiv.org/abs/2503.23819v1)**  `arXiv:2503.23819`  `cs.LG` `cs.CV` `cs.AI`  
  _Swarnava Bhattacharyya, Umapada Pal, Tapabrata Chakraborti_
  <details open><summary>Abstract</summary>
  Deep learning based diagnostic AI systems based on medical images arestarting to provide similar performance as human experts. However these datahungry complex systems are inherently black boxes and therefore slow to beadopted for high risk applications like healthcare. This problem of lack oftransparency is exacerbated in the case of recent large foundation models,which are trained in a self supervised manner on millions of data points toprovide robust generalisation across a range of downstream tasks, but theembeddings generated from them happen through a process that is notinterpretable, and hence not easily trustable for clinical applications. Toaddress this timely issue, we deploy conformal analysis to quantify thepredictive uncertainty of a vision transformer (ViT) based foundation modelacross patient demographics with respect to sex, age and ethnicity for thetasks of skin lesion classification using several public benchmark datasets.The significant advantage of this method is that conformal analysis is methodindependent and it not only provides a coverage guarantee at population levelbut also provides an uncertainty score for each individual. We used amodel-agnostic dynamic F1-score-based sampling during model training, whichhelped to stabilize the class imbalance and we investigate the effects onuncertainty quantification (UQ) with or without this bias mitigation step. Thuswe show how this can be used as a fairness metric to evaluate the robustness ofthe feature embeddings of the foundation model (Google DermFoundation) and thusadvance the trustworthiness and fairness of clinical AI.
  </details>

- **[Bridge the Gap Between Visual and Linguistic Comprehension for Generalized Zero-shot Semantic Segmentation](http://arxiv.org/abs/2503.23806v1)**  `arXiv:2503.23806`  `cs.CV`  
  _Xiaoqing Guo, Wuyang Li, Yixuan Yuan_
  <details open><summary>Abstract</summary>
  Generalized zero-shot semantic segmentation (GZS3) aims to achieve thehuman-level capability of segmenting not only seen classes but also novel classregions unseen in the training data through introducing the bridge of semanticrepresentations, e.g., word vector. While effective, the way of utilizing onesemantic representation to associate the corresponding class and to enable theknowledge transfer from seen to unseen classes is insufficient as well asincompatible with human cognition. Inspired by the observation that humansoften use some `part' and `state' information to comprehend the seen objectsand imagine unseen classes, we decouple each class into detailed descriptions,including object parts and states. Based on the decoupling formulation, wepropose a Decoupled Vision-Language Matching (DeVLMatch) framework, composed ofspatial-part (SPMatch) and channel-state (CSMatch) matching modules, for GZS3.In SPMatch, we comprehend objects with spatial part information from bothvisual and linguistic perspectives and perform graph matching to bridge thegap. In CSMatch, states of objects from the linguistic perspective are matchedto compatible channel information from the visual perspective. By decouplingand matching objects across visual and linguistic comprehension, we canexplicitly introspect the relationship between seen and unseen classes infine-grained object part and state levels, thereby facilitating the knowledgetransfer from seen to unseen classes in visual space. The proposed DeVLMatchframework surpasses the previous GZS3 methods on standard benchmarks, includingPASCAL VOC, COCO-Stuff, and CATARACTS, demonstrating its effectiveness.
  </details>

- **[Adaptive Layer-skipping in Pre-trained LLMs](http://arxiv.org/abs/2503.23798v1)**  `arXiv:2503.23798`  `cs.CL` `cs.AI`  
  _Xuan Luo, Weizhi Wang, Xifeng Yan_
  <details open><summary>Abstract</summary>
  Various layer-skipping methods have been proposed to accelerate tokengeneration in large language models (LLMs). However, they have overlooked afundamental question: How do computational demands vary across the generationof different tokens? In this work, we introduce FlexiDepth, a method thatdynamically adjusts the number of Transformer layers used in text generation.By incorporating a plug-in router and adapter, FlexiDepth enables adaptivelayer-skipping in LLMs without modifying their original parameters. IntroducingFlexiDepth to Llama-3-8B model achieves layer skipping of 8 layers out of 32,and meanwhile maintains the full 100\% benchmark performance. Experimentalresults with FlexiDepth demonstrate that computational demands in LLMssignificantly vary based on token type. Specifically, generating repetitivetokens or fixed phrases requires fewer layers, whereas producing tokensinvolving computation or high uncertainty requires more layers. Interestingly,this adaptive allocation pattern aligns with human intuition. To advanceresearch in this area, we open sourced FlexiDepth and a dataset documentingFlexiDepth's layer allocation patterns for future exploration.
  </details>

- **[DebFlow: Automating Agent Creation via Agent Debate](http://arxiv.org/abs/2503.23781v1)**  `arXiv:2503.23781`  `cs.AI`  
  _Jinwei Su, Yinghui Xia, Ronghua Shi, Jianhui Wang, Jianuo Huang, Yijin Wang, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) have demonstrated strong potential andimpressive performance in automating the generation and optimization ofworkflows. However, existing approaches are marked by limited reasoningcapabilities, high computational demands, and significant resourcerequirements. To address these issues, we propose DebFlow, a framework thatemploys a debate mechanism to optimize workflows and integrates reflexion toimprove based on previous experiences. We evaluated our method across sixbenchmark datasets, including HotpotQA, MATH, and ALFWorld. Our approachachieved a 3\% average performance improvement over the latest baselines,demonstrating its effectiveness in diverse problem domains. In particular,during training, our framework reduces resource consumption by 37\% compared tothe state-of-the-art baselines. Additionally, we performed ablation studies.Removing the Debate component resulted in a 4\% performance drop across twobenchmark datasets, significantly greater than the 2\% drop observed when theReflection component was removed. These findings strongly demonstrate thecritical role of Debate in enhancing framework performance, while alsohighlighting the auxiliary contribution of reflexion to overall optimization.
  </details>

- **[WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common Sense Categorization](http://arxiv.org/abs/2503.23779v1)**  `arXiv:2503.23779`  `cs.CL` `cs.AI`  
  _Ine Gevers, Victor De Marez, Luna De Bruyne, Walter Daelemans_
  <details open><summary>Abstract</summary>
  In this study, we take a closer look at how Winograd schema challenges can beused to evaluate common sense reasoning in LLMs. Specifically, we evaluategenerative models of different sizes on the popular WinoGrande benchmark. Werelease WinoWhat, a new corpus, in which each instance of the WinoGrandevalidation set is paraphrased. Additionally, we evaluate the performance on thechallenge across five common sense knowledge categories, giving morefine-grained insights on what types of knowledge are more challenging for LLMs.Surprisingly, all models perform significantly worse on WinoWhat, implying thatLLM reasoning capabilities are overestimated on WinoGrande. To verify whetherthis is an effect of benchmark memorization, we match benchmark instances toLLM trainingdata and create two test-suites. We observe that memorization has aminimal effect on model performance on WinoGrande.
  </details>

- **[XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?](http://arxiv.org/abs/2503.23771v1)**  `arXiv:2503.23771`  `cs.CV`  
  _Fengxiang Wang, Hongzhen Wang, Mingshuo Chen, Di Wang, Yulin Wang, Zonghao Guo, et al._
  <details open><summary>Abstract</summary>
  The astonishing breakthrough of multimodal large language models (MLLMs) hasnecessitated new benchmarks to quantitatively assess their capabilities, revealtheir limitations, and indicate future research directions. However, this ischallenging in the context of remote sensing (RS), since the imagery featuresultra-high resolution that incorporates extremely complex semanticrelationships. Existing benchmarks usually adopt notably smaller image sizesthan real-world RS scenarios, suffer from limited annotation quality, andconsider insufficient dimensions of evaluation. To address these issues, wepresent XLRS-Bench: a comprehensive benchmark for evaluating the perception andreasoning capabilities of MLLMs in ultra-high-resolution RS scenarios.XLRS-Bench boasts the largest average image size (8500$\times$8500) observedthus far, with all evaluation samples meticulously annotated manually, assistedby a novel semi-automatic captioner on ultra-high-resolution RS images. On topof the XLRS-Bench, 16 sub-tasks are defined to evaluate MLLMs' 10 kinds ofperceptual capabilities and 6 kinds of reasoning capabilities, with a primaryemphasis on advanced cognitive processes that facilitate real-worlddecision-making and the capture of spatiotemporal changes. The results of bothgeneral and RS-focused MLLMs on XLRS-Bench indicate that further efforts areneeded for real-world RS applications. We have open-sourced XLRS-Bench tosupport further research in developing more powerful MLLMs for remote sensing.
  </details>

- **[Texture or Semantics? Vision-Language Models Get Lost in Font Recognition](http://arxiv.org/abs/2503.23768v1)**  `arXiv:2503.23768`  `cs.CV` `cs.CL`  
  _Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, Yiwei Wang_
  <details open><summary>Abstract</summary>
  Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguisticcapabilities, achieving impressive performance in various tasks such as imagerecognition and object localization. However, their effectiveness infine-grained tasks remains an open question. In everyday scenarios, individualsencountering design materials, such as magazines, typography tutorials,research papers, or branding content, may wish to identify aestheticallypleasing fonts used in the text. Given their multimodal capabilities and freeaccessibility, many VLMs are often considered potential tools for fontrecognition. This raises a fundamental question: Do VLMs truly possess thecapability to recognize fonts? To investigate this, we introduce the FontRecognition Benchmark (FRB), a compact and well-structured dataset comprising15 commonly used fonts. FRB includes two versions: (i) an easy version, where10 sentences are rendered in different fonts, and (ii) a hard version, whereeach text sample consists of the names of the 15 fonts themselves, introducinga stroop effect that challenges model perception. Through extensive evaluationof various VLMs on font recognition tasks, we arrive at the following keyfindings: (i) Current VLMs exhibit limited font recognition capabilities, withmany state-of-the-art models failing to achieve satisfactory performance. (ii)Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefitsin improving font recognition accuracy across different VLMs. (iii) Attentionanalysis sheds light on the inherent limitations of VLMs in capturing semanticfeatures.
  </details>

- **[STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?](http://arxiv.org/abs/2503.23765v1)**  `arXiv:2503.23765`  `cs.CV`  
  _Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, et al._
  <details open><summary>Abstract</summary>
  The use of Multimodal Large Language Models (MLLMs) as an end-to-end solutionfor Embodied AI and Autonomous Driving has become a prevailing trend. WhileMLLMs have been extensively studied for visual semantic understanding tasks,their ability to perform precise and quantitative spatial-temporalunderstanding in real-world applications remains largely unexamined, leading touncertain prospects. To evaluate models' Spatial-Temporal Intelligence, weintroduce STI-Bench, a benchmark designed to evaluate MLLMs' spatial-temporalunderstanding through challenging tasks such as estimating and predicting theappearance, pose, displacement, and motion of objects. Our benchmarkencompasses a wide range of robot and vehicle operations across desktop,indoor, and outdoor scenarios. The extensive experiments reveals that thestate-of-the-art MLLMs still struggle in real-world spatial-temporalunderstanding, especially in tasks requiring precise distance estimation andmotion analysis.
  </details>

- **[Decoupled Distillation to Erase: A General Unlearning Method for Any Class-centric Tasks](http://arxiv.org/abs/2503.23751v1)**  `arXiv:2503.23751`  `cs.CV`  
  _Yu Zhou, Dian Zheng, Qijie Mo, Renjie Lu, Kun-Yu Lin, Wei-Shi Zheng_
  <details open><summary>Abstract</summary>
  In this work, we present DEcoupLEd Distillation To Erase (DELETE), a generaland strong unlearning method for any class-centric tasks. To derive this, wefirst propose a theoretical framework to analyze the general form of unlearningloss and decompose it into forgetting and retention terms. Through thetheoretical framework, we point out that a class of previous methods could bemainly formulated as a loss that implicitly optimizes the forgetting term whilelacking supervision for the retention term, disturbing the distribution ofpre-trained model and struggling to adequately preserve knowledge of theremaining classes. To address it, we refine the retention term using "darkknowledge" and propose a mask distillation unlearning method. By applying amask to separate forgetting logits from retention logits, our approachoptimizes both the forgetting and refined retention components simultaneously,retaining knowledge of the remaining classes while ensuring thorough forgettingof the target class. Without access to the remaining data or intervention(i.e., used in some works), we achieve state-of-the-art performance acrossvarious benchmarks. What's more, DELETE is a general solution that can beapplied to various downstream tasks, including face recognition, backdoordefense, and semantic segmentation with great performance.
  </details>

- **[Consistency-aware Self-Training for Iterative-based Stereo Matching](http://arxiv.org/abs/2503.23747v1)**  `arXiv:2503.23747`  `cs.CV`  
  _Jingyi Zhou, Peng Ye, Haoyu Zhang, Jiakang Yuan, Rao Qiang, Liu YangChenXu, et al._
  <details open><summary>Abstract</summary>
  Iterative-based methods have become mainstream in stereo matching due totheir high performance. However, these methods heavily rely on labeled data andface challenges with unlabeled real-world data. To this end, we propose aconsistency-aware self-training framework for iterative-based stereo matchingfor the first time, leveraging real-world unlabeled data in a teacher-studentmanner. We first observe that regions with larger errors tend to exhibit morepronounced oscillation characteristics during model prediction.Based on this,we introduce a novel consistency-aware soft filtering module to evaluate thereliability of teacher-predicted pseudo-labels, which consists of amulti-resolution prediction consistency filter and an iterative predictionconsistency filter to assess the prediction fluctuations of multipleresolutions and iterative optimization respectively. Further, we introduce aconsistency-aware soft-weighted loss to adjust the weight of pseudo-labelsaccordingly, relieving the error accumulation and performance degradationproblem due to incorrect pseudo-labels. Extensive experiments demonstrate thatour method can improve the performance of various iterative-based stereomatching approaches in various scenarios. In particular, our method can achievefurther enhancements over the current SOTA methods on several benchmarkdatasets.
  </details>

- **[AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization](http://arxiv.org/abs/2503.23733v1)**  `arXiv:2503.23733`  `cs.CV` `cs.CL`  
  _Yiyang Du, Xiaochen Wang, Chi Chen, Jiabo Ye, Yiru Wang, Peng Li, et al._
  <details open><summary>Abstract</summary>
  Recently, model merging methods have demonstrated powerful strengths incombining abilities on various tasks from multiple Large Language Models(LLMs). While previous model merging methods mainly focus on merginghomogeneous models with identical architecture, they meet challenges whendealing with Multimodal Large Language Models (MLLMs) with inherentheterogeneous property, including differences in model architecture and theasymmetry in the parameter space. In this work, we propose AdaMMS, a novelmodel merging method tailored for heterogeneous MLLMs. Our method tackles thechallenges in three steps: mapping, merging and searching. Specifically, wefirst design mapping function between models to apply model merging on MLLMswith different architecture. Then we apply linear interpolation on modelweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally inthe hyper-parameter searching step, we propose an unsupervised hyper-parameterselection method for model merging. As the first model merging method capableof merging heterogeneous MLLMs without labeled data, extensive experiments onvarious model combinations demonstrated that AdaMMS outperforms previous modelmerging methods on various vision-language benchmarks.
  </details>

- **[KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language](http://arxiv.org/abs/2503.23730v1)**  `arXiv:2503.23730`  `cs.CV` `cs.CL` `cs.AI`  
  _Yoonshik Kim, Jaeyoon Jung_
  <details open><summary>Abstract</summary>
  The recent emergence of Large Vision-Language Models(VLMs) has resulted in avariety of different benchmarks for evaluating such models. Despite this, weobserve that most existing evaluation methods suffer from the fact that theyeither require the model to choose from pre-determined responses, sacrificingopen-endedness, or evaluate responses using a judge model, resulting insubjective and unreliable evaluation. In addition, we observe a lack ofbenchmarks for VLMs in the Korean language, which are necessary as a separatemetric from more common English language benchmarks, as the performance ofgenerative language models can differ significantly based on the language beingused. Therefore, we present KOFFVQA, a general-purpose free-form visualquestion answering benchmark in the Korean language for the evaluation of VLMs.Our benchmark consists of 275 carefully crafted questions each paired with animage and grading criteria covering 10 different aspects of VLM performance.The grading criteria eliminate the problem of unreliability by allowing thejudge model to grade each response based on a pre-determined set of rules. Bydefining the evaluation criteria in an objective manner, even a smallopen-source model can be used to evaluate models on our benchmark reliably. Inaddition to evaluating a large number of existing VLMs on our benchmark, wealso experimentally verify that our method of using pre-existing gradingcriteria for evaluation is much more reliable than existing methods. Ourevaluation code is available at https://github.com/maum-ai/KOFFVQA
  </details>

- **[LATex: Leveraging Attribute-based Text Knowledge for Aerial-Ground Person Re-Identification](http://arxiv.org/abs/2503.23722v1)**  `arXiv:2503.23722`  `cs.CV`  
  _Xiang Hu, Yuhao Wang, Pingping Zhang, Huchuan Lu_
  <details open><summary>Abstract</summary>
  Aerial-Ground person Re-IDentification (AG-ReID) aims to retrieve specificpersons across heterogeneous cameras in different views. Previous methodsusually adopt large-scale models, focusing on view-invariant features. However,they overlook the semantic information in person attributes. Additionally,existing training strategies often rely on full fine-tuning large-scale models,which significantly increases training costs. To address these issues, wepropose a novel framework named LATex for AG-ReID, which adopts prompt-tuningstrategies to leverage attribute-based text knowledge. More specifically, wefirst introduce the Contrastive Language-Image Pre-training (CLIP) model as thebackbone, and propose an Attribute-aware Image Encoder (AIE) to extract globalsemantic features and attribute-aware features. Then, with these features, wepropose a Prompted Attribute Classifier Group (PACG) to generate personattribute predictions and obtain the encoded representations of predictedattributes. Finally, we design a Coupled Prompt Template (CPT) to transformattribute tokens and view information into structured sentences. Thesesentences are processed by the text encoder of CLIP to generate morediscriminative features. As a result, our framework can fully leverageattribute-based text knowledge to improve the AG-ReID. Extensive experiments onthree AG-ReID benchmarks demonstrate the effectiveness of our proposed LATex.The source code will be available.
  </details>

- **[Expanding-and-Shrinking Binary Neural Networks](http://arxiv.org/abs/2503.23709v1)**  `arXiv:2503.23709`  `cs.CV`  
  _Xulong Shi, Caiyi Sun, Zhi Qi, Liu Hao, Xiaodong Yang_
  <details open><summary>Abstract</summary>
  While binary neural networks (BNNs) offer significant benefits in terms ofspeed, memory and energy, they encounter substantial accuracy degradation inchallenging tasks compared to their real-valued counterparts. Due to thebinarization of weights and activations, the possible values of each entry inthe feature maps generated by BNNs are strongly constrained. To tackle thislimitation, we propose the expanding-and-shrinking operation, which enhancesbinary feature maps with negligible increase of computation complexity, therebystrengthening the representation capacity. Extensive experiments conducted onmultiple benchmarks reveal that our approach generalizes well across diverseapplications ranging from image classification, object detection to generativediffusion model, while also achieving remarkable improvement over variousleading binarization algorithms based on different architectures including bothCNNs and Transformers.
  </details>

- **[Towards Benchmarking and Assessing the Safety and Robustness of Autonomous Driving on Safety-critical Scenarios](http://arxiv.org/abs/2503.23708v1)**  `arXiv:2503.23708`  `cs.RO` `cs.AI`  
  _Jingzheng Li, Xianglong Liu, Shikui Wei, Zhijun Chen, Bing Li, Qing Guo, et al._
  <details open><summary>Abstract</summary>
  Autonomous driving has made significant progress in both academia andindustry, including performance improvements in perception task and thedevelopment of end-to-end autonomous driving systems. However, the safety androbustness assessment of autonomous driving has not received sufficientattention. Current evaluations of autonomous driving are typically conducted innatural driving scenarios. However, many accidents often occur in edge cases,also known as safety-critical scenarios. These safety-critical scenarios aredifficult to collect, and there is currently no clear definition of whatconstitutes a safety-critical scenario. In this work, we explore the safety androbustness of autonomous driving in safety-critical scenarios. First, weprovide a definition of safety-critical scenarios, including static trafficscenarios such as adversarial attack scenarios and natural distribution shifts,as well as dynamic traffic scenarios such as accident scenarios. Then, wedevelop an autonomous driving safety testing platform to comprehensivelyevaluate autonomous driving systems, encompassing not only the assessment ofperception modules but also system-level evaluations. Our work systematicallyconstructs a safety verification process for autonomous driving, providingtechnical support for the industry to establish standardized test framework andreduce risks in real-world road deployment.
  </details>

- **[The Devil is in the Distributions: Explicit Modeling of Scene Content is Key in Zero-Shot Video Captioning](http://arxiv.org/abs/2503.23679v1)**  `arXiv:2503.23679`  `cs.CV`  
  _Mingkai Tian, Guorong Li, Yuankai Qi, Amin Beheshti, Javen Qinfeng Shi, Anton van den Hengel, et al._
  <details open><summary>Abstract</summary>
  Zero-shot video captioning requires that a model generate high-qualitycaptions without human-annotated video-text pairs for training.State-of-the-art approaches to the problem leverage CLIP to extractvisual-relevant textual prompts to guide language models in generatingcaptions. These methods tend to focus on one key aspect of the scene and builda caption that ignores the rest of the visual input. To address this issue, andgenerate more accurate and complete captions, we propose a novel progressivemulti-granularity textual prompting strategy for zero-shot video captioning.Our approach constructs three distinct memory banks, encompassing noun phrases,scene graphs of noun phrases, and entire sentences. Moreover, we introduce acategory-aware retrieval mechanism that models the distribution of naturallanguage surrounding the specific topics in question. Extensive experimentsdemonstrate the effectiveness of our method with 5.7%, 16.2%, and 3.4%improvements in terms of the main metric CIDEr on MSR-VTT, MSVD, and VATEXbenchmarks compared to existing state-of-the-art.
  </details>

- **[WHERE and WHICH: Iterative Debate for Biomedical Synthetic Data Augmentation](http://arxiv.org/abs/2503.23673v1)**  `arXiv:2503.23673`  `cs.CL`  
  _Zhengyi Zhao, Shubo Zhang, Bin Liang, Binyang Li, Kam-Fai Wong_
  <details open><summary>Abstract</summary>
  In Biomedical Natural Language Processing (BioNLP) tasks, such as RelationExtraction, Named Entity Recognition, and Text Classification, the scarcity ofhigh-quality data remains a significant challenge. This limitation poisonslarge language models to correctly understand relationships between biologicalentities, such as molecules and diseases, or drug interactions, and furtherresults in potential misinterpretation of biomedical documents. To address thisissue, current approaches generally adopt the Synthetic Data Augmentationmethod which involves similarity computation followed by word replacement, butcounterfactual data are usually generated. As a result, these methods disruptmeaningful word sets or produce sentences with meanings that deviatesubstantially from the original context, rendering them ineffective inimproving model performance. To this end, this paper proposes abiomedical-dedicated rationale-based synthetic data augmentation method. Beyondthe naive lexicon similarity, specific bio-relation similarity is measured tohold the augmented instance having a strong correlation with bio-relationinstead of simply increasing the diversity of augmented data. Moreover, amulti-agents-involved reflection mechanism helps the model iterativelydistinguish different usage of similar entities to escape falling into themis-replace trap. We evaluate our method on the BLURB and BigBIO benchmark,which includes 9 common datasets spanning four major BioNLP tasks. Ourexperimental results demonstrate consistent performance improvements across alltasks, highlighting the effectiveness of our approach in addressing thechallenges associated with data scarcity and enhancing the overall performanceof biomedical NLP models.
  </details>

- **[CrossFormer: Cross-Segment Semantic Fusion for Document Segmentation](http://arxiv.org/abs/2503.23671v1)**  `arXiv:2503.23671`  `cs.CL`  
  _Tongke Ni, Yang Fan, Junru Zhou, Xiangping Wu, Qingcai Chen_
  <details open><summary>Abstract</summary>
  Text semantic segmentation involves partitioning a document into multipleparagraphs with continuous semantics based on the subject matter, contextualinformation, and document structure. Traditional approaches have typicallyrelied on preprocessing documents into segments to address input lengthconstraints, resulting in the loss of critical semantic information acrosssegments. To address this, we present CrossFormer, a transformer-based modelfeaturing a novel cross-segment fusion module that dynamically models latentsemantic dependencies across document segments, substantially elevatingsegmentation accuracy. Additionally, CrossFormer can replace rule-based chunkmethods within the Retrieval-Augmented Generation (RAG) system, producing moresemantically coherent chunks that enhance its efficacy. Comprehensiveevaluations confirm CrossFormer's state-of-the-art performance on public textsemantic segmentation datasets, alongside considerable gains on RAG benchmarks.
  </details>

- **[MolGround: A Benchmark for Molecular Grounding](http://arxiv.org/abs/2503.23668v1)**  `arXiv:2503.23668`  `cs.AI`  
  _Jiaxin Wu, Ting Zhang, Rubing Chen, Wengyu Zhang, Chen Jason Zhang, Xiaoyong Wei, et al._
  <details open><summary>Abstract</summary>
  Current molecular understanding approaches predominantly focus on thedescriptive aspect of human perception, providing broad, topic-level insights.However, the referential aspect -- linking molecular concepts to specificstructural components -- remains largely unexplored. To address this gap, wepropose a molecular grounding benchmark designed to evaluate a model'sreferential abilities. We align molecular grounding with establishedconventions in NLP, cheminformatics, and molecular science, showcasing thepotential of NLP techniques to advance molecular understanding within the AIfor Science movement. Furthermore, we constructed the largest molecularunderstanding benchmark to date, comprising 79k QA pairs, and developed amulti-agent grounding prototype as proof of concept. This system outperformsexisting models, including GPT-4o, and its grounding outputs have beenintegrated to enhance traditional tasks such as molecular captioning and ATC(Anatomical, Therapeutic, Chemical) classification.
  </details>

- **[ActionStudio: A Lightweight Framework for Data and Training of Large Action Models](http://arxiv.org/abs/2503.22673v2)**  `arXiv:2503.22673`  `cs.CL` `cs.AI`  
  _Jianguo Zhang, Thai Hoang, Ming Zhu, Zuxin Liu, Shiyu Wang, Tulika Awalgaonkar, et al._
  <details open><summary>Abstract</summary>
  Action models are essential for enabling autonomous agents to perform complextasks. However, training large action models remains challenging due to thediversity of agent environments and the complexity of agentic data. Despitegrowing interest, existing infrastructure provides limited support forscalable, agent-specific fine-tuning. We present ActionStudio, a lightweightand extensible data and training framework designed for large action models.ActionStudio unifies heterogeneous agent trajectories through a standardizedformat, supports diverse training paradigms including LoRA, full fine-tuning,and distributed setups, and integrates robust preprocessing and verificationtools. We validate its effectiveness across both public and realistic industrybenchmarks, demonstrating strong performance and practical scalability. Weopen-sourced code and data at https://github.com/SalesforceAIResearch/xLAM tofacilitate research in the community.
  </details>

- **[Resilient Sensor Fusion under Adverse Sensor Failures via Multi-Modal Expert Fusion](http://arxiv.org/abs/2503.19776v2)**  `arXiv:2503.19776`  `cs.CV`  
  _Konyul Park, Yecheol Kim, Daehun Kim, Jun Won Choi_
  <details open><summary>Abstract</summary>
  Modern autonomous driving perception systems utilize complementarymulti-modal sensors, such as LiDAR and cameras. Although sensor fusionarchitectures enhance performance in challenging environments, they stillsuffer significant performance drops under severe sensor failures, such asLiDAR beam reduction, LiDAR drop, limited field of view, camera drop, andocclusion. This limitation stems from inter-modality dependencies in currentsensor fusion frameworks. In this study, we introduce an efficient and robustLiDAR-camera 3D object detector, referred to as MoME, which can achieve robustperformance through a mixture of experts approach. Our MoME fully decouplesmodality dependencies using three parallel expert decoders, which use camerafeatures, LiDAR features, or a combination of both to decode object queries,respectively. We propose Multi-Expert Decoding (MED) framework, where eachquery is decoded selectively using one of three expert decoders. MoME utilizesan Adaptive Query Router (AQR) to select the most appropriate expert decoderfor each query based on the quality of camera and LiDAR features. This ensuresthat each query is processed by the best-suited expert, resulting in robustperformance across diverse sensor failure scenarios. We evaluated theperformance of MoME on the nuScenes-R benchmark. Our MoME achievedstate-of-the-art performance in extreme weather and sensor failure conditions,significantly outperforming the existing models across various sensor failurescenarios.
  </details>

- **[Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language Models](http://arxiv.org/abs/2503.18334v2)**  `arXiv:2503.18334`  `cs.CV`  
  _Haotian Zhai, Xinyu Chen, Can Zhang, Tianming Sha, Ruirui Li_
  <details open><summary>Abstract</summary>
  Test-time adaptation (TTA) of visual language models has recently attractedsignificant attention as a solution to the performance degradation caused bydistribution shifts in downstream tasks. However, existing cache-based TTAmethods have certain limitations. They mainly rely on the accuracy of cachedfeature labels, and the presence of noisy pseudo-labels can cause thesefeatures to deviate from their true distribution. This makes cache retrievalmethods based on similarity matching highly sensitive to outliers or extremesamples. Moreover, current methods lack effective mechanisms to model classdistributions, which limits their ability to fully exploit the potential ofcached information. To address these challenges, we introduce a comprehensiveand reliable caching mechanism and propose a novel zero-shot TTA method called"Cache, Residual, Gaussian" (CRG). This method not only employs learnableresidual parameters to better align positive and negative visual prototypeswith text prototypes, thereby optimizing the quality of cached features, butalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically modelintra-class feature distributions, further mitigating the impact of noisyfeatures. Experimental results on 13 benchmarks demonstrate that CRGoutperforms state-of-the-art TTA methods, showcasing exceptional robustness andadaptability.
  </details>

- **[Image as an IMU: Estimating Camera Motion from a Single Motion-Blurred Image](http://arxiv.org/abs/2503.17358v2)**  `arXiv:2503.17358`  `cs.CV`  
  _Jerred Chen, Ronald Clark_
  <details open><summary>Abstract</summary>
  In many robotics and VR/AR applications, fast camera motions cause a highlevel of motion blur, causing existing camera pose estimation methods to fail.In this work, we propose a novel framework that leverages motion blur as a richcue for motion estimation rather than treating it as an unwanted artifact. Ourapproach works by predicting a dense motion flow field and a monocular depthmap directly from a single motion-blurred image. We then recover theinstantaneous camera velocity by solving a linear least squares problem underthe small motion assumption. In essence, our method produces an IMU-likemeasurement that robustly captures fast and aggressive camera movements. Totrain our model, we construct a large-scale dataset with realistic syntheticmotion blur derived from ScanNet++v2 and further refine our model by trainingend-to-end on real data using our fully differentiable pipeline. Extensiveevaluations on real-world benchmarks demonstrate that our method achievesstate-of-the-art angular and translational velocity estimates, outperformingcurrent methods like MASt3R and COLMAP.
  </details>

- **[DCAD-2000: A Multilingual Dataset across 2000+ Languages with Data Cleaning as Anomaly Detection](http://arxiv.org/abs/2502.11546v2)**  `arXiv:2502.11546`  `cs.CL`  
  _Yingli Shen, Wen Lai, Shuo Wang, Xueren Zhang, Kangyang Luo, Alexander Fraser, et al._
  <details open><summary>Abstract</summary>
  The rapid development of multilingual large language models (LLMs) highlightsthe need for high-quality, diverse, and clean multilingual datasets. In thispaper, we introduce DCAD-2000 (Data Cleaning as Anomaly Detection), alarge-scale multilingual corpus built using newly extracted Common Crawl dataand existing multilingual datasets. DCAD-2000 includes over 2,282 languages,46.72TB of data, and 8.63 billion documents, spanning 155 high- andmedium-resource languages and 159 writing scripts. To overcome the limitationsof current data cleaning methods, which rely on manual heuristic thresholds, wepropose reframing data cleaning as an anomaly detection task. This dynamicfiltering approach significantly enhances data quality by identifying andremoving noisy or anomalous content. We evaluate the quality of DCAD-2000 onthe FineTask benchmark, demonstrating substantial improvements in multilingualdataset quality and task performance.
  </details>

- **[Quantifying the Capability Boundary of DeepSeek Models: An Application-Driven Performance Analysis](http://arxiv.org/abs/2502.11164v4)**  `arXiv:2502.11164`  `cs.LG` `cs.AI`  
  _Kaikai Zhao, Zhaoxiang Liu, Xuejiao Lei, Jiaojiao Zhao, Zhenhong Long, Zipeng Wang, et al._
  <details open><summary>Abstract</summary>
  DeepSeek-R1, known for its low training cost and exceptional reasoningcapabilities, has achieved state-of-the-art performance on various benchmarks.However, detailed evaluations for DeepSeek Series models from the perspectiveof real-world applications are lacking, making it challenging for users toselect the most suitable DeepSeek models for their specific needs. To addressthis gap, we conduct a systematic evaluation of the DeepSeek-V3, DeepSeek-R1,DeepSeek-R1-Distill-Qwen series, DeepSeek-R1-Distill-Llama series, theircorresponding 4-bit quantized models, and the reasoning model QwQ-32B using theenhanced A-Eval benchmark, A-Eval-2.0. Through a comparative analysis oforiginal instruction-tuned models and their distilled counterparts, weinvestigate how reasoning enhancements impact performance across diversepractical tasks. To assist users in model selection, we quantify the capabilityboundary of DeepSeek models through performance tier classifications. Based onthe quantification results, we develop a model selection handbook that clearlyillustrates the relation among models, their capabilities and practicalapplications. This handbook enables users to select the most cost-effectivemodels without efforts, ensuring optimal performance and resource efficiency inreal-world applications. It should be noted that, despite our efforts toestablish a comprehensive, objective, and authoritative evaluation benchmark,the selection of test samples, characteristics of data distribution, and thesetting of evaluation criteria may inevitably introduce certain biases into theevaluation results. We will continuously optimize the evaluation benchmarks andperiodically update this paper to provide more comprehensive and accurateevaluation results. Please refer to the latest version of the paper for themost current results and conclusions.
  </details>

- **[PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models](http://arxiv.org/abs/2502.01584v3)**  `arXiv:2502.01584`  `cs.LG` `cs.AI`  
  _Zixuan Wu, Francesca Lucchetti, Aleksander Boruch-Gruszecki, Jingmiao Zhao, Carolyn Jane Anderson, Joydeep Biswas, et al._
  <details open><summary>Abstract</summary>
  Existing benchmarks for frontier models often test specialized, "PhD-level"knowledge that is difficult for non-experts to grasp. In contrast, we present abenchmark with 594 problems based on the NPR Sunday Puzzle Challenge thatrequires only general knowledge. Our benchmark is challenging for both humansand models; however correct solutions are easy to verify, and models' mistakesare easy to spot. As LLMs are more widely deployed in society, we believe it isuseful to develop benchmarks for frontier models that humans can understandwithout the need for deep domain expertise.  Our work reveals capability gaps that are not evident in existing benchmarks:OpenAI o1 significantly outperforms other reasoning models on our benchmark,despite being on par with other models when tested on benchmarks that testspecialized knowledge. Furthermore, our analysis of reasoning outputs uncoversnew kinds of failures. DeepSeek R1, for instance, often concedes with "I giveup" before providing an answer that it knows is wrong. R1 can also beremarkably "uncertain" in its output and in rare cases, it does not "finishthinking," which suggests the need for techniques to "wrap up" before thecontext window limit is reached. We also quantify the effectiveness ofreasoning longer to identify the point beyond which more reasoning is unlikelyto improve accuracy on our benchmark.
  </details>

- **[Know "No'' Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP](http://arxiv.org/abs/2501.10913v2)**  `arXiv:2501.10913`  `cs.CV` `cs.CL`  
  _Junsung Park, Jungbeom Lee, Jongyoon Song, Sangwon Yu, Dahuin Jung, Sungroh Yoon_
  <details open><summary>Abstract</summary>
  While CLIP has significantly advanced multimodal understanding by bridgingvision and language, the inability to grasp negation - such as failing todifferentiate concepts like "parking" from "no parking" - poses substantialchallenges. By analyzing the data used in the public CLIP model's pre-training,we posit this limitation stems from a lack of negation-inclusive data. Toaddress this, we introduce data generation pipelines that employ a largelanguage model (LLM) and a multimodal LLM to produce negation-inclusivecaptions. Fine-tuning CLIP with data generated from our pipelines, we developNegationCLIP, which enhances negation awareness while preserving thegenerality. Moreover, to enable a comprehensive evaluation of negationunderstanding, we propose NegRefCOCOg-a benchmark tailored to test VLMs'ability to interpret negation across diverse expressions and positions within asentence. Experiments on various CLIP architectures validate the effectivenessof our data generation pipelines in enhancing CLIP's ability to perceivenegation accurately. Additionally, NegationCLIP's enhanced negation awarenesshas practical applications across various multimodal tasks, demonstrated byperformance gains in text-to-image generation and referring image segmentation.
  </details>

- **[CALMM-Drive: Confidence-Aware Autonomous Driving with Large Multimodal Model](http://arxiv.org/abs/2412.04209v2)**  `arXiv:2412.04209`  `cs.RO`  
  _Ruoyu Yao, Yubin Wang, Haichao Liu, Rui Yang, Zengqi Peng, Lei Zhu, et al._
  <details open><summary>Abstract</summary>
  Decision-making and motion planning constitute critical components forensuring the safety and efficiency of autonomous vehicles (AVs). Existingmethodologies typically adopt two paradigms: decision then planning orgeneration then scoring. However, the former architecture often suffers fromdecision-planning misalignment that incurs risky situations. Meanwhile, thelatter struggles to balance short-term operational metrics (e.g., immediatemotion smoothness) with long-term tactical goals (e.g., route efficiency),resulting in myopic or overly conservative behaviors. To address these issues,we introduce CALMM-Drive, a novel Confidence-Aware Large Multimodal Model (LMM)empowered Autonomous Driving framework. Our approach integrates drivingtask-oriented Chain-of-Thought (CoT) reasoning coupled with Top-K confidenceelicitation, which facilitates high-level reasoning to generate multiplecandidate decisions with their confidence levels. Furthermore, we propose anovel planning module that integrates a diffusion model for trajectorygeneration and a hierarchical refinement process to find the optimaltrajectory. This framework enables the selection over trajectory candidatesaccounting for both low-level solution quality and high-level tacticalconfidence, which avoids the risks within one-shot decisions and overcomes thelimitations in short-sighted scoring mechanisms. Comprehensive evaluations innuPlan closed-loop simulation environments demonstrate the competitiveperformance of CALMM-Drive across both common and long-tail benchmarks,showcasing a significant advancement in the integration of uncertainty inLMM-empowered AVs. The code will be released upon acceptance.
  </details>

- **[Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis](http://arxiv.org/abs/2411.19655v3)**  `arXiv:2411.19655`  `cs.CL`  
  _Alessandro Scir√®, Andrei Stefan Bejgu, Simone Tedeschi, Karim Ghonim, Federico Martelli, Roberto Navigli_
  <details open><summary>Abstract</summary>
  After the introduction of Large Language Models (LLMs), there have beensubstantial improvements in the performance of Natural Language Generation(NLG) tasks, including Text Summarization and Machine Translation. However,LLMs still produce outputs containing hallucinations, that is, content notgrounded in factual information. Therefore, developing methods to assess thefactuality of LLMs has become urgent.  Indeed, resources for factuality evaluation have recently emerged. Althoughchallenging, these resources face one or more of the following limitations: (i)they are tailored to a specific task or domain; (ii) they are limited in size,thereby preventing the training of new factuality evaluators; (iii) they aredesigned for simpler verification tasks, such as claim verification.  To address these issues, we introduce LLM-Oasis, to the best of our knowledgethe largest resource for training end-to-end factuality evaluators. LLM-Oasisis constructed by extracting claims from Wikipedia, falsifying a subset ofthese claims, and generating pairs of factual and unfactual texts. We then relyon human annotators to both validate the quality of our dataset and to create agold standard test set for benchmarking factuality evaluation systems.  Our experiments demonstrate that LLM-Oasis presents a significant challengefor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in ourproposed end-to-end factuality evaluation task, highlighting its potential todrive future research in the field.
  </details>

- **[Emphasizing Discriminative Features for Dataset Distillation in Complex Scenarios](http://arxiv.org/abs/2410.17193v2)**  `arXiv:2410.17193`  `cs.CV` `cs.AI`  
  _Kai Wang, Zekai Li, Zhi-Qi Cheng, Samir Khaki, Ahmad Sajedi, Ramakrishna Vedantam, et al._
  <details open><summary>Abstract</summary>
  Dataset distillation has demonstrated strong performance on simple datasetslike CIFAR, MNIST, and TinyImageNet but struggles to achieve similar results inmore complex scenarios. In this paper, we propose EDF (emphasizes thediscriminative features), a dataset distillation method that enhances keydiscriminative regions in synthetic images using Grad-CAM activation maps. Ourapproach is inspired by a key observation: in simple datasets, high-activationareas typically occupy most of the image, whereas in complex scenarios, thesize of these areas is much smaller. Unlike previous methods that treat allpixels equally when synthesizing images, EDF uses Grad-CAM activation maps toenhance high-activation areas. From a supervision perspective, we downplaysupervision signals that have lower losses, as they contain common patterns.Additionally, to help the DD community better explore complex scenarios, webuild the Complex Dataset Distillation (Comp-DD) benchmark by meticulouslyselecting sixteen subsets, eight easy and eight hard, from ImageNet-1K. Inparticular, EDF consistently outperforms SOTA results in complex scenarios,such as ImageNet-1K subsets. Hopefully, more researchers will be inspired andencouraged to improve the practicality and efficacy of DD. Our code andbenchmark will be made public at https://github.com/NUS-HPC-AI-Lab/EDF.
  </details>

- **[MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](http://arxiv.org/abs/2410.10139v2)**  `arXiv:2410.10139`  `cs.LG` `cs.CV` `cs.CL`  
  _Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, et al._
  <details open><summary>Abstract</summary>
  Interleaved multimodal comprehension and generation, enabling models toproduce and interpret both images and text in arbitrary sequences, have becomea pivotal area in multimodal learning. Despite significant advancements, theevaluation of this capability remains insufficient. Existing benchmarks sufferfrom limitations in data scale, scope, and evaluation depth, while currentevaluation metrics are often costly or biased, lacking in reliability forpractical applications. To address these challenges, we introduce MMIE, alarge-scale knowledge-intensive benchmark for evaluating interleaved multimodalcomprehension and generation in Large Vision-Language Models (LVLMs). MMIEcomprises 20K meticulously curated multimodal queries, spanning 3 categories,12 fields, and 102 subfields, including mathematics, coding, physics,literature, health, and arts. It supports both interleaved inputs and outputs,offering a mix of multiple-choice and open-ended question formats to evaluatediverse competencies. Moreover, we propose a reliable automated evaluationmetric, leveraging a scoring model fine-tuned with human-annotated data andsystematic evaluation criteria, aimed at reducing bias and improving evaluationaccuracy. Extensive experiments demonstrate the effectiveness of our benchmarkand metrics in providing a comprehensive evaluation of interleaved LVLMs.Specifically, we evaluate eight LVLMs, revealing that even the best models showsignificant room for improvement, with most achieving only moderate results. Webelieve MMIE will drive further advancements in the development of interleavedLVLMs. We publicly release our benchmark and code inhttps://mmie-bench.github.io/.
  </details>

- **[Reversible Decoupling Network for Single Image Reflection Removal](http://arxiv.org/abs/2410.08063v2)**  `arXiv:2410.08063`  `cs.CV`  
  _Hao Zhao, Mingjia Li, Qiming Hu, Xiaojie Guo_
  <details open><summary>Abstract</summary>
  Recent deep-learning-based approaches to single-image reflection removal haveshown promising advances, primarily for two reasons: 1) the utilization ofrecognition-pretrained features as inputs, and 2) the design of dual-streaminteraction networks. However, according to the Information Bottleneckprinciple, high-level semantic clues tend to be compressed or discarded duringlayer-by-layer propagation. Additionally, interactions in dual-stream networksfollow a fixed pattern across different layers, limiting overall performance.To address these limitations, we propose a novel architecture called ReversibleDecoupling Network (RDNet), which employs a reversible encoder to securevaluable information while flexibly decoupling transmission- andreflection-relevant features during the forward pass. Furthermore, we customizea transmission-rate-aware prompt generator to dynamically calibrate features,further boosting performance. Extensive experiments demonstrate the superiorityof RDNet over existing SOTA methods on five widely-adopted benchmark datasets.RDNet achieves the best performance in the NTIRE 2025 Single Image ReflectionRemoval in the Wild Challenge in both fidelity and perceptual comparison. Ourcode is available at https://github.com/lime-j/RDNet
  </details>

- **[ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery](http://arxiv.org/abs/2410.05080v3)**  `arXiv:2410.05080`  `cs.LG` `cs.CL` `cs.AI`  
  _Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, et al._
  <details open><summary>Abstract</summary>
  The advancements of large language models (LLMs) have piqued growing interestin developing LLM-based language agents to automate scientific discoveryend-to-end, which has sparked both excitement and skepticism about their truecapabilities. In this work, we call for rigorous assessment of agents onindividual tasks in a scientific workflow before making bold claims onend-to-end automation. To this end, we present ScienceAgentBench, a newbenchmark for evaluating language agents for data-driven scientific discovery.To ensure the scientific authenticity and real-world relevance of ourbenchmark, we extract 102 tasks from 44 peer-reviewed publications in fourdisciplines and engage nine subject matter experts to validate them. We unifythe target output for every task to a self-contained Python program file andemploy an array of evaluation metrics to examine the generated programs,execution results, and costs. Each task goes through multiple rounds of manualvalidation by annotators and subject matter experts to ensure its annotationquality and scientific plausibility. We also propose two effective strategiesto mitigate data contamination concerns. Using ScienceAgentBench, we evaluatefive open-weight and proprietary LLMs, each with three frameworks: directprompting, OpenHands CodeAct, and self-debug. Given three attempts for eachtask, the best-performing agent can only solve 32.4% of the tasks independentlyand 34.3% with expert-provided knowledge. In addition, we evaluate OpenAIo1-preview with direct prompting and self-debug, which can boost theperformance to 42.2%, demonstrating the effectiveness of increasinginference-time compute but with more than 10 times the cost of other LLMs.Still, our results underscore the limitations of current language agents ingenerating code for data-driven discovery, let alone end-to-end automation forscientific research.
  </details>

- **[Cropper: Vision-Language Model for Image Cropping through In-Context Learning](http://arxiv.org/abs/2408.07790v2)**  `arXiv:2408.07790`  `cs.CV`  
  _Seung Hyun Lee, Jijun Jiang, Yiran Xu, Zhuofang Li, Junjie Ke, Yinxiao Li, et al._
  <details open><summary>Abstract</summary>
  The goal of image cropping is to identify visually appealing crops in animage. Conventional methods are trained on specific datasets and fail to adaptto new requirements. Recent breakthroughs in large vision-language models(VLMs) enable visual in-context learning without explicit training. However,downstream tasks with VLMs remain under explored. In this paper, we propose aneffective approach to leverage VLMs for image cropping. First, we propose anefficient prompt retrieval mechanism for image cropping to automate theselection of in-context examples. Second, we introduce an iterative refinementstrategy to iteratively enhance the predicted crops. The proposed framework, werefer to as Cropper, is applicable to a wide range of cropping tasks, includingfree-form cropping, subject-aware cropping, and aspect ratio-aware cropping.Extensive experiments demonstrate that Cropper significantly outperformsstate-of-the-art methods across several benchmarks.
  </details>

- **[Training-Free Exponential Context Extension via Cascading KV Cache](http://arxiv.org/abs/2406.17808v4)**  `arXiv:2406.17808`  `cs.LG` `cs.CL` `cs.AI`  
  _Jeffrey Willette, Heejun Lee, Youngwan Lee, Myeongjae Jeon, Sung Ju Hwang_
  <details open><summary>Abstract</summary>
  The transformer's context window is vital for tasks such as few-shot learningand conditional generation as it preserves previous tokens for active memory.However, as the context lengths increase, the computational costs growquadratically, hindering the deployment of large language models (LLMs) inreal-world, long sequence scenarios. Although some recent key-value caching (KVCache) methods offer linear inference complexity, they naively manage thestored context, prematurely evicting tokens and losing valuable information.Moreover, they lack an optimized prefill/prompt stage strategy, resulting inhigher latency than even quadratic attention for realistic context sizes. Inresponse, we introduce a novel mechanism that leverages cascading sub-cachebuffers to selectively retain the most relevant tokens, enabling the model tomaintain longer context histories without increasing the cache size. Ourapproach outperforms linear caching baselines across key benchmarks, includingstreaming perplexity, question answering, book summarization, and passkeyretrieval, where it retains better retrieval accuracy at 1M tokens after fourdoublings of the cache size of 65K. Additionally, our method reduces prefillstage latency by a factor of 6.8 when compared to flash attention on 1M tokens.These innovations not only enhance the computational efficiency of LLMs butalso pave the way for their effective deployment in resource-constrainedenvironments, enabling large-scale, real-time applications with significantlyreduced latency.
  </details>

- **[Adaptive Multi-step Refinement Network for Robust Point Cloud Registration](http://arxiv.org/abs/2312.03053v2)**  `arXiv:2312.03053`  `cs.CV`  
  _Zhi Chen, Yufan Ren, Tong Zhang, Zheng Dang, Wenbing Tao, Sabine S√ºsstrunk, et al._
  <details open><summary>Abstract</summary>
  Point Cloud Registration (PCR) estimates the relative rigid transformationbetween two point clouds of the same scene. Despite significant progress withlearning-based approaches, existing methods still face challenges when theoverlapping region between the two point clouds is small. In this paper, wepropose an adaptive multi-step refinement network that refines the registrationquality at each step by leveraging the information from the preceding step. Toachieve this, we introduce a training procedure and a refinement network.Firstly, to adapt the network to the current step, we utilize a generalizedone-way attention mechanism, which prioritizes the last step's estimatedoverlapping region, and we condition the network on step indices. Secondly,instead of training the network to map either random transformations or a fixedpre-trained model's estimations to the ground truth, we train it ontransformations with varying registration qualities, ranging from accurate toinaccurate, thereby enhancing the network's adaptiveness and robustness.Despite its conceptual simplicity, our method achieves state-of-the-artperformance on both the 3DMatch/3DLoMatch and KITTI benchmarks. Notably, on3DLoMatch, our method reaches 80.4% recall rate, with an absolute improvementof 1.2%.
  </details>
