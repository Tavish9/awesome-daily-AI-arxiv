# üîç LLM Papers ¬∑ 2025-03-31

[![Total Papers](https://img.shields.io/badge/Papers-41-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `LLM` `Large Language Model` `GPT`  
**Filter**: `None`

---

## üìö Paper List

- **[Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation](http://arxiv.org/abs/2503.24379v1)**  `arXiv:2503.24379`  `cs.CV` `cs.AI`  
  _Shengqiong Wu, Weicai Ye, Jiahao Wang, Quande Liu, Xintao Wang, Pengfei Wan, et al._
  <details open><summary>Abstract</summary>
  To address the bottleneck of accurate user intent interpretation within thecurrent video generation community, we present Any2Caption, a novel frameworkfor controllable video generation under any condition. The key idea is todecouple various condition interpretation steps from the video synthesis step.By leveraging modern multimodal large language models (MLLMs), Any2Captioninterprets diverse inputs--text, images, videos, and specialized cues such asregion, motion, and camera poses--into dense, structured captions that offerbackbone video generators with better guidance. We also introduce Any2CapIns, alarge-scale dataset with 337K instances and 407K conditions forany-condition-to-caption instruction tuning. Comprehensive evaluationsdemonstrate significant improvements of our system in controllability and videoquality across various aspects of existing video generation models. ProjectPage: https://sqwu.top/Any2Cap/
  </details>

- **[Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models](http://arxiv.org/abs/2503.24377v1)**  `arXiv:2503.24377`  `cs.CL` `cs.AI`  
  _Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in Large Language Models (LLMs) have significantlyenhanced their ability to perform complex reasoning tasks, transitioning fromfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).While System 2 reasoning improves task accuracy, it often incurs substantialcomputational costs due to its slow thinking nature and inefficient orunnecessary reasoning behaviors. In contrast, System 1 reasoning iscomputationally efficient but leads to suboptimal performance. Consequently, itis critical to balance the trade-off between performance (benefits) andcomputational costs (budgets), giving rise to the concept of reasoning economy.In this survey, we provide a comprehensive analysis of reasoning economy inboth the post-training and test-time inference stages of LLMs, encompassing i)the cause of reasoning inefficiency, ii) behavior analysis of differentreasoning patterns, and iii) potential solutions to achieve reasoning economy.By offering actionable insights and highlighting open challenges, we aim toshed light on strategies for improving the reasoning economy of LLMs, therebyserving as a valuable resource for advancing research in this evolving area. Wealso provide a public repository to continually track developments in thisfast-evolving field.
  </details>

- **[Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](http://arxiv.org/abs/2503.24376v1)**  `arXiv:2503.24376`  `cs.CL` `cs.CV` `cs.AI` `cs.LG`  
  _Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in Chain of Thought (COT) generation have significantlyimproved the reasoning capabilities of Large Language Models (LLMs), withreinforcement learning (RL) emerging as an effective post-training approach.Multimodal Large Language Models (MLLMs) inherit this reasoning potential butremain underexplored in tasks requiring both perception and logical reasoning.To address this, we introduce SEED-Bench-R1, a benchmark designed tosystematically evaluate post-training methods for MLLMs in video understanding.It includes intricate real-world videos and complex everyday planning tasks inthe format of multiple-choice questions, requiring sophisticated perception andreasoning. SEED-Bench-R1 assesses generalization through a three-levelhierarchy: in-distribution, cross-environment, and cross-environment-taskscenarios, equipped with a large-scale training dataset with easily verifiableground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RLwith supervised fine-tuning (SFT), demonstrating RL's data efficiency andsuperior performance on both in-distribution and out-of-distribution tasks,even outperforming SFT on general video understanding benchmarks likeLongVideoBench. Our detailed analysis reveals that RL enhances visualperception but often produces less logically coherent reasoning chains. Weidentify key limitations such as inconsistent reasoning and overlooked visualcues, and suggest future improvements in base model reasoning, reward modeling,and RL robustness against noisy signals.
  </details>

- **[Effectively Controlling Reasoning Models through Thinking Intervention](http://arxiv.org/abs/2503.24370v1)**  `arXiv:2503.24370`  `cs.CL` `cs.AI` `cs.LG`  
  _Tong Wu, Chong Xiang, Jiachen T. Wang, Prateek Mittal_
  <details open><summary>Abstract</summary>
  Reasoning-enhanced large language models (LLMs) explicitly generateintermediate reasoning steps prior to generating final answers, helping themodel excel in complex problem-solving. In this paper, we demonstrate that thisemerging generation framework offers a unique opportunity for more fine-grainedcontrol over model behavior. We propose Thinking Intervention, a novel paradigmdesigned to explicitly guide the internal reasoning processes of LLMs bystrategically inserting or revising specific thinking tokens. We conductcomprehensive evaluations across multiple tasks, including instructionfollowing on IFEval, instruction hierarchy on SEP, and safety alignment onXSTest and SORRY-Bench. Our results demonstrate that Thinking Interventionsignificantly outperforms baseline prompting approaches, achieving up to 6.7%accuracy gains in instruction-following scenarios, 15.4% improvements inreasoning about instruction hierarchies, and a 40.0% increase in refusal ratesfor unsafe prompts using open-source DeepSeek R1 models. Overall, our workopens a promising new research avenue for controlling reasoning LLMs.
  </details>

- **[ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion](http://arxiv.org/abs/2503.24354v1)**  `arXiv:2503.24354`  `cs.CL` `cs.AI` `cs.CV` `cs.LG`  
  _Rana Muhammad Shahroz Khan, Dongwen Tang, Pingzhi Li, Kai Wang, Tianlong Chen_
  <details open><summary>Abstract</summary>
  Parameter generation has emerged as a novel paradigm for neural networkdevelopment, offering an alternative to traditional neural network training bysynthesizing high-quality model weights directly. In the context of Low-RankAdaptation (LoRA) for evolving ($\textit{i.e.}$, constantly updated) largelanguage models (LLMs), this approach promises efficient adaptation withoutcostly retraining. However, existing methods face critical limitations insimultaneously achieving scalability and controllability. In this paper, weintroduce $\texttt{ORAL}$, a novel $\textbf{conditional recurrent diffusion}$framework that addresses these challenges. $\texttt{ORAL}$ incorporates a novelconditioning mechanism that integrates model architecture and textual taskspecifications, enabling the generation of task-specific LoRA parameters thatcan seamlessly transfer across evolving foundation models. Our approachsuccessfully scales to billions-of-parameter LLMs and maintainscontrollability. Through extensive experiments across seven language tasks,four vision tasks, and three multimodal tasks using five pre-trained LLMs, wedemonstrate that $\texttt{ORAL}$ generates high-quality LoRA parameters thatachieve comparable or superior performance to vanilla trained counterparts.
  </details>

- **[Is analogy enough to draw novel adjective-noun inferences?](http://arxiv.org/abs/2503.24293v1)**  `arXiv:2503.24293`  `cs.CL`  
  _Hayley Ross, Kathryn Davidson, Najoung Kim_
  <details open><summary>Abstract</summary>
  Recent work (Ross et al., 2025, 2024) has argued that the ability of humansand LLMs respectively to generalize to novel adjective-noun combinations showsthat they each have access to a compositional mechanism to determine thephrase's meaning and derive inferences. We study whether these inferences caninstead be derived by analogy to known inferences, without need forcomposition. We investigate this by (1) building a model of analogicalreasoning using similarity over lexical items, and (2) asking humanparticipants to reason by analogy. While we find that this strategy works wellfor a large proportion of the dataset of Ross et al. (2025), there are novelcombinations for which both humans and LLMs derive convergent inferences butwhich are not well handled by analogy. We thus conclude that the mechanismhumans and LLMs use to generalize in these cases cannot be fully reduced toanalogy, and likely involves composition.
  </details>

- **[Evaluating and Designing Sparse Autoencoders by Approximating Quasi-Orthogonality](http://arxiv.org/abs/2503.24277v1)**  `arXiv:2503.24277`  `cs.AI` `cs.LG`  
  _Sewoong Lee, Adam Davies, Marc E. Canby, Julia Hockenmaier_
  <details open><summary>Abstract</summary>
  Sparse autoencoders (SAEs) have emerged as a workhorse of modern mechanisticinterpretability, but leading SAE approaches with top-$k$ style activationfunctions lack theoretical grounding for selecting the hyperparameter $k$. SAEsare based on the linear representation hypothesis (LRH), which assumes that therepresentations of large language models (LLMs) are linearly encoded, and thesuperposition hypothesis (SH), which states that there can be more features inthe model than its dimensionality. We show that, based on the formaldefinitions of the LRH and SH, the magnitude of sparse feature vectors (thelatent representations learned by SAEs of the dense embeddings of LLMs) can beapproximated using their corresponding dense vector with a closed-form errorbound. To visualize this, we propose the ZF plot, which reveals a previouslyunknown relationship between LLM hidden embeddings and SAE feature vectors,allowing us to make the first empirical measurement of the extent to whichfeature vectors of pre-trained SAEs are over- or under-activated for a giveninput. Correspondingly, we introduce Approximate Feature Activation (AFA),which approximates the magnitude of the ground-truth sparse feature vector, andpropose a new evaluation metric derived from AFA to assess the alignmentbetween inputs and activations. We also leverage AFA to introduce a novel SAEarchitecture, the top-AFA SAE, leading to SAEs that: (a) are more in line withtheoretical justifications; and (b) obviate the need to tune SAE sparsityhyperparameters. Finally, we empirically demonstrate that top-AFA SAEs achievereconstruction loss comparable to that of state-of-the-art top-k SAEs, withoutrequiring the hyperparameter $k$ to be tuned. Our code is available at:https://github.com/SewoongLee/top-afa-sae.
  </details>

- **[Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation](http://arxiv.org/abs/2503.24245v1)**  `arXiv:2503.24245`  `cs.CL`  
  _Dun Yuan, Hao Zhou, Di Wu, Xue Liu, Hao Chen, Yan Xin, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) have made significant progress ingeneral-purpose natural language processing tasks. However, LLMs are stillfacing challenges when applied to domain-specific areas liketelecommunications, which demands specialized expertise and adaptability toevolving standards. This paper presents a novel framework that combinesknowledge graph (KG) and retrieval-augmented generation (RAG) techniques toenhance LLM performance in the telecom domain. The framework leverages a KG tocapture structured, domain-specific information about network protocols,standards, and other telecom-related entities, comprehensively representingtheir relationships. By integrating KG with RAG, LLMs can dynamically accessand utilize the most relevant and up-to-date knowledge during responsegeneration. This hybrid approach bridges the gap between structured knowledgerepresentation and the generative capabilities of LLMs, significantly enhancingaccuracy, adaptability, and domain-specific comprehension. Our resultsdemonstrate the effectiveness of the KG-RAG framework in addressing complextechnical queries with precision. The proposed KG-RAG model attained anaccuracy of 88% for question answering tasks on a frequently usedtelecom-specific dataset, compared to 82% for the RAG-only and 48% for theLLM-only approaches.
  </details>

- **[What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models](http://arxiv.org/abs/2503.24235v1)**  `arXiv:2503.24235`  `cs.CL` `cs.AI`  
  _Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, et al._
  <details open><summary>Abstract</summary>
  As enthusiasm for scaling computation (data and parameters) in thepretraining era gradually diminished, test-time scaling (TTS), also referred toas ``test-time computing'' has emerged as a prominent research focus. Recentstudies demonstrate that TTS can further elicit the problem-solvingcapabilities of large language models (LLMs), enabling significantbreakthroughs not only in specialized reasoning tasks, such as mathematics andcoding, but also in general tasks like open-ended Q&A. However, despite theexplosion of recent efforts in this area, there remains an urgent need for acomprehensive survey offering a systemic understanding. To fill this gap, wepropose a unified, multidimensional framework structured along four coredimensions of TTS research: what to scale, how to scale, where to scale, andhow well to scale. Building upon this taxonomy, we conduct an extensive reviewof methods, application scenarios, and assessment aspects, and present anorganized decomposition that highlights the unique functional roles ofindividual techniques within the broader TTS landscape. From this analysis, wedistill the major developmental trajectories of TTS to date and offer hands-onguidelines for practical deployment. Furthermore, we identify several openchallenges and offer insights into promising future directions, includingfurther scaling, clarifying the functional essence of techniques, generalizingto more tasks, and more attributions.
  </details>

- **[PAARS: Persona Aligned Agentic Retail Shoppers](http://arxiv.org/abs/2503.24228v1)**  `arXiv:2503.24228`  `cs.CL` `cs.MA` `cs.AI`  
  _Saab Mansour, Leonardo Perelli, Lorenzo Mainetti, George Davidson, Stefano D'Amato_
  <details open><summary>Abstract</summary>
  In e-commerce, behavioral data is collected for decision making which can becostly and slow. Simulation with LLM powered agents is emerging as a promisingalternative for representing human population behavior. However, LLMs are knownto exhibit certain biases, such as brand bias, review rating bias and limitedrepresentation of certain groups in the population, hence they need to becarefully benchmarked and aligned to user behavior. Ultimately, our goal is tosynthesise an agent population and verify that it collectively approximates areal sample of humans. To this end, we propose a framework that: (i) createssynthetic shopping agents by automatically mining personas from anonymisedhistorical shopping data, (ii) equips agents with retail-specific tools tosynthesise shopping sessions and (iii) introduces a novel alignment suitemeasuring distributional differences between humans and shopping agents at thegroup (i.e. population) level rather than the traditional "individual" level.Experimental results demonstrate that using personas improves performance onthe alignment suite, though a gap remains to human behaviour. We showcase aninitial application of our framework for automated agentic A/B testing andcompare the findings to human results. Finally, we discuss applications,limitations and challenges setting the stage for impactful future work.
  </details>

- **[Synthetic News Generation for Fake News Classification](http://arxiv.org/abs/2503.24206v1)**  `arXiv:2503.24206`  `cs.CL`  
  _Abdul Sittar, Luka Golob, Mateja Smiljanic_
  <details open><summary>Abstract</summary>
  This study explores the generation and evaluation of synthetic fake newsthrough fact based manipulations using large language models (LLMs). Weintroduce a novel methodology that extracts key facts from real articles,modifies them, and regenerates content to simulate fake news while maintainingcoherence. To assess the quality of the generated content, we propose a set ofevaluation metrics coherence, dissimilarity, and correctness. The research alsoinvestigates the application of synthetic data in fake news classification,comparing traditional machine learning models with transformer based modelssuch as BERT. Our experiments demonstrate that transformer models, especiallyBERT, effectively leverage synthetic data for fake news detection, showingimprovements with smaller proportions of synthetic data. Additionally, we findthat fact verification features, which focus on identifying factualinconsistencies, provide the most promising results in distinguishing syntheticfake news. The study highlights the potential of synthetic data to enhance fakenews detection systems, offering valuable insights for future research andsuggesting that targeted improvements in synthetic data generation can furtherstrengthen detection models.
  </details>

- **[TwT: Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers' Guidance](http://arxiv.org/abs/2503.24198v1)**  `arXiv:2503.24198`  `cs.CL`  
  _Jingxian Xu, Mengyu Zhou, Weichang Liu, Hanbing Liu, Shi Han, Dongmei Zhang_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have made significant strides in problem-solvingby incorporating reasoning processes. However, this enhanced reasoningcapability results in an increased number of output tokens during inference,leading to higher computational costs. To address this challenge, we proposeTwT (Thinking without Tokens), a method that reduces inference-time coststhrough habitual reasoning distillation with multi-teachers' guidance, whilemaintaining high performance. Our approach introduces a Habitual ReasoningDistillation method, which internalizes explicit reasoning into the model'shabitual behavior through a Teacher-Guided compression strategy inspired byhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling(DCRS), a technique that generates a high-quality and diverse distillationdataset using multiple teacher models, making our method suitable forunsupervised scenarios. Experimental results demonstrate that TwT effectivelyreduces inference costs while preserving superior performance, achieving up toa 13.6% improvement in accuracy with fewer output tokens compared to otherdistillation methods, offering a highly practical solution for efficient LLMdeployment.
  </details>

- **[Implicit In-Context Learning: Evidence from Artificial Language Experiments](http://arxiv.org/abs/2503.24190v1)**  `arXiv:2503.24190`  `cs.CL`  
  _Xiaomeng Ma, Qihui Xu_
  <details open><summary>Abstract</summary>
  Humans acquire language through implicit learning, absorbing complex patternswithout explicit awareness. While LLMs demonstrate impressive linguisticcapabilities, it remains unclear whether they exhibit human-like patternrecognition during in-context learning at inferencing level. We adapted threeclassic artificial language learning experiments spanning morphology,morphosyntax, and syntax to systematically evaluate implicit learning atinferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini.Our results reveal linguistic domain-specific alignment between models andhuman behaviors, o3-mini aligns better in morphology while both models align insyntax.
  </details>

- **[LLM4FS: Leveraging Large Language Models for Feature Selection and How to Improve It](http://arxiv.org/abs/2503.24157v1)**  `arXiv:2503.24157`  `cs.LG`  
  _Jianhao Li, Xianchao Xiu_
  <details open><summary>Abstract</summary>
  Recent advances in large language models (LLMs) have provided newopportunities for decision-making, particularly in the task of automatedfeature selection. In this paper, we first comprehensively evaluate LLM-basedfeature selection methods, covering the state-of-the-art DeepSeek-R1,GPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy calledLLM4FS that integrates LLMs with traditional data-driven methods. Specifically,input data samples into LLMs, and directly call traditional data-driventechniques such as random forest and forward sequential selection. Notably, ouranalysis reveals that the hybrid strategy leverages the contextualunderstanding of LLMs and the high statistical reliability of traditionaldata-driven methods to achieve excellent feature selection performance, evensurpassing LLMs and traditional data-driven methods. Finally, we point out thelimitations of its application in decision-making.
  </details>

- **[Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to Embodied Cognition](http://arxiv.org/abs/2503.24110v1)**  `arXiv:2503.24110`  `cs.CL` `cs.AI`  
  _Fran√ßois Olivier, Zied Bouraoui_
  <details open><summary>Abstract</summary>
  Despite advances in embodied AI, agent reasoning systems still struggle tocapture the fundamental conceptual structures that humans naturally use tounderstand and interact with their environment. To address this, we propose anovel framework that bridges embodied cognition theory and agent systems byleveraging a formal characterization of image schemas, which are defined asrecurring patterns of sensorimotor experience that structure human cognition.By customizing LLMs to translate natural language descriptions into formalrepresentations based on these sensorimotor patterns, we will be able to createa neurosymbolic system that grounds the agent's understanding in fundamentalconceptual structures. We argue that such an approach enhances both efficiencyand interpretability while enabling more intuitive human-agent interactionsthrough shared embodied understanding.
  </details>

- **[Is LLM the Silver Bullet to Low-Resource Languages Machine Translation?](http://arxiv.org/abs/2503.24102v1)**  `arXiv:2503.24102`  `cs.CL`  
  _Yewei Song, Lujun Li, Cedric Lothritz, Saad Ezzini, Lama Sleem, Niccolo Gentile, et al._
  <details open><summary>Abstract</summary>
  Low-Resource Languages (LRLs) present significant challenges in naturallanguage processing due to their limited linguistic resources andunderrepresentation in standard datasets. While recent advancements in LargeLanguage Models (LLMs) and Neural Machine Translation (NMT) have substantiallyimproved translation capabilities for high-resource languages, performancedisparities persist for LRLs, particularly impacting privacy-sensitive andresource-constrained scenarios. This paper systematically evaluates thelimitations of current LLMs across 200 languages using benchmarks such asFLORES-200. We also explore alternative data sources, including news articlesand bilingual dictionaries, and demonstrate how knowledge distillation fromlarge pre-trained models can significantly improve smaller LRL translations.Additionally, we investigate various fine-tuning strategies, revealing thatincremental enhancements markedly reduce performance gaps on smaller LLMs.
  </details>

- **[DANTE-AD: Dual-Vision Attention Network for Long-Term Audio Description](http://arxiv.org/abs/2503.24096v1)**  `arXiv:2503.24096`  `cs.CV`  
  _Adrienne Deganutti, Simon Hadfield, Andrew Gilbert_
  <details open><summary>Abstract</summary>
  Audio Description is a narrated commentary designed to aid vision-impairedaudiences in perceiving key visual elements in a video. While short-form videounderstanding has advanced rapidly, a solution for maintaining coherentlong-term visual storytelling remains unresolved. Existing methods rely solelyon frame-level embeddings, effectively describing object-based content butlacking contextual information across scenes. We introduce DANTE-AD, anenhanced video description model leveraging a dual-vision Transformer-basedarchitecture to address this gap. DANTE-AD sequentially fuses both frame andscene level embeddings to improve long-term contextual understanding. Wepropose a novel, state-of-the-art method for sequential cross-attention toachieve contextual grounding for fine-grained audio description generation.Evaluated on a broad range of key scenes from well-known movie clips, DANTE-ADoutperforms existing methods across traditional NLP metrics and LLM-basedevaluations.
  </details>

- **[TransMamba: Flexibly Switching between Transformer and Mamba](http://arxiv.org/abs/2503.24067v1)**  `arXiv:2503.24067`  `cs.LG`  
  _Yixing Li, Ruobing Xie, Zhen Yang, Xingwu Sun, Shuaipeng Li, Weidong Han, et al._
  <details open><summary>Abstract</summary>
  Transformers are the cornerstone of modern large language models, but theirquadratic computational complexity limits efficiency in long-sequenceprocessing. Recent advancements in Mamba, a state space model (SSM) with linearcomplexity, offer promising efficiency gains but suffer from unstablecontextual learning and multitask generalization. This paper proposesTransMamba, a novel framework that unifies Transformer and Mamba through sharedparameter matrices (e.g., QKV and CBx), and thus could dynamically switchbetween attention and SSM mechanisms at different token lengths and layers. Wedesign the Memory converter to bridge Transformer and Mamba by convertingattention outputs into SSM-compatible states, ensuring seamless informationflow at TransPoints where the transformation happens. The TransPoint schedulingis also thoroughly explored for further improvements. We conducted extensiveexperiments demonstrating that TransMamba achieves superior training efficiencyand performance compared to baselines, and validated the deeper consistencybetween Transformer and Mamba paradigms, offering a scalable solution fornext-generation sequence modeling.
  </details>

- **[Artificial Conversations, Real Results: Fostering Language Detection with Synthetic Data](http://arxiv.org/abs/2503.24062v1)**  `arXiv:2503.24062`  `cs.CL` `cs.LG` `cs.AI`  
  _Fatemeh Mohammadi, Tommaso Romano, Samira Maghool, Paolo Ceravolo_
  <details open><summary>Abstract</summary>
  Collecting high-quality training data is essential for fine-tuning LargeLanguage Models (LLMs). However, acquiring such data is often costly andtime-consuming, especially for non-English languages such as Italian. Recently,researchers have begun to explore the use of LLMs to generate syntheticdatasets as a viable alternative. This study proposes a pipeline for generatingsynthetic data and a comprehensive approach for investigating the factors thatinfluence the validity of synthetic data generated by LLMs by examining howmodel performance is affected by metrics such as prompt strategy, text lengthand target position in a specific task, i.e. inclusive language detection inItalian job advertisements. Our results show that, in most cases and acrossdifferent metrics, the fine-tuned models trained on synthetic data consistentlyoutperformed other models on both real and synthetic test datasets. The studydiscusses the practical implications and limitations of using synthetic datafor language detection tasks with LLMs.
  </details>

- **[Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents](http://arxiv.org/abs/2503.24047v1)**  `arXiv:2503.24047`  `cs.MA` `cs.AI`  
  _Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, Jiajun Zhang_
  <details open><summary>Abstract</summary>
  As scientific research becomes increasingly complex, innovative tools areneeded to manage vast data, facilitate interdisciplinary collaboration, andaccelerate discovery. Large language models (LLMs) are now evolving intoLLM-based scientific agents that automate critical tasks, ranging fromhypothesis generation and experiment design to data analysis and simulation.Unlike general-purpose LLMs, these specialized agents integrate domain-specificknowledge, advanced tool sets, and robust validation mechanisms, enabling themto handle complex data types, ensure reproducibility, and drive scientificbreakthroughs. This survey provides a focused review of the architectures,design, benchmarks, applications, and ethical considerations surroundingLLM-based scientific agents. We highlight why they differ from general agentsand the ways in which they advance research across various scientific fields.By examining their development and challenges, this survey offers acomprehensive roadmap for researchers and practitioners to harness these agentsfor more efficient, reliable, and ethically sound scientific discovery.
  </details>

- **[Pay More Attention to the Robustness of Prompt for Instruction Data Mining](http://arxiv.org/abs/2503.24028v1)**  `arXiv:2503.24028`  `cs.AI`  
  _Qiang Wang, Dawei Feng, Xu Zhang, Ao Shen, Yang Xu, Bo Ding, et al._
  <details open><summary>Abstract</summary>
  Instruction tuning has emerged as a paramount method for tailoring thebehaviors of LLMs. Recent work has unveiled the potential for LLMs to achievehigh performance through fine-tuning with a limited quantity of high-qualityinstruction data. Building upon this approach, we further explore the impact ofprompt's robustness on the selection of high-quality instruction data. Thispaper proposes a pioneering framework of high-quality online instruction datamining for instruction tuning, focusing on the impact of prompt's robustness onthe data mining process. Our notable innovation, is to generate the adversarialinstruction data by conducting the attack for the prompt of online instructiondata. Then, we introduce an Adversarial Instruction-Following Difficulty metricto measure how much help the adversarial instruction data can provide to thegeneration of the corresponding response. Apart from it, we propose a novelAdversarial Instruction Output Embedding Consistency approach to selecthigh-quality online instruction data. We conduct extensive experiments on twobenchmark datasets to assess the performance. The experimental results serve tounderscore the effectiveness of our proposed two methods. Moreover, the resultsunderscore the critical practical significance of considering prompt'srobustness.
  </details>

- **[H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding](http://arxiv.org/abs/2503.24008v1)**  `arXiv:2503.24008`  `cs.CV` `cs.AI`  
  _Qi Wu, Quanlong Zheng, Yanhao Zhang, Junlin Xie, Jinguo Luo, Kuo Wang, et al._
  <details open><summary>Abstract</summary>
  With the rapid development of multimodal models, the demand for assessingvideo understanding capabilities has been steadily increasing. However,existing benchmarks for evaluating video understanding exhibit significantlimitations in coverage, task diversity, and scene adaptability. Theseshortcomings hinder the accurate assessment of models' comprehensive videounderstanding capabilities. To tackle this challenge, we propose a hierarchicaland holistic video understanding (H2VU) benchmark designed to evaluate bothgeneral video and online streaming video comprehension. This benchmarkcontributes three key features:  Extended video duration: Spanning videos from brief 3-second clips tocomprehensive 1.5-hour recordings, thereby bridging the temporal gaps found incurrent benchmarks. Comprehensive assessment tasks: Beyond traditionalperceptual and reasoning tasks, we have introduced modules forcountercommonsense comprehension and trajectory state tracking. These additionstest the models' deep understanding capabilities beyond mere prior knowledge.Enriched video data: To keep pace with the rapid evolution of current AIagents, we have expanded first-person streaming video datasets. This expansionallows for the exploration of multimodal models' performance in understandingstreaming videos from a first-person perspective. Extensive results from H2VUreveal that existing multimodal large language models (MLLMs) possesssubstantial potential for improvement in our newly proposed evaluation tasks.We expect that H2VU will facilitate advancements in video understandingresearch by offering a comprehensive and in-depth analysis of MLLMs.
  </details>

- **[Rethinking Key-Value Cache Compression Techniques for Large Language Model Serving](http://arxiv.org/abs/2503.24000v1)**  `arXiv:2503.24000`  `cs.AI` `cs.LG`  
  _Wei Gao, Xinyu Zhou, Peng Sun, Tianwei Zhang, Yonggang Wen_
  <details open><summary>Abstract</summary>
  Key-Value cache (\texttt{KV} \texttt{cache}) compression has emerged as apromising technique to optimize Large Language Model (LLM) serving. Itprimarily decreases the memory consumption of \texttt{KV} \texttt{cache} toreduce the computation cost. Despite the development of many compressionalgorithms, their applications in production environments are still notprevalent. In this paper, we revisit mainstream \texttt{KV} \texttt{cache}compression solutions from a practical perspective. Our contributions arethree-fold. First, we comprehensively review existing algorithmic designs andbenchmark studies for \texttt{KV} \texttt{cache} compression and identifymissing pieces in their performance measurement, which could hinder theiradoption in practice. Second, we empirically evaluate representative\texttt{KV} \texttt{cache} compression methods to uncover two key issues thataffect the computational efficiency: (1) while compressing \texttt{KV}\texttt{cache} can reduce memory consumption, current implementations (e.g.,FlashAttention, PagedAttention) do not optimize for production-level LLMserving, resulting in suboptimal throughput performance; (2) compressing\texttt{KV} \texttt{cache} may lead to longer outputs, resulting in increasedend-to-end latency. We further investigate the accuracy performance ofindividual samples rather than the overall performance, revealing the intrinsiclimitations in \texttt{KV} \texttt{cache} compression when handling specificLLM tasks. Third, we provide tools to shed light on future \texttt{KV}\texttt{cache} compression studies and facilitate their practical deployment inproduction. They are open-sourced in\href{https://github.com/LLMkvsys/rethink-kv-compression}{https://github.com/LLMkvsys/rethink-kv-compression}.
  </details>

- **[Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in Discriminative and Generative AI Operations](http://arxiv.org/abs/2503.23934v1)**  `arXiv:2503.23934`  `cs.AI` `cs.LG`  
  _Adri√°n S√°nchez-Momp√≥, Ioannis Mavromatis, Peizheng Li, Konstantinos Katsaros, Aftab Khan_
  <details open><summary>Abstract</summary>
  This study presents an empirical investigation into the energy consumption ofDiscriminative and Generative AI models within real-world MLOps pipelines. ForDiscriminative models, we examine various architectures and hyperparametersduring training and inference and identify energy-efficient practices. ForGenerative AI, Large Language Models (LLMs) are assessed, focusing primarily onenergy consumption across different model sizes and varying service requests.Our study employs software-based power measurements, ensuring ease ofreplication across diverse configurations, models, and datasets. We analysemultiple models and hardware setups to uncover correlations among variousmetrics, identifying key contributors to energy consumption. The resultsindicate that for Discriminative models, optimising architectures,hyperparameters, and hardware can significantly reduce energy consumptionwithout sacrificing performance. For LLMs, energy efficiency depends onbalancing model size, reasoning complexity, and request-handling capacity, aslarger models do not necessarily consume more energy when utilisation remainslow. This analysis provides practical guidelines for designing green andsustainable ML operations, emphasising energy consumption and carbon footprintreductions while maintaining performance. This paper can serve as a benchmarkfor accurately estimating total energy use across different types of AI models.
  </details>

- **[Model Hemorrhage and the Robustness Limits of Large Language Models](http://arxiv.org/abs/2503.23924v1)**  `arXiv:2503.23924`  `cs.CL` `cs.LG`  
  _Ziyang Ma, Zuchao Li, Lefei Zhang, Gui-Song Xia, Bo Du, Liangpei Zhang, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) demonstrate strong performance across naturallanguage processing tasks, yet undergo significant performance degradation whenmodified for deployment through quantization, pruning, or decoding strategyadjustments. We define this phenomenon as model hemorrhage - performancedecline caused by parameter alterations and architectural changes. Throughsystematic analysis of various LLM frameworks, we identify key vulnerabilitypatterns: layer expansion frequently disrupts attention mechanisms, compressiontechniques induce information loss cascades, and decoding adjustments amplifyprediction divergences. Our investigation reveals transformer architecturesexhibit inherent robustness thresholds that determine hemorrhage severityacross modification types. We propose three mitigation strategies:gradient-aware pruning preserves critical weight pathways, dynamic quantizationscaling maintains activation integrity, and decoding calibration alignsgeneration trajectories with original model distributions. This workestablishes foundational metrics for evaluating model stability duringadaptation, providing practical guidelines for maintaining performance whileenabling efficient LLM deployment. Our findings advance understanding of neuralnetwork resilience under architectural transformations, particularly forlarge-scale language models.
  </details>

- **[Entropy-Based Adaptive Weighting for Self-Training](http://arxiv.org/abs/2503.23913v1)**  `arXiv:2503.23913`  `cs.CL`  
  _Xiaoxuan Wang, Yihe Deng, Mingyu Derek Ma, Wei Wang_
  <details open><summary>Abstract</summary>
  The mathematical problem-solving capabilities of large language models havebecome a focal point of research, with growing interests in leveragingself-generated reasoning paths as a promising way to refine and enhance thesemodels. These paths capture step-by-step logical processes while requiring onlythe correct answer for supervision. The self-training method has been shown tobe effective in reasoning tasks while eliminating the need for external modelsand manual annotations. However, optimizing the use of self-generated data formodel training remains an open challenge. In this work, we proposeEntropy-Based Adaptive Weighting for Self-Training (EAST), an adaptiveweighting strategy designed to prioritize uncertain data during self-training.Specifically, EAST employs a mapping function with a tunable parameter thatcontrols the sharpness of the weighting, assigning higher weights to data wherethe model exhibits greater uncertainty. This approach guides the model to focuson more informative and challenging examples, thereby enhancing its reasoningability. We evaluate our approach on GSM8K and MATH benchmarks. Empiricalresults show that, while the vanilla method yields virtually no improvement(0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K,EAST attains a further 1-2% performance boost compared to the vanilla method.
  </details>

- **[Better wit than wealth: Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement](http://arxiv.org/abs/2503.23895v1)**  `arXiv:2503.23895`  `cs.CL` `cs.AI`  
  _Yuqiao Tan, Shizhu He, Huanxuan Liao, Jun Zhao, Kang Liu_
  <details open><summary>Abstract</summary>
  Retrieval-augmented generation (RAG) enhances large language models (LLMs) byretrieving relevant documents from external sources and incorporating them intothe context. While it improves reliability by providing factual texts, itsignificantly increases inference costs as context length grows and introduceschallenging issue of RAG hallucination, primarily caused by the lack ofcorresponding parametric knowledge in LLMs. An efficient solution is to enhancethe knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this byembedding document into LLMs parameters to perform test-time knowledgeenhancement, effectively reducing inference costs through offline training.However, its high training and storage costs, along with limited generalizationability, significantly restrict its practical adoption. To address thesechallenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework thatleverages a lightweight parameter translator model to efficiently convertdocuments into parametric knowledge. DyPRAG not only reduces inference,training, and storage costs but also dynamically generates parametricknowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledgeconflicts in a plug-and-play manner at test-time. Extensive experiments onmultiple datasets demonstrate the effectiveness and generalization capabilitiesof DyPRAG, offering a powerful and practical RAG paradigm which enablessuperior knowledge fusion and mitigates RAG hallucination in real-worldapplications. Our code is available at https://github.com/Trae1ounG/DyPRAG.
  </details>

- **[GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via Language Models](http://arxiv.org/abs/2503.23875v1)**  `arXiv:2503.23875`  `cs.MA` `cs.RO` `cs.AI`  
  _Wenkang Ji, Huaben Chen, Mingyang Chen, Guobin Zhu, Lufeng Xu, Roderich Gro√ü, et al._
  <details open><summary>Abstract</summary>
  The development of control policies for multi-robot systems traditionallyfollows a complex and labor-intensive process, often lacking the flexibility toadapt to dynamic tasks. This has motivated research on methods to automaticallycreate control policies. However, these methods require iterative processes ofmanually crafting and refining objective functions, thereby prolonging thedevelopment cycle. This work introduces \textit{GenSwarm}, an end-to-end systemthat leverages large language models to automatically generate and deploycontrol policies for multi-robot tasks based on simple user instructions innatural language. As a multi-language-agent system, GenSwarm achieves zero-shotlearning, enabling rapid adaptation to altered or unseen tasks. The white-boxnature of the code policies ensures strong reproducibility andinterpretability. With its scalable software and hardware architectures,GenSwarm supports efficient policy deployment on both simulated and real-worldmulti-robot systems, realizing an instruction-to-execution end-to-endfunctionality that could prove valuable for robotics specialists andnon-specialists alike.The code of the proposed GenSwarm system is availableonline: https://github.com/WindyLab/GenSwarm.
  </details>

- **[Communication-Efficient and Personalized Federated Foundation Model Fine-Tuning via Tri-Matrix Adaptation](http://arxiv.org/abs/2503.23869v1)**  `arXiv:2503.23869`  `cs.LG`  
  _Yongle Li, Bo Liu, Sheng Huang, ZHeng ZHang, Xiaotong Yuan, Richang Hong_
  <details open><summary>Abstract</summary>
  In federated learning, fine-tuning pre-trained foundation models posessignificant challenges, particularly regarding high communication cost andsuboptimal model performance due to data heterogeneity between the clients. Toaddress these issues, this paper introduces communication-efficient federatedLoRA adaption (CE-LoRA), a method that employs a tri-factorization low-rankadaptation approach with personalized model parameter aggregation. We firstpresents a novel LoRA parameter factorization by introducing a small-size densematrix, which can significantly reduce the communication cost and achievecomparable empirical performance than transferring the low-rank parametermatrix used by existing methods. Without violating data privacy, the serverconsiders the client similarity in both training dataset and model parameterspace, and learns personalized weights for model aggregation. Our experimentson various LLM and VLM fine-tuning tasks demonstrate that CE-LoRA not onlysignificantly reduces communication overhead but also improves performanceunder not independently and identically distributed data conditions. Inaddition, CE-LoRA improves data privacy protection, effectively mitigatinggradient-based data reconstruction attacks.
  </details>

- **[SpeechDialogueFactory: Generating High-Quality Speech Dialogue Data to Accelerate Your Speech-LLM Development](http://arxiv.org/abs/2503.23848v1)**  `arXiv:2503.23848`  `cs.CL`  
  _Minghan Wang, Ye Bai, Yuxia Wang, Thuy-Trang Vu, Ehsan Shareghi, Gholamreza Haffari_
  <details open><summary>Abstract</summary>
  High-quality speech dialogue datasets are crucial for Speech-LLM development,yet existing acquisition methods face significant limitations. Human recordingsincur high costs and privacy concerns, while synthetic approaches often lackconversational authenticity. To address these challenges, we introduce\textsc{SpeechDialogueFactory}, a production-ready framework for generatingnatural speech dialogues efficiently. Our solution employs a comprehensivepipeline including metadata generation, dialogue scripting,paralinguistic-enriched utterance simulation, and natural speech synthesis withvoice cloning. Additionally, the system provides an interactive UI for detailedsample inspection and a high-throughput batch synthesis mode. Evaluations showthat dialogues generated by our system achieve a quality comparable to humanrecordings while significantly reducing production costs. We release our workas an open-source toolkit, alongside example datasets available in English andChinese, empowering researchers and developers in Speech-LLM research anddevelopment.
  </details>

- **[Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning](http://arxiv.org/abs/2503.22456v2)**  `arXiv:2503.22456`  `cs.AI` `cs.LG`  
  _Abdullah Vanlioglu_
  <details open><summary>Abstract</summary>
  We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach thatenhances the exploration-exploitation tradeoff by dynamically assigning weightsto generated outputs based on their advantage and entropy for ReinforcementLearning-based Large Language Model fine-tuning. EGSW integrates entropyregularization with advantage-based weighting to balance policy updates,enabling efficient exploration in high-dimensional state spaces. By employingtemperature-scaled softmax weighting over sequences, EGSW prioritizinghigh-reward, high-uncertainty steps while maintaining training stability.Although originally developed to improve Group Relative Policy Optimization(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable toother reinforcement learning (RL) algorithms and can be implemented in bothstep-wise and trajectory-wise settings. Empirical evaluations demonstrate thatEGSW enhances GRPO reasoning ability, yielding improvements in sampleefficiency. Future work will explore the application of EGSW to advanced RLmethodologies.
  </details>

- **[Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback](http://arxiv.org/abs/2503.22230v2)**  `arXiv:2503.22230`  `cs.LG`  
  _Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, et al._
  <details open><summary>Abstract</summary>
  Reinforcement Learning from Human Feedback (RLHF) is crucial for aligninglarge language models with human preferences. While recent research has focusedon algorithmic improvements, the importance of prompt-data construction hasbeen overlooked. This paper addresses this gap by exploring data-drivenbottlenecks in RLHF performance scaling, particularly reward hacking anddecreasing response diversity. We introduce a hybrid reward system combiningreasoning task verifiers (RTV) and a generative reward model (GenRM) tomitigate reward hacking. We also propose a novel prompt-selection method,Pre-PPO, to maintain response diversity and enhance learning effectiveness.Additionally, we find that prioritizing mathematical and coding tasks early inRLHF training significantly improves performance. Experiments across two modelsizes validate our methods' effectiveness and scalability. Results show thatRTV is most resistant to reward hacking, followed by GenRM with ground truth,and then GenRM with SFT Best-of-N responses. Our strategies enable rapidcapture of subtle task-specific distinctions, leading to substantialimprovements in overall RLHF performance. This work highlights the importanceof careful data construction and provides practical methods to overcomeperformance barriers in RLHF.
  </details>

- **[EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues](http://arxiv.org/abs/2503.21080v3)**  `arXiv:2503.21080`  `cs.CL`  
  _Yuhan Liu, Yunbo Long_
  <details open><summary>Abstract</summary>
  While large language model (LLM)-based chatbots have been applied foreffective engagement in credit dialogues, their capacity for dynamic emotionalexpression remains limited. Current agents primarily rely on passive empathyrather than affective reasoning. For instance, when faced with persistentclient negativity, the agent should employ strategic emotional adaptation byexpressing measured anger to discourage counterproductive behavior and guidethe conversation toward resolution. This context-aware emotional modulation isessential for imitating the nuanced decision-making of human negotiators. Thispaper introduces an EQ-negotiator that combines emotion sensing frompre-trained language models (PLMs) with emotional reasoning based on GameTheory and Hidden Markov Models. It takes into account both the current andhistorical emotions of the client to better manage and address negativeemotions during interactions. By fine-tuning pre-trained language models (PLMs)on public emotion datasets and validating them on the credit dialogue datasets,our approach enables LLM-based agents to effectively capture shifts in clientemotions and dynamically adjust their response tone based on our emotiondecision policies in real-world financial negotiations. This EQ-negotiator canalso help credit agencies foster positive client relationships, enhancingsatisfaction in credit services.
  </details>

- **[Surgical Action Planning with Large Language Models](http://arxiv.org/abs/2503.18296v2)**  `arXiv:2503.18296`  `cs.CL`  
  _Mengya Xu, Zhongzhen Huang, Jie Zhang, Xiaofan Zhang, Qi Dou_
  <details open><summary>Abstract</summary>
  In robot-assisted minimally invasive surgery, we introduce the SurgicalAction Planning (SAP) task, which generates future action plans from visualinputs to address the absence of intraoperative predictive planning in currentintelligent applications. SAP shows great potential for enhancingintraoperative guidance and automating procedures. However, it faces challengessuch as understanding instrument-action relationships and tracking surgicalprogress. Large Language Models (LLMs) show promise in understanding surgicalvideo content but remain underexplored for predictive decision-making in SAP,as they focus mainly on retrospective analysis. Challenges like data privacy,computational demands, and modality-specific constraints further highlightsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, aLarge Language Models-based Surgical Action Planning framework that predictsfuture actions and generates text responses by interpreting natural languageprompts of surgical goals. The text responses potentially support surgicaleducation, intraoperative decision-making, procedure documentation, and skillanalysis. LLM-SAP integrates two novel modules: the Near-History Focus MemoryModule (NHF-MM) for modeling historical states and the prompts factory foraction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP datasetusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness innext-action prediction. Pre-trained LLMs are tested in a zero-shot setting, andsupervised fine-tuning (SFT) with LoRA is implemented. Our experiments showthat Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy.
  </details>

- **[PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models](http://arxiv.org/abs/2502.01584v3)**  `arXiv:2502.01584`  `cs.LG` `cs.AI`  
  _Zixuan Wu, Francesca Lucchetti, Aleksander Boruch-Gruszecki, Jingmiao Zhao, Carolyn Jane Anderson, Joydeep Biswas, et al._
  <details open><summary>Abstract</summary>
  Existing benchmarks for frontier models often test specialized, "PhD-level"knowledge that is difficult for non-experts to grasp. In contrast, we present abenchmark with 594 problems based on the NPR Sunday Puzzle Challenge thatrequires only general knowledge. Our benchmark is challenging for both humansand models; however correct solutions are easy to verify, and models' mistakesare easy to spot. As LLMs are more widely deployed in society, we believe it isuseful to develop benchmarks for frontier models that humans can understandwithout the need for deep domain expertise.  Our work reveals capability gaps that are not evident in existing benchmarks:OpenAI o1 significantly outperforms other reasoning models on our benchmark,despite being on par with other models when tested on benchmarks that testspecialized knowledge. Furthermore, our analysis of reasoning outputs uncoversnew kinds of failures. DeepSeek R1, for instance, often concedes with "I giveup" before providing an answer that it knows is wrong. R1 can also beremarkably "uncertain" in its output and in rare cases, it does not "finishthinking," which suggests the need for techniques to "wrap up" before thecontext window limit is reached. We also quantify the effectiveness ofreasoning longer to identify the point beyond which more reasoning is unlikelyto improve accuracy on our benchmark.
  </details>

- **[Truth or Mirage? Towards End-to-End Factuality Evaluation with LLM-Oasis](http://arxiv.org/abs/2411.19655v3)**  `arXiv:2411.19655`  `cs.CL`  
  _Alessandro Scir√®, Andrei Stefan Bejgu, Simone Tedeschi, Karim Ghonim, Federico Martelli, Roberto Navigli_
  <details open><summary>Abstract</summary>
  After the introduction of Large Language Models (LLMs), there have beensubstantial improvements in the performance of Natural Language Generation(NLG) tasks, including Text Summarization and Machine Translation. However,LLMs still produce outputs containing hallucinations, that is, content notgrounded in factual information. Therefore, developing methods to assess thefactuality of LLMs has become urgent.  Indeed, resources for factuality evaluation have recently emerged. Althoughchallenging, these resources face one or more of the following limitations: (i)they are tailored to a specific task or domain; (ii) they are limited in size,thereby preventing the training of new factuality evaluators; (iii) they aredesigned for simpler verification tasks, such as claim verification.  To address these issues, we introduce LLM-Oasis, to the best of our knowledgethe largest resource for training end-to-end factuality evaluators. LLM-Oasisis constructed by extracting claims from Wikipedia, falsifying a subset ofthese claims, and generating pairs of factual and unfactual texts. We then relyon human annotators to both validate the quality of our dataset and to create agold standard test set for benchmarking factuality evaluation systems.  Our experiments demonstrate that LLM-Oasis presents a significant challengefor state-of-the-art LLMs, with GPT-4o achieving up to 60% accuracy in ourproposed end-to-end factuality evaluation task, highlighting its potential todrive future research in the field.
  </details>

- **[How Does A Text Preprocessing Pipeline Affect Ontology Syntactic Matching?](http://arxiv.org/abs/2411.03962v5)**  `arXiv:2411.03962`  `cs.CL`  
  _Zhangcheng Qiang, Kerry Taylor, Weiqing Wang_
  <details open><summary>Abstract</summary>
  The classic text preprocessing pipeline, comprising Tokenisation,Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has beenimplemented in many systems for syntactic ontology matching (OM). However, thelack of standardisation in text preprocessing creates diversity in mappingresults. In this paper we investigate the effect of the text preprocessingpipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)tracks with 49 distinct alignments. We find that Phase 1 text preprocessing(Tokenisation and Normalisation) is more effective than Phase 2 textpreprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair theunwanted false mappings caused by Phase 2 text preprocessing, we propose anovel context-based pipeline repair approach that employs a post hoc check tofind common words that cause false mappings. These words are stored in areserved word set and applied in text preprocessing. The experimental resultsshow that our approach improves the matching correctness and the overallmatching performance. We then consider the broader integration of the classictext preprocessing pipeline with modern large language models (LLMs) for OM. Werecommend that (1) the text preprocessing pipeline be injected via functioncalling into LLMs to avoid the tendency towards unstable true mappings producedby LLM prompting; or (2) LLMs be used to repair non-existent andcounter-intuitive false mappings generated by the text preprocessing pipeline.
  </details>

- **[CASA: Class-Agnostic Shared Attributes in Vision-Language Models for Efficient Incremental Object Detection](http://arxiv.org/abs/2410.05804v3)**  `arXiv:2410.05804`  `cs.CV`  
  _Mingyi Guo, Yuyang Liu, Zhiyuan Yan, Zongying Lin, Peixi Peng, Yonghong Tian_
  <details open><summary>Abstract</summary>
  Incremental object detection is fundamentally challenged by catastrophicforgetting. A major factor contributing to this issue is background shift,where background categories in sequential tasks may overlap with eitherpreviously learned or future unseen classes. To address this, we propose anovel method called Class-Agnostic Shared Attribute Base (CASA) that encouragesthe model to learn category-agnostic attributes shared across incrementalclasses. Our approach leverages an LLM to generate candidate textualattributes, selects the most relevant ones based on the current training data,and records their importance in an assignment matrix. For subsequent tasks, theretained attributes are frozen, and new attributes are selected from theremaining candidates, ensuring both knowledge retention and adaptability.Extensive experiments on the COCO dataset demonstrate the state-of-the-artperformance of our method.
  </details>

- **[ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery](http://arxiv.org/abs/2410.05080v3)**  `arXiv:2410.05080`  `cs.CL` `cs.LG` `cs.AI`  
  _Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, et al._
  <details open><summary>Abstract</summary>
  The advancements of large language models (LLMs) have piqued growing interestin developing LLM-based language agents to automate scientific discoveryend-to-end, which has sparked both excitement and skepticism about their truecapabilities. In this work, we call for rigorous assessment of agents onindividual tasks in a scientific workflow before making bold claims onend-to-end automation. To this end, we present ScienceAgentBench, a newbenchmark for evaluating language agents for data-driven scientific discovery.To ensure the scientific authenticity and real-world relevance of ourbenchmark, we extract 102 tasks from 44 peer-reviewed publications in fourdisciplines and engage nine subject matter experts to validate them. We unifythe target output for every task to a self-contained Python program file andemploy an array of evaluation metrics to examine the generated programs,execution results, and costs. Each task goes through multiple rounds of manualvalidation by annotators and subject matter experts to ensure its annotationquality and scientific plausibility. We also propose two effective strategiesto mitigate data contamination concerns. Using ScienceAgentBench, we evaluatefive open-weight and proprietary LLMs, each with three frameworks: directprompting, OpenHands CodeAct, and self-debug. Given three attempts for eachtask, the best-performing agent can only solve 32.4% of the tasks independentlyand 34.3% with expert-provided knowledge. In addition, we evaluate OpenAIo1-preview with direct prompting and self-debug, which can boost theperformance to 42.2%, demonstrating the effectiveness of increasinginference-time compute but with more than 10 times the cost of other LLMs.Still, our results underscore the limitations of current language agents ingenerating code for data-driven discovery, let alone end-to-end automation forscientific research.
  </details>

- **[Fast and Accurate Task Planning using Neuro-Symbolic Language Models and Multi-level Goal Decomposition](http://arxiv.org/abs/2409.19250v2)**  `arXiv:2409.19250`  `cs.RO`  
  _Minseo Kwon, Yaesol Kim, Young J. Kim_
  <details open><summary>Abstract</summary>
  In robotic task planning, symbolic planners using rule-based representationslike PDDL are effective but struggle with long-sequential tasks in complicatedenvironments due to exponentially increasing search space. Meanwhile, LLM-basedapproaches, which are grounded in artificial neural networks, offer fasterinference and commonsense reasoning but suffer from lower success rates. Toaddress the limitations of the current symbolic (slow speed) or LLM-basedapproaches (low accuracy), we propose a novel neuro-symbolic task planner thatdecomposes complex tasks into subgoals using LLM and carries out task planningfor each subgoal using either symbolic or MCTS-based LLM planners, depending onthe subgoal complexity. This decomposition reduces planning time and improvessuccess rates by narrowing the search space and enabling LLMs to focus on moremanageable tasks. Our method significantly reduces planning time whilemaintaining high success rates across task planning domains, as well asreal-world and simulated robotics environments. More details are available athttp://graphics.ewha.ac.kr/LLMTAMP/.
  </details>

- **[MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty](http://arxiv.org/abs/2408.06816v2)**  `arXiv:2408.06816`  `cs.CL` `cs.AI`  
  _Yongjin Yang, Haneul Yoo, Hwaran Lee_
  <details open><summary>Abstract</summary>
  Despite the massive advancements in large language models (LLMs), they stillsuffer from producing plausible but incorrect responses. To improve thereliability of LLMs, recent research has focused on uncertainty quantificationto predict whether a response is correct or not. However, most uncertaintyquantification methods have been evaluated on single-labeled questions, whichremoves data uncertainty: the irreducible randomness often present in userqueries, which can arise from factors like multiple possible answers. Thislimitation may cause uncertainty quantification results to be unreliable inpractical settings. In this paper, we investigate previous uncertaintyquantification methods under the presence of data uncertainty. Ourcontributions are two-fold: 1) proposing a new Multi-Answer Question Answeringdataset, MAQA, consisting of world knowledge, mathematical reasoning, andcommonsense reasoning tasks to evaluate uncertainty quantification regardingdata uncertainty, and 2) assessing 5 uncertainty quantification methods ofdiverse white- and black-box LLMs. Our findings show that previous methodsrelatively struggle compared to single-answer settings, though this variesdepending on the task. Moreover, we observe that entropy- and consistency-basedmethods effectively estimate model uncertainty, even in the presence of datauncertainty. We believe these observations will guide future work onuncertainty quantification in more realistic settings.
  </details>
