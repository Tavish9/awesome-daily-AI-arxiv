# üîç LLM Papers ¬∑ 2025-03-27

[![Total Papers](https://img.shields.io/badge/Papers-80-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `LLM` `Large Language Model` `GPT`  
**Filter**: `None`

---

## üìö Paper List

- **[Video-R1: Reinforcing Video Reasoning in MLLMs](http://arxiv.org/abs/2503.21776v1)**  `arXiv:2503.21776`  `cs.CV`  
  _Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, et al._
  <details open><summary>Abstract</summary>
  Inspired by DeepSeek-R1's success in eliciting reasoning abilities throughrule-based reinforcement learning (RL), we introduce Video-R1 as the firstattempt to systematically explore the R1 paradigm for eliciting video reasoningwithin multimodal large language models (MLLMs). However, directly applying RLtraining with the GRPO algorithm to video reasoning presents two primarychallenges: (i) a lack of temporal modeling for video reasoning, and (ii) thescarcity of high-quality video-reasoning data. To address these issues, wefirst propose the T-GRPO algorithm, which encourages models to utilize temporalinformation in videos for reasoning. Additionally, instead of relying solely onvideo data, we incorporate high-quality image-reasoning data into the trainingprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold startand Video-R1-260k for RL training, both comprising image and video data.Experimental results demonstrate that Video-R1 achieves significantimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, aswell as on general video benchmarks including MVBench and TempCompass, etc.Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoningbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. Allcodes, models, data are released.
  </details>

- **[MemInsight: Autonomous Memory Augmentation for LLM Agents](http://arxiv.org/abs/2503.21760v1)**  `arXiv:2503.21760`  `cs.CL`  
  _Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, et al._
  <details open><summary>Abstract</summary>
  Large language model (LLM) agents have evolved to intelligently processinformation, make decisions, and interact with users or tools. A key capabilityis the integration of long-term memory capabilities, enabling these agents todraw upon historical interactions and knowledge. However, the growing memorysize and need for semantic structuring pose significant challenges. In thiswork, we propose an autonomous memory augmentation approach, MemInsight, toenhance semantic data representation and retrieval mechanisms. By leveragingautonomous augmentation to historical interactions, LLM agents are shown todeliver more accurate and contextualized responses. We empirically validate theefficacy of our proposed approach in three task scenarios; conversationalrecommendation, question answering and event summarization. On the LLM-REDIALdataset, MemInsight boosts persuasiveness of recommendations by up to 14%.Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.Our empirical results show the potential of MemInsight to enhance thecontextual performance of LLM agents across multiple tasks.
  </details>

- **[Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck](http://arxiv.org/abs/2503.21757v1)**  `arXiv:2503.21757`  `cs.LG` `cs.AI` `cs.CV`  
  _Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos_
  <details open><summary>Abstract</summary>
  In this work, we aim to compress the vision tokens of a Large Vision LanguageModel (LVLM) into a representation that is simultaneously suitable for (a)generative and (b) discriminative tasks, (c) is nearly lossless, and (d) isstorage-efficient. We propose a novel compression approach, called Fwd2Bot,that uses the LVLM itself to compress the visual information in a task-agnosticmanner. At the core of Fwd2bot there exists a "double-forward pass" trainingstrategy, whereby, during the first forward pass, the LLM (of the LVLM) createsa bottleneck by condensing the visual information into a small number ofsummary tokens. Then, using the same LLM, the second forward pass processes thelanguage instruction(s) alongside the summary tokens, used as a directreplacement for the image ones. The training signal is provided by two losses:an autoregressive one applied after the second pass that provides a directoptimization objective for compression, and a contrastive loss, applied afterthe first pass, that further boosts the representation strength, especially fordiscriminative tasks. The training is further enhanced by stage-specificadapters. We accompany the proposed method by an in-depth ablation study.Overall, Fwd2Bot results in highly-informative compressed representationssuitable for both generative and discriminative tasks. For generative tasks, weoffer a 2x higher compression rate without compromising the generativecapabilities, setting a new state-of-the-art result. For discriminative tasks,we set a new state-of-the-art on image retrieval and compositionality.
  </details>

- **[VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness](http://arxiv.org/abs/2503.21755v1)**  `arXiv:2503.21755`  `cs.CV`  
  _Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, et al._
  <details open><summary>Abstract</summary>
  Video generation has advanced significantly, evolving from producingunrealistic outputs to generating videos that appear visually convincing andtemporally coherent. To evaluate these video generative models, benchmarks suchas VBench have been developed to assess their faithfulness, measuring factorslike per-frame aesthetics, temporal consistency, and basic prompt adherence.However, these aspects mainly represent superficial faithfulness, which focuson whether the video appears visually convincing rather than whether it adheresto real-world principles. While recent models perform increasingly well onthese metrics, they still struggle to generate videos that are not justvisually plausible but fundamentally realistic. To achieve real "world models"through video generation, the next frontier lies in intrinsic faithfulness toensure that generated videos adhere to physical laws, commonsense reasoning,anatomical correctness, and compositional integrity. Achieving this level ofrealism is essential for applications such as AI-assisted filmmaking andsimulated world modeling. To bridge this gap, we introduce VBench-2.0, anext-generation benchmark designed to automatically evaluate video generativemodels for their intrinsic faithfulness. VBench-2.0 assesses five keydimensions: Human Fidelity, Controllability, Creativity, Physics, andCommonsense, each further broken down into fine-grained capabilities. Tailoredfor individual dimensions, our evaluation framework integrates generalists suchas state-of-the-art VLMs and LLMs, and specialists, including anomaly detectionmethods proposed for video generation. We conduct extensive annotations toensure alignment with human judgment. By pushing beyond superficialfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a newstandard for the next generation of video generative models in pursuit ofintrinsic faithfulness.
  </details>

- **[Effective Skill Unlearning through Intervention and Abstention](http://arxiv.org/abs/2503.21730v1)**  `arXiv:2503.21730`  `cs.LG` `cs.CL`  
  _Yongce Li, Chung-En Sun, Tsui-Wei Weng_
  <details open><summary>Abstract</summary>
  Large language Models (LLMs) have demonstrated remarkable skills acrossvarious domains. Understanding the mechanisms behind their abilities andimplementing controls over them is becoming increasingly important fordeveloping better models. In this paper, we focus on skill unlearning in LLMs,specifically unlearning a particular skill while retaining their overallcapabilities. We introduce two lightweight, training-free machine skillunlearning techniques for LLMs. First, we observe that the pre-activationdistribution of neurons in each Feed-Forward Layer (FFL) differs when the modeldemonstrates different skills. Additionally, we find that queries triggeringthe same skill cluster within the FFL key space and can be separated from otherqueries using a hypercube. Based on these observations, we propose twolightweight, training-free skill unlearning methods via \textit{intervention}and \textit{abstention} respectively: \texttt{Neuron Adjust} and \texttt{KeySpace Detection}. We evaluate our methods on unlearning math-solving,Python-coding, and comprehension skills across seven different languages. Theresults demonstrate their strong unlearning capabilities for the designatedskills. Specifically, \texttt{Key Space Detection} achieves over 80\% relativeperformance drop on the forgetting skill and less than 10\% relativeperformance drop on other skills and the model's general knowledge (MMLU) formost unlearning tasks. Our code is available athttps://github.com/Trustworthy-ML-Lab/effective_skill_unlearning
  </details>

- **[Collab: Controlled Decoding using Mixture of Agents for LLM Alignment](http://arxiv.org/abs/2503.21720v1)**  `arXiv:2503.21720`  `cs.AI` `cs.CL`  
  _Souradip Chakraborty, Sujay Bhatt, Udari Madhushani Sehwag, Soumya Suvra Ghosal, Jiahao Qiu, Mengdi Wang, et al._
  <details open><summary>Abstract</summary>
  Alignment of Large Language models (LLMs) is crucial for safe and trustworthydeployment in applications. Reinforcement learning from human feedback (RLHF)has emerged as an effective technique to align LLMs to human preferences andbroader utilities, but it requires updating billions of model parameters, whichis computationally expensive. Controlled Decoding, by contrast, provides amechanism for aligning a model at inference time without retraining. However,single-agent decoding approaches often struggle to adapt to diverse tasks dueto the complexity and variability inherent in these tasks. To strengthen thetest-time performance w.r.t the target task, we propose a mixture ofagent-based decoding strategies leveraging the existing off-the-shelf alignedLLM policies. Treating each prior policy as an agent in the spirit of mixtureof agent collaboration, we develop a decoding method that allows forinference-time alignment through a token-level selection strategy amongmultiple agents. For each token, the most suitable LLM is dynamically chosenfrom a pool of models based on a long-term utility metric. Thispolicy-switching mechanism ensures optimal model selection at each step,enabling efficient collaboration and alignment among LLMs during decoding.Theoretical analysis of our proposed algorithm establishes optimal performancewith respect to the target task represented via a target reward for the givenoff-the-shelf models. We conduct comprehensive empirical evaluations withopen-source aligned models on diverse tasks and preferences, which demonstratesthe merits of this approach over single-agent decoding baselines. Notably,Collab surpasses the current SoTA decoding strategy, achieving an improvementof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.
  </details>

- **[CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?](http://arxiv.org/abs/2503.21717v1)**  `arXiv:2503.21717`  `cs.CL`  
  _Jiefu Ou, William Gantt Walden, Kate Sanders, Zhengping Jiang, Kaiser Sun, Jeffrey Cheng, et al._
  <details open><summary>Abstract</summary>
  A core part of scientific peer review involves providing expert critiquesthat directly assess the scientific claims a paper makes. While it is nowpossible to automatically generate plausible (if generic) reviews, ensuringthat these reviews are sound and grounded in the papers' claims remainschallenging. To facilitate LLM benchmarking on these challenges, we introduceCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions andreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts forweakness statements in the reviews and the paper claims that they dispute, aswell as fine-grained labels of the validity, objectivity, and type of theidentified weaknesses. We benchmark several LLMs on three claim-centric taskssupported by CLAIMCHECK, requiring models to (1) associate weaknesses with theclaims they dispute, (2) predict fine-grained labels for weaknesses and rewritethe weaknesses to enhance their specificity, and (3) verify a paper's claimswith grounded reasoning. Our experiments reveal that cutting-edge LLMs, whilecapable of predicting weakness labels in (2), continue to underperform relativeto human experts on all other tasks.
  </details>

- **[LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning](http://arxiv.org/abs/2503.21683v1)**  `arXiv:2503.21683`  `cs.AI` `cs.CL`  
  _Hui Wang_
  <details open><summary>Abstract</summary>
  In recent years, large language models (LLMs) have shown significantadvancements in natural language processing (NLP), with strong capa-bilities ingeneration, comprehension, and rea-soning. These models have found applicationsin education, intelligent decision-making, and gaming. However, effectivelyutilizing LLMs for strategic planning and decision-making in the game of Gomokuremains a challenge. This study aims to develop a Gomoku AI system based onLLMs, simulating the human learning process of playing chess. The system isde-signed to understand and apply Gomoku strat-egies and logic to make rationaldecisions. The research methods include enabling the model to "read the board,""understand the rules," "select strategies," and "evaluate positions," whileen-hancing its abilities through self-play and rein-forcement learning. Theresults demonstrate that this approach significantly improves the se-lection ofmove positions, resolves the issue of generating illegal positions, and reducespro-cess time through parallel position evaluation. After extensive self-playtraining, the model's Gomoku-playing capabilities have been notably enhanced.
  </details>

- **[How do language models learn facts? Dynamics, curricula and hallucinations](http://arxiv.org/abs/2503.21676v1)**  `arXiv:2503.21676`  `cs.LG` `cs.CL`  
  _Nicolas Zucchet, J√∂rg Bornschein, Stephanie Chan, Andrew Lampinen, Razvan Pascanu, Soham De_
  <details open><summary>Abstract</summary>
  Large language models accumulate vast knowledge during pre-training, yet thedynamics governing this acquisition remain poorly understood. This workinvestigates the learning dynamics of language models on a synthetic factualrecall task, uncovering three key findings: First, language models learn inthree phases, exhibiting a performance plateau before acquiring precise factualknowledge. Mechanistically, this plateau coincides with the formation ofattention-based circuits that support recall. Second, the training datadistribution significantly impacts learning dynamics, as imbalanceddistributions lead to shorter plateaus. Finally, hallucinations emergesimultaneously with knowledge, and integrating new knowledge into the modelthrough fine-tuning is challenging, as it quickly corrupts its existingparametric memories. Our results emphasize the importance of data distributionin knowledge acquisition and suggest novel data scheduling strategies toaccelerate neural network training.
  </details>

- **[COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing](http://arxiv.org/abs/2503.21670v1)**  `arXiv:2503.21670`  `cs.AI` `cs.CL`  
  _Rajvee Sheth, Himanshu Beniwal, Mayank Singh_
  <details open><summary>Abstract</summary>
  The rapid growth of digital communication has driven the widespread use ofcode-mixing, particularly Hindi-English, in multilingual communities. Existingdatasets often focus on romanized text, have limited scope, or rely onsynthetic data, which fails to capture realworld language nuances. Humanannotations are crucial for assessing the naturalness and acceptability ofcode-mixed text. To address these challenges, We introduce COMI-LINGUA, thelargest manually annotated dataset for code-mixed text, comprising 100,970instances evaluated by three expert annotators in both Devanagari and Romanscripts. The dataset supports five fundamental NLP tasks: LanguageIdentification, Matrix Language Identification, Part-of-Speech Tagging, NamedEntity Recognition, and Translation. We evaluate LLMs on these tasks usingCOMILINGUA, revealing limitations in current multilingual modeling strategiesand emphasizing the need for improved code-mixed text processing capabilities.COMI-LINGUA is publically availabe at:https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.
  </details>

- **[UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning](http://arxiv.org/abs/2503.21620v1)**  `arXiv:2503.21620`  `cs.AI`  
  _Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, et al._
  <details open><summary>Abstract</summary>
  The recent DeepSeek-R1 has showcased the emergence of reasoning capabilitiesin LLMs through reinforcement learning (RL) with rule-based rewards. Buildingon this idea, we are the first to explore how rule-based RL can enhance thereasoning capabilities of multimodal large language models (MLLMs) for graphicuser interface (GUI) action prediction tasks. To this end, we curate a smallyet high-quality dataset of 136 challenging tasks, encompassing five commonaction types on mobile devices. We also introduce a unified rule-based actionreward, enabling model optimization via policy-based algorithms such as GroupRelative Policy Optimization (GRPO). Experimental results demonstrate that ourproposed data-efficient model, UI-R1-3B, achieves substantial improvements onboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the IDbenchmark AndroidControl, the action type accuracy improves by 15%, whilegrounding accuracy increases by 10.3%, compared with the base model (i.e.Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our modelsurpasses the base model by 6.0% and achieves competitive performance withlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning(SFT) on 76K data. These results underscore the potential of rule-basedreinforcement learning to advance GUI understanding and control, paving the wayfor future research in this domain.
  </details>

- **[Evaluating book summaries from internal knowledge in Large Language Models: a cross-model and semantic consistency approach](http://arxiv.org/abs/2503.21613v1)**  `arXiv:2503.21613`  `cs.CL`  
  _Javier Coronado-Bl√°zquez_
  <details open><summary>Abstract</summary>
  We study the ability of large language models (LLMs) to generatecomprehensive and accurate book summaries solely from their internal knowledge,without recourse to the original text. Employing a diverse set of books andmultiple LLM architectures, we examine whether these models can synthesizemeaningful narratives that align with established human interpretations.Evaluation is performed with a LLM-as-a-judge paradigm: each AI-generatedsummary is compared against a high-quality, human-written summary via across-model assessment, where all participating LLMs evaluate not only theirown outputs but also those produced by others. This methodology enables theidentification of potential biases, such as the proclivity for models to favortheir own summarization style over others. In addition, alignment between thehuman-crafted and LLM-generated summaries is quantified using ROUGE andBERTScore metrics, assessing the depth of grammatical and semanticcorrespondence. The results reveal nuanced variations in content representationand stylistic preferences among the models, highlighting both strengths andlimitations inherent in relying on internal knowledge for summarization tasks.These findings contribute to a deeper understanding of LLM internal encodingsof factual information and the dynamics of cross-model evaluation, withimplications for the development of more robust natural language generativesystems.
  </details>

- **[GenEdit: Compounding Operators and Continuous Improvement to Tackle Text-to-SQL in the Enterprise](http://arxiv.org/abs/2503.21602v1)**  `arXiv:2503.21602`  `cs.AI`  
  _Karime Maamari, Connor Landy, Amine Mhedhbi_
  <details open><summary>Abstract</summary>
  Recent advancements in Text-to-SQL, driven by large language models, aredemocratizing data access. Despite these advancements, enterprise deploymentsremain challenging due to the need to capture business-specific knowledge,handle complex queries, and meet expectations of continuous improvements. Toaddress these issues, we designed and implemented GenEdit: our Text-to-SQLgeneration system that improves with user feedback. GenEdit builds andmaintains a company-specific knowledge set, employs a pipeline of operatorsdecomposing SQL generation, and uses feedback to update its knowledge set toimprove future SQL generations.  We describe GenEdit's architecture made of two core modules: (i) decomposedSQL generation; and (ii) knowledge set edits based on user feedback. Forgeneration, GenEdit leverages compounding operators to improve knowledgeretrieval and to create a plan as chain-of-thought steps that guidesgeneration. GenEdit first retrieves relevant examples in an initial retrievalstage where original SQL queries are decomposed into sub-statements, clauses orsub-queries. It then also retrieves instructions and schema elements. Using theretrieved contextual information, GenEdit then generates step-by-step plan innatural language on how to produce the query. Finally, GenEdit uses the plan togenerate SQL, minimizing the need for model reasoning, which enhances complexSQL generation. If necessary, GenEdit regenerates the query based on syntacticand semantic errors. The knowledge set edits are recommended through aninteractive copilot, allowing users to iterate on their feedback and toregenerate SQL queries as needed. Each generation uses staged edits whichupdate the generation prompt. Once the feedback is submitted, it gets mergedafter passing regression testing and obtaining an approval, improving futuregenerations.
  </details>

- **[Cooking Task Planning using LLM and Verified by Graph Network](http://arxiv.org/abs/2503.21564v1)**  `arXiv:2503.21564`  `cs.RO`  
  _Ryunosuke Takebayashi, Vitor Hideyo Isume, Takuya Kiyokawa, Weiwei Wan, Kensuke Harada_
  <details open><summary>Abstract</summary>
  Cooking tasks remain a challenging problem for robotics due to theircomplexity. Videos of people cooking are a valuable source of information forsuch task, but introduces a lot of variability in terms of how to translatethis data to a robotic environment. This research aims to streamline thisprocess, focusing on the task plan generation step, by using a Large LanguageModel (LLM)-based Task and Motion Planning (TAMP) framework to autonomouslygenerate cooking task plans from videos with subtitles, and execute them.Conventional LLM-based task planning methods are not well-suited forinterpreting the cooking video data due to uncertainty in the videos, and therisk of hallucination in its output. To address both of these problems, weexplore using LLMs in combination with Functional Object-Oriented Networks(FOON), to validate the plan and provide feedback in case of failure. Thiscombination can generate task sequences with manipulation motions that arelogically correct and executable by a robot. We compare the execution of thegenerated plans for 5 cooking recipes from our approach against the plansgenerated by a few-shot LLM-only approach for a dual-arm robot setup. It couldsuccessfully execute 4 of the plans generated by our approach, whereas only 1of the plans generated by solely using the LLM could be executed.
  </details>

- **[Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models](http://arxiv.org/abs/2503.21530v1)**  `arXiv:2503.21530`  `cs.AI` `cs.CL`  
  _Umer Butt, Stalin Veranasi, G√ºnter Neumann_
  <details open><summary>Abstract</summary>
  As the Information Retrieval (IR) field increasingly recognizes theimportance of inclusivity, addressing the needs of low-resource languagesremains a significant challenge. Transliteration between Urdu and its Romanizedform, Roman Urdu, remains underexplored despite the widespread use of bothscripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl datasetshowed promising results but suffered from poor domain adaptability and limitedevaluation. We propose a transformer-based approach using the m2m100multilingual translation model, enhanced with masked language modeling (MLM)pretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverseDakshina dataset. To address previous evaluation flaws, we introduce rigorousdataset splits and assess performance using BLEU, character-level BLEU, andCHRF. Our model achieves strong transliteration performance, with Char-BLEUscores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. Theseresults outperform both RNN baselines and GPT-4o Mini and demonstrate theeffectiveness of multilingual transfer learning for low-resourcetransliteration tasks.
  </details>

- **[Keyword-Oriented Multimodal Modeling for Euphemism Identification](http://arxiv.org/abs/2503.21504v1)**  `arXiv:2503.21504`  `cs.CV` `cs.AI` `cs.CL`  
  _Yuxue Hu, Junsong Li, Meixuan Chen, Dongyu Su, Tongguan Wang, Ying Sha_
  <details open><summary>Abstract</summary>
  Euphemism identification deciphers the true meaning of euphemisms, such aslinking "weed" (euphemism) to "marijuana" (target keyword) in illicit texts,aiding content moderation and combating underground markets. While existingmethods are primarily text-based, the rise of social media highlights the needfor multimodal analysis, incorporating text, images, and audio. However, thelack of multimodal datasets for euphemisms limits further research. To addressthis, we regard euphemisms and their corresponding target keywords as keywordsand first introduce a keyword-oriented multimodal corpus of euphemisms(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), includingtext, images, and speech. We further propose a keyword-oriented multimodaleuphemism identification method (KOM-EI), which uses cross-modal featurealignment and dynamic fusion modules to explicitly utilize the visual and audiofeatures of the keywords for efficient euphemism identification. Extensiveexperiments demonstrate that KOM-EI outperforms state-of-the-art models andlarge language models, and show the importance of our multimodal datasets.
  </details>

- **[OpenHuEval: Evaluating Large Language Model on Hungarian Specifics](http://arxiv.org/abs/2503.21500v1)**  `arXiv:2503.21500`  `cs.CL`  
  _Haote Yang, Xingjian Wei, Jiang Wu, No√©mi Ligeti-Nagy, Jiaxing Sun, Yinfan Wang, et al._
  <details open><summary>Abstract</summary>
  We introduce OpenHuEval, the first benchmark for LLMs focusing on theHungarian language and specifics. OpenHuEval is constructed from a vastcollection of Hungarian-specific materials sourced from multiple origins. Inthe construction, we incorporated the latest design principles for evaluatingLLMs, such as using real user queries from the internet, emphasizing theassessment of LLMs' generative capabilities, and employing LLM-as-judge toenhance the multidimensionality and accuracy of evaluations. Ultimately,OpenHuEval encompasses eight Hungarian-specific dimensions, featuring fivetasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive,in-depth, and scientifically accurate assessment of LLM performance in thecontext of the Hungarian language and its specifics. We evaluated currentmainstream LLMs, including both traditional LLMs and recently developed LargeReasoning Models. The results demonstrate the significant necessity forevaluation and model optimization tailored to the Hungarian language andspecifics. We also established the framework for analyzing the thinkingprocesses of LRMs with OpenHuEval, revealing intrinsic patterns and mechanismsof these models in non-English languages, with Hungarian serving as arepresentative example. We will release OpenHuEval athttps://github.com/opendatalab/OpenHuEval .
  </details>

- **[OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs](http://arxiv.org/abs/2503.21480v1)**  `arXiv:2503.21480`  `cs.CL`  
  _John Murzaku, Owen Rambow_
  <details open><summary>Abstract</summary>
  The use of omni-LLMs (large language models that accept any modality asinput), particularly for multimodal cognitive state tasks involving speech, isunderstudied. We present OmniVox, the first systematic evaluation of fouromni-LLMs on the zero-shot emotion recognition task. We evaluate on two widelyused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shotomni-LLMs outperform or are competitive with fine-tuned audio models. Alongsideour audio-only evaluation, we also evaluate omni-LLMs on text only and text andaudio. We present acoustic prompting, an audio-specific prompting strategy foromni-LLMs which focuses on acoustic feature analysis, conversation contextanalysis, and step-by-step reasoning. We compare our acoustic prompting tominimal prompting and full chain-of-thought prompting techniques. We perform acontext window analysis on IEMOCAP and MELD, and find that using context helps,especially on IEMOCAP. We conclude with an error analysis on the generatedacoustic reasoning outputs from the omni-LLMs.
  </details>

- **[Large Language Model Agent: A Survey on Methodology, Applications and Challenges](http://arxiv.org/abs/2503.21460v1)**  `arXiv:2503.21460`  `cs.CL`  
  _Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, et al._
  <details open><summary>Abstract</summary>
  The era of intelligent agents is upon us, driven by revolutionaryadvancements in large language models. Large Language Model (LLM) agents, withgoal-driven behaviors and dynamic adaptation capabilities, potentiallyrepresent a critical pathway toward artificial general intelligence. Thissurvey systematically deconstructs LLM agent systems through amethodology-centered taxonomy, linking architectural foundations, collaborationmechanisms, and evolutionary pathways. We unify fragmented research threads byrevealing fundamental connections between agent design principles and theiremergent behaviors in complex environments. Our work provides a unifiedarchitectural perspective, examining how agents are constructed, how theycollaborate, and how they evolve over time, while also addressing evaluationmethodologies, tool applications, practical challenges, and diverse applicationdomains. By surveying the latest developments in this rapidly evolving field,we offer researchers a structured taxonomy for understanding LLM agents andidentify promising directions for future research. The collection is availableat https://github.com/luo-junyu/Awesome-Agent-Papers.
  </details>

- **[RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives](http://arxiv.org/abs/2503.21459v1)**  `arXiv:2503.21459`  `cs.CV`  
  _Chirag Parikh, Deepti Rawat, Rakshitha R. T., Tathagata Ghosh, Ravi Kiran Sarvadevabhatla_
  <details open><summary>Abstract</summary>
  We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored forgeneric road event understanding from social media narratives. Unlike existingdatasets limited by regional bias, viewpoint bias and expert-drivenannotations, RoadSocial captures the global complexity of road events withvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich socialdiscourse. Our scalable semi-automatic annotation framework leverages Text LLMsand Video LLMs to generate comprehensive question-answer pairs across 12challenging QA tasks, pushing the boundaries of road event understanding.RoadSocial is derived from social media videos spanning 14M frames and 414Ksocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260Khigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,driving-specific and general-purpose) on our road event understandingbenchmark. We also demonstrate RoadSocial's utility in improving road eventunderstanding capabilities of general-purpose Video LLMs.
  </details>

- **[FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs](http://arxiv.org/abs/2503.21457v1)**  `arXiv:2503.21457`  `cs.CV`  
  _Xiaoqin Wang, Xusen Ma, Xianxu Hou, Meidan Ding, Yudong Li, Junliang Chen, et al._
  <details open><summary>Abstract</summary>
  Multimodal large language models (MLLMs) have demonstrated remarkablecapabilities in various tasks. However, effectively evaluating these MLLMs onface perception remains largely unexplored. To address this gap, we introduceFaceBench, a dataset featuring hierarchical multi-view and multi-levelattributes specifically designed to assess the comprehensive face perceptionabilities of MLLMs. Initially, we construct a hierarchical facial attributestructure, which encompasses five views with up to three levels of attributes,totaling over 210 attributes and 700 attribute values. Based on the structure,the proposed FaceBench consists of 49,919 visual question-answering (VQA) pairsfor evaluation and 23,841 pairs for fine-tuning. Moreover, we further develop arobust face perception MLLM baseline, Face-LLaVA, by training with our proposedface VQA data. Extensive experiments on various mainstream MLLMs and Face-LLaVAare conducted to test their face perception ability, with results also comparedagainst human performance. The results reveal that, the existing MLLMs are farfrom satisfactory in understanding the fine-grained facial attributes, whileour Face-LLaVA significantly outperforms existing open-source models with asmall amount of training data and is comparable to commercial ones like GPT-4oand Gemini. The dataset will be released athttps://github.com/CVI-SZU/FaceBench.
  </details>

- **[Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In \& Out Learning](http://arxiv.org/abs/2503.21419v1)**  `arXiv:2503.21419`  `cs.AI`  
  _Yupei Li, Manuel Milling, Bj√∂rn W. Schuller_
  <details open><summary>Abstract</summary>
  Artificial Intelligence (AI) has achieved new levels of performance andspread in public usage with the rise of deep neural networks (DNNs). Initiallyinspired by human neurons and their connections, NNs have become the foundationof AI models for many advanced architectures. However, some of the mostintegral processes in the human brain, particularly neurogenesis andneuroplasticity in addition to the more spread neuroapoptosis have largely beenignored in DNN architecture design. Instead, contemporary AI developmentpredominantly focuses on constructing advanced frameworks, such as largelanguage models, which retain a static structure of neural connections duringtraining and inference. In this light, we explore how neurogenesis,neuroapoptosis, and neuroplasticity can inspire future AI advances.Specifically, we examine analogous activities in artificial NNs, introducingthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' andstructural pruning for neuroapoptosis. We additionally suggest neuroplasticitycombining the two for future large NNs in ``life-long learning'' settingsfollowing the biological inspiration. We conclude by advocating for greaterresearch efforts in this interdisciplinary domain and identifying promisingdirections for future exploration.
  </details>

- **[Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap](http://arxiv.org/abs/2503.21411v1)**  `arXiv:2503.21411`  `cs.AI`  
  _Tong Nie, Jian Sun, Wei Ma_
  <details open><summary>Abstract</summary>
  Modern transportation systems face pressing challenges due to increasingdemand, dynamic environments, and heterogeneous information integration. Therapid evolution of Large Language Models (LLMs) offers transformative potentialto address these challenges. Extensive knowledge and high-level capabilitiesderived from pretraining evolve the default role of LLMs as text generators tobecome versatile, knowledge-driven task solvers for intelligent transportationsystems. This survey first presents LLM4TR, a novel conceptual framework thatsystematically categorizes the roles of LLMs in transportation into foursynergetic dimensions: information processors, knowledge encoders, componentgenerators, and decision facilitators. Through a unified taxonomy, wesystematically elucidate how LLMs bridge fragmented data pipelines, enhancepredictive analytics, simulate human-like reasoning, and enable closed-loopinteractions across sensing, learning, modeling, and managing tasks intransportation systems. For each role, our review spans diverse applications,from traffic prediction and autonomous driving to safety analytics and urbanmobility optimization, highlighting how emergent capabilities of LLMs such asin-context learning and step-by-step reasoning can enhance the operation andmanagement of transportation systems. We further curate practical guidance,including available resources and computational guidelines, to supportreal-world deployment. By identifying challenges in existing LLM-basedsolutions, this survey charts a roadmap for advancing LLM-driven transportationresearch, positioning LLMs as central actors in the next generation ofcyber-physical-social mobility ecosystems. Online resources can be found in theproject page: https://github.com/tongnie/awesome-llm4tr.
  </details>

- **[VALLR: Visual ASR Language Model for Lip Reading](http://arxiv.org/abs/2503.21408v1)**  `arXiv:2503.21408`  `cs.CV`  
  _Marshall Thomas, Edward Fish, Richard Bowden_
  <details open><summary>Abstract</summary>
  Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complextask requiring the interpretation of spoken language exclusively from visualcues, primarily lip movements and facial expressions. This task is especiallychallenging due to the absence of auditory information and the inherentambiguity when visually distinguishing phonemes that have overlapping visemeswhere different phonemes appear identical on the lips. Current methodstypically attempt to predict words or characters directly from these visualcues, but this approach frequently encounters high error rates due tocoarticulation effects and viseme ambiguity. We propose a novel two-stage,phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) thataddresses these longstanding challenges. First, our model predicts a compactsequence of phonemes from visual inputs using a Video Transformer with a CTChead, thereby reducing the task complexity and achieving robust speakerinvariance. This phoneme output then serves as the input to a fine-tuned LargeLanguage Model (LLM), which reconstructs coherent words and sentences byleveraging broader linguistic context. Unlike existing methods that eitherpredict words directly-often faltering on visually similar phonemes-or rely onlarge-scale multimodal pre-training, our approach explicitly encodesintermediate linguistic structure while remaining highly data efficient. Wedemonstrate state-of-the-art performance on two challenging datasets, LRS2 andLRS3, where our method achieves significant reductions in Word Error Rate (WER)achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled datathan the next best approach.
  </details>

- **[An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses](http://arxiv.org/abs/2503.21393v1)**  `arXiv:2503.21393`  `cs.AI` `cs.CL`  
  _Rohitash Chandra, Aryan Chaudhary, Yeshwanth Rayavarapu_
  <details open><summary>Abstract</summary>
  Large Language models (LLMs) have been prominent for language translation,including low-resource languages. There has been limited study about theassessment of the quality of translations generated by LLMs, including Gemini,GPT and Google Translate. In this study, we address this limitation by usingsemantic and sentiment analysis of selected LLMs for Indian languages,including Sanskrit, Telugu and Hindi. We select prominent texts that have beenwell translated by experts and use LLMs to generate their translations toEnglish, and then we provide a comparison with selected expert (human)translations. Our findings suggest that while LLMs have made significantprogress in translation accuracy, challenges remain in preserving sentiment andsemantic integrity, especially in figurative and philosophical contexts. Thesentiment analysis revealed that GPT-4o and GPT-3.5 are better at preservingthe sentiments for the Bhagavad Gita (Sanskrit-English) translations whencompared to Google Translate. We observed a similar trend for the case of Tamas(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performssimilarly to GPT-3.5 in the translation in terms of sentiments for the threelanguages. We found that LLMs are generally better at translation for capturingsentiments when compared to Google Translate.
  </details>

- **[Controlling Large Language Model with Latent Actions](http://arxiv.org/abs/2503.21383v1)**  `arXiv:2503.21383`  `cs.LG` `cs.CL`  
  _Chengxing Jia, Ziniu Li, Pengyuan Wang, Yi-Chen Li, Zhenyu Hou, Yuxiao Dong, et al._
  <details open><summary>Abstract</summary>
  Adapting Large Language Models (LLMs) to downstream tasks using ReinforcementLearning (RL) has proven to be an effective approach. However, LLMs do notinherently define the structure of an agent for RL training, particularly interms of defining the action space. This paper studies learning a compactlatent action space to enhance the controllability and exploration of RL forLLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),a framework that integrates a latent action space into pre-trained LLMs. Weapply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,compared to RL with token-level actions, CoLA's latent action enables greatersemantic diversity in text generation. For enhancing downstream tasks, we showthat CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassingthe baseline score of 38.2, and reaches 68.2 when augmented with a Monte CarloTree Search variant. Furthermore, CoLA with RL consistently improvesperformance on agent-based tasks without degrading the pre-trained LLM'scapabilities, unlike the baseline. Finally, CoLA reduces computation time byhalf in tasks involving enhanced thinking prompts for LLMs by RL. These resultshighlight CoLA's potential to advance RL-based adaptation of LLMs fordownstream applications.
  </details>

- **[Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models](http://arxiv.org/abs/2503.21380v1)**  `arXiv:2503.21380`  `cs.CL`  
  _Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, et al._
  <details open><summary>Abstract</summary>
  In recent years, the rapid development of large reasoning models has resultedin the saturation of existing benchmarks for evaluating mathematical reasoning,highlighting the urgent need for more challenging and rigorous evaluationframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-levelmathematical benchmark, designed to rigorously test the complex reasoningcapabilities of LLMs. OlymMATH features 200 meticulously curated problems, eachmanually verified and available in parallel English and Chinese versions. Theproblems are systematically organized into two distinct difficulty tiers: (1)AIME-level problems (easy) that establish a baseline for mathematical reasoningassessment, and (2) significantly more challenging problems (hard) designed topush the boundaries of current state-of-the-art models. In our benchmark, theseproblems span four core mathematical fields, each including a verifiablenumerical solution to enable objective, rule-based evaluation. Empiricalresults underscore the significant challenge presented by OlymMATH, withstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-minidemonstrating notably limited accuracy on the hard subset. Furthermore, thebenchmark facilitates comprehensive bilingual assessment of mathematicalreasoning abilities-a critical dimension that remains largely unaddressed inmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmarkat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.
  </details>

- **[From User Preferences to Optimization Constraints Using Large Language Models](http://arxiv.org/abs/2503.21360v1)**  `arXiv:2503.21360`  `cs.CL`  
  _Manuela Sanguinetti, Alessandra Perniciano, Luca Zedda, Andrea Loddo, Cecilia Di Ruberto, Maurizio Atzori_
  <details open><summary>Abstract</summary>
  This work explores using Large Language Models (LLMs) to translate userpreferences into energy optimization constraints for home appliances. Wedescribe a task where natural language user utterances are converted intoformal constraints for smart appliances, within the broader context of arenewable energy community (REC) and in the Italian scenario. We evaluate theeffectiveness of various LLMs currently available for Italian in translatingthese preferences resorting to classical zero-shot, one-shot, and few-shotlearning settings, using a pilot dataset of Italian user requests paired withcorresponding formal constraint representation. Our contributions includeestablishing a baseline performance for this task, publicly releasing thedataset and code for further research, and providing insights on observed bestpractices and limitations of LLMs in this particular domain
  </details>

- **[InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression](http://arxiv.org/abs/2503.21307v1)**  `arXiv:2503.21307`  `cs.AI` `cs.CV`  
  _Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, et al._
  <details open><summary>Abstract</summary>
  Most multimodal large language models (MLLMs) treat visual tokens as "asequence of text", integrating them with text tokens into a large languagemodel (LLM). However, a great quantity of visual tokens significantly increasesthe demand for computational resources and time. In this paper, we proposeInternVL-X, which outperforms the InternVL model in both performance andefficiency by incorporating three visual token compression methods. First, wepropose a novel vision-language projector, PVTC. This component integratesadjacent visual embeddings to form a local query and utilizes the transformedCLS token as a global query, then performs point-to-region cross-attentionthrough these local and global queries to more effectively convert visualfeatures. Second, we present a layer-wise visual token compression module,LVTC, which compresses tokens in the LLM shallow layers and then expands themthrough upsampling and residual connections in the deeper layers. Thissignificantly enhances the model computational efficiency. Futhermore, wepropose an efficient high resolution slicing method, RVTC, which dynamicallyadjusts the number of visual tokens based on image area or length filtering.RVTC greatly enhances training efficiency with only a slight reduction inperformance. By utilizing 20% or fewer visual tokens, InternVL-X achievesstate-of-the-art performance on 7 public MLLM benchmarks, and improves theaverage metric by 2.34% across 12 tasks.
  </details>

- **[R-PRM: Reasoning-Driven Process Reward Modeling](http://arxiv.org/abs/2503.21295v1)**  `arXiv:2503.21295`  `cs.CL`  
  _Shuaijie She, Junxiao Liu, Yifeng Liu, Jiajun Chen, Xin Huang, Shujian Huang_
  <details open><summary>Abstract</summary>
  Large language models (LLMs) inevitably make mistakes when performingstep-by-step mathematical reasoning. Process Reward Models (PRMs) have emergedas a promising solution by evaluating each reasoning step. However, existingPRMs typically output evaluation scores directly, limiting both learningefficiency and evaluation accuracy, which is further exacerbated by thescarcity of annotated data. To address these issues, we proposeReasoning-Driven Process Reward Modeling (R-PRM). First, we leverage strongerLLMs to generate seed data from limited annotations, effectively bootstrappingour model's reasoning capabilities and enabling comprehensive step-by-stepevaluation. Second, we further enhance performance through preferenceoptimization, without requiring additional annotated data. Third, we introduceinference-time scaling to fully harness the model's reasoning potential.Extensive experiments demonstrate R-PRM's effectiveness: on ProcessBench andPRMBench, it surpasses strong baselines by 11.9 and 8.5 points in F1 scores,respectively. When applied to guide mathematical reasoning, R-PRM achievesconsistent accuracy improvements of over 8.5 points across six challengingdatasets. Further analysis reveals that R-PRM exhibits more comprehensiveevaluation and stronger generalization capabilities, thereby highlighting itssignificant potential.
  </details>

- **[Reinforced Model Merging](http://arxiv.org/abs/2503.21272v1)**  `arXiv:2503.21272`  `cs.AI`  
  _Jiaqi Han, Jingwen Ye, Shunyu Liu, Haofei Zhang, Jie Song, Zunlei Feng, et al._
  <details open><summary>Abstract</summary>
  The success of large language models has garnered widespread attention formodel merging techniques, especially training-free methods which combine modelcapabilities within the parameter space. However, two challenges remain: (1)uniform treatment of all parameters leads to performance degradation; (2)search-based algorithms are often inefficient. In this paper, we present aninnovative framework termed Reinforced Model Merging (RMM), which encompassesan environment and agent tailored for merging tasks. These components interactto execute layer-wise merging actions, aiming to search the optimal mergingarchitecture. Notably, RMM operates without any gradient computations on theoriginal models, rendering it feasible for edge devices. Furthermore, byutilizing data subsets during the evaluation process, we addressed thebottleneck in the reward feedback phase, thereby accelerating RMM by up to 100times. Extensive experiments demonstrate that RMM achieves state-of-the-artperformance across various vision and NLP datasets and effectively overcomesthe limitations of the existing baseline methods. Our code is available athttps://github.com/WuDiHJQ/Reinforced-Model-Merging.
  </details>

- **[LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models](http://arxiv.org/abs/2503.21227v1)**  `arXiv:2503.21227`  `cs.CL`  
  _Hengyuan Zhao, Ziqin Wang, Qixin Sun, Kaiyou Song, Yilin Li, Xiaolin Hu, et al._
  <details open><summary>Abstract</summary>
  Although applying Mixture of Experts to large language models for learningnew tasks is widely regarded as an effective strategy for continuous learning,there still remain two major challenges: (1) As the number of tasks grows,simple parameter expansion strategies can lead to excessively large models. (2)Modifying the parameters of the existing router results in the erosion ofpreviously acquired knowledge. In this paper, we present an innovativeframework named LLaVA-CMoE, which is a continuous Mixture of Experts (MoE)architecture without any replay data. Specifically, we have developed a methodcalled Probe-Guided Knowledge Extension (PGKE), which employs probe experts toassess whether additional knowledge is required for a specific layer. Thisapproach enables the model to adaptively expand its network parameters based ontask distribution, thereby significantly improving the efficiency of parameterexpansion. Additionally, we introduce a hierarchical routing algorithm calledProbabilistic Task Locator (PTL), where high-level routing captures inter-taskinformation and low-level routing focuses on intra-task details, ensuring thatnew task experts do not interfere with existing ones. Our experiments showsthat our efficient architecture has substantially improved model performance onthe Coin benchmark while maintaining a reasonable parameter count.
  </details>

- **[Rethinking Graph Structure Learning in the Era of LLMs](http://arxiv.org/abs/2503.21223v1)**  `arXiv:2503.21223`  `cs.LG`  
  _Zhihan Zhang, Xunkai Li, Guang Zeng, Hongchao Qin, Ronghua Li, Guoren Wang_
  <details open><summary>Abstract</summary>
  Recently, the emergence of large language models (LLMs) has promptedresearchers to explore the integration of language descriptions into graphs,aiming to enhance model encoding capabilities from a data-centric perspective.This graph representation is called text-attributed graphs (TAGs). A review ofprior advancements highlights that graph structure learning (GSL) is a pivotaltechnique for improving data utility, making it highly relevant to efficientTAG learning. However, most GSL methods are tailored for traditional graphswithout textual information, underscoring the necessity of developing a new GSLparadigm. Despite clear motivations, it remains challenging: (1) How can wedefine a reasonable optimization objective for GSL in the era of LLMs,considering the massive parameters in LLM? (2) How can we design an efficientmodel architecture that enables seamless integration of LLM for thisoptimization objective? For Question 1, we reformulate existing GSLoptimization objectives as a tree optimization framework, shifting the focusfrom obtaining a well-trained edge predictor to a language-aware tree sampler.For Question 2, we propose decoupled and training-free model design principlesfor LLM integration, shifting the focus from computation-intensive fine-tuningto more efficient inference. Based on this, we propose Large Language and TreeAssistant (LLaTA), which leverages tree-based LLM in-context learning toenhance the understanding of topology and text, enabling reliable inference andgenerating improved graph structure. Extensive experiments on 10 TAG datasetsdemonstrate that LLaTA enjoys flexibility - incorporated with any backbone;scalability - outperforms other LLM-based GSL methods in terms of runningefficiency; effectiveness - achieves SOTA performance.
  </details>

- **[Resource-Efficient Federated Fine-Tuning Large Language Models for Heterogeneous Data](http://arxiv.org/abs/2503.21213v1)**  `arXiv:2503.21213`  `cs.LG`  
  _Jun Liu, Yunming Liao, Hongli Xu, Yang Xu_
  <details open><summary>Abstract</summary>
  Fine-tuning large language models (LLMs) via federated learning, i.e.,FedLLM, has been proposed to adapt LLMs for various downstream applications ina privacy-preserving way. To reduce the fine-tuning costs onresource-constrained devices, FedLoRA is proposed to fine-tune only a smallsubset of model parameters by integrating low-rank adaptation (LoRA) intoFedLLM. However, apart from resource constraints, there is still anothercritical challenge, i.e., data heterogeneity, severely hindering theimplementation of FedLoRA in practical applications. Herein, inspired by theprevious group-based federated learning paradigm, we propose a hierarchicalFedLoRA framework, termed HierFedLoRA, to address these challenges.Specifically, HierFedLoRA partitions all devices into multiple near-IID groupsand adjusts the intra-group aggregation frequency for each group to eliminatethe negative effects of non-IID data. Meanwhile, to reduce the computation andcommunication cost, HierFedLoRA dynamically assigns diverse and suitablefine-tuning depth (i.e., the number of continuous fine-tuning layers from theoutput) for each group. HierFedLoRA explores jointly optimizing aggregationfrequency and depth upon their coupled relationship to better enhance theperformance of FedLoRA. Extensive experiments are conducted on a physicalplatform with 80 commercial devices. The results show that HierFedLoRA improvesthe final model accuracy by 1.6% to 4.2%, speeding up the fine-tuning processby at least 2.1$\times$, compared to the strong baselines.
  </details>

- **[Leveraging LLMs with Iterative Loop Structure for Enhanced Social Intelligence in Video Question Answering](http://arxiv.org/abs/2503.21190v1)**  `arXiv:2503.21190`  `cs.CV`  
  _Erika Mori, Yue Qiu, Hirokatsu Kataoka, Yoshimitsu Aoki_
  <details open><summary>Abstract</summary>
  Social intelligence, the ability to interpret emotions, intentions, andbehaviors, is essential for effective communication and adaptive responses. Asrobots and AI systems become more prevalent in caregiving, healthcare, andeducation, the demand for AI that can interact naturally with humans grows.However, creating AI that seamlessly integrates multiple modalities, such asvision and speech, remains a challenge. Current video-based methods for socialintelligence rely on general video recognition or emotion recognitiontechniques, often overlook the unique elements inherent in human interactions.To address this, we propose the Looped Video Debating (LVD) framework, whichintegrates Large Language Models (LLMs) with visual information, such as facialexpressions and body movements, to enhance the transparency and reliability ofquestion-answering tasks involving human interaction videos. Our results on theSocial-IQ 2.0 benchmark show that LVD achieves state-of-the-art performancewithout fine-tuning. Furthermore, supplementary human annotations on existingdatasets provide insights into the model's accuracy, guiding futureimprovements in AI-driven social intelligence.
  </details>

- **[Integrating Large Language Models For Monte Carlo Simulation of Chemical Reaction Networks](http://arxiv.org/abs/2503.21178v1)**  `arXiv:2503.21178`  `cs.AI`  
  _Sadikshya Gyawali, Ashwini Mandal, Manish Dahal, Manish Awale, Sanjay Rijal, Shital Adhikari, et al._
  <details open><summary>Abstract</summary>
  Chemical reaction network is an important method for modeling and exploringcomplex biological processes, bio-chemical interactions and the behavior ofdifferent dynamics in system biology. But, formulating such reaction kineticstakes considerable time. In this paper, we leverage the efficiency of modernlarge language models to automate the stochastic monte carlo simulation ofchemical reaction networks and enable the simulation through the reactiondescription provided in the form of natural languages. We also integrate thisprocess into widely used simulation tool Copasi to further give the edge andease to the modelers and researchers. In this work, we show the efficacy andlimitations of the modern large language models to parse and create reactionkinetics for modelling complex chemical reaction processes.
  </details>

- **[Model as a Game: On Numerical and Spatial Consistency for Generative Games](http://arxiv.org/abs/2503.21172v1)**  `arXiv:2503.21172`  `cs.CV`  
  _Jingye Chen, Yuzhong Zhao, Yupan Huang, Lei Cui, Li Dong, Tengchao Lv, et al._
  <details open><summary>Abstract</summary>
  Recent advances in generative models have significantly impacted gamegeneration. However, despite producing high-quality graphics and adequatelyreceiving player input, existing models often fail to maintain fundamental gameproperties such as numerical and spatial consistency. Numerical consistencyensures gameplay mechanics correctly reflect score changes and otherquantitative elements, while spatial consistency prevents jarring scenetransitions, providing seamless player experiences. In this paper, we revisitthe paradigm of generative games to explore what truly constitutes a Model as aGame (MaaG) with a well-developed mechanism. We begin with an empirical studyon ``Traveler'', a 2D game created by an LLM featuring minimalist rules yetchallenging generative models in maintaining consistency. Based on the DiTarchitecture, we design two specialized modules: (1) a numerical module thatintegrates a LogicNet to determine event triggers, with calculations processedexternally as conditions for image generation; and (2) a spatial module thatmaintains a map of explored areas, retrieving location-specific informationduring generation and linking new observations to ensure continuity.Experiments across three games demonstrate that our integrated modulessignificantly enhance performance on consistency metrics compared to baselines,while incurring minimal time overhead during inference.
  </details>

- **[Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?](http://arxiv.org/abs/2503.21157v1)**  `arXiv:2503.21157`  `cs.LG`  
  _Ashish Sardana_
  <details open><summary>Abstract</summary>
  This article surveys Evaluation models to automatically detect hallucinationsin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmarkof their performance across six RAG applications. Methods included in our studyinclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination EvaluationModel (HHEM), and the Trustworthy Language Model (TLM). These approaches areall reference-free, requiring no ground-truth answers/labels to catch incorrectLLM responses. Our study reveals that, across diverse RAG applications, some ofthese approaches consistently detect incorrect RAG responses with highprecision/recall.
  </details>

- **[Embedding Domain-Specific Knowledge from LLMs into the Feature Engineering Pipeline](http://arxiv.org/abs/2503.21155v1)**  `arXiv:2503.21155`  `cs.LG`  
  _Jo√£o Eduardo Batista_
  <details open><summary>Abstract</summary>
  Feature engineering is mandatory in the machine learning pipeline to obtainrobust models. While evolutionary computation is well-known for its greatresults both in feature selection and feature construction, its methods arecomputationally expensive due to the large number of evaluations required toinduce the final model. Part of the reason why these algorithms require a largenumber of evaluations is their lack of domain-specific knowledge, resulting ina lot of random guessing during evolution. In this work, we propose using LargeLanguage Models (LLMs) as an initial feature construction step to add knowledgeto the dataset. By doing so, our results show that the evolution can convergefaster, saving us computational resources. The proposed approach only providesthe names of the features in the dataset and the target objective to the LLM,making it usable even when working with datasets containing private data. Whileconsistent improvements to test performance were only observed for one-third ofthe datasets (CSS, PM, and IM10), possibly due to problems being easilyexplored by LLMs, this approach only decreased the model performance in 1/77test cases. Additionally, this work introduces the M6GP feature engineeringalgorithm to symbolic regression, showing it can improve the results of therandom forest regressor and produce competitive results with its predecessor,M3GP.
  </details>

- **[MoQa: Rethinking MoE Quantization with Multi-stage Data-model Distribution Awareness](http://arxiv.org/abs/2503.21135v1)**  `arXiv:2503.21135`  `cs.LG`  
  _Zihao Zheng, Xiuping Cui, Size Zheng, Maoliang Li, Jiayu Chen, Yun, et al._
  <details open><summary>Abstract</summary>
  With the advances in artificial intelligence, Mix-of-Experts (MoE) has becomethe main form of Large Language Models (LLMs), and its demand for modelcompression is increasing. Quantization is an effective method that not onlycompresses the models but also significantly accelerates their performance.Existing quantization methods have gradually shifted the focus from parameterscaling to the analysis of data distributions. However, their analysis isdesigned for dense LLMs and relies on the simple one-model-all-data mapping,which is unsuitable for MoEs. This paper proposes a new quantization frameworkcalled MoQa. MoQa decouples the data-model distribution complexity of MoEs inmultiple analysis stages, quantitively revealing the dynamics during sparsedata activation, data-parameter mapping, and inter-expert correlations. Basedon these, MoQa identifies particular experts' and parameters' significance withoptimal data-model distribution awareness and proposes a series of fine-grainedmix-quantization strategies adaptive to various data activation and expertcombination scenarios. Moreover, MoQa discusses the limitations of existingquantization and analyzes the impact of each stage analysis, showing novelinsights for MoE quantization. Experiments show that MoQa achieves a 1.69~2.18perplexity decrease in language modeling tasks and a 1.58%~8.91% accuracyimprovement in zero-shot inference tasks. We believe MoQa will play a role infuture MoE construction, optimization, and compression.
  </details>

- **[Leveraging Large Language Models for Risk Assessment in Hyperconnected Logistic Hub Network Deployment](http://arxiv.org/abs/2503.21115v1)**  `arXiv:2503.21115`  `cs.CL`  
  _Yinzhu Quan, Yujia Xu, Guanlin Chen, Frederick Benaben, Benoit Montreuil_
  <details open><summary>Abstract</summary>
  The growing emphasis on energy efficiency and environmental sustainability inglobal supply chains introduces new challenges in the deployment ofhyperconnected logistic hub networks. In current volatile, uncertain, complex,and ambiguous (VUCA) environments, dynamic risk assessment becomes essential toensure successful hub deployment. However, traditional methods often struggleto effectively capture and analyze unstructured information. In this paper, wedesign an Large Language Model (LLM)-driven risk assessment pipeline integratedwith multiple analytical tools to evaluate logistic hub deployment. Thisframework enables LLMs to systematically identify potential risks by analyzingunstructured data, such as geopolitical instability, financial trends,historical storm events, traffic conditions, and emerging risks from newssources. These data are processed through a suite of analytical tools, whichare automatically called by LLMs to support a structured and data-drivendecision-making process for logistic hub selection. In addition, we designprompts that instruct LLMs to leverage these tools for assessing thefeasibility of hub selection by evaluating various risk types and levels.Through risk-based similarity analysis, LLMs cluster logistic hubs withcomparable risk profiles, enabling a structured approach to risk assessment. Inconclusion, the framework incorporates scalability with long-term memory andenhances decision-making through explanation and interpretation, enablingcomprehensive risk assessments for logistic hub deployment in hyperconnectedsupply chain networks.
  </details>

- **[EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues](http://arxiv.org/abs/2503.21080v1)**  `arXiv:2503.21080`  `cs.CL`  
  _Yuhan Liu, Yunbo Long_
  <details open><summary>Abstract</summary>
  While large language model (LLM)-based chatbots have been applied foreffective engagement in credit dialogues, their capacity for dynamic emotionalexpression remains limited. Current agents primarily rely on passive empathyrather than affective reasoning. For instance, when faced with persistentclient negativity, the agent should employ strategic emotional adaptation byexpressing measured anger to discourage counterproductive behavior and guidethe conversation toward resolution. This context-aware emotional modulation isessential for imitating the nuanced decision-making of human negotiators. Thispaper introduces an EQ-negotiator that combines emotion sensing frompre-trained language models (PLMs) with emotional reasoning based on GameTheory and Hidden Markov Models. It takes into account both the current andhistorical emotions of the client to better manage and address negativeemotions during interactions. By fine-tuning pre-trained language models (PLMs)on public emotion datasets and validating them on the credit dialogue datasets,our approach enables LLM-based agents to effectively capture shifts in clientemotions and dynamically adjust their response tone based on our emotiondecision policies in real-world financial negotiations. This EQ-negotiator canalso help credit agencies foster positive client relationships, enhancingsatisfaction in credit services.
  </details>

- **[What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning](http://arxiv.org/abs/2503.21055v1)**  `arXiv:2503.21055`  `cs.CV`  
  _Chi-Hsi Kung, Frangil Ramirez, Juhyung Ha, Yi-Ting Chen, David Crandall, Yi-Hsuan Tsai_
  <details open><summary>Abstract</summary>
  Understanding a procedural activity requires modeling both how action stepstransform the scene, and how evolving scene transformations can influence thesequence of action steps, even those that are accidental or erroneous. Existingwork has studied procedure-aware video representations by proposing novelapproaches such as modeling the temporal order of actions and has notexplicitly learned the state changes (scene transformations). In this work, westudy procedure-aware video representation learning by incorporatingstate-change descriptions generated by Large Language Models (LLMs) assupervision signals for video encoders. Moreover, we generate state-changecounterfactuals that simulate hypothesized failure outcomes, allowing models tolearn by imagining the unseen ``What if'' scenarios. This counterfactualreasoning facilitates the model's ability to understand the cause and effect ofeach step in an activity. To verify the procedure awareness of our model, weconduct extensive experiments on procedure-aware tasks, including temporalaction segmentation and error detection. Our results demonstrate theeffectiveness of the proposed state-change descriptions and theircounterfactuals and achieve significant improvements on multiple tasks. We willmake our source code and data publicly available soon.
  </details>

- **[Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs](http://arxiv.org/abs/2503.20749v2)**  `arXiv:2503.20749`  `cs.CL`  
  _Yuxuan Lu, Jing Huang, Yan Han, Bennet Bei, Yaochen Xie, Dakuo Wang, et al._
  <details open><summary>Abstract</summary>
  Recent research shows that LLMs can simulate ``believable'' human behaviorsto power LLM agents via prompt-only methods. In this work, we focus onevaluating and improving LLM's objective ``accuracy'' rather than thesubjective ``believability'' in the web action generation task, leveraging alarge-scale, real-world dataset collected from online shopping human actions.We present the first comprehensive quantitative evaluation of state-of-the-artLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web actiongeneration. Our results show that fine-tuning LLMs on real-world behavioraldata substantially improves their ability to generate actions compared toprompt-only methods. Furthermore, incorporating synthesized reasoning tracesinto model training leads to additional performance gains, demonstrating thevalue of explicit rationale in behavior modeling. This work establishes a newbenchmark for evaluating LLMs in behavior simulation and offers actionableinsights into how real-world action data and reasoning augmentation can enhancethe fidelity of LLM agents.
  </details>

- **[Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy](http://arxiv.org/abs/2503.20673v2)**  `arXiv:2503.20673`  `cs.CV`  
  _Yinan Sun, Xiongkuo Min, Zicheng Zhang, Yixuan Gao, Yuqin Cao, Guangtao Zhai_
  <details open><summary>Abstract</summary>
  The rapid development of multimodal large language models has resulted inremarkable advancements in visual perception and understanding, consolidatingseveral tasks into a single visual question-answering framework. However, thesemodels are prone to hallucinations, which limit their reliability as artificialintelligence systems. While this issue is extensively researched in naturallanguage processing and image captioning, there remains a lack of investigationof hallucinations in Low-level Visual Perception and Understanding (HLPU),especially in the context of image quality assessment tasks. We consider thatthese hallucinations arise from an absence of clear self-awareness within themodels. To address this issue, we first introduce the HLPU instructiondatabase, the first instruction database specifically focused on hallucinationsin low-level vision tasks. This database contains approximately 200Kquestion-answer pairs and comprises four subsets, each covering different typesof instructions. Subsequently, we propose the Self-Awareness FailureElimination (SAFEQA) model, which utilizes image features, salient regionfeatures and quality features to improve the perception and comprehensionabilities of the model in low-level vision tasks. Furthermore, we propose theEnhancing Self-Awareness Preference Optimization (ESA-PO) framework to increasethe model's awareness of knowledge boundaries, thereby mitigating the incidenceof hallucination. Finally, we conduct comprehensive experiments on low-levelvision tasks, with the results demonstrating that our proposed methodsignificantly enhances self-awareness of the model in these tasks and reduceshallucinations. Notably, our proposed method improves both accuracy andself-awareness of the proposed model and outperforms close-source models interms of various evaluation metrics.
  </details>

- **[Cross-Tokenizer Distillation via Approximate Likelihood Matching](http://arxiv.org/abs/2503.20083v2)**  `arXiv:2503.20083`  `cs.CL`  
  _Benjamin Minixhofer, Ivan Vuliƒá, Edoardo Maria Ponti_
  <details open><summary>Abstract</summary>
  Distillation has shown remarkable success in transferring knowledge from aLarge Language Model (LLM) teacher to a student LLM. However, currentdistillation methods predominantly require the same tokenizer between theteacher and the student, restricting their applicability to only a small subsetof teacher-student pairs. In this work, we develop a cross-tokenizerdistillation method to solve this crucial deficiency. Our method is the firstto enable cross-tokenizer distillation without a next-token prediction loss asthe main objective, instead purely maximizing the student predictions'similarity to the teacher's predictions (known as pure distillation), whilealso being robust to large mismatches between the teacher and the studenttokenizer function and vocabulary. Empirically, our method enablessubstantially improved performance as tested on two use cases. First, we showthat viewing tokenizer transfer as self-distillation enables unprecedentlyeffective transfer across tokenizers. We transfer (subword-level) Llama andGemma models to byte-level tokenization more effectively than prior methodstransfer to a similar subword tokenizer under a comparable training budget.Transferring different base models to the same tokenizer also enablesensembling them (e.g., via averaging their predicted probabilities) whichboosts performance. Second, we use our cross-tokenizer distillation method todistil a large maths-specialized LLM into a smaller model, achievingcompetitive maths problem-solving performance. Overall, our results makesubstantial strides toward better adaptability and enhanced interaction betweendifferent LLMs.
  </details>

- **[ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning](http://arxiv.org/abs/2503.19470v2)**  `arXiv:2503.19470`  `cs.AI` `cs.CL`  
  _Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, et al._
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have shown remarkable capabilities in reasoning,exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integratingreasoning with external search processes remains challenging, especially forcomplex multi-hop questions requiring multiple retrieval steps. We proposeReSearch, a novel framework that trains LLMs to Reason with Search viareinforcement learning without using any supervised data on reasoning steps.Our approach treats search operations as integral components of the reasoningchain, where when and how to perform searches is guided by text-based thinking,and search results subsequently influence further reasoning. We train ReSearchon Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conductextensive experiments. Despite being trained on only one dataset, our modelsdemonstrate strong generalizability across various benchmarks. Analysis revealsthat ReSearch naturally elicits advanced reasoning capabilities such asreflection and self-correction during the reinforcement learning process.
  </details>

- **[SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding](http://arxiv.org/abs/2503.18943v2)**  `arXiv:2503.18943`  `cs.CV`  
  _Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, et al._
  <details open><summary>Abstract</summary>
  We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family ofvideo large language models (LLMs) offering a token-efficient solution forlong-form video understanding. We incorporate the two-stream SlowFast mechanisminto a streamlined training pipeline, and perform joint video-image training ona carefully curated data mixture of only publicly available datasets. Ourprimary focus is on highly efficient model scales (1B and 3B), demonstratingthat even relatively small Video LLMs can achieve state-of-the-art performanceon video understanding, meeting the demand for mobile-friendly models.Experimental results demonstrate that SF-LLaVA-1.5 achieves superiorperformance on a wide range of video and image tasks, with robust results atall model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achievesstate-of-the-art results in long-form video understanding (e.g., LongVideoBenchand MLVU) and excels at small scales across various video benchmarks.
  </details>

- **[AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning](http://arxiv.org/abs/2503.18769v2)**  `arXiv:2503.18769`  `cs.CL` `cs.RO`  
  _Alan Dao, Dinh Bach Vu, Bui Quang Huy_
  <details open><summary>Abstract</summary>
  This paper presents AlphaSpace, a novel methodology designed to enhance thespatial reasoning capabilities of language models for robotic manipulation in3D Cartesian space. AlphaSpace employs a hierarchical semantics-basedtokenization strategy that encodes spatial information at both coarse andfine-grained levels. Our approach represents objects with their attributes,positions, and height information through structured tokens, enabling precisespatial reasoning without relying on traditional vision-based embeddings. Thisapproach enables LLMs to accurately manipulate objects by positioning them atspecific (x, y, z) coordinates. Experimental results suggest that AlphaSpacedemonstrates promising potential for improving manipulation tasks, achieving atotal accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude3.5 Sonnet. These results demonstrate the potential of structured spatialencoding for manipulation tasks and warrant further exploration.
  </details>

- **[Image-to-Text for Medical Reports Using Adaptive Co-Attention and Triple-LSTM Module](http://arxiv.org/abs/2503.18297v2)**  `arXiv:2503.18297`  `cs.CV`  
  _Yishen Liu, Shengda Liu, Hudan Pan_
  <details open><summary>Abstract</summary>
  Medical report generation requires specialized expertise that general largemodels often fail to accurately capture. Moreover, the inherent repetition andsimilarity in medical data make it difficult for models to extract meaningfulfeatures, resulting in a tendency to overfit. So in this paper, we propose amultimodal model, Co-Attention Triple-LSTM Network (CA-TriNet), a deep learningmodel that combines transformer architectures with a Multi-LSTM network. ItsCo-Attention module synergistically links a vision transformer with a texttransformer to better differentiate medical images with similarities, augmentedby an adaptive weight operator to catch and amplify image labels with minorsimilarities. Furthermore, its Triple-LSTM module refines generated sentencesusing targeted image objects. Extensive evaluations over three public datasetshave demonstrated that CA-TriNet outperforms state-of-the-art models in termsof comprehensive ability, even pre-trained large language models on somemetrics.
  </details>

- **[WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for Efficient LLM Inference](http://arxiv.org/abs/2503.17922v2)**  `arXiv:2503.17922`  `cs.CL`  
  _Youhui Zuo, Sibo Wei, Chen Zhang, Zhuorui Liu, Wenpeng Lu, Dawei Song_
  <details open><summary>Abstract</summary>
  With the advancements in long-context inference capabilities of largelanguage models (LLMs), the KV cache has become one of the foundationalcomponents. However, its substantial GPU memory consumption makes KV cachecompression a key technique for enabling efficient LLM inference in industrialscenarios. While recent studies have focused on optimizing the memory occupiedby the KV cache, they overlook two critical factors: preserving semanticcoherence and considering task-specific characteristic during compression. Toaddress these limitations, we propose a novel task-adaptive KV cache windowselection method, WindowKV. WindowKV dynamically selects local semantic windowsconsisting of consecutive tokens, according to task-specific characteristics,ensuring the retained KV cache captures continuous, essential context.Additionally, we introduce an intra-group layer KV cache indices sharingstrategy to reduce computational overhead, achieving a balance betweenperformance and efficiency. We rigorously evaluate WindowKV on the LongBenchbenchmark, and the results demonstrate that it maintains a performancecomparable to full KV cache retention while using only 12% of the original KVcache, significantly reducing memory requirements. Furthermore, our method alsoachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,highlighting its effectiveness and robustness.
  </details>

- **[Accelerating Antibiotic Discovery with Large Language Models and Knowledge Graphs](http://arxiv.org/abs/2503.16655v2)**  `arXiv:2503.16655`  `cs.CL`  
  _Maxime Delmas, Magdalena Wysocka, Danilo Gusicuma, Andr√© Freitas_
  <details open><summary>Abstract</summary>
  The discovery of novel antibiotics is critical to address the growingantimicrobial resistance (AMR). However, pharmaceutical industries face highcosts (over $1 billion), long timelines, and a high failure rate, worsened bythe rediscovery of known compounds. We propose an LLM-based pipeline that actsas an alarm system, detecting prior evidence of antibiotic activity to preventcostly rediscoveries. The system integrates organism and chemical literatureinto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,and multi-level evidence classification. We tested the pipeline on a privatelist of 73 potential antibiotic-producing organisms, disclosing 12 negativehits for evaluation. The results highlight the effectiveness of the pipelinefor evidence reviewing, reducing false negatives, and acceleratingdecision-making. The KG for negative hits and the user interface forinteractive exploration will be made publicly available.
  </details>

- **[Bias Evaluation and Mitigation in Retrieval-Augmented Medical Question-Answering Systems](http://arxiv.org/abs/2503.15454v3)**  `arXiv:2503.15454`  `cs.CL`  
  _Yuelyu Ji, Hang Zhang, Yanshan Wang_
  <details open><summary>Abstract</summary>
  Medical Question Answering systems based on Retrieval Augmented Generation ispromising for clinical decision support because they can integrate externalknowledge, thus reducing inaccuracies inherent in standalone large languagemodels (LLMs). However, these systems may unintentionally propagate or amplifybiases associated with sensitive demographic attributes like race, gender, andsocioeconomic factors. This study systematically evaluates demographic biaseswithin medical RAG pipelines across multiple QA benchmarks, including MedQA,MedMCQA, MMLU, and EquityMedQA. We quantify disparities in retrievalconsistency and answer correctness by generating and analyzing queriessensitive to demographic variations. We further implement and compare severalbias mitigation strategies to address identified biases, including Chain ofThought reasoning, Counterfactual filtering, Adversarial prompt refinement, andMajority Vote aggregation. Experimental results reveal significant demographicdisparities, highlighting that Majority Vote aggregation notably improvesaccuracy and fairness metrics. Our findings underscore the critical need forexplicitly fairness-aware retrieval methods and prompt engineering strategiesto develop truly equitable medical QA systems.
  </details>

- **[TLUE: A Tibetan Language Understanding Evaluation Benchmark](http://arxiv.org/abs/2503.12051v2)**  `arXiv:2503.12051`  `cs.CL`  
  _Fan Gao, Cheng Huang, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) have made tremendous progress in recent years,but low-resource languages, such as Tibetan, remain significantlyunderrepresented in their evaluation. Despite Tibetan being spoken by overseven million people, it has largely been neglected in the development andassessment of LLMs. To address this gap, we present TLUE (A Tibetan LanguageUnderstanding Evaluation Benchmark), the first large-scale benchmark forassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:(1) a comprehensive multi-task understanding benchmark spanning 5 domains and67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate adiverse set of state-of-the-art LLMs. Experimental results demonstrate thatmost LLMs perform below the random baseline, highlighting the considerablechallenges LLMs face in processing Tibetan, a low-resource language. TLUEprovides an essential foundation for driving future research and progress inTibetan language understanding and underscores the need for greater inclusivityin LLM development.
  </details>

- **[Cognitive-Mental-LLM: Evaluating Reasoning in Large Language Models for Mental Health Prediction via Online Text](http://arxiv.org/abs/2503.10095v2)**  `arXiv:2503.10095`  `cs.AI` `cs.CL`  
  _Avinash Patil, Amardeep Kour Gedhu_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have demonstrated potential in predicting mentalhealth outcomes from online text, yet traditional classification methods oftenlack interpretability and robustness. This study evaluates structured reasoningtechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), andTree-of-Thought (ToT)-to improve classification accuracy across multiple mentalhealth datasets sourced from Reddit. We analyze reasoning-driven promptingstrategies, including Zero-shot CoT and Few-shot CoT, using key performancemetrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Ourfindings indicate that reasoning-enhanced techniques improve classificationperformance over direct prediction, particularly in complex cases. Compared tobaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trainedtransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMssuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notablegains on datasets like Dreaddit (+0.52\% over M-LLM, +0.82\% over BERT) andSDCNL (+4.67\% over M-LLM, +2.17\% over BERT). However, performance declines inDepression Severity, and CSSRS predictions suggest dataset-specificlimitations, likely due to our using a more extensive test set. Among promptingstrategies, Few-shot CoT consistently outperforms others, reinforcing theeffectiveness of reasoning-driven LLMs. Nonetheless, dataset variabilityhighlights challenges in model reliability and interpretability. This studyprovides a comprehensive benchmark of reasoning-based LLM techniques for mentalhealth text classification. It offers insights into their potential forscalable clinical applications while identifying key challenges for futureimprovements.
  </details>

- **[Starjob: Dataset for LLM-Driven Job Shop Scheduling](http://arxiv.org/abs/2503.01877v2)**  `arXiv:2503.01877`  `cs.LG` `cs.AI`  
  _Henrik Abgaryan, Tristan Cazenave, Ararat Harutyunyan_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have shown remarkable capabilities acrossvarious domains, but their potential for solving combinatorial optimizationproblems remains largely unexplored. In this paper, we investigate theapplicability of LLMs to the Job Shop Scheduling Problem (JSSP), a classicchallenge in combinatorial optimization that requires efficient job allocationto machines to minimize makespan. To this end, we introduce Starjob, the firstsupervised dataset for JSSP, comprising 130k instances specifically designedfor training LLMs. Leveraging this dataset, we fine-tune the LLaMA 8B 4-bitquantized model with the LoRA method to develop an end-to-end schedulingapproach. Our evaluation on standard benchmarks demonstrates that the proposedLLM-based method not only surpasses traditional Priority Dispatching Rules(PDRs) but also achieves notable improvements over state-of-the-art neuralapproaches like L2D, with an average improvement of 15.36% on DMU and 7.85% onTaillard benchmarks. These results highlight the untapped potential of LLMs intackling combinatorial optimization problems, paving the way for futureadvancements in this area.
  </details>

- **[R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs](http://arxiv.org/abs/2502.12767v4)**  `arXiv:2502.12767`  `cs.AI` `cs.CL`  
  _Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi_
  <details open><summary>Abstract</summary>
  Recent studies have combined Large Language Models (LLMs) with KnowledgeGraphs (KGs) to enhance reasoning, improving inference accuracy withoutadditional training while mitigating hallucination. However, existingframeworks are often rigid, struggling to adapt to KG or task changes. Theyalso rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.To address this, We introduce R2-KG, a plug-and-play, dual-agent framework thatseparates reasoning into two roles: an Operator (a low-capacity LLM) thatgathers evidence and a Supervisor (a high-capacity LLM) that makes finaljudgments. This design is cost-efficient for LLM inference while stillmaintaining strong reasoning accuracy. Additionally, R2-KG employs anAbstention mechanism, generating answers only when sufficient evidence iscollected from KG, which significantly enhances reliability. Experiments acrossmultiple KG-based reasoning tasks show that R2-KG consistently outperformsbaselines in both accuracy and reliability, regardless of the inherentcapability of LLMs used as the Operator. Further experiments reveal that thesingle-agent version of R2-KG, equipped with a strict self-consistencystrategy, achieves significantly higher-than-baseline reliability whilereducing inference cost. However, it also leads to a higher abstention rate incomplex KGs. Our findings establish R2-KG as a flexible and cost-effectivesolution for KG-based reasoning. It reduces reliance on high-capacity LLMswhile ensuring trustworthy inference. The code is available athttps://github.com/ekrxjwh2009/R2-KG/.
  </details>

- **[Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging -- An Open Recipe](http://arxiv.org/abs/2502.09056v3)**  `arXiv:2502.09056`  `cs.AI` `cs.CL`  
  _Kunat Pipatanakul, Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai_
  <details open><summary>Abstract</summary>
  This paper investigates data selection and model merging methodologies aimedat incorporating advanced reasoning capabilities such as those of DeepSeek R1into language-specific large language models (LLMs), with a particular focus onthe Thai LLM. Our goal is to enhance the reasoning capabilities oflanguage-specific LLMs while maintaining their target language abilities.DeepSeek R1 excels in reasoning but primarily benefits high-resource languagessuch as English and Chinese. However, low-resource languages remain underserveddue to the dominance of English-centric training data and model optimizations,which limit performance in these languages. This limitation results inunreliable code-switching and diminished effectiveness on tasks in low-resourcelanguages. Meanwhile, local and regional LLM initiatives have attempted tobridge this gap by developing language-specific LLMs that focus on improvinglocal linguistic fidelity. We demonstrate that, with only publicly availabledatasets and a computational budget of $120, it is possible to enhance thereasoning capabilities of language-specific LLMs to match the level of DeepSeekR1, without compromising their performance on target language tasks.
  </details>

- **[Typhoon T1: An Open Thai Reasoning Model](http://arxiv.org/abs/2502.09042v2)**  `arXiv:2502.09042`  `cs.AI` `cs.CL`  
  _Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai, Kunat Pipatanakul_
  <details open><summary>Abstract</summary>
  This paper introduces Typhoon T1, an open effort to develop an open Thaireasoning model. A reasoning model is a relatively new type of generative modelbuilt on top of large language models (LLMs). A reasoning model generates along chain of thought before arriving at a final answer, an approach found toimprove performance on complex tasks. However, details on developing such amodel are limited, especially for reasoning models that can generate traces ina low-resource language. Typhoon T1 presents an open effort that dives into thedetails of developing a reasoning model in a more cost-effective way byleveraging supervised fine-tuning using open datasets, instead of reinforcementlearning. This paper shares the details about synthetic data generation andtraining, as well as our dataset and model weights. Additionally, we provideinsights gained from developing a reasoning model that generalizes acrossdomains and is capable of generating reasoning traces in a low-resourcelanguage, using Thai as an example. We hope this open effort provides afoundation for further research in this field.
  </details>

- **[Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning](http://arxiv.org/abs/2502.08972v2)**  `arXiv:2502.08972`  `cs.AI` `cs.CL`  
  _Hyundong Cho, Karishma Sharma, Nicolaas Jedema, Leonardo F. R. Ribeiro, Alessandro Moschitti, Ravi Krishnan, et al._
  <details open><summary>Abstract</summary>
  Language models are aligned to the collective voice of many, resulting ingeneric outputs that do not align with specific users' styles. In this work, wepresent Trial-Error-Explain In-Context Learning} (ITCL), a tuning-free methodthat personalizes language models for text generation tasks with fewer than 10examples per user. TICL iteratively expands an in-context learning prompt via atrial-error-explain process, adding model-generated negative samples andexplanations that provide fine-grained guidance towards a specific user'sstyle. TICL achieves favorable win rates on pairwise comparisons withLLM-as-a-judge up to 91.5% against the previous state-of-the-art andoutperforms competitive tuning-free baselines for personalized alignment tasksof writing emails, essays and news articles. Both lexical and qualitativeanalyses show that the negative samples and explanations enable language modelsto learn stylistic context more effectively and overcome the bias towardsstructural and formal phrases observed in their zero-shot outputs. Byfront-loading inference compute to create a user-specific in-context learningprompt that does not require extra generation steps at test time, TICL presentsa novel yet simple approach for personalized alignment.
  </details>

- **[Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG](http://arxiv.org/abs/2502.08356v3)**  `arXiv:2502.08356`  `cs.CL`  
  _Kushagra Bhushan, Yatin Nandwani, Dinesh Khandelwal, Sonam Gupta, Gaurav Pandey, Dinesh Raghu, et al._
  <details open><summary>Abstract</summary>
  Retrieval-Augmented Generation (RAG) has emerged as a prominent method forincorporating domain knowledge into Large Language Models (LLMs). While RAGenhances response relevance by incorporating retrieved domain knowledge in thecontext, retrieval errors can still lead to hallucinations and incorrectanswers. To recover from retriever failures, domain knowledge is injected byfine-tuning the model to generate the correct response, even in the case ofretrieval errors. However, we observe that without systematic knowledgeaugmentation, fine-tuned LLMs may memorize new information but still fail toextract relevant domain knowledge, leading to poor performance. In this work,we present a novel framework that significantly enhances the fine-tuningprocess by augmenting the training data in two ways -- context augmentation andknowledge paraphrasing. In context augmentation, we create multiple trainingsamples for a given QA pair by varying the relevance of the retrievedinformation, teaching the model when to ignore and when to rely on retrievedcontent. In knowledge paraphrasing, we fine-tune with multiple answers to thesame question, enabling LLMs to better internalize specialized knowledge. Tomitigate catastrophic forgetting due to fine-tuning, we add a domain-specificidentifier to a question and also utilize a replay buffer containing general QApairs. Experimental results demonstrate the efficacy of our method overexisting techniques, achieving up to 10\% relative gain in token-level recallwhile preserving the LLM's generalization capabilities.
  </details>

- **[Enhancing LLM Character-Level Manipulation via Divide and Conquer](http://arxiv.org/abs/2502.08180v2)**  `arXiv:2502.08180`  `cs.AI` `cs.CL`  
  _Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Zhecheng Li, Yiwei Wang_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have demonstrated strong generalizationcapabilities across a wide range of natural language processing (NLP) tasks.However, they exhibit notable weaknesses in character-level stringmanipulation, struggling with fundamental operations such as characterdeletion, insertion, and substitution. These challenges stem primarily fromtokenization constraints, despite the critical role of such operations in datapreprocessing and code generation. Through systematic analysis, we derive twokey insights: (1) LLMs face significant difficulties in leveraging intrinsictoken knowledge for character-level reasoning, and (2) atomized word structurescan substantially enhance LLMs' ability to process token-level structuralinformation. Building on these insights, we propose Character-LevelManipulation via Divide and Conquer, a novel approach designed to bridge thegap between token-level processing and character-level manipulation. Our methoddecomposes complex operations into explicit character-level subtasks coupledwith controlled token reconstruction phases, leading to significantimprovements in accuracy. Without additional training, our method significantlyimproves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and$\texttt{Substitution}$ tasks. To support further research, we open-source ourimplementation and benchmarks.
  </details>

- **[Reinforced Lifelong Editing for Language Models](http://arxiv.org/abs/2502.05759v3)**  `arXiv:2502.05759`  `cs.CL`  
  _Zherui Li, Houcheng Jiang, Hao Chen, Baolong Bi, Zhenhong Zhou, Fei Sun, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) acquire information from pre-training corpora,but their stored knowledge can become inaccurate or outdated over time. Modelediting addresses this challenge by modifying model parameters withoutretraining, and prevalent approaches leverage hypernetworks to generate theseparameter updates. However, they face significant challenges in lifelongediting due to their incompatibility with LLM parameters that dynamicallychange during the editing process. To address this, we observed thathypernetwork-based lifelong editing aligns with reinforcement learning modelingand proposed RLEdit, an RL-based editing method. By treating editing losses asrewards and optimizing hypernetwork parameters at the full knowledge sequencelevel, we enable it to precisely capture LLM changes and generate appropriateparameter updates. Our extensive empirical evaluation across several LLMsdemonstrates that RLEdit outperforms existing methods in lifelong editing withsuperior effectiveness and efficiency, achieving a 59.24% improvement whilerequiring only 2.11% of the time compared to most approaches. Our code isavailable at: https://github.com/zhrli324/RLEdit.
  </details>

- **[AnyEdit: Edit Any Knowledge Encoded in Language Models](http://arxiv.org/abs/2502.05628v2)**  `arXiv:2502.05628`  `cs.CL`  
  _Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Guojun Ma, Mingyang Wan, Xiang Wang, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) often produce incorrect or outdated information,necessitating efficient and precise knowledge updates. Current model editingmethods, however, struggle with long-form knowledge in diverse formats, such aspoetry, code snippets, and mathematical derivations. These limitations arisefrom their reliance on editing a single token's hidden state, a limitation weterm "efficacy barrier". To solve this, we propose AnyEdit, a newautoregressive editing paradigm. It decomposes long-form knowledge intosequential chunks and iteratively edits the key token in each chunk, ensuringconsistent and accurate outputs. Theoretically, we ground AnyEdit in the ChainRule of Mutual Information, showing its ability to update any knowledge withinLLMs. Empirically, it outperforms strong baselines by 21.5% on benchmarksincluding UnKEBench, AKEW, and our new EditEverything dataset for long-formdiverse-formatted knowledge. Additionally, AnyEdit serves as a plug-and-playframework, enabling current editing methods to update knowledge with arbitrarylength and format, significantly advancing the scope and practicality of LLMknowledge editing.
  </details>

- **[Group Reasoning Emission Estimation Networks](http://arxiv.org/abs/2502.06874v2)**  `arXiv:2502.06874`  `cs.LG` `cs.AI` `cs.CL`  
  _Yanming Guo, Xiao Qian, Kevin Credit, Jin Ma_
  <details open><summary>Abstract</summary>
  Accurate greenhouse gas (GHG) emission reporting is critical for governments,businesses, and investors. However, adoption remains limited particularly amongsmall and medium enterprises due to high implementation costs, fragmentedemission factor databases, and a lack of robust sector classification methods.To address these challenges, we introduce Group Reasoning Emission EstimationNetworks (GREEN), an AI-driven carbon accounting framework that standardizesenterprise-level emission estimation, constructs a large-scale benchmarkdataset, and leverages a novel reasoning approach with large language models(LLMs). Specifically, we compile textual descriptions for 20,850 companies withvalidated North American Industry Classification System (NAICS) labels andalign these with an economic model of carbon intensity factors. By reframingsector classification as an information retrieval task, we fine-tuneSentence-BERT models using a contrastive learning loss. To overcome thelimitations of single-stage models in handling thousands of hierarchicalcategories, we propose a Group Reasoning method that ensembles LLM classifiersbased on the natural NAICS ontology, decomposing the task into multiplesub-classification steps. We theoretically prove that this approach reducesclassification uncertainty and computational complexity. Experiments on 1,114NAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%Top-10 accuracy), and case studies on 20 companies report a mean absolutepercentage error (MAPE) of 45.88%. The project is available at:https://huggingface.co/datasets/Yvnminc/ExioNAICS.
  </details>

- **[Leveraging Textual Anatomical Knowledge for Class-Imbalanced Semi-Supervised Multi-Organ Segmentation](http://arxiv.org/abs/2501.13470v2)**  `arXiv:2501.13470`  `cs.CV`  
  _Yuliang Gu, Weilun Tsao, Bo Du, Thierry G√©raud, Yongchao Xu_
  <details open><summary>Abstract</summary>
  Annotating 3D medical images demands substantial time and expertise, drivingthe adoption of semi-supervised learning (SSL) for segmentation tasks. However,the complex anatomical structures of organs often lead to significant classimbalances, posing major challenges for deploying SSL in real-world scenarios.Despite the availability of valuable prior information, such as inter-organrelative positions and organ shape priors, existing SSL methods have yet tofully leverage these insights. To address this gap, we propose a novel approachthat integrates textual anatomical knowledge (TAK) into the segmentation model.Specifically, we use GPT-4o to generate textual descriptions of anatomicalpriors, which are then encoded using a CLIP-based model. These encoded priorsare injected into the segmentation model as parameters of the segmentationhead. Additionally, contrastive learning is employed to enhance the alignmentbetween textual priors and visual features. Extensive experiments demonstratethe superior performance of our method, significantly surpassingstate-of-the-art approaches. The source code will be available at:https://github.com/Lunn88/TAK-Semi.
  </details>

- **[iTool: Boosting Tool Use of Large Language Models via Iterative Reinforced Fine-Tuning](http://arxiv.org/abs/2501.09766v3)**  `arXiv:2501.09766`  `cs.LG` `cs.AI` `cs.CL`  
  _Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, et al._
  <details open><summary>Abstract</summary>
  Augmenting large language models (LLMs) with external tools is known as apromising approach to enhancing their capabilities, especially for complextasks. Synthesizing tool-use data through real-world simulations is aneffective way to achieve it. Nevertheless, our investigation reveals that (1)training gains significantly decay as synthetic data increases. The modelstruggles to benefit from more synthetic data due to potential data diversityissues, resulting in poor performance in complex scenarios. Moreover, we findthat (2) this challenge primarily manifests as minor discrepancies between themodel's output and the ground truth response (termed as deficiency), such aserrors in parameter values that require complex reasoning from the context toresolve. To this end, we propose an iterative reinforced fine-tuning strategydesigned to alleviate these challenges. This strategy involves: (1) enhancingthe diversity of synthetic data through path exploration of Monte Carlo TreeSearch. (2) iteratively identifying deficiency-related data, constructingfine-grained preference pairs to pinpoint deficiencies, and then applyingpreference optimization to optimize these deficiencies. Our experiments showthat models trained using our method achieve about 12\% better performance thanbaseline models, outperforming larger open-source and closed-source models.
  </details>

- **[OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?](http://arxiv.org/abs/2501.05510v2)**  `arXiv:2501.05510`  `cs.AI` `cs.CV`  
  _Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, et al._
  <details open><summary>Abstract</summary>
  Temporal Awareness, the ability to reason dynamically based on the timestampwhen a question is raised, is the key distinction between offline and onlinevideo LLMs. Unlike offline models, which rely on complete videos for static,post hoc analysis, online models process video streams incrementally anddynamically adapt their responses based on the timestamp at which the questionis posed. Despite its significance, temporal awareness has not been adequatelyevaluated in existing benchmarks. To fill this gap, we present OVO-Bench(Online-VideO-Benchmark), a novel video benchmark that emphasizes theimportance of timestamps for advanced online video understanding capabilitybenchmarking. OVO-Bench evaluates the ability of video LLMs to reason andrespond to events occurring at specific timestamps under three distinctscenarios: (1) Backward tracing: trace back to past events to answer thequestion. (2) Real-time understanding: understand and respond to events as theyunfold at the current timestamp. (3) Forward active responding: delay theresponse until sufficient future information becomes available to answer thequestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videosand approximately human-curated 2,800 fine-grained meta-annotations withprecise timestamps. We combine automated generation pipelines with humancuration. With these high-quality samples, we further developed an evaluationpipeline to systematically query video LLMs along the video timeline.Evaluations of nine Video-LLMs reveal that, despite advancements on traditionalbenchmarks, current models struggle with online video understanding, showing asignificant gap compared to human agents. We hope OVO-Bench will drive progressin video LLMs and inspire future research in online video reasoning. Ourbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.
  </details>

- **[Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine](http://arxiv.org/abs/2501.02471v2)**  `arXiv:2501.02471`  `cs.AI` `cs.CL`  
  _Yishen Liu, Shengda Luo, Zishao Zhong, Tongtong Wu, Jianguo Zhang, Peiyao Ou, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) primarily trained on English texts, often facebiases and inaccuracies in Chinese contexts. Their limitations are pronouncedin fields like Traditional Chinese Medicine (TCM), where cultural and clinicalsubtleties are vital, further hindered by a lack of domain-specific data, suchas rheumatoid arthritis (RA). To address these issues, this paper introducesHengqin-RA-v1, the first large language model specifically tailored for TCMwith a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, acomprehensive RA-specific dataset curated from ancient Chinese medicalliterature, classical texts, and modern clinical studies. This dataset empowersHengqin-RA-v1 to deliver accurate and culturally informed responses,effectively bridging the gaps left by general-purpose models. Extensiveexperiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models,even surpassing the diagnostic accuracy of TCM practitioners in certain cases.
  </details>

- **[Understanding the Logic of Direct Preference Alignment through Logic](http://arxiv.org/abs/2412.17696v2)**  `arXiv:2412.17696`  `cs.CL`  
  _Kyle Richardson, Vivek Srikumar, Ashish Sabharwal_
  <details open><summary>Abstract</summary>
  Recent direct preference alignment algorithms (DPA), such as DPO, have showngreat promise in aligning large language models to human preferences. Whilethis has motivated the development of many new variants of the original DPOloss, understanding the differences between these recent proposals, as well asdeveloping new DPA loss functions, remains difficult given the lack of atechnical and conceptual framework for reasoning about the underlying semanticsof these algorithms. In this paper, we attempt to remedy this by formalizingDPA losses in terms of discrete reasoning problems. Specifically, we ask: Givenan existing DPA loss, can we systematically derive a symbolic program thatcharacterizes its semantics? We propose a novel formalism for characterizingpreference losses for single model and reference model based approaches, andidentify symbolic forms for a number of commonly used DPA variants. Further, weshow how this formal view of preference learning sheds new light on both thesize and structure of the DPA loss landscape, making it possible to not onlyrigorously characterize the relationships between recent loss proposals butalso to systematically explore the landscape and derive new loss functions fromfirst principles. We hope our framework and findings will help provide usefulguidance to those working on human AI alignment.
  </details>

- **[Do Multimodal Large Language Models See Like Humans?](http://arxiv.org/abs/2412.09603v2)**  `arXiv:2412.09603`  `cs.CV`  
  _Jiaying Lin, Shuquan Ye, Rynson W. H. Lau_
  <details open><summary>Abstract</summary>
  Multimodal Large Language Models (MLLMs) have achieved impressive results onvarious vision tasks, leveraging recent advancements in large language models.However, a critical question remains unaddressed: do MLLMs perceive visualinformation similarly to humans? Current benchmarks lack the ability toevaluate MLLMs from this perspective. To address this challenge, we introduceHVSBench, a large-scale benchmark designed to assess the alignment betweenMLLMs and the human visual system (HVS) on fundamental vision tasks that mirrorhuman vision. HVSBench curated over 85K multimodal samples, spanning 13categories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,Free-Viewing, and Searching. Extensive experiments demonstrate theeffectiveness of our benchmark in providing a comprehensive evaluation ofMLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best modelsshow significant room for improvement, with most achieving only moderateresults. Our experiments reveal that HVSBench presents a new and significantchallenge for cutting-edge MLLMs. Diverse human participants attained strongperformance, significantly outperforming MLLMs, which further underscores thebenchmark's high quality. We believe that HVSBench will facilitate research onhuman-aligned and explainable MLLMs, marking a key step in understanding howMLLMs perceive and process visual information.
  </details>

- **[Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding](http://arxiv.org/abs/2412.00493v2)**  `arXiv:2412.00493`  `cs.CV` `cs.CL`  
  _Duo Zheng, Shijia Huang, Liwei Wang_
  <details open><summary>Abstract</summary>
  The rapid advancement of Multimodal Large Language Models (MLLMs) hassignificantly impacted various multimodal tasks. However, these models facechallenges in tasks that require spatial understanding within 3D environments.Efforts to enhance MLLMs, such as incorporating point cloud features, have beenmade, yet a considerable gap remains between the models' learnedrepresentations and the inherent complexity of 3D scenes. This discrepancylargely stems from the training of MLLMs on predominantly 2D data, whichrestricts their effectiveness in comprehending 3D spaces. To address thisissue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,for 3D scene understanding. By treating 3D scenes as dynamic videos andincorporating 3D position encoding into these representations, our Video-3D LLMaligns video representations with real-world spatial contexts more accurately.In addition, we have implemented a maximum coverage sampling technique tooptimize the trade-off between computational cost and performance. Extensiveexperiments demonstrate that our model achieves state-of-the-art performance onseveral 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,Scan2Cap, ScanQA, and SQA3D.
  </details>

- **[ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom](http://arxiv.org/abs/2410.14138v2)**  `arXiv:2410.14138`  `cs.AI` `cs.CV`  
  _Jingqi Zhou, Sheng Wang, Jingwei Dong, Lei Li, Jiahui Gao, Jiyue Jiang, et al._
  <details open><summary>Abstract</summary>
  Large vision-language models (LVLMs) have witnessed significant progress onvisual understanding tasks. However, they often prioritize language knowledgeover image information on visual reasoning tasks, incurring performancedegradation. To tackle this issue, we first identify the drawbacks of existingsolutions (i.e., insufficient and irrelevant visual descriptions, and limitedmulti-modal capacities). We then decompose visual reasoning process into twostages: visual perception (i.e., eyesight) and textual reasoning (i.e.,wisdom), and introduce a novel visual reasoning framework named ProReason. Thisframework features multi-run proactive perception and decoupledvision-reasoning capabilities. Briefly, given a multi-modal question, ProReasoniterates proactive information collection and reasoning until the answer can beconcluded with necessary and sufficient visual descriptions. Notably, thedisassociation of capabilities allows seamless integration of existing largelanguage models (LLMs) to compensate for the reasoning deficits of LVLMs. Ourextensive experiments demonstrate that ProReason outperforms both existingmulti-step reasoning frameworks and passive peer methods on a wide range ofbenchmarks for both open-source and closed-source models. In addition, with theassistance of LLMs, ProReason achieves a performance improvement of up to 15%on MMMU benchmark. Our insights into existing solutions and the decoupledperspective for feasible integration of LLMs illuminate future research onvisual reasoning techniques, especially LLM-assisted ones.
  </details>

- **[Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems](http://arxiv.org/abs/2409.19804v2)**  `arXiv:2409.19804`  `cs.CL`  
  _Xuyang Wu, Shuowei Li, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang_
  <details open><summary>Abstract</summary>
  Retrieval-Augmented Generation (RAG) has recently gained significantattention for its enhanced ability to integrate external knowledge sources intoopen-domain question answering (QA) tasks. However, it remains unclear howthese models address fairness concerns, particularly with respect to sensitiveattributes such as gender, geographic location, and other demographic factors.First, as language models evolve to prioritize utility, like improving exactmatch accuracy, fairness considerations may have been largely overlooked.Second, the complex, multi-component architecture of RAG methods poseschallenges in identifying and mitigating biases, as each component is optimizedfor distinct objectives. In this paper, we aim to empirically evaluate fairnessin several RAG methods. We propose a fairness evaluation framework tailored toRAG, using scenario-based questions and analyzing disparities acrossdemographic attributes. Our experimental results indicate that, despite recentadvances in utility-driven optimization, fairness issues persist in both theretrieval and generation stages. These findings underscore the need fortargeted interventions to address fairness concerns throughout the RAGpipeline. The dataset and code used in this study are publicly available atthis GitHub Repository https://github.com/elviswxy/RAG_fairness .
  </details>

- **[Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography](http://arxiv.org/abs/2409.18119v2)**  `arXiv:2409.18119`  `cs.LG` `cs.AI` `cs.CV`  
  _Yuexi Du, John Onofrey, Nicha C. Dvornek_
  <details open><summary>Abstract</summary>
  Contrastive Language-Image Pre-training (CLIP) demonstrates strong potentialin medical image analysis but requires substantial data and computationalresources. Due to these restrictions, existing CLIP applications in medicalimaging focus mainly on modalities like chest X-rays that have abundantimage-report data available, leaving many other important modalitiesunderexplored. Here, we propose one of the first adaptations of the full CLIPmodel to mammography, which presents significant challenges due to labeled datascarcity, high-resolution images with small regions of interest, and class-wiseimbalance. We first develop a specialized supervision framework for mammographythat leverages its multi-view nature. Furthermore, we design a symmetric localalignment module to better focus on detailed features in high-resolutionimages. Lastly, we incorporate a parameter-efficient fine-tuning approach forlarge language models pre-trained with medical knowledge to address datalimitations. Our multi-view and multi-scale alignment (MaMA) method outperformsstate-of-the-art baselines for three different tasks on two large real-worldmammography datasets, EMBED and RSNA-Mammo, with only 52% model size comparedwith the largest baseline. The code is available athttps://github.com/XYPB/MaMA
  </details>

- **[OmniBench: Towards The Future of Universal Omni-Language Models](http://arxiv.org/abs/2409.15272v4)**  `arXiv:2409.15272`  `cs.CV` `cs.AI` `cs.CL`  
  _Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in multimodal large language models (MLLMs) have focusedon integrating multiple modalities, yet their ability to simultaneously processand reason across different inputs remains underexplored. We introduceOmniBench, a novel benchmark designed to evaluate models' ability to recognize,interpret, and reason across visual, acoustic, and textual inputssimultaneously. We define language models capable of such tri-modal processingas omni-language models (OLMs). OmniBench features high-quality humanannotations that require integrated understanding across all modalities. Ourevaluation reveals that: i) open-source OLMs show significant limitations ininstruction-following and reasoning in tri-modal contexts; and ii) mostbaseline models perform poorly (around 50% accuracy) even with textualalternatives to image/audio inputs. To address these limitations, we developOmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. Weadvocate for developing more robust tri-modal integration techniques andtraining strategies to enhance OLM performance. Codes and data could be foundat our repo (https://github.com/multimodal-art-projection/OmniBench).
  </details>

- **[Joint Estimation and Prediction of City-wide Delivery Demand: A Large Language Model Empowered Graph-based Learning Approach](http://arxiv.org/abs/2408.17258v3)**  `arXiv:2408.17258`  `cs.LG`  
  _Tong Nie, Junlin He, Yuewen Mei, Guoyang Qin, Guilong Li, Jian Sun, et al._
  <details open><summary>Abstract</summary>
  The proliferation of e-commerce and urbanization has significantlyintensified delivery operations in urban areas, boosting the volume andcomplexity of delivery demand. Data-driven predictive methods, especially thoseutilizing machine learning techniques, have emerged to handle thesecomplexities in urban delivery demand management problems. One particularlypressing issue that has yet to be sufficiently addressed is the jointestimation and prediction of city-wide delivery demand, as well as thegeneralization of the model to new cities. To this end, we formulate thisproblem as a transferable graph-based spatiotemporal learning task. First, anindividual-collective message-passing neural network model is formalized tocapture the interaction between demand patterns of associated regions. Second,by exploiting recent advances in large language models (LLMs), we extractgeneral geospatial knowledge encodings from the unstructured locational datausing the embedding generated by LLMs. Last, to encourage the cross-citygeneralization of the model, we integrate the encoding into the demandpredictor in a transferable way. Comprehensive empirical evaluation results ontwo real-world delivery datasets, including eight cities in China and the US,demonstrate that our model significantly outperforms state-of-the-art baselinesin accuracy, efficiency, and transferability.
  </details>

- **[Vision language models are blind: Failing to translate detailed visual features into words](http://arxiv.org/abs/2407.06581v6)**  `arXiv:2407.06581`  `cs.AI` `cs.CV`  
  _Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen_
  <details open><summary>Abstract</summary>
  While large language models with vision capabilities (VLMs), e.g., GPT-4o andGemini 1.5 Pro, score high on many vision-understanding benchmarks, they arestill struggling with low-level vision tasks that are easy to humans.Specifically, on BlindTest, our suite of 7 very simple tasks, includingidentifying (a) whether two circles overlap; (b) how many times two linesintersect; (c) which letter is being circled in a word; and (d) the number ofcircles in an Olympic-like logo, four state-of-the-art VLMs are only 58.07%accurate on average. Claude 3.5 Sonnet performs the best at 77.84% accuracy,far from the human expected accuracy of 100%. Across different imageresolutions and line widths, VLMs including slow-thinking models consistentlystruggle with those tasks that require precise spatial information whengeometric primitives overlap or are close. Yet, VLMs perform at near-100%accuracy when much more space is added to separate shapes and letters. Linearprobing experiments show that vision encoders contain sufficient visualinformation to solve BlindTest and that language models fail to decode thisinformation into correct answers. Code and data are at:https://vlmsareblind.github.io
  </details>

- **[What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models](http://arxiv.org/abs/2405.15668v4)**  `arXiv:2405.15668`  `cs.CV`  
  _Abdelrahman Abdelhamed, Mahmoud Afifi, Alec Go_
  <details open><summary>Abstract</summary>
  Large language models (LLMs) have been effectively used for many computervision tasks, including image classification. In this paper, we present asimple yet effective approach for zero-shot image classification usingmultimodal LLMs. Using multimodal LLMs, we generate comprehensive textualrepresentations from input images. These textual representations are thenutilized to generate fixed-dimensional features in a cross-modal embeddingspace. Subsequently, these features are fused together to perform zero-shotclassification using a linear classifier. Our method does not require promptengineering for each dataset; instead, we use a single, straightforward set ofprompts across all datasets. We evaluated our method on several datasets andour results demonstrate its remarkable effectiveness, surpassing benchmarkaccuracy on multiple datasets. On average, for ten benchmarks, our methodachieved an accuracy gain of 6.2 percentage points, with an increase of 6.8percentage points on the ImageNet dataset, compared to prior methodsre-evaluated with the same setup. Our findings highlight the potential ofmultimodal LLMs to enhance computer vision tasks such as zero-shot imageclassification, offering a significant improvement over traditional methods.
  </details>

- **[Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems](http://arxiv.org/abs/2404.14963v5)**  `arXiv:2404.14963`  `cs.AI` `cs.CL`  
  _Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du_
  <details open><summary>Abstract</summary>
  Chain-of-Thought (CoT) prompting has enhanced the performance of LargeLanguage Models (LLMs) across various reasoning tasks. However, CoT still fallsshort in dealing with complex math word problems, as it usually suffers fromthree pitfalls: semantic misunderstanding errors, calculation errors, andstep-missing errors. Prior studies involve addressing the calculation errorsand step-missing errors, but neglect the semantic misunderstanding errors,which is the major factor limiting the reasoning performance of LLMs. To thisend, we propose a simple-yet-effective method, namely Deeply Understanding theProblems (DUP), to improve the LLMs' math problem-solving ability by addressingsemantic misunderstanding errors. The core of our method is to encourage theLLMs to deeply understand the problems and extract the key problem-solvinginformation used for better reasoning. Extensive experiments on 10 diversereasoning benchmarks show that our DUP method consistently outperforms theother counterparts by a large margin. More encouragingly, DUP achieves a newSOTA result on the GSM8K benchmark, with an accuracy of 97.1% under thezero-shot setting.
  </details>
