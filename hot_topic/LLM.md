# üîç LLM Papers ¬∑ 2025-03-28

[![Total Papers](https://img.shields.io/badge/Papers-62-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `LLM` `Large Language Model` `GPT`  
**Filter**: `None`

---

## üìö Paper List

- **[Q-Insight: Understanding Image Quality via Visual Reinforcement Learning](http://arxiv.org/abs/2503.22679v1)**  `arXiv:2503.22679`  `cs.CV`  
  _Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, et al._
  <details open><summary>Abstract</summary>
  Image quality assessment (IQA) focuses on the perceptual visual quality ofimages, playing a crucial role in downstream tasks such as imagereconstruction, compression, and generation. The rapid advancement ofmulti-modal large language models (MLLMs) has significantly broadened the scopeof IQA, moving toward comprehensive image quality understanding thatincorporates content analysis, degradation perception, and comparison reasoningbeyond mere numerical scoring. Previous MLLM-based methods typically eithergenerate numerical scores lacking interpretability or heavily rely onsupervised fine-tuning (SFT) using large-scale annotated datasets to providedescriptive assessments, limiting their flexibility and applicability. In thispaper, we propose Q-Insight, a reinforcement learning-based model built upongroup relative policy optimization (GRPO), which demonstrates strong visualreasoning capability for image quality understanding while requiring only alimited amount of rating scores and degradation labels. By jointly optimizingscore regression and degradation perception tasks with carefully designedreward functions, our approach effectively exploits their mutual benefits forenhanced performance. Extensive experiments demonstrate that Q-Insightsubstantially outperforms existing state-of-the-art methods in both scoreregression and degradation perception tasks, while exhibiting impressivezero-shot generalization to comparison reasoning tasks. Code will be availableat https://github.com/lwq20020127/Q-Insight.
  </details>

- **[Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions](http://arxiv.org/abs/2503.22678v1)**  `arXiv:2503.22678`  `cs.CL`  
  _Mohammad Almansoori, Komal Kumar, Hisham Cholakkal_
  <details open><summary>Abstract</summary>
  In this work, we introduce MedAgentSim, an open-source simulated clinicalenvironment with doctor, patient, and measurement agents designed to evaluateand enhance LLM performance in dynamic diagnostic settings. Unlike priorapproaches, our framework requires doctor agents to actively engage withpatients through multi-turn conversations, requesting relevant medicalexaminations (e.g., temperature, blood pressure, ECG) and imaging results(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnosticprocess. Additionally, we incorporate self improvement mechanisms that allowmodels to iteratively refine their diagnostic strategies. We enhance LLMperformance in our simulated setting by integrating multi-agent discussions,chain-of-thought reasoning, and experience-based knowledge retrieval,facilitating progressive learning as doctor agents interact with more patients.We also introduce an evaluation benchmark for assessing the LLM's ability toengage in dynamic, context-aware diagnostic interactions. While MedAgentSim isfully automated, it also supports a user-controlled mode, enabling humaninteraction with either the doctor or patient agent. Comprehensive evaluationsin various simulated diagnostic scenarios demonstrate the effectiveness of ourapproach. Our code, simulation tool, and benchmark are available at\href{https://medagentsim.netlify.app/}.
  </details>

- **[QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?](http://arxiv.org/abs/2503.22674v1)**  `arXiv:2503.22674`  `cs.CL` `cs.AI` `cs.LG`  
  _Belinda Z. Li, Been Kim, Zi Wang_
  <details open><summary>Abstract</summary>
  Recently, a large amount of work has focused on improving large languagemodels' (LLMs') performance on reasoning benchmarks such as math and logic.However, past work has largely assumed that tasks are well-defined. In the realworld, queries to LLMs are often underspecified, only solvable throughacquiring missing information. We formalize this as a constraint satisfactionproblem (CSP) with missing variable assignments. Using a special case of thisformalism where only one necessary variable assignment is missing, we canrigorously evaluate an LLM's ability to identify the minimal necessary questionto ask and quantify axes of difficulty levels for each problem. We presentQuestBench, a set of underspecified reasoning tasks solvable by asking at mostone question, which includes: (1) Logic-Q: Logical reasoning tasks with onemissing proposition, (2) Planning-Q: PDDL planning problems with initial statesthat are partially-observed, (3) GSM-Q: Human-annotated grade school mathproblems with one missing variable assignment, and (4) GSME-Q: a version ofGSM-Q where word problems are translated into equations by human annotators.The LLM is tasked with selecting the correct clarification question(s) from alist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, theiraccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates thatthe ability to solve well-specified reasoning problems may not be sufficientfor success on our benchmark: models have difficulty identifying the rightquestion to ask, even when they can solve the fully specified version of theproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, evenwhen explicitly presented with the option to predict ``not sure.'' Thishighlights the need for deeper investigation into models' informationacquisition capabilities.
  </details>

- **[Beyond Vanilla Fine-Tuning: Leveraging Multistage, Multilingual, and Domain-Specific Methods for Low-Resource Machine Translation](http://arxiv.org/abs/2503.22582v1)**  `arXiv:2503.22582`  `cs.CL`  
  _Sarubi Thillainathan, Songchen Yuan, En-Shiun Annie Lee, Sanath Jayasena, Surangika Ranathunga_
  <details open><summary>Abstract</summary>
  Fine-tuning multilingual sequence-to-sequence large language models (msLLMs)has shown promise in developing neural machine translation (NMT) systems forlow-resource languages (LRLs). However, conventional single-stage fine-tuningmethods struggle in extremely low-resource NMT settings, where training data isvery limited. This paper contributes to artificial intelligence by proposingtwo approaches for adapting msLLMs in these challenging scenarios: (1)continual pre-training (CPT), where the msLLM is further trained withdomain-specific monolingual data to compensate for the under-representation ofLRLs, and (2) intermediate task transfer learning (ITTL), a method thatfine-tunes the msLLM with both in-domain and out-of-domain parallel data toenhance its translation capabilities across various domains and tasks. As anapplication in engineering, these methods are implemented in NMT systems forSinhala, Tamil, and English (six language pairs) in domain-specific, extremelylow-resource settings (datasets containing fewer than 100,000 samples). Ourexperiments reveal that these approaches enhance translation performance by anaverage of +1.47 bilingual evaluation understudy (BLEU) score compared to thestandard single-stage fine-tuning baseline across all translation directions.Additionally, a multi-model ensemble further improves performance by anadditional BLEU score.
  </details>

- **[Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction in Transformers through Token Correlation](http://arxiv.org/abs/2503.22547v1)**  `arXiv:2503.22547`  `cs.CL` `cs.LG`  
  _Zhuo-Yang Song, Zeyu Li, Qing-Hong Cao, Ming-xing Luo, Hua Xing Zhu_
  <details open><summary>Abstract</summary>
  The geometric evolution of token representations in large language models(LLMs) presents a fundamental paradox: while human language inherentlyorganizes semantic information in low-dimensional spaces ($\sim 10^1$dimensions), modern LLMs employ high-dimensional embeddings ($\sim 10^3$dimensions) processed through Transformer architectures. To resolve thisparadox, this work bridges this conceptual gap by developing a geometricframework that tracks token dynamics across Transformers layers. Throughlayer-wise analysis of intrinsic dimensions across multiple architectures, wereveal an expansion-contraction pattern where tokens diffuse to a "workingspace" and then progressively project onto lower-dimensional submanifolds. Ourfinding implies a negative correlation between the working space dimension andparameter-sensitive performance of the LLMs, and indicates that effectivemodels tend to compress tokens into approximately 10-dimensional submanifolds,closely resembling human semantic spaces. This work not only advances LLMinterpretability by reframing Transformers layers as projectors that mediatebetween high-dimensional computation and low-dimensional semantics, but alsoprovides practical tools for model diagnostics that do not rely ontask-specific evaluations.
  </details>

- **[Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities](http://arxiv.org/abs/2503.22517v1)**  `arXiv:2503.22517`  `cs.CL` `cs.CV` `cs.AI`  
  _Raman Dutt, Harleen Hanspal, Guoxuan Xia, Petru-Daniel Tudosiu, Alexander Black, Yongxin Yang, et al._
  <details open><summary>Abstract</summary>
  In this work, we undertake the challenge of augmenting the existinggenerative capabilities of pre-trained text-only large language models (LLMs)with multi-modal generation capability while satisfying two core constraints:C1 preserving the preservation of original language generative capabilitieswith negligible performance degradation, and C2 adhering to a small parameterbudget to learn the new modality, ensuring scalability and efficiency. Incontrast to current approaches that add dedicated modules, therebysignificantly increasing the parameter count, we propose a method thatleverages the underutilized capacity inherent in deep models. Specifically, weexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a sourceof additional capacity for learning a new modality, enabling better parameterefficiency (C1). Moreover, we preserve the original language generationcapabilities by applying low-rank adaptation exclusively to the tokens of thenew modality (C2). Furthermore, we introduce a novel parameter initializationscheme based on the Gromov-Wasserstein distance to improve convergence andtraining stability. Through an extensive analysis of the routing mechanism, weuncover the emergence of modality-specific pathways and decreased redundancywithin the experts that can efficiently unlock multi-modal generativecapabilities. Overall, our method can be seamlessly applied to a wide range ofcontemporary LLMs, providing a new pathway for transitioning from uni-modal tomulti-modal architectures.
  </details>

- **[Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model](http://arxiv.org/abs/2503.22480v1)**  `arXiv:2503.22480`  `cs.LG`  
  _Wangtao Sun, Xiang Cheng, Xing Yu, Haotian Xu, Zhao Yang, Shizhu He, et al._
  <details open><summary>Abstract</summary>
  Reinforcement Learning from Human Feedback (RLHF) has emerged as a criticaltechnique for training large language models. However, reward hacking-aphenomenon where models exploit flaws in the reward model-remains a significantbarrier to achieving robust and scalable intelligence through long-termtraining. Existing studies have proposed uncertain reward model to addressreward hacking, however, they often lack systematic or theoretical foundations,failing to model the uncertainty intrinsically emerging from preference data.In this paper, we propose the Probabilistic Uncertain Reward Model (PURM), anatural generalization of the classical Bradley-Terry reward model. PURM learnsreward distributions directly from preference data and quantifies per-sampleuncertainty via the average overlap area between reward distributions. Tomitigate reward hacking, we further introduce an uncertainty-aware penalty intoProximal Policy Optimization (PPO), which leverages the learned uncertainty todynamically balance reward optimization and exploration. We propose alightweight and easy-to-use implementation of PURM. Experiments demonstratethat PURM significantly delays the onset of reward hacking while improvingfinal reward performance, outperforming baseline methods in both stability andeffectiveness.
  </details>

- **[WorkTeam: Constructing Workflows from Natural Language with Multi-Agents](http://arxiv.org/abs/2503.22473v1)**  `arXiv:2503.22473`  `cs.CL`  
  _Hanchao Liu, Rongjun Li, Weimin Xiong, Ziyu Zhou, Wei Peng_
  <details open><summary>Abstract</summary>
  Workflows play a crucial role in enhancing enterprise efficiency byorchestrating complex processes with multiple tools or components. However,hand-crafted workflow construction requires expert knowledge, presentingsignificant technical barriers. Recent advancements in Large Language Models(LLMs) have improved the generation of workflows from natural languageinstructions (aka NL2Workflow), yet existing single LLM agent-based methodsface performance degradation on complex tasks due to the need for specializedknowledge and the strain of task-switching. To tackle these challenges, wepropose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor,orchestrator, and filler agent, each with distinct roles that collaborativelyenhance the conversion process. As there are currently no publicly availableNL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, whichincludes 3,695 real-world business samples for training and evaluation.Experimental results show that our approach significantly increases the successrate of workflow construction, providing a novel and effective solution forenterprise NL2Workflow services.
  </details>

- **[Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey](http://arxiv.org/abs/2503.22458v1)**  `arXiv:2503.22458`  `cs.CL` `cs.AI`  
  _Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, Jian-guang Lou_
  <details open><summary>Abstract</summary>
  This survey examines evaluation methods for large language model (LLM)-basedagents in multi-turn conversational settings. Using a PRISMA-inspiredframework, we systematically reviewed nearly 250 scholarly sources, capturingthe state of the art from various venues of publication, and establishing asolid foundation for our analysis. Our study offers a structured approach bydeveloping two interrelated taxonomy systems: one that defines \emph{what toevaluate} and another that explains \emph{how to evaluate}. The first taxonomyidentifies key components of LLM-based agents for multi-turn conversations andtheir evaluation dimensions, including task completion, response quality, userexperience, memory and context retention, as well as planning and toolintegration. These components ensure that the performance of conversationalagents is assessed in a holistic and meaningful manner. The second taxonomysystem focuses on the evaluation methodologies. It categorizes approaches intoannotation-based evaluations, automated metrics, hybrid strategies that combinehuman assessments with quantitative measures, and self-judging methodsutilizing LLMs. This framework not only captures traditional metrics derivedfrom language understanding, such as BLEU and ROUGE scores, but alsoincorporates advanced techniques that reflect the dynamic, interactive natureof multi-turn dialogues.
  </details>

- **[Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning](http://arxiv.org/abs/2503.22456v1)**  `arXiv:2503.22456`  `cs.AI` `cs.LG`  
  _Abdullah Vanlioglu_
  <details open><summary>Abstract</summary>
  We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach thatenhances the exploration-exploitation tradeoff by dynamically assigning weightsto generated outputs based on their advantage and entropy for ReinforcementLearning-based Large Language Model fine-tuning. EGSW integrates entropyregularization with advantage-based weighting to balance policy updates,enabling efficient exploration in high-dimensional state spaces. By employingtemperature-scaled softmax weighting over sequences, EGSW prioritizinghigh-reward, high-uncertainty steps while maintaining training stability.Although originally developed to improve Group Relative Policy Optimization(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable toother reinforcement learning (RL) algorithms and can be implemented in bothstep-wise and trajectory-wise settings. Empirical evaluations demonstrate thatEGSW enhances GRPO reasoning ability, yielding improvements in sampleefficiency. Future work will explore the application of EGSW to advanced RLmethodologies.
  </details>

- **[STADE: Standard Deviation as a Pruning Metric](http://arxiv.org/abs/2503.22451v1)**  `arXiv:2503.22451`  `cs.LG`  
  _Diego Coello de Portugal Mecke, Haya Alyoussef, Ilia Koloiarov, Maximilian Stubbemann, Lars Schmidt-Thieme_
  <details open><summary>Abstract</summary>
  Recently, Large Language Models (LLMs) have become very widespread and areused to solve a wide variety of tasks. To successfully handle these tasks, LLMsrequire longer training times and larger model sizes. This makes LLMs idealcandidates for pruning methods that reduce computational demands whilemaintaining performance. Previous methods require a retraining phase afterpruning to maintain the original model's performance. However, state-of-the-artpruning methods, such as Wanda, prune the model without retraining, making thepruning process faster and more efficient. Building upon Wanda's work, thisstudy provides a theoretical explanation of why the method is effective andleverages these insights to enhance the pruning process. Specifically, atheoretical analysis of the pruning problem reveals a common scenario inMachine Learning where Wanda is the optimal pruning method. Furthermore, thisanalysis is extended to cases where Wanda is no longer optimal, leading to thedevelopment of a new method, STADE, based on the standard deviation of theinput. From a theoretical standpoint, STADE demonstrates better generalityacross different scenarios. Finally, extensive experiments on Llama and OpenPre-trained Transformers (OPT) models validate these theoretical findings,showing that depending on the training conditions, Wanda's optimal performancevaries as predicted by the theoretical framework. These insights contribute toa more robust understanding of pruning strategies and their practicalimplications. Code is available at: https://github.com/Coello-dev/STADE/
  </details>

- **[NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous Driving](http://arxiv.org/abs/2503.22436v1)**  `arXiv:2503.22436`  `cs.CV`  
  _Fuhao Li, Huan Jin, Bin Gao, Liaoyuan Fan, Lihui Jiang, Long Zeng_
  <details open><summary>Abstract</summary>
  Multi-view 3D visual grounding is critical for autonomous driving vehicles tointerpret natural languages and localize target objects in complexenvironments. However, existing datasets and methods suffer from coarse-grainedlanguage instructions, and inadequate integration of 3D geometric reasoningwith linguistic comprehension. To this end, we introduce NuGrounding, the firstlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.We present a Hierarchy of Grounding (HoG) method to construct NuGrounding togenerate hierarchical multi-level instructions, ensuring comprehensive coverageof human instruction patterns. To tackle this challenging dataset, we propose anovel paradigm that seamlessly combines instruction comprehension abilities ofmulti-modal LLMs (MLLMs) with precise localization abilities of specialistdetection models. Our approach introduces two decoupled task tokens and acontext query to aggregate 3D geometric information and semantic instructions,followed by a fusion decoder to refine spatial-semantic feature fusion forprecise localization. Extensive experiments demonstrate that our methodsignificantly outperforms the baselines adapted from representative 3D sceneunderstanding methods by a significant margin and achieves 0.59 in precisionand 0.64 in recall, with improvements of 50.8% and 54.7%.
  </details>

- **[Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis](http://arxiv.org/abs/2503.22420v1)**  `arXiv:2503.22420`  `cs.CV`  
  _Jiangyong Huang, Baoxiong Jia, Yan Wang, Ziyu Zhu, Xiongkun Linghu, Qing Li, et al._
  <details open><summary>Abstract</summary>
  Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VLmodels, creating a "mist" that obscures rigorous insights into modelcapabilities and 3D-VL tasks. This mist persists due to three key limitations.First, flawed test data, like ambiguous referential text in the grounding task,can yield incorrect and unreliable test results. Second, oversimplified metricssuch as simply averaging accuracy per question answering (QA) pair, cannotreveal true model capability due to their vulnerability to language variations.Third, existing benchmarks isolate the grounding and QA tasks, disregarding theunderlying coherence that QA should be based on solid grounding capabilities.To unveil the "mist", we propose Beacon3D, a benchmark for 3D-VL grounding andQA tasks, delivering a perspective shift in the evaluation of 3D-VLunderstanding. Beacon3D features (i) high-quality test data with precise andnatural language, (ii) object-centric evaluation with multiple tests per objectto ensure robustness, and (iii) a novel chain-of-analysis paradigm to addresslanguage robustness and model performance coherence across grounding and QA.Our evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)object-centric evaluation elicits true model performance and particularly weakgeneralization in QA; (ii) grounding-QA coherence remains fragile in current3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VLmodels, though as a prevalent practice, hinders grounding capabilities and hasyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysiscould benefit the 3D-VL community towards faithful developments.
  </details>

- **[Negation: A Pink Elephant in the Large Language Models' Room?](http://arxiv.org/abs/2503.22395v1)**  `arXiv:2503.22395`  `cs.CL`  
  _Tereza Vrabcov√°, Marek Kadlƒç√≠k, Petr Sojka, Michal ≈†tef√°nik, Michal Spiegel_
  <details open><summary>Abstract</summary>
  Negations are key to determining sentence meaning, making them essential forlogical reasoning. Despite their importance, negations pose a substantialchallenge for large language models (LLMs) and remain underexplored.  We construct two multilingual natural language inference (NLI) datasets with\textit{paired} examples differing in negation. We investigate how model sizeand language impact its ability to handle negation correctly by evaluatingpopular LLMs.  Contrary to previous work, we show that increasing the model sizeconsistently improves the models' ability to handle negations. Furthermore, wefind that both the models' reasoning accuracy and robustness to negation arelanguage-dependent and that the length and explicitness of the premise have agreater impact on robustness than language.  Our datasets can facilitate further research and improvements of languagemodel reasoning in multilingual settings.
  </details>

- **[Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors](http://arxiv.org/abs/2503.22388v1)**  `arXiv:2503.22388`  `cs.CL`  
  _Zhiyu Yang, Shuo Wang, Yukun Yan, Yang Deng_
  <details open><summary>Abstract</summary>
  LLMs are transforming software development, yet current code generation andcode repair benchmarks mainly assess syntactic and functional correctness insimple, single-error cases. LLMs' capabilities to autonomously find and fixruntime logical errors in complex data science code remain largely unexplored.To address this gap, we introduce DSDBench: the Data Science DebuggingBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hoperror tracing and multi-bug detection in data science code debugging. DSDBenchadapts datasets from existing data science task benchmarks, such as DABench andMatPlotBench, featuring realistic data science debugging tasks withautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes1,117 annotated samples with 741 cause-effect error pairs and runtime errormessages. Evaluations of state-of-the-art LLMs on DSDBench show significantperformance gaps, highlighting challenges in debugging logical runtime errorsin data science code. DSDBench offers a crucial resource to evaluate andimprove LLMs' debugging and reasoning capabilities, enabling more reliableAI-assisted data science in the future.DSDBench is publicly available athttps://github.com/KevinCL16/DSDBench.
  </details>

- **[ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch Recognition and Generation](http://arxiv.org/abs/2503.22374v1)**  `arXiv:2503.22374`  `cs.CV` `cs.AI`  
  _Giulio Federico, Giuseppe Amato, Fabio Carrara, Claudio Gennaro, Marco Di Benedetto_
  <details open><summary>Abstract</summary>
  Understanding the nature of human sketches is challenging because of the widevariation in how they are created. Recognizing complex structural patternsimproves both the accuracy in recognizing sketches and the fidelity of thegenerated sketches. In this work, we introduce ViSketch-GPT, a novel algorithmdesigned to address these challenges through a multi-scale context extractionapproach. The model captures intricate details at multiple scales and combinesthem using an ensemble-like mechanism, where the extracted features workcollaboratively to enhance the recognition and generation of key detailscrucial for classification and generation tasks.  The effectiveness of ViSketch-GPT is validated through extensive experimentson the QuickDraw dataset. Our model establishes a new benchmark, significantlyoutperforming existing methods in both classification and generation tasks,with substantial improvements in accuracy and the fidelity of generatedsketches.  The proposed algorithm offers a robust framework for understanding complexstructures by extracting features that collaborate to recognize intricatedetails, enhancing the understanding of structures like sketches and making ita versatile tool for various applications in computer vision and machinelearning.
  </details>

- **[Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs](http://arxiv.org/abs/2503.22362v1)**  `arXiv:2503.22362`  `cs.CL`  
  _Yuan He, Bailan He, Zifeng Ding, Alisia Lupidi, Yuqicheng Zhu, Shuo Chen, et al._
  <details open><summary>Abstract</summary>
  Understanding and mitigating hallucinations in Large Language Models (LLMs)is crucial for ensuring reliable content generation. While previous researchhas primarily focused on "when" LLMs hallucinate, our work explains "why" anddirectly links model behaviour to the pre-training data that forms their priorknowledge. Specifically, we demonstrate that an asymmetry exists in therecognition of logically equivalent facts, which can be attributed to frequencydiscrepancies of entities appearing as subjects versus objects. Given that mostpre-training datasets are inaccessible, we leverage the fully open-source OLMoseries by indexing its Dolma dataset to estimate entity frequencies. Usingrelational facts (represented as triples) from Wikidata5M, we construct probingdatasets to isolate this effect. Our experiments reveal that facts with ahigh-frequency subject and a low-frequency object are better recognised thantheir inverse, despite their logical equivalence. The pattern reverses inlow-to-high frequency settings, and no statistically significant asymmetryemerges when both entities are high-frequency. These findings highlight theinfluential role of pre-training data in shaping model predictions and provideinsights for inferring the characteristics of pre-training data in closed orpartially closed LLMs.
  </details>

- **[Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions](http://arxiv.org/abs/2503.22353v1)**  `arXiv:2503.22353`  `cs.CL` `cs.AI`  
  _Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have shown remarkable capabilities acrossvarious tasks, but their deployment in high-stake domains requires consistentperformance across multiple interaction rounds. This paper introduces acomprehensive framework for evaluating and improving LLM response consistency,making three key contributions. First, we propose a novel Position-WeightedConsistency (PWC) score that captures both the importance of early-stagestability and recovery patterns in multi-turn interactions. Second, we presenta carefully curated benchmark dataset spanning diverse domains and difficultylevels, specifically designed to evaluate LLM consistency under variouschallenging follow-up scenarios. Third, we introduce Confidence-Aware ResponseGeneration (CARG), a framework that significantly improves response stabilityby incorporating model confidence signals into the generation process.Empirical results demonstrate that CARG significantly improves responsestability without sacrificing accuracy, underscoring its potential for reliableLLM deployment in critical applications.
  </details>

- **[SKDU at De-Factify 4.0: Natural Language Features for AI-Generated Text-Detection](http://arxiv.org/abs/2503.22338v1)**  `arXiv:2503.22338`  `cs.CL`  
  _Shrikant Malviya, Pablo Arnau-Gonz√°lez, Miguel Arevalillo-Herr√°ez, Stamos Katsigiannis_
  <details open><summary>Abstract</summary>
  The rapid advancement of large language models (LLMs) has introduced newchallenges in distinguishing human-written text from AI-generated content. Inthis work, we explored a pipelined approach for AI-generated text detectionthat includes a feature extraction step (i.e. prompt-based rewriting featuresinspired by RAIDAR and content-based features derived from the NELA toolkit)followed by a classification module. Comprehensive experiments were conductedon the Defactify4.0 dataset, evaluating two tasks: binary classification todifferentiate human-written and AI-generated text, and multi-classclassification to identify the specific generative model used to generate theinput text. Our findings reveal that NELA features significantly outperformRAIDAR features in both tasks, demonstrating their ability to capture nuancedlinguistic, stylistic, and content-based differences. Combining RAIDAR and NELAfeatures provided minimal improvement, highlighting the redundancy introducedby less discriminative features. Among the classifiers tested, XGBoost emergedas the most effective, leveraging the rich feature sets to achieve highaccuracy and generalisation.
  </details>

- **[A Refined Analysis of Massive Activations in LLMs](http://arxiv.org/abs/2503.22329v1)**  `arXiv:2503.22329`  `cs.CL`  
  _Louis Owen, Nilabhra Roy Chowdhury, Abhay Kumar, Fabian G√ºra_
  <details open><summary>Abstract</summary>
  Motivated in part by their relevance for low-precision training andquantization, massive activations in large language models (LLMs) have recentlyemerged as a topic of interest. However, existing analyses are limited inscope, and generalizability across architectures is unclear. This paper helpsaddress some of these gaps by conducting an analysis of massive activationsacross a broad range of LLMs, including both GLU-based and non-GLU-basedarchitectures. Our findings challenge several prior assumptions, mostimportantly: (1) not all massive activations are detrimental, i.e. suppressingthem does not lead to an explosion of perplexity or a collapse in downstreamtask performance; (2) proposed mitigation strategies such as Attention KV biasare model-specific and ineffective in certain cases. We consequentlyinvestigate novel hybrid mitigation strategies; in particular pairing TargetVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)successfully balances the mitigation of massive activations with preserveddownstream model performance in the scenarios we investigated. Our code isavailable at: https://github.com/bluorion-com/refine_massive_activations.
  </details>

- **[MultiClaimNet: A Massively Multilingual Dataset of Fact-Checked Claim Clusters](http://arxiv.org/abs/2503.22280v1)**  `arXiv:2503.22280`  `cs.CL`  
  _Rrubaa Panchendrarajan, Rub√©n M√≠guez, Arkaitz Zubiaga_
  <details open><summary>Abstract</summary>
  In the context of fact-checking, claims are often repeated across variousplatforms and in different languages, which can benefit from a process thatreduces this redundancy. While retrieving previously fact-checked claims hasbeen investigated as a solution, the growing number of unverified claims andexpanding size of fact-checked databases calls for alternative, more efficientsolutions. A promising solution is to group claims that discuss the sameunderlying facts into clusters to improve claim retrieval and validation.However, research on claim clustering is hindered by the lack of suitabledatasets. To bridge this gap, we introduce \textit{MultiClaimNet}, a collectionof three multilingual claim cluster datasets containing claims in 86 languagesacross diverse topics. Claim clusters are formed automatically fromclaim-matching pairs with limited manual intervention. We leverage two existingclaim-matching datasets to form the smaller datasets within\textit{MultiClaimNet}. To build the larger dataset, we propose and validate anapproach involving retrieval of approximate nearest neighbors to form candidateclaim pairs and an automated annotation of claim similarity using largelanguage models. This larger dataset contains 85.3K fact-checked claims writtenin 78 languages. We further conduct extensive experiments using variousclustering techniques and sentence embedding models to establish baselineperformance. Our datasets and findings provide a strong foundation for scalableclaim clustering, contributing to efficient fact-checking pipelines.
  </details>

- **[Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback](http://arxiv.org/abs/2503.22230v1)**  `arXiv:2503.22230`  `cs.LG`  
  _Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, et al._
  <details open><summary>Abstract</summary>
  Reinforcement Learning from Human Feedback (RLHF) is crucial for aligninglarge language models with human preferences. While recent research has focusedon algorithmic improvements, the importance of prompt-data construction hasbeen overlooked. This paper addresses this gap by exploring data-drivenbottlenecks in RLHF performance scaling, particularly reward hacking anddecreasing response diversity. We introduce a hybrid reward system combiningreasoning task verifiers (RTV) and a generative reward model (GenRM) tomitigate reward hacking. We also propose a novel prompt-selection method,Pre-PPO, to maintain response diversity and enhance learning effectiveness.Additionally, we find that prioritizing mathematical and coding tasks early inRLHF training significantly improves performance. Experiments across two modelsizes validate our methods' effectiveness and scalability. Results show thatRTV is most resistant to reward hacking, followed by GenRM with ground truth,and then GenRM with SFT Best-of-N responses. Our strategies enable rapidcapture of subtle task-specific distinctions, leading to substantialimprovements in overall RLHF performance. This work highlights the importanceof careful data construction and provides practical methods to overcomeperformance barriers in RLHF.
  </details>

- **[Learning to Instruct for Visual Instruction Tuning](http://arxiv.org/abs/2503.22215v1)**  `arXiv:2503.22215`  `cs.CL` `cs.CV` `cs.AI` `cs.LG`  
  _Zhihan Zhou, Feng Hong, Jiaan Luo, Jiangchao Yao, Dongsheng Li, Bo Han, et al._
  <details open><summary>Abstract</summary>
  We propose LIT, an advancement of visual instruction tuning (VIT). While VITequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, thecurrent design choices for VIT often result in overfitting and shortcutlearning, potentially degrading performance. This gap arises from anoveremphasis on instruction-following abilities, while neglecting the proactiveunderstanding of visual information. Inspired by this, LIT adopts a simple yeteffective approach by incorporating the loss function into both the instructionand response sequences. It seamlessly expands the training data, andregularizes the MLLMs from overly relying on language priors. Based on thismerit, LIT achieves a significant relative improvement of up to 9% oncomprehensive multimodal benchmarks, requiring no additional training data andincurring negligible computational overhead. Surprisingly, LIT attainsexceptional fundamental visual capabilities, yielding up to an 18% improvementin captioning performance, while simultaneously alleviating hallucination inMLLMs.
  </details>

- **[EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge Devices](http://arxiv.org/abs/2503.22196v1)**  `arXiv:2503.22196`  `cs.CL`  
  _Jiyu Chen, Shuang Peng, Daxiong Luo, Fan Yang, Renshou Wu, Fangyuan Li, et al._
  <details open><summary>Abstract</summary>
  Transformer-based large language models (LLMs) encounter challenges inprocessing long sequences on edge devices due to the quadratic complexity ofattention mechanisms and growing memory demands from Key-Value (KV) cache.Existing KV cache optimizations struggle with irreversible token eviction inlong-output tasks, while alternative sequence modeling architectures provecostly to adopt within established Transformer infrastructure. We presentEdgeInfinite, a memory-efficient solution for infinite contexts that integratescompressed memory into Transformer-based LLMs through a trainable memory-gatingmodule. This approach maintains full compatibility with standard Transformerarchitectures, requiring fine-tuning only a small part of parameters, andenables selective activation of the memory-gating module for long and shortcontext task routing. The experimental result shows that EdgeInfinite achievescomparable performance to baseline Transformer-based LLM on long contextbenchmarks while optimizing memory consumption and time to first token.
  </details>

- **[Reasoning of Large Language Models over Knowledge Graphs with Super-Relations](http://arxiv.org/abs/2503.22166v1)**  `arXiv:2503.22166`  `cs.LG`  
  _Song Wang, Junhong Lin, Xiaojie Guo, Julian Shun, Jundong Li, Yada Zhu_
  <details open><summary>Abstract</summary>
  While large language models (LLMs) have made significant progress inprocessing and reasoning over knowledge graphs, current methods suffer from ahigh non-retrieval rate. This limitation reduces the accuracy of answeringquestions based on these graphs. Our analysis reveals that the combination ofgreedy search and forward reasoning is a major contributor to this issue. Toovercome these challenges, we introduce the concept of super-relations, whichenables both forward and backward reasoning by summarizing and connectingvarious relational paths within the graph. This holistic approach not onlyexpands the search space, but also significantly improves retrieval efficiency.In this paper, we propose the ReKnoS framework, which aims to Reason overKnowledge Graphs with Super-Relations. Our framework's key advantages includethe inclusion of multiple relation paths through super-relations, enhancedforward and backward reasoning capabilities, and increased efficiency inquerying LLMs. These enhancements collectively lead to a substantialimprovement in the successful retrieval rate and overall reasoning performance.We conduct extensive experiments on nine real-world datasets to evaluateReKnoS, and the results demonstrate the superior performance of ReKnoS overexisting state-of-the-art baselines, with an average accuracy gain of 2.92%.
  </details>

- **[Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models](http://arxiv.org/abs/2503.22165v1)**  `arXiv:2503.22165`  `cs.LG`  
  _Zhanke Zhou, Zhaocheng Zhu, Xuan Li, Mikhail Galkin, Xiao Feng, Sanmi Koyejo, et al._
  <details open><summary>Abstract</summary>
  Numerous applications of large language models (LLMs) rely on their abilityto perform step-by-step reasoning. However, the reasoning behavior of LLMsremains poorly understood, posing challenges to research, development, andsafety. To address this gap, we introduce landscape of thoughts-the firstvisualization tool for users to inspect the reasoning paths of chain-of-thoughtand its derivatives on any multi-choice dataset. Specifically, we represent thestates in a reasoning path as feature vectors that quantify their distances toall answer choices. These features are then visualized in two-dimensional plotsusing t-SNE. Qualitative and quantitative analysis with the landscape ofthoughts effectively distinguishes between strong and weak models, correct andincorrect answers, as well as different reasoning tasks. It also uncoversundesirable reasoning patterns, such as low consistency and high uncertainty.Additionally, users can adapt our tool to a model that predicts the propertythey observe. We showcase this advantage by adapting our tool to a lightweightverifier that evaluates the correctness of reasoning paths. The code ispublicly available at: https://github.com/tmlr-group/landscape-of-thoughts.
  </details>

- **[EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos](http://arxiv.org/abs/2503.22152v1)**  `arXiv:2503.22152`  `cs.CV` `cs.AI`  
  _Yuxuan Li, Vijay Veerabadran, Michael L. Iuzzolino, Brett D. Roads, Asli Celikyilmaz, Karl Ridgeway_
  <details open><summary>Abstract</summary>
  We introduce EgoToM, a new video question-answering benchmark that extendsTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToMmodel, we generate multi-choice video QA instances for the Ego4D dataset tobenchmark the ability to predict a camera wearer's goals, beliefs, and nextactions. We study the performance of both humans and state of the artmultimodal large language models (MLLMs) on these three interconnectedinference problems. Our evaluation shows that MLLMs achieve close tohuman-level accuracy on inferring goals from egocentric videos. However, MLLMs(including the largest ones we tested with over 100B parameters) fall short ofhuman performance when inferring the camera wearers' in-the-moment beliefstates and future actions that are most consistent with the unseen videofuture. We believe that our results will shape the future design of animportant class of egocentric digital assistants which are equipped with areasonable model of the user's internal mental states.
  </details>

- **[FRASE: Structured Representations for Generalizable SPARQL Query Generation](http://arxiv.org/abs/2503.22144v1)**  `arXiv:2503.22144`  `cs.CL` `cs.AI`  
  _Papa Abdou Karim Karou Diallo, Amal Zouaq_
  <details open><summary>Abstract</summary>
  Translating natural language questions into SPARQL queries enables KnowledgeBase querying for factual and up-to-date responses. However, existing datasetsfor this task are predominantly template-based, leading models to learnsuperficial mappings between question and query templates rather thandeveloping true generalization capabilities. As a result, models struggle whenencountering naturally phrased, template-free questions. This paper introducesFRASE (FRAme-based Semantic Enhancement), a novel approach that leverages FrameSemantic Role Labeling (FSRL) to address this limitation. We also presentLC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question isenriched using FRASE through frame detection and the mapping of frame-elementsto their argument. We evaluate the impact of this approach through extensiveexperiments on recent large language models (LLMs) under different fine-tuningconfigurations. Our results demonstrate that integrating frame-based structuredrepresentations consistently improves SPARQL generation performance,particularly in challenging generalization scenarios when test questionsfeature unseen templates (unknown template splits) and when they are allnaturally phrased (reformulated questions).
  </details>

- **[Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF](http://arxiv.org/abs/2503.22137v1)**  `arXiv:2503.22137`  `cs.AI` `cs.LG`  
  _Syrine Belakaria, Joshua Kazdan, Charles Marx, Chris Cundy, Willie Neiswanger, Sanmi Koyejo, et al._
  <details open><summary>Abstract</summary>
  Reinforcement learning from human feedback (RLHF) has become a cornerstone ofthe training and alignment pipeline for large language models (LLMs). Recentadvances, such as direct preference optimization (DPO), have simplified thepreference learning step. However, collecting preference data remains achallenging and costly process, often requiring expert annotation. This costcan be mitigated by carefully selecting the data points presented forannotation. In this work, we propose an active learning approach to efficientlyselect prompt and preference pairs using a risk assessment strategy based onthe Sharpe Ratio. To address the challenge of unknown preferences prior toannotation, our method evaluates the gradients of all potential preferenceannotations to assess their impact on model updates. These gradient-basedevaluations enable risk assessment of data points regardless of the annotationoutcome. By leveraging the DPO loss derivations, we derive a closed-formexpression for computing these Sharpe ratios on a per-tuple basis, ensuring ourapproach remains both tractable and computationally efficient. We alsointroduce two variants of our method, each making different assumptions aboutprior information. Experimental results demonstrate that our method outperformsthe baseline by up to 5% in win rates against the chosen completion withlimited human preference data across several language models and real-worlddatasets.
  </details>

- **[Few-Shot Graph Out-of-Distribution Detection with LLMs](http://arxiv.org/abs/2503.22097v1)**  `arXiv:2503.22097`  `cs.CL` `cs.LG`  
  _Haoyan Xu, Zhengtao Yao, Yushun Dong, Ziyi Wang, Ryan A. Rossi, Mengyuan Li, et al._
  <details open><summary>Abstract</summary>
  Existing methods for graph out-of-distribution (OOD) detection typicallydepend on training graph neural network (GNN) classifiers using a substantialamount of labeled in-distribution (ID) data. However, acquiring high-qualitylabeled nodes in text-attributed graphs (TAGs) is challenging and costly due totheir complex textual and structural characteristics. Large language models(LLMs), known for their powerful zero-shot capabilities in textual tasks, showpromise but struggle to naturally capture the critical structural informationinherent to TAGs, limiting their direct effectiveness.  To address these challenges, we propose LLM-GOOD, a general framework thateffectively combines the strengths of LLMs and GNNs to enhance data efficiencyin graph OOD detection. Specifically, we first leverage LLMs' strong zero-shotcapabilities to filter out likely OOD nodes, significantly reducing the humanannotation burden. To minimize the usage and cost of the LLM, we employ it onlyto annotate a small subset of unlabeled nodes. We then train a lightweight GNNfilter using these noisy labels, enabling efficient predictions of ID statusfor all other unlabeled nodes by leveraging both textual and structuralinformation. After obtaining node embeddings from the GNN filter, we can applyinformativeness-based methods to select the most valuable nodes for precisehuman annotation. Finally, we train the target ID classifier using theseaccurately annotated ID nodes. Extensive experiments on four real-world TAGdatasets demonstrate that LLM-GOOD significantly reduces human annotation costsand outperforms state-of-the-art baselines in terms of both ID classificationaccuracy and OOD detection performance.
  </details>

- **[How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark](http://arxiv.org/abs/2503.22093v1)**  `arXiv:2503.22093`  `cs.CV` `cs.AI`  
  _Ximing Wen, Mallika Mainali, Anik Sen_
  <details open><summary>Abstract</summary>
  Vision Language Models (VLMs) have demonstrated strong reasoning capabilitiesin Visual Question Answering (VQA) tasks; However, their ability to performTheory of Mind (ToM) tasks such as accurately inferring human intentions,beliefs, and other mental states remains underexplored. In this work, wepropose an open-ended question framework to comprehensively evaluate VLMs'performance across diverse categories of ToM tasks. We curated and annotated abenchmark dataset composed of 30 images. We then assessed the performance offour VLMs of varying sizes on this dataset. Our experimental results show thatthe GPT-4 model outperformed all others, with only one smaller model,GPT-4o-mini, achieving comparable performance. Additionally, we observed thatVLMs often struggle to accurately infer intentions in complex scenarios such asbullying or cheating. Moreover, our findings also reveal that smaller modelscan sometimes infer correct intentions despite relying on incorrect visualcues.
  </details>

- **[Leveraging LLMs for Predicting Unknown Diagnoses from Clinical Notes](http://arxiv.org/abs/2503.22092v1)**  `arXiv:2503.22092`  `cs.CL`  
  _Dina Albassam, Adam Cross, Chengxiang Zhai_
  <details open><summary>Abstract</summary>
  Electronic Health Records (EHRs) often lack explicit links betweenmedications and diagnoses, making clinical decision-making and research moredifficult. Even when links exist, diagnosis lists may be incomplete, especiallyduring early patient visits. Discharge summaries tend to provide more completeinformation, which can help infer accurate diagnoses, especially with the helpof large language models (LLMs). This study investigates whether LLMs canpredict implicitly mentioned diagnoses from clinical notes and link them tocorresponding medications. We address two research questions: (1) Does majorityvoting across diverse LLM configurations outperform the best singleconfiguration in diagnosis prediction? (2) How sensitive is majority votingaccuracy to LLM hyperparameters such as temperature, top-p, and summary length?To evaluate, we created a new dataset of 240 expert-annotatedmedication-diagnosis pairs from 20 MIMIC-IV notes. Using GPT-3.5 Turbo, we ran18 prompting configurations across short and long summary lengths, generating8568 test cases. Results show that majority voting achieved 75 percentaccuracy, outperforming the best single configuration at 66 percent. No singlehyperparameter setting dominated, but combining deterministic, balanced, andexploratory strategies improved performance. Shorter summaries generally led tohigher accuracy.In conclusion, ensemble-style majority voting with diverse LLMconfigurations improves diagnosis prediction in EHRs and offers a promisingmethod to link medications and diagnoses in clinical texts.
  </details>

- **[Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A General Framework for Domain-Specific Large Language Model Adaptation](http://arxiv.org/abs/2503.22074v1)**  `arXiv:2503.22074`  `cs.CL` `cs.AI`  
  _Chuan-Wei Kuo, Siyu Chen, Chenqi Yan, Yu Yang Fredrik Liu_
  <details open><summary>Abstract</summary>
  Large language models (LLMs) hold great promise for specialized scientificdomains such as materials science, yet adapting them efficiently and accuratelyto domain-specific knowledge remains challenging due to limited data and highknowledge density. We propose a two-stage framework that combines structuredmodel compression with a scientific fine-tuning regimen to address thischallenge. In the compression stage, we decompose the LLM's weight matricesinto local low-rank "rank blocks" and arrange these blocks in a Penrose-likenon-periodic tiling pattern. Each block is then compacted via spectraltransformations (e.g., discrete cosine or Fourier transforms), and aKullback-Leibler (KL) divergence-based alignment loss preserves thedistributional similarity between the compressed model's representations andthose of the original full model. In the adaptation stage, the compressed modelis further tuned using a human-like scientific reading protocol: it processestechnical materials science documents section by section, engaging in astructured question-and-answer routine for each section. This section-wise Q&Afine-tuning strategy extracts explicit reasoning traces and gradually injectsdomain knowledge, while minimizing catastrophic forgetting of the model'sgeneral language capabilities. By balancing efficient compression with targetedadaptation, our two-stage approach enables precise specialization of LLMs tohigh-value domains under data-scarce conditions. We present this principled yetexploratory pipeline and outline its potential for advancing materials scienceknowledge integration, laying the groundwork for comprehensive empiricalevaluation in future work.
  </details>

- **[Arch-LLM: Taming LLMs for Neural Architecture Generation via Unsupervised Discrete Representation Learning](http://arxiv.org/abs/2503.22063v1)**  `arXiv:2503.22063`  `cs.LG`  
  _Deshani Geethika Poddenige, Sachith Seneviratne, Damith Senanayake, Mahesan Niranjan, PN Suganthan, Saman Halgamuge_
  <details open><summary>Abstract</summary>
  Unsupervised representation learning has been widely explored across variousmodalities, including neural architectures, where it plays a key role indownstream applications like Neural Architecture Search (NAS). These methodstypically learn an unsupervised representation space before generating/sampling architectures for the downstream search. A common approach involvesthe use of Variational Autoencoders (VAEs) to map discrete architectures onto acontinuous representation space, however, sampling from these spaces oftenleads to a high percentage of invalid or duplicate neural architectures. Thiscould be due to the unnatural mapping of inherently discrete architecturalspace onto a continuous space, which emphasizes the need for a robust discreterepresentation of these architectures. To address this, we introduce a VectorQuantized Variational Autoencoder (VQ-VAE) to learn a discrete latent spacemore naturally aligned with the discrete neural architectures. In contrast toVAEs, VQ-VAEs (i) map each architecture into a discrete code sequence and (ii)allow the prior to be learned by any generative model rather than assuming anormal distribution. We then represent these architecture latent codes asnumerical sequences and train a text-to-text model leveraging a Large LanguageModel to learn and generate sequences representing architectures. We experimentour method with Inception/ ResNet-like cell-based search spaces, namelyNAS-Bench-101 and NAS-Bench-201. Compared to VAE-based methods, our approachimproves the generation of valid and unique architectures by over 80% onNASBench-101 and over 8% on NASBench-201. Finally, we demonstrate theapplicability of our method in NAS employing a sequence-modeling-based NASalgorithm.
  </details>

- **[OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs](http://arxiv.org/abs/2503.21480v2)**  `arXiv:2503.21480`  `cs.CL`  
  _John Murzaku, Owen Rambow_
  <details open><summary>Abstract</summary>
  The use of omni-LLMs (large language models that accept any modality asinput), particularly for multimodal cognitive state tasks involving speech, isunderstudied. We present OmniVox, the first systematic evaluation of fouromni-LLMs on the zero-shot emotion recognition task. We evaluate on two widelyused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shotomni-LLMs outperform or are competitive with fine-tuned audio models. Alongsideour audio-only evaluation, we also evaluate omni-LLMs on text only and text andaudio. We present acoustic prompting, an audio-specific prompting strategy foromni-LLMs which focuses on acoustic feature analysis, conversation contextanalysis, and step-by-step reasoning. We compare our acoustic prompting tominimal prompting and full chain-of-thought prompting techniques. We perform acontext window analysis on IEMOCAP and MELD, and find that using context helps,especially on IEMOCAP. We conclude with an error analysis on the generatedacoustic reasoning outputs from the omni-LLMs.
  </details>

- **[Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In & Out Learning](http://arxiv.org/abs/2503.21419v2)**  `arXiv:2503.21419`  `cs.AI`  
  _Yupei Li, Manuel Milling, Bj√∂rn W. Schuller_
  <details open><summary>Abstract</summary>
  Artificial Intelligence (AI) has achieved new levels of performance andspread in public usage with the rise of deep neural networks (DNNs). Initiallyinspired by human neurons and their connections, NNs have become the foundationof AI models for many advanced architectures. However, some of the mostintegral processes in the human brain, particularly neurogenesis andneuroplasticity in addition to the more spread neuroapoptosis have largely beenignored in DNN architecture design. Instead, contemporary AI developmentpredominantly focuses on constructing advanced frameworks, such as largelanguage models, which retain a static structure of neural connections duringtraining and inference. In this light, we explore how neurogenesis,neuroapoptosis, and neuroplasticity can inspire future AI advances.Specifically, we examine analogous activities in artificial NNs, introducingthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' andstructural pruning for neuroapoptosis. We additionally suggest neuroplasticitycombining the two for future large NNs in ``life-long learning'' settingsfollowing the biological inspiration. We conclude by advocating for greaterresearch efforts in this interdisciplinary domain and identifying promisingdirections for future exploration.
  </details>

- **[EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues](http://arxiv.org/abs/2503.21080v2)**  `arXiv:2503.21080`  `cs.CL`  
  _Yuhan Liu, Yunbo Long_
  <details open><summary>Abstract</summary>
  While large language model (LLM)-based chatbots have been applied foreffective engagement in credit dialogues, their capacity for dynamic emotionalexpression remains limited. Current agents primarily rely on passive empathyrather than affective reasoning. For instance, when faced with persistentclient negativity, the agent should employ strategic emotional adaptation byexpressing measured anger to discourage counterproductive behavior and guidethe conversation toward resolution. This context-aware emotional modulation isessential for imitating the nuanced decision-making of human negotiators. Thispaper introduces an EQ-negotiator that combines emotion sensing frompre-trained language models (PLMs) with emotional reasoning based on GameTheory and Hidden Markov Models. It takes into account both the current andhistorical emotions of the client to better manage and address negativeemotions during interactions. By fine-tuning pre-trained language models (PLMs)on public emotion datasets and validating them on the credit dialogue datasets,our approach enables LLM-based agents to effectively capture shifts in clientemotions and dynamically adjust their response tone based on our emotiondecision policies in real-world financial negotiations. This EQ-negotiator canalso help credit agencies foster positive client relationships, enhancingsatisfaction in credit services.
  </details>

- **[Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields](http://arxiv.org/abs/2503.20776v2)**  `arXiv:2503.20776`  `cs.CV`  
  _Shijie Zhou, Hui Ren, Yijia Weng, Shuwang Zhang, Zhen Wang, Dejia Xu, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in 2D and multimodal models have achieved remarkablesuccess by leveraging large-scale training on extensive datasets. However,extending these achievements to enable free-form interactions and high-levelsemantic operations with complex 3D/4D scenes remains challenging. Thisdifficulty stems from the limited availability of large-scale, annotated 3D/4Dor multi-view datasets, which are crucial for generalizable vision and languagetasks such as open-vocabulary and prompt-based segmentation, language-guidedediting, and visual question answering (VQA). In this paper, we introduceFeature4X, a universal framework designed to extend any functionality from 2Dvision foundation model into the 4D realm, using only monocular video input,which is widely available from user-generated content. The "X" in Feature4Xrepresents its versatility, enabling any task through adaptable,model-conditioned 4D feature field distillation. At the core of our frameworkis a dynamic optimization strategy that unifies multiple model capabilitiesinto a single representation. Additionally, to the best of our knowledge,Feature4X is the first method to distill and lift the features of videofoundation models (e.g., SAM2, InternVideo2) into an explicit 4D feature fieldusing Gaussian Splatting. Our experiments showcase novel view segment anything,geometric and appearance scene editing, and free-form VQA across all timesteps, empowered by LLMs in feedback loops. These advancements broaden thescope of agentic AI applications by providing a foundation for scalable,contextually and spatiotemporally aware systems capable of immersive dynamic 4Dscene interaction.
  </details>

- **[DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts](http://arxiv.org/abs/2503.19498v2)**  `arXiv:2503.19498`  `cs.CL`  
  _Ling Zhong, Yujing Lu, Jing Yang, Weiming Li, Peng Wei, Yongheng Wang, et al._
  <details open><summary>Abstract</summary>
  Chart Question Answering (CQA) benchmarks are essential for evaluating thecapability of Multimodal Large Language Models (MLLMs) to interpret visualdata. However, current benchmarks focus primarily on the evaluation ofgeneral-purpose CQA but fail to adequately capture domain-specific challenges.We introduce DomainCQA, a systematic methodology for constructingdomain-specific CQA benchmarks, and demonstrate its effectiveness by developingAstroChart, a CQA benchmark in the field of astronomy. Our evaluation showsthat chart reasoning and combining chart information with domain knowledge fordeeper analysis and summarization, rather than domain-specific knowledge, posethe primary challenge for existing MLLMs, highlighting a critical gap incurrent benchmarks. By providing a scalable and rigorous framework, DomainCQAenables more precise assessment and improvement of MLLMs for domain-specificapplications.
  </details>

- **[Overtrained Language Models Are Harder to Fine-Tune](http://arxiv.org/abs/2503.19206v2)**  `arXiv:2503.19206`  `cs.CL` `cs.AI`  
  _Jacob Mitchell Springer, Sachin Goyal, Kaiyue Wen, Tanishq Kumar, Xiang Yue, Sadhika Malladi, et al._
  <details open><summary>Abstract</summary>
  Large language models are pre-trained on ever-growing token budgets under theassumption that better pre-training performance translates to improveddownstream models. In this work, we challenge this assumption and show thatextended pre-training can make models harder to fine-tune, leading to degradedfinal performance. We term this phenomenon catastrophic overtraining. Forexample, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads toover 2% worse performance on multiple standard LLM benchmarks than its 2.3Ttoken counterpart. Through controlled experiments and theoretical analysis, weshow that catastrophic overtraining arises from a systematic increase in thebroad sensitivity of pre-trained parameters to modifications, including but notlimited to fine-tuning. Our findings call for a critical reassessment ofpre-training design that considers the downstream adaptability of the model.
  </details>

- **[Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2503.18352v2)**  `arXiv:2503.18352`  `cs.CV`  
  _Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, Di Huang_
  <details open><summary>Abstract</summary>
  In this paper, we present Diffusion-4K, a novel framework for directultra-high-resolution image synthesis using text-to-image diffusion models. Thecore advancements include: (1) Aesthetic-4K Benchmark: addressing the absenceof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,a comprehensive benchmark for ultra-high-resolution image generation. Wecurated a high-quality 4K dataset with carefully selected images and captionsgenerated by GPT-4o. Additionally, we introduce GLCM Score and CompressionRatio metrics to evaluate fine details, combined with holistic measures such asFID, Aesthetics and CLIPScore for a comprehensive assessment ofultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose awavelet-based fine-tuning approach for direct training with photorealistic 4Kimages, applicable to various latent diffusion models, demonstrating itseffectiveness in synthesizing highly detailed 4K images. Consequently,Diffusion-4K achieves impressive performance in high-quality image synthesisand text prompt adherence, especially when powered by modern large-scalediffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental resultsfrom our benchmark demonstrate the superiority of Diffusion-4K inultra-high-resolution image synthesis.
  </details>

- **[Sun-Shine: A Large Language Model for Tibetan Culture](http://arxiv.org/abs/2503.18288v2)**  `arXiv:2503.18288`  `cs.CL`  
  _Cheng Huang, Fan Gao, Nyima Tashi, Yutong Liu, Xiangxiang Wang, Thupten Tsering, et al._
  <details open><summary>Abstract</summary>
  Tibetan, a minority language in China, features a highly intricategrammatical structure, characterized by four verb tenses and a tense systemwith frequent irregularities, contributing to its extensive inflectionaldiversity. Recently, advances in Large Language Models (LLMs) have transformedthe paradigm in many domains. Despite the success in other fields, current LLMsoften fall short in catering to the needs of domain experts like Tibetans, andthe potential of LLMs for Tibetan culture is under-explored. The intrinsicreasons are the immense and intricate nature of Tibetan culture as well as thenecessity for higher granularity and richness in knowledge. Simultaneously, thecomplexity and uniqueness of its grammatical structure, coupled with its statusas a minority ethnic language, contribute to data scarcity, which remains afundamental challenge. To alleviate these issues, we introduce Llama-Sunshine(Sun-Shine), the first large language model for Tibetan culture, which isexpert in various Tibetan language processing tasks. Sun-Shine incorporatesstate-of-the-art model architectures optimized for Tibetan's linguisticfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverseTibetan texts such as literature, religious scripts, news, and conversationaldata, which is also the first large-scale dataset for Tibetan culture. Thoughcomprehensive experiments, Sun-Shine not only demonstrates a higher level ofknowledge expertise for Tibetan culture but also gains preliminary embodiedintelligence capabilities in Tibetan language processing tasks, like languagemodeling, text classification, machine translation, and syntactic analysis.Moreover, it excels in low-resource scenarios, showcasing strong generalizationcapabilities.
  </details>

- **[Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering](http://arxiv.org/abs/2503.18172v2)**  `arXiv:2503.18172`  `cs.CL` `cs.AI`  
  _Zixin Chen, Sicheng Song, Kashun Shum, Yanna Lin, Rui Sheng, Huamin Qu_
  <details open><summary>Abstract</summary>
  Misleading chart visualizations, which intentionally manipulate datarepresentations to support specific claims, can distort perceptions and lead toincorrect conclusions. Despite decades of research, misleading visualizationsremain a widespread and pressing issue. Recent advances in multimodal largelanguage models (MLLMs) have demonstrated strong chart comprehensioncapabilities, yet no existing work has systematically evaluated their abilityto detect and interpret misleading charts. This paper introduces the MisleadingChart Question Answering (Misleading ChartQA) Benchmark, a large-scalemultimodal dataset designed to assess MLLMs in identifying and reasoning aboutmisleading charts. It contains over 3,000 curated examples, covering 21 typesof misleaders and 10 chart types. Each example includes standardized chartcode, CSV data, and multiple-choice questions with labeled explanations,validated through multi-round MLLM checks and exhausted expert human review. Webenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitationsin identifying visually deceptive practices. We also propose a novel pipelinethat detects and localizes misleaders, enhancing MLLMs' accuracy in misleadingchart interpretation. Our work establishes a foundation for advancingMLLM-driven misleading chart comprehension. We publicly release the sampledataset to support further research in this critical area.
  </details>

- **[Can Language Models Follow Multiple Turns of Entangled Instructions?](http://arxiv.org/abs/2503.13222v2)**  `arXiv:2503.13222`  `cs.CL` `cs.AI`  
  _Chi Han_
  <details open><summary>Abstract</summary>
  Despite significant achievements in improving the instruction-followingcapabilities of large language models (LLMs), the ability to process multiplepotentially entangled or conflicting instructions remains a considerablechallenge. Real-world scenarios often require consistency across multipleinstructions over time, such as secret privacy, personal preferences, andprioritization, which demand sophisticated abilities to integrate multipleturns and carefully balance competing objectives when instructions intersect orconflict. This work presents a systematic investigation of LLMs' capabilitiesin handling multiple turns of instructions, covering three levels ofdifficulty: (1) retrieving information from instructions, (2) tracking andreasoning across turns, and (3) resolving conflicts among instructions. Weconstruct MultiTurnInstruct with around 1.1K high-quality multi-turnconversations through the human-in-the-loop approach and result in ninecapability categories, including statics and dynamics, reasoning, andmultitasking. Our finding reveals an intriguing trade-off between differentcapabilities. While GPT models demonstrate superior memorization, they showreduced effectiveness in privacy-protection tasks requiring selectiveinformation withholding. Larger models exhibit stronger reasoning capabilitiesbut still struggle with resolving conflicting instructions. Importantly, theseperformance gaps cannot be attributed solely to information loss, as modelsdemonstrate strong BLEU scores on memorization tasks but their attentionmechanisms fail to integrate multiple related instructions effectively. Thesefindings highlight critical areas for improvement in complex real-world tasksinvolving multi-turn instructions.
  </details>

- **[Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical Investigation](http://arxiv.org/abs/2503.12854v2)**  `arXiv:2503.12854`  `cs.CL`  
  _Songjun Tu, Jiahao Lin, Xiangyu Tian, Qichao Zhang, Linjing Li, Yuqian Fu, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in post-training methodologies for large language models(LLMs) have highlighted reinforcement learning (RL) as a critical component forenhancing reasoning. However, the substantial computational costs associatedwith RL-based approaches have led to growing interest in alternative paradigms,such as Direct Preference Optimization (DPO). In this study, we investigate theeffectiveness of DPO in facilitating self-improvement for LLMs throughiterative preference-based learning. We demonstrate that a single round of DPOwith coarse filtering significantly enhances mathematical reasoningperformance, particularly for strong base model. Furthermore, we design aniterative enhancement framework for both the generator and the reward model(RM), enabling their mutual improvement through online interaction acrossmultiple rounds of DPO. Finally, with simple verifiable rewards, our modelDPO-VP achieves RL-level performance with significantly lower computationaloverhead. These findings highlight DPO as a scalable and cost-effectivealternative to RL, offering a practical solution for enhancing LLM reasoning inresource-constrained situations.
  </details>

- **[StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through Event-Gated Cognition](http://arxiv.org/abs/2503.06220v2)**  `arXiv:2503.06220`  `cs.CV` `cs.LG`  
  _Xin Ding, Hao Wu, Yifan Yang, Shiqi Jiang, Donglin Bai, Zhibo Chen, et al._
  <details open><summary>Abstract</summary>
  With the rise of real-world human-AI interaction applications, such as AIassistants, the need for Streaming Video Dialogue is critical. To address thisneed, we introduce StreamMind, a video LLM framework that achieves ultra-FPSstreaming video processing (100 fps on a single A100) and enables proactive,always-on responses in real time, without explicit user intervention.  To solve the key challenge of the contradiction between linear videostreaming speed and quadratic transformer computation cost, we propose a novelperception-cognition interleaving paradigm named ''event-gated LLMinvocation'', in contrast to the existing per-time-step LLM invocation. Byintroducing a Cognition Gate network between the video encoder and the LLM, LLMis only invoked when relevant events occur. To realize the event featureextraction with constant cost, we propose Event-Preserving Feature Extractor(EPFE) based on state-space method, generating a single perception token forspatiotemporal features. These techniques enable the video LLM with full-FPSperception and real-time cognition response.  Experiments on Ego4D and SoccerNet streaming tasks, as well as standardoffline benchmarks, demonstrate state-of-the-art performance in both modelcapability and real-time efficiency, paving the way for ultra-high-FPSapplications, such as Game AI and interactive media. The code and data isavailable at https://aka.ms/StreamMind.
  </details>

- **[Retrieval Backward Attention without Additional Training: Enhance Embeddings of Large Language Models via Repetition](http://arxiv.org/abs/2502.20726v2)**  `arXiv:2502.20726`  `cs.CL` `cs.LG`  
  _Yifei Duan, Raphael Shang, Deng Liang, Yongqiang Cai_
  <details open><summary>Abstract</summary>
  Language models can be viewed as functions that embed text into Euclideanspace, where the quality of the embedding vectors directly determines modelperformance, training such neural networks involves various uncertainties. Thispaper focuses on improving the performance of pre-trained language models inzero-shot settings through a simple and easily implementable method. We proposea novel backward attention mechanism to enhance contextual informationencoding. Evaluated on the Chinese Massive Text Embedding Benchmark (C-MTEB),our approach achieves significant improvements across multiple tasks, providingvaluable insights for advancing zero-shot learning capabilities.
  </details>

- **[Foot-In-The-Door: A Multi-turn Jailbreak for LLMs](http://arxiv.org/abs/2502.19820v3)**  `arXiv:2502.19820`  `cs.CL` `cs.AI`  
  _Zixuan Weng, Xiaolong Jin, Jinyuan Jia, Xiangyu Zhang_
  <details open><summary>Abstract</summary>
  Ensuring AI safety is crucial as large language models become increasinglyintegrated into real-world applications. A key challenge is jailbreak, whereadversarial prompts bypass built-in safeguards to elicit harmful disallowedoutputs. Inspired by psychological foot-in-the-door principles, we introduceFITD,a novel multi-turn jailbreak method that leverages the phenomenon whereminor initial commitments lower resistance to more significant or moreunethical transgressions. Our approach progressively escalates the maliciousintent of user queries through intermediate bridge prompts and aligns themodel's response by itself to induce toxic responses. Extensive experimentalresults on two jailbreak benchmarks demonstrate that FITD achieves an averageattack success rate of 94% across seven widely used models, outperformingexisting state-of-the-art methods. Additionally, we provide an in-depthanalysis of LLM self-corruption, highlighting vulnerabilities in currentalignment strategies and emphasizing the risks inherent in multi-turninteractions. The code is available athttps://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak.
  </details>

- **[SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines](http://arxiv.org/abs/2502.14739v4)**  `arXiv:2502.14739`  `cs.CL`  
  _M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) have demonstrated remarkable proficiency inmainstream academic disciplines such as mathematics, physics, and computerscience. However, human knowledge encompasses over 200 specialized disciplines,far exceeding the scope of existing benchmarks. The capabilities of LLMs inmany of these specialized fields-particularly in light industry, agriculture,and service-oriented disciplines-remain inadequately evaluated. To address thisgap, we present SuperGPQA, a comprehensive benchmark that evaluatesgraduate-level knowledge and reasoning capabilities across 285 disciplines. Ourbenchmark employs a novel Human-LLM collaborative filtering mechanism toeliminate trivial or ambiguous questions through iterative refinement based onboth LLM responses and expert feedback. Our experimental results revealsignificant room for improvement in the performance of current state-of-the-artLLMs across diverse knowledge domains (e.g., the reasoning-focused modelDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlightingthe considerable gap between current model capabilities and artificial generalintelligence. Additionally, we present comprehensive insights from ourmanagement of a large-scale annotation process, involving over 80 expertannotators and an interactive Human-LLM collaborative system, offering valuablemethodological guidance for future research initiatives of comparable scope.
  </details>

- **[Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance](http://arxiv.org/abs/2502.08127v2)**  `arXiv:2502.08127`  `cs.CL`  
  _Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Han Yi, Jimin Huang, et al._
  <details open><summary>Abstract</summary>
  While large language models (LLMs) have shown strong general reasoningcapabilities, their effectiveness in financial reasoning, which is crucial forreal-world financial applications remains underexplored. In this study, weconduct a comprehensive evaluation of 24 state-of-the-art general andreasoning-focused LLMs across four complex financial reasoning tasks involvingfinancial text, tabular data, and equations. We assess key capabilities such asnumerical reasoning, tabular interpretation, financial terminologycomprehension, long-context understanding, and equation-based problem solving.Our analysis reveals that while data quality and pretraining contribute toperformance, general techniques like chain-of-thought (CoT) fine-tuning offerlimited gains in financial tasks. To address this, we propose twodomain-adapted models, Fino1-8B and Fino1-14B, trained with CoT fine-tuning andreinforcement learning using domain-specific reasoning paths. Our models aretrained on a carefully curated dataset integrating high-quality examples fromdiverse sources, covering financial reports, tables, equations, and structuredXBRL texts. Despite limited training data, they achieve an 7-9% performanceimprovement, outperforming several advanced LLMs, including GPT-o1,GPT-o3-mini, GPT-4.5, and comparable with DeepSeek models (V3 and R1),demonstrating strong practical value in resource, constrained scenarios. Ourfindings highlight the need for domain-specific adaptations in financialreasoning, and we release all datasets, models, and code for future research.
  </details>

- **[Visual Agentic AI for Spatial Reasoning with a Dynamic API](http://arxiv.org/abs/2502.06787v2)**  `arXiv:2502.06787`  `cs.CV`  
  _Damiano Marsili, Rohun Agrawal, Yisong Yue, Georgia Gkioxari_
  <details open><summary>Abstract</summary>
  Visual reasoning -- the ability to interpret the visual world -- is crucialfor embodied agents that operate within three-dimensional scenes. Progress inAI has led to vision and language models capable of answering questions fromimages. However, their performance declines when tasked with 3D spatialreasoning. To tackle the complexity of such reasoning problems, we introduce anagentic program synthesis approach where LLM agents collaboratively generate aPythonic API with new functions to solve common subproblems. Our methodovercomes limitations of prior approaches that rely on a static, human-definedAPI, allowing it to handle a wider range of queries. To assess AI capabilitiesfor 3D understanding, we introduce a new benchmark of queries involvingmultiple steps of grounding and inference. We show that our method outperformsprior zero-shot models for visual reasoning in 3D and empirically validate theeffectiveness of our agentic framework for 3D spatial reasoning tasks. Projectwebsite: https://glab-caltech.github.io/vadar/
  </details>

- **[Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based Planning and Manoeuvrability-Driven Controls](http://arxiv.org/abs/2412.06931v3)**  `arXiv:2412.06931`  `cs.RO`  
  _Hoi-Yin Lee, Peng Zhou, Anqing Duan, Wanyu Ma, Chenguang Yang, David Navarro-Alarcon_
  <details open><summary>Abstract</summary>
  The ability to wield tools was once considered exclusive to humanintelligence, but it's now known that many other animals, like crows, possessthis capability. Yet, robotic systems still fall short of matching biologicaldexterity. In this paper, we investigate the use of Large Language Models(LLMs), tool affordances, and object manoeuvrability for non-prehensiletool-based manipulation tasks. Our novel method leverages LLMs based on sceneinformation and natural language instructions to enable symbolic task planningfor tool-object manipulation. This approach allows the system to convert thehuman language sentence into a sequence of feasible motion functions. We havedeveloped a novel manoeuvrability-driven controller using a new tool affordancemodel derived from visual feedback. This controller helps guide the robot'stool utilization and manipulation actions, even within confined areas, using astepping incremental approach. The proposed methodology is evaluated withexperiments to prove its effectiveness under various manipulation scenarios.
  </details>

- **[RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for Boosting 2-bit Large Language Model Accuracy](http://arxiv.org/abs/2412.01129v3)**  `arXiv:2412.01129`  `cs.AI` `cs.LG`  
  _Geonho Lee, Janghwan Lee, Sukjin Hong, Minsoo Kim, Euijai Ahn, Du-Seong Chang, et al._
  <details open><summary>Abstract</summary>
  Low-rank adaptation (LoRA) has become the dominant method forparameter-efficient LLM fine-tuning, with LoRA-based quantization errorcompensation (LQEC) emerging as a powerful tool for recovering accuracy incompressed LLMs. However, LQEC has underperformed in sub-4-bit scenarios, withno prior investigation into understanding this limitation. We propose RILQ(Rank-Insensitive LoRA-based Quantization Error Compensation) to understandfundamental limitation and boost 2-bit LLM accuracy. Based on rank analysisrevealing model-wise activation discrepancy loss's rank-insensitive nature,RILQ employs this loss to adjust adapters cooperatively across layers, enablingrobust error compensation with low-rank adapters. Evaluations on LLaMA-2 andLLaMA-3 demonstrate RILQ's consistent improvements in 2-bit quantized inferenceacross various state-of-the-art quantizers and enhanced accuracy intask-specific fine-tuning. RILQ maintains computational efficiency comparableto existing LoRA methods, enabling adapter-merged weight-quantized LLMinference with significantly enhanced accuracy, making it a promising approachfor boosting 2-bit LLM performance. Our code is available athttps://github.com/aiha-lab/RILQ.
  </details>

- **[DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models](http://arxiv.org/abs/2411.15024v3)**  `arXiv:2411.15024`  `cs.CV` `cs.LG`  
  _Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang_
  <details open><summary>Abstract</summary>
  Video large language models (VLLMs) have significantly advanced recently inprocessing complex video content, yet their inference efficiency remainsconstrained because of the high computational cost stemming from the thousandsof visual tokens generated from the video inputs. We empirically observe that,unlike single image inputs, VLLMs typically attend visual tokens from differentframes at different decoding iterations, making a one-shot pruning strategyprone to removing important tokens by mistake. Motivated by this, we presentDyCoke, a training-free token compression method to optimize tokenrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-playtemporal compression module to minimize temporal redundancy by mergingredundant tokens across frames, and applies dynamic KV cache reduction to prunespatially redundant tokens selectively. It ensures high-quality inference bydynamically retaining the critical tokens at each decoding step. Extensiveexperimental results demonstrate that DyCoke can outperform the prior SoTAcounterparts, achieving 1.5X inference speedup, 1.4X memory reduction againstthe baseline VLLM, while still improving the performance, with no training.
  </details>

- **[Do LLMs estimate uncertainty well in instruction-following?](http://arxiv.org/abs/2410.14582v4)**  `arXiv:2410.14582`  `cs.CL` `cs.AI`  
  _Juyeon Heo, Miao Xiong, Christina Heinze-Deml, Jaya Narain_
  <details open><summary>Abstract</summary>
  Large language models (LLMs) could be valuable personal AI agents acrossvarious domains, provided they can precisely follow user instructions. However,recent studies have shown significant limitations in LLMs'instruction-following capabilities, raising concerns about their reliability inhigh-stakes applications. Accurately estimating LLMs' uncertainty in adheringto instructions is critical to mitigating deployment risks. We present, to ourknowledge, the first systematic evaluation of the uncertainty estimationabilities of LLMs in the context of instruction-following. Our study identifieskey challenges with existing instruction-following benchmarks, where multiplefactors are entangled with uncertainty stems from instruction-following,complicating the isolation and comparison across methods and models. To addressthese issues, we introduce a controlled evaluation setup with two benchmarkversions of data, enabling a comprehensive comparison of uncertainty estimationmethods under various conditions. Our findings show that existing uncertaintymethods struggle, particularly when models make subtle errors in instructionfollowing. While internal model states provide some improvement, they remaininadequate in more complex scenarios. The insights from our controlledevaluation setups provide a crucial understanding of LLMs' limitations andpotential for uncertainty estimation in instruction-following tasks, paving theway for more trustworthy AI agents.
  </details>

- **[Do LLMs "know" internally when they follow instructions?](http://arxiv.org/abs/2410.14516v5)**  `arXiv:2410.14516`  `cs.CL` `cs.AI`  
  _Juyeon Heo, Christina Heinze-Deml, Oussama Elachqar, Kwan Ho Ryan Chan, Shirley Ren, Udhay Nallasamy, et al._
  <details open><summary>Abstract</summary>
  Instruction-following is crucial for building AI agents with large languagemodels (LLMs), as these models must adhere strictly to user-providedconstraints and guidelines. However, LLMs often fail to follow even simple andclear instructions. To improve instruction-following behavior and preventundesirable outputs, a deeper understanding of how LLMs' internal states relateto these outcomes is required. In this work, we investigate whether LLMs encodeinformation in their representations that correlate with instruction-followingsuccess - a property we term knowing internally. Our analysis identifies adirection in the input embedding space, termed the instruction-followingdimension, that predicts whether a response will comply with a giveninstruction. We find that this dimension generalizes well across unseen tasksbut not across unseen instruction types. We demonstrate that modifyingrepresentations along this dimension improves instruction-following successrates compared to random changes, without compromising response quality.Further investigation reveals that this dimension is more closely related tothe phrasing of prompts rather than the inherent difficulty of the task orinstructions. This work provides insight into the internal workings of LLMs'instruction-following, paving the way for reliable LLM agents.
  </details>

- **[AnyAttack: Towards Large-scale Self-supervised Adversarial Attacks on Vision-language Models](http://arxiv.org/abs/2410.05346v3)**  `arXiv:2410.05346`  `cs.AI` `cs.LG`  
  _Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Yunhao Chen, et al._
  <details open><summary>Abstract</summary>
  Due to their multimodal capabilities, Vision-Language Models (VLMs) havefound numerous impactful applications in real-world scenarios. However, recentstudies have revealed that VLMs are vulnerable to image-based adversarialattacks. Traditional targeted adversarial attacks require specific targets andlabels, limiting their real-world impact.We present AnyAttack, aself-supervised framework that transcends the limitations of conventionalattacks through a novel foundation model approach. By pre-training on themassive LAION-400M dataset without label supervision, AnyAttack achievesunprecedented flexibility - enabling any image to be transformed into an attackvector targeting any desired output across different VLMs.This approachfundamentally changes the threat landscape, making adversarial capabilitiesaccessible at an unprecedented scale. Our extensive validation across fiveopen-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) demonstratesAnyAttack's effectiveness across diverse multimodal tasks. Most concerning,AnyAttack seamlessly transfers to commercial systems including Google Gemini,Claude Sonnet, Microsoft Copilot and OpenAI GPT, revealing a systemicvulnerability requiring immediate attention.
  </details>

- **[Output Scouting: Auditing Large Language Models for Catastrophic Responses](http://arxiv.org/abs/2410.05305v2)**  `arXiv:2410.05305`  `cs.CL` `cs.AI`  
  _Andrew Bell, Joao Fonseca_
  <details open><summary>Abstract</summary>
  Recent high profile incidents in which the use of Large Language Models(LLMs) resulted in significant harm to individuals have brought about a growinginterest in AI safety. One reason LLM safety issues occur is that models oftenhave at least some non-zero probability of producing harmful outputs. In thiswork, we explore the following scenario: imagine an AI safety auditor issearching for catastrophic responses from an LLM (e.g. a "yes" responses to"can I fire an employee for being pregnant?"), and is able to query the model alimited number times (e.g. 1000 times). What is a strategy for querying themodel that would efficiently find those failure responses? To this end, wepropose output scouting: an approach that aims to generate semantically fluentoutputs to a given prompt matching any target probability distribution. We thenrun experiments using two LLMs and find numerous examples of catastrophicresponses. We conclude with a discussion that includes advice for practitionerswho are looking to implement LLM auditing for catastrophic responses. We alsorelease an open-source toolkit (https://github.com/joaopfonseca/outputscouting)that implements our auditing framework using the Hugging Face transformerslibrary.
  </details>

- **[Frame-Voyager: Learning to Query Frames for Video Large Language Models](http://arxiv.org/abs/2410.03226v4)**  `arXiv:2410.03226`  `cs.CL` `cs.CV`  
  _Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, et al._
  <details open><summary>Abstract</summary>
  Video Large Language Models (Video-LLMs) have made remarkable progress invideo understanding tasks. However, they are constrained by the maximum lengthof input tokens, making it impractical to input entire videos. Existing frameselection approaches, such as uniform frame sampling and text-frame retrieval,fail to account for the information density variations in the videos or thecomplex instructions in the tasks, leading to sub-optimal performance. In thispaper, we propose Frame-Voyager that learns to query informative framecombinations, based on the given textual queries in the task. To trainFrame-Voyager, we introduce a new data collection and labeling pipeline, byranking frame combinations using a pre-trained Video-LLM. Given a video of Mframes, we traverse its T-frame combinations, feed them into a Video-LLM, andrank them based on Video-LLM's prediction losses. Using this ranking assupervision, we train Frame-Voyager to query the frame combinations with lowerlosses. In experiments, we evaluate Frame-Voyager on four Video QuestionAnswering benchmarks by plugging it into two different Video-LLMs. Theexperimental results demonstrate that Frame-Voyager achieves impressive resultsin all settings, highlighting its potential as a plug-and-play solution forVideo-LLMs.
  </details>

- **[Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models](http://arxiv.org/abs/2405.12523v3)**  `arXiv:2405.12523`  `cs.CV` `cs.AI`  
  _Jiaqi Li, Qianshan Wei, Chuanyi Zhang, Guilin Qi, Miaozeng Du, Yongrui Chen, et al._
  <details open><summary>Abstract</summary>
  Machine unlearning empowers individuals with the `right to be forgotten' byremoving their private or sensitive information encoded in machine learningmodels. However, it remains uncertain whether MU can be effectively applied toMultimodal Large Language Models (MLLMs), particularly in scenarios offorgetting the leaked visual data of concepts. To overcome the challenge, wepropose an efficient method, Single Image Unlearning (SIU), to unlearn thevisual recognition of a concept by fine-tuning a single associated image forfew steps. SIU consists of two key aspects: (i) Constructing Multifacetedfine-tuning data. We introduce four targets, based on which we constructfine-tuning data for the concepts to be forgotten; (ii) Jointly training loss.To synchronously forget the visual recognition of concepts and preserve theutility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergenceLoss combined with Cross Entropy loss. Alongside our method, we establishMMUBench, a new benchmark for MU in MLLMs and introduce a collection of metricsfor its evaluation. Experimental results on MMUBench show that SIU completelysurpasses the performance of existing methods. Furthermore, we surprisinglyfind that SIU can avoid invasive membership inference attacks and jailbreakattacks. To the best of our knowledge, we are the first to explore MU in MLLMs.We will release the code and benchmark in the near future.
  </details>

- **[A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](http://arxiv.org/abs/2402.07877v3)**  `arXiv:2402.07877`  `cs.AI`  
  _Yangxinyu Xie, Bowen Jiang, Tanwi Mallick, Joshua David Bergerson, John K. Hutchison, Duane R. Verner, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) are a transformational capability at thefrontier of artificial intelligence and machine learning that can supportdecision-makers in addressing pressing societal challenges such as extremenatural hazard events. As generalized models, LLMs often struggle to providecontext-specific information, particularly in areas requiring specializedknowledge. In this work, we propose a Retrieval-Augmented Generation(RAG)-based multi-agent LLM system to support analysis and decision-making inthe context of natural hazards and extreme weather events. As a proof ofconcept, we present WildfireGPT, a specialized system focused on wildfirescenarios. The architecture employs a user-centered, multi-agent design todeliver tailored risk insights across diverse stakeholder groups. Byintegrating domain-specific projection data, observational datasets, andscientific literature through a RAG framework, the system ensures both accuracyand contextual relevance of the information it provides. Evaluation across tenexpert-led case studies demonstrates that WildfireGPT significantly outperformsexisting LLM-based solutions for decision support in natural hazard and extremeweather contexts.
  </details>

- **[Self-Rewarding Language Models](http://arxiv.org/abs/2401.10020v3)**  `arXiv:2401.10020`  `cs.CL` `cs.AI`  
  _Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, et al._
  <details open><summary>Abstract</summary>
  We posit that to achieve superhuman agents, future models require superhumanfeedback in order to provide an adequate training signal. Current approachescommonly train reward models from human preferences, which may then bebottlenecked by human performance level, and secondly these separate frozenreward models cannot then learn to improve during LLM training. In this work,we study Self-Rewarding Language Models, where the language model itself isused via LLM-as-a-Judge prompting to provide its own rewards during training.We show that during Iterative DPO training that not only does instructionfollowing ability improve, but also the ability to provide high-quality rewardsto itself. Fine-tuning Llama 2 70B on three iterations of our approach yields amodel that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left stillto explore, this work opens the door to the possibility of models that cancontinually improve in both axes.
  </details>
