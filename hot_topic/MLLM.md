# üîç MLLM Papers ¬∑ 2025-03-28

[![Total Papers](https://img.shields.io/badge/Papers-20-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `VLM` `MLM` `MLLM` `Multi-Modal Language Model` `Multimodal Language Model` `Vision Language Model`  
**Filter**: `None`

---

## üìö Paper List

- **[Q-Insight: Understanding Image Quality via Visual Reinforcement Learning](http://arxiv.org/abs/2503.22679v1)**  `arXiv:2503.22679`  `cs.CV`  
  _Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, et al._
  <details open><summary>Abstract</summary>
  Image quality assessment (IQA) focuses on the perceptual visual quality ofimages, playing a crucial role in downstream tasks such as imagereconstruction, compression, and generation. The rapid advancement ofmulti-modal large language models (MLLMs) has significantly broadened the scopeof IQA, moving toward comprehensive image quality understanding thatincorporates content analysis, degradation perception, and comparison reasoningbeyond mere numerical scoring. Previous MLLM-based methods typically eithergenerate numerical scores lacking interpretability or heavily rely onsupervised fine-tuning (SFT) using large-scale annotated datasets to providedescriptive assessments, limiting their flexibility and applicability. In thispaper, we propose Q-Insight, a reinforcement learning-based model built upongroup relative policy optimization (GRPO), which demonstrates strong visualreasoning capability for image quality understanding while requiring only alimited amount of rating scores and degradation labels. By jointly optimizingscore regression and degradation perception tasks with carefully designedreward functions, our approach effectively exploits their mutual benefits forenhanced performance. Extensive experiments demonstrate that Q-Insightsubstantially outperforms existing state-of-the-art methods in both scoreregression and degradation perception tasks, while exhibiting impressivezero-shot generalization to comparison reasoning tasks. Code will be availableat https://github.com/lwq20020127/Q-Insight.
  </details>

- **[Understanding Co-speech Gestures in-the-wild](http://arxiv.org/abs/2503.22668v1)**  `arXiv:2503.22668`  `cs.CV`  
  _Sindhu B Hegde, K R Prajwal, Taein Kwon, Andrew Zisserman_
  <details open><summary>Abstract</summary>
  Co-speech gestures play a vital role in non-verbal communication. In thispaper, we introduce a new framework for co-speech gesture understanding in thewild. Specifically, we propose three new tasks and benchmarks to evaluate amodel's capability to comprehend gesture-text-speech associations: (i)gesture-based retrieval, (ii) gestured word spotting, and (iii) active speakerdetection using gestures. We present a new approach that learns a tri-modalspeech-text-video-gesture representation to solve these tasks. By leveraging acombination of global phrase contrastive loss and local gesture-word couplingloss, we demonstrate that a strong gesture representation can be learned in aweakly supervised manner from videos in the wild. Our learned representationsoutperform previous methods, including large vision-language models (VLMs),across all three tasks. Further analysis reveals that speech and textmodalities capture distinct gesture-related signals, underscoring theadvantages of learning a shared tri-modal embedding space. The dataset, model,and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal
  </details>

- **[Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization](http://arxiv.org/abs/2503.22577v1)**  `arXiv:2503.22577`  `cs.CV` `cs.AI`  
  _I√±igo Pikabea, I√±aki Lacunza, Oriol Pareras, Carlos Escolano, Aitor Gonzalez-Agirre, Javier Hernando, et al._
  <details open><summary>Abstract</summary>
  Rapid advancements in Visual Language Models (VLMs) have transformedmultimodal understanding but are often constrained by generating Englishresponses regardless of the input language. This phenomenon has been termed asImage-induced Fidelity Loss (IFL) and stems from limited multimodalmultilingual training data. To address this, we propose a continuousmultilingual integration strategy that injects text-only multilingual dataduring visual instruction tuning, preserving the language model's originalmultilingual capabilities. Extensive evaluations demonstrate that our approachsignificantly improves linguistic fidelity across languages without degradationin visual performance. We also explore model merging, which improves languagefidelity but comes at the cost of visual performance. In contrast, our coremethod achieves robust multilingual alignment without trade-offs, offering ascalable and effective path to mitigating IFL for global VLM adoption.
  </details>

- **[NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous Driving](http://arxiv.org/abs/2503.22436v1)**  `arXiv:2503.22436`  `cs.CV`  
  _Fuhao Li, Huan Jin, Bin Gao, Liaoyuan Fan, Lihui Jiang, Long Zeng_
  <details open><summary>Abstract</summary>
  Multi-view 3D visual grounding is critical for autonomous driving vehicles tointerpret natural languages and localize target objects in complexenvironments. However, existing datasets and methods suffer from coarse-grainedlanguage instructions, and inadequate integration of 3D geometric reasoningwith linguistic comprehension. To this end, we introduce NuGrounding, the firstlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.We present a Hierarchy of Grounding (HoG) method to construct NuGrounding togenerate hierarchical multi-level instructions, ensuring comprehensive coverageof human instruction patterns. To tackle this challenging dataset, we propose anovel paradigm that seamlessly combines instruction comprehension abilities ofmulti-modal LLMs (MLLMs) with precise localization abilities of specialistdetection models. Our approach introduces two decoupled task tokens and acontext query to aggregate 3D geometric information and semantic instructions,followed by a fusion decoder to refine spatial-semantic feature fusion forprecise localization. Extensive experiments demonstrate that our methodsignificantly outperforms the baselines adapted from representative 3D sceneunderstanding methods by a significant margin and achieves 0.59 in precisionand 0.64 in recall, with improvements of 50.8% and 54.7%.
  </details>

- **[VisTa: Visual-contextual and Text-augmented Zero-shot Object-level OOD Detection](http://arxiv.org/abs/2503.22291v1)**  `arXiv:2503.22291`  `cs.CV`  
  _Bin Zhang, Xiaoyang Qu, Guokuan Li, Jiguang Wan, Jianzong Wang_
  <details open><summary>Abstract</summary>
  As object detectors are increasingly deployed as black-box cloud services orpre-trained models with restricted access to the original training data, thechallenge of zero-shot object-level out-of-distribution (OOD) detection arises.This task becomes crucial in ensuring the reliability of detectors inopen-world settings. While existing methods have demonstrated success inimage-level OOD detection using pre-trained vision-language models like CLIP,directly applying such models to object-level OOD detection presents challengesdue to the loss of contextual information and reliance on image-levelalignment. To tackle these challenges, we introduce a new method that leveragesvisual prompts and text-augmented in-distribution (ID) space construction toadapt CLIP for zero-shot object-level OOD detection. Our method preservescritical contextual information and improves the ability to differentiatebetween ID and OOD objects, achieving competitive performance across differentbenchmarks.
  </details>

- **[FLIP: Towards Comprehensive and Reliable Evaluation of Federated Prompt Learning](http://arxiv.org/abs/2503.22263v1)**  `arXiv:2503.22263`  `cs.CV` `cs.LG`  
  _Dongping Liao, Xitong Gao, Yabo Xu, Chengzhong Xu_
  <details open><summary>Abstract</summary>
  The increasing emphasis on privacy and data security has driven the adoptionof federated learning, a decentralized approach to train machine learningmodels without sharing raw data. Prompt learning, which fine-tunes promptembeddings of pretrained models, offers significant advantages in federatedsettings by reducing computational costs and communication overheads whileleveraging the strong performance and generalization capabilities ofvision-language models such as CLIP. This paper addresses the intersection offederated learning and prompt learning, particularly for vision-languagemodels. In this work, we introduce a comprehensive framework, named FLIP, toevaluate federated prompt learning algorithms. FLIP assesses the performance of8 state-of-the-art federated prompt learning methods across 4 federatedlearning protocols and 12 open datasets, considering 6 distinct evaluationscenarios. Our findings demonstrate that prompt learning maintains stronggeneralization performance in both in-distribution and out-of-distributionsettings with minimal resource consumption. This work highlights theeffectiveness of federated prompt learning in environments characterized bydata scarcity, unseen classes, and cross-domain distributional shifts. Weopen-source the code for all implemented algorithms in FLIP to facilitatefurther research in this domain.
  </details>

- **[Learning to Instruct for Visual Instruction Tuning](http://arxiv.org/abs/2503.22215v1)**  `arXiv:2503.22215`  `cs.CL` `cs.CV` `cs.AI` `cs.LG`  
  _Zhihan Zhou, Feng Hong, Jiaan Luo, Jiangchao Yao, Dongsheng Li, Bo Han, et al._
  <details open><summary>Abstract</summary>
  We propose LIT, an advancement of visual instruction tuning (VIT). While VITequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, thecurrent design choices for VIT often result in overfitting and shortcutlearning, potentially degrading performance. This gap arises from anoveremphasis on instruction-following abilities, while neglecting the proactiveunderstanding of visual information. Inspired by this, LIT adopts a simple yeteffective approach by incorporating the loss function into both the instructionand response sequences. It seamlessly expands the training data, andregularizes the MLLMs from overly relying on language priors. Based on thismerit, LIT achieves a significant relative improvement of up to 9% oncomprehensive multimodal benchmarks, requiring no additional training data andincurring negligible computational overhead. Surprisingly, LIT attainsexceptional fundamental visual capabilities, yielding up to an 18% improvementin captioning performance, while simultaneously alleviating hallucination inMLLMs.
  </details>

- **[Multi-modal Knowledge Distillation-based Human Trajectory Forecasting](http://arxiv.org/abs/2503.22201v1)**  `arXiv:2503.22201`  `cs.CV`  
  _Jaewoo Jeong, Seohee Lee, Daehee Park, Giwon Lee, Kuk-Jin Yoon_
  <details open><summary>Abstract</summary>
  Pedestrian trajectory forecasting is crucial in various applications such asautonomous driving and mobile robot navigation. In such applications,camera-based perception enables the extraction of additional modalities (humanpose, text) to enhance prediction accuracy. Indeed, we find that textualdescriptions play a crucial role in integrating additional modalities into aunified understanding. However, online extraction of text requires the use ofVLM, which may not be feasible for resource-constrained systems. To addressthis challenge, we propose a multi-modal knowledge distillation framework: astudent model with limited modality is distilled from a teacher model trainedwith full range of modalities. The comprehensive knowledge of a teacher modeltrained with trajectory, human pose, and text is distilled into a student modelusing only trajectory or human pose as a sole supplement. In doing so, weseparately distill the core locomotion insights from intra-agent multi-modalityand inter-agent interaction. Our generalizable framework is validated with twostate-of-the-art models across three datasets on both ego-view (JRDB, SIT) andBEV-view (ETH/UCY) setups, utilizing both annotated and VLM-generated textcaptions. Distilled student models show consistent improvement in allprediction metrics for both full and instantaneous observations, improving upto ~13%. The code is available at https://github.com/Jaewoo97/KDTF.
  </details>

- **[EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos](http://arxiv.org/abs/2503.22152v1)**  `arXiv:2503.22152`  `cs.CV` `cs.AI`  
  _Yuxuan Li, Vijay Veerabadran, Michael L. Iuzzolino, Brett D. Roads, Asli Celikyilmaz, Karl Ridgeway_
  <details open><summary>Abstract</summary>
  We introduce EgoToM, a new video question-answering benchmark that extendsTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToMmodel, we generate multi-choice video QA instances for the Ego4D dataset tobenchmark the ability to predict a camera wearer's goals, beliefs, and nextactions. We study the performance of both humans and state of the artmultimodal large language models (MLLMs) on these three interconnectedinference problems. Our evaluation shows that MLLMs achieve close tohuman-level accuracy on inferring goals from egocentric videos. However, MLLMs(including the largest ones we tested with over 100B parameters) fall short ofhuman performance when inferring the camera wearers' in-the-moment beliefstates and future actions that are most consistent with the unseen videofuture. We believe that our results will shape the future design of animportant class of egocentric digital assistants which are equipped with areasonable model of the user's internal mental states.
  </details>

- **[REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation](http://arxiv.org/abs/2503.22122v1)**  `arXiv:2503.22122`  `cs.CL` `cs.CV` `cs.RO` `cs.AI`  
  _Puzhen Yuan, Angyuan Ma, Yunchao Yao, Huaxiu Yao, Masayoshi Tomizuka, Mingyu Ding_
  <details open><summary>Abstract</summary>
  Vision-language models (VLMs) have demonstrated remarkable capabilities inrobotic planning, particularly for long-horizon tasks that require a holisticunderstanding of the environment for task decomposition. Existing methodstypically rely on prior environmental knowledge or carefully designedtask-specific prompts, making them struggle with dynamic scene changes orunexpected task conditions, e.g., a robot attempting to put a carrot in themicrowave but finds the door was closed. Such challenges underscore twocritical issues: adaptability and efficiency. To address them, in this work, wepropose an adaptive multi-agent planning framework, termed REMAC, that enablesefficient, scene-agnostic multi-robot long-horizon task planning and executionthrough continuous reflection and self-evolution. REMAC incorporates two keymodules: a self-reflection module performing pre-condition and post-conditionchecks in the loop to evaluate progress and refine plans, and a self-evolvementmodule dynamically adapting plans based on scene-specific reasoning. It offersseveral appealing benefits: 1) Robots can initially explore and reason aboutthe environment without complex prompt design. 2) Robots can keep reflecting onpotential planning errors and adapting the plan based on task-specificinsights. 3) After iterations, a robot can call another one to coordinate tasksin parallel, maximizing the task execution efficiency. To validate REMAC'seffectiveness, we build a multi-agent environment for long-horizon robotmanipulation and navigation based on RoboCasa, featuring 4 task categories with27 task styles and 50+ different objects. Based on it, we further benchmarkstate-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, andGrok3, demonstrating REMAC's superiority by boosting average success rates by40% and execution efficiency by 52.7% over the single robot baseline.
  </details>

- **[How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark](http://arxiv.org/abs/2503.22093v1)**  `arXiv:2503.22093`  `cs.CV` `cs.AI`  
  _Ximing Wen, Mallika Mainali, Anik Sen_
  <details open><summary>Abstract</summary>
  Vision Language Models (VLMs) have demonstrated strong reasoning capabilitiesin Visual Question Answering (VQA) tasks; However, their ability to performTheory of Mind (ToM) tasks such as accurately inferring human intentions,beliefs, and other mental states remains underexplored. In this work, wepropose an open-ended question framework to comprehensively evaluate VLMs'performance across diverse categories of ToM tasks. We curated and annotated abenchmark dataset composed of 30 images. We then assessed the performance offour VLMs of varying sizes on this dataset. Our experimental results show thatthe GPT-4 model outperformed all others, with only one smaller model,GPT-4o-mini, achieving comparable performance. Additionally, we observed thatVLMs often struggle to accurately infer intentions in complex scenarios such asbullying or cheating. Moreover, our findings also reveal that smaller modelscan sometimes infer correct intentions despite relying on incorrect visualcues.
  </details>

- **[A Survey on Remote Sensing Foundation Models: From Vision to Multimodality](http://arxiv.org/abs/2503.22081v1)**  `arXiv:2503.22081`  `cs.CV`  
  _Ziyue Huang, Hongxi Yan, Qiqi Zhan, Shuai Yang, Mingming Zhang, Chenkai Zhang, et al._
  <details open><summary>Abstract</summary>
  The rapid advancement of remote sensing foundation models, particularlyvision and multimodal models, has significantly enhanced the capabilities ofintelligent geospatial data interpretation. These models combine various datamodalities, such as optical, radar, and LiDAR imagery, with textual andgeographic information, enabling more comprehensive analysis and understandingof remote sensing data. The integration of multiple modalities allows forimproved performance in tasks like object detection, land cover classification,and change detection, which are often challenged by the complex andheterogeneous nature of remote sensing data. However, despite theseadvancements, several challenges remain. The diversity in data types, the needfor large-scale annotated datasets, and the complexity of multimodal fusiontechniques pose significant obstacles to the effective deployment of thesemodels. Moreover, the computational demands of training and fine-tuningmultimodal models require significant resources, further complicating theirpractical application in remote sensing image interpretation tasks. This paperprovides a comprehensive review of the state-of-the-art in vision andmultimodal foundation models for remote sensing, focusing on theirarchitecture, training methods, datasets and application scenarios. We discussthe key challenges these models face, such as data alignment, cross-modaltransfer learning, and scalability, while also identifying emerging researchdirections aimed at overcoming these limitations. Our goal is to provide aclear understanding of the current landscape of remote sensing foundationmodels and inspire future research that can push the boundaries of what thesemodels can achieve in real-world applications. The list of resources collectedby the paper can be found in thehttps://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.
  </details>

- **[DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts](http://arxiv.org/abs/2503.19498v2)**  `arXiv:2503.19498`  `cs.CL`  
  _Ling Zhong, Yujing Lu, Jing Yang, Weiming Li, Peng Wei, Yongheng Wang, et al._
  <details open><summary>Abstract</summary>
  Chart Question Answering (CQA) benchmarks are essential for evaluating thecapability of Multimodal Large Language Models (MLLMs) to interpret visualdata. However, current benchmarks focus primarily on the evaluation ofgeneral-purpose CQA but fail to adequately capture domain-specific challenges.We introduce DomainCQA, a systematic methodology for constructingdomain-specific CQA benchmarks, and demonstrate its effectiveness by developingAstroChart, a CQA benchmark in the field of astronomy. Our evaluation showsthat chart reasoning and combining chart information with domain knowledge fordeeper analysis and summarization, rather than domain-specific knowledge, posethe primary challenge for existing MLLMs, highlighting a critical gap incurrent benchmarks. By providing a scalable and rigorous framework, DomainCQAenables more precise assessment and improvement of MLLMs for domain-specificapplications.
  </details>

- **[Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering](http://arxiv.org/abs/2503.18172v2)**  `arXiv:2503.18172`  `cs.CL` `cs.AI`  
  _Zixin Chen, Sicheng Song, Kashun Shum, Yanna Lin, Rui Sheng, Huamin Qu_
  <details open><summary>Abstract</summary>
  Misleading chart visualizations, which intentionally manipulate datarepresentations to support specific claims, can distort perceptions and lead toincorrect conclusions. Despite decades of research, misleading visualizationsremain a widespread and pressing issue. Recent advances in multimodal largelanguage models (MLLMs) have demonstrated strong chart comprehensioncapabilities, yet no existing work has systematically evaluated their abilityto detect and interpret misleading charts. This paper introduces the MisleadingChart Question Answering (Misleading ChartQA) Benchmark, a large-scalemultimodal dataset designed to assess MLLMs in identifying and reasoning aboutmisleading charts. It contains over 3,000 curated examples, covering 21 typesof misleaders and 10 chart types. Each example includes standardized chartcode, CSV data, and multiple-choice questions with labeled explanations,validated through multi-round MLLM checks and exhausted expert human review. Webenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitationsin identifying visually deceptive practices. We also propose a novel pipelinethat detects and localizes misleaders, enhancing MLLMs' accuracy in misleadingchart interpretation. Our work establishes a foundation for advancingMLLM-driven misleading chart comprehension. We publicly release the sampledataset to support further research in this critical area.
  </details>

- **[Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding](http://arxiv.org/abs/2503.16707v2)**  `arXiv:2503.16707`  `cs.CV`  
  _Jinlong Li, Cristiano Saltori, Fabio Poiesi, Nicu Sebe_
  <details open><summary>Abstract</summary>
  The lack of a large-scale 3D-text corpus has led recent works to distillopen-vocabulary knowledge from vision-language models (VLMs). However, thesemethods typically rely on a single VLM to align the feature spaces of 3D modelswithin a common language space, which limits the potential of 3D models toleverage the diverse spatial and semantic capabilities encapsulated in variousfoundation models. In this paper, we propose Cross-modal and Uncertainty-awareAgglomeration for Open-vocabulary 3D Scene Understanding dubbed CUA-O3D, thefirst model to integrate multiple foundation models-such as CLIP, DINOv2, andStable Diffusion-into 3D scene understanding. We further introduce adeterministic uncertainty estimation to adaptively distill and harmonize theheterogeneous 2D feature embeddings from these models. Our method addresses twokey challenges: (1) incorporating semantic priors from VLMs alongside thegeometric knowledge of spatially-aware vision foundation models, and (2) usinga novel deterministic uncertainty estimation to capture model-specificuncertainties across diverse semantic and geometric sensitivities, helping toreconcile heterogeneous representations during training. Extensive experimentson ScanNetV2 and Matterport3D demonstrate that our method not only advancesopen-vocabulary segmentation but also achieves robust cross-domain alignmentand competitive spatial perception capabilities. The code will be available at:https://github.com/TyroneLi/CUA_O3D.
  </details>

- **[Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](http://arxiv.org/abs/2503.12496v2)**  `arXiv:2503.12496`  `cs.CV`  
  _Tianyuan Qu, Longxiang Tang, Bohao Peng, Senqiao Yang, Bei Yu, Jiaya Jia_
  <details open><summary>Abstract</summary>
  The rise of Large Vision-Language Models (LVLMs) has significantly advancedvideo understanding. However, efficiently processing long videos remains achallenge due to the ``Sampling Dilemma'': low-density sampling risks missingcritical information, while high-density sampling introduces redundancy. Toaddress this issue, we introduce LSDBench, the first benchmark designed toevaluate LVLMs on long-video tasks by constructing high Necessary SamplingDensity (NSD) questions, where NSD represents the minimum sampling densityrequired to accurately answer a given question. LSDBench focuses on dense,short-duration actions to rigorously assess the sampling strategies employed byLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novelReasoning-Driven Hierarchical Sampling (RHS) framework, which combines globallocalization of question-relevant cues with local dense sampling for preciseinference. Additionally, we develop a lightweight Semantic-Guided FrameSelector to prioritize informative frames, enabling RHS to achieve comparableor superior performance with significantly fewer sampled frames. Together, ourLSDBench and RHS framework address the unique challenges of high-NSD long-videotasks, setting a new standard for evaluating and improving LVLMs in thisdomain. Our benchmark and evaluation codes has been released at:https://github.com/dvlab-research/LSDBench
  </details>

- **[Generalizable Prompt Learning of CLIP: A Brief Overview](http://arxiv.org/abs/2503.01263v3)**  `arXiv:2503.01263`  `cs.CL` `cs.CV`  
  _Fangming Cui, Yonggang Zhang, Xuan Wang, Xule Wang, Liang Xiao_
  <details open><summary>Abstract</summary>
  Existing vision-language models (VLMs) such as CLIP have showcased animpressive capability to generalize well across various downstream tasks. Thesemodels leverage the synergy between visual and textual information, enablingthem to understand and reason about the content present in images and text in aunified manner. This article provides a brief overview of CLIP based onfew-shot prompt learning, including experimental data and technicalcharacteristics of some methods. The purpose of this review is to provide areference for researchers who have just started their research in generalizableprompting of CLIP through few-shot training for classification across 15datasets and also to facilitate the integration of this field by researchers inother downstream tasks.
  </details>

- **[TULIP: Token-length Upgraded CLIP](http://arxiv.org/abs/2410.10034v2)**  `arXiv:2410.10034`  `cs.CV`  
  _Ivona Najdenkoska, Mohammad Mahdi Derakhshani, Yuki M. Asano, Nanne van Noord, Marcel Worring, Cees G. M. Snoek_
  <details open><summary>Abstract</summary>
  We address the challenge of representing long captions in vision-languagemodels, such as CLIP. By design these models are limited by fixed, absolutepositional encodings, restricting inputs to a maximum of 77 tokens andhindering performance on tasks requiring longer descriptions. Although recentwork has attempted to overcome this limit, their proposed approaches struggleto model token relationships over longer distances and simply extend to a fixednew token length. Instead, we propose a generalizable method, named TULIP, ableto upgrade the token length to any length for CLIP-like models. We do so byimproving the architecture with relative position encodings, followed by atraining procedure that (i) distills the original CLIP text encoder into anencoder with relative position encodings and (ii) enhances the model foraligning longer captions with images. By effectively encoding captions longerthan the default 77 tokens, our model outperforms baselines on cross-modaltasks such as retrieval and text-to-image generation. The code repository isavailable at https://github.com/ivonajdenkoska/tulip.
  </details>

- **[AnyAttack: Towards Large-scale Self-supervised Adversarial Attacks on Vision-language Models](http://arxiv.org/abs/2410.05346v3)**  `arXiv:2410.05346`  `cs.AI` `cs.LG`  
  _Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Yunhao Chen, et al._
  <details open><summary>Abstract</summary>
  Due to their multimodal capabilities, Vision-Language Models (VLMs) havefound numerous impactful applications in real-world scenarios. However, recentstudies have revealed that VLMs are vulnerable to image-based adversarialattacks. Traditional targeted adversarial attacks require specific targets andlabels, limiting their real-world impact.We present AnyAttack, aself-supervised framework that transcends the limitations of conventionalattacks through a novel foundation model approach. By pre-training on themassive LAION-400M dataset without label supervision, AnyAttack achievesunprecedented flexibility - enabling any image to be transformed into an attackvector targeting any desired output across different VLMs.This approachfundamentally changes the threat landscape, making adversarial capabilitiesaccessible at an unprecedented scale. Our extensive validation across fiveopen-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) demonstratesAnyAttack's effectiveness across diverse multimodal tasks. Most concerning,AnyAttack seamlessly transfers to commercial systems including Google Gemini,Claude Sonnet, Microsoft Copilot and OpenAI GPT, revealing a systemicvulnerability requiring immediate attention.
  </details>

- **[Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models](http://arxiv.org/abs/2405.12523v3)**  `arXiv:2405.12523`  `cs.CV` `cs.AI`  
  _Jiaqi Li, Qianshan Wei, Chuanyi Zhang, Guilin Qi, Miaozeng Du, Yongrui Chen, et al._
  <details open><summary>Abstract</summary>
  Machine unlearning empowers individuals with the `right to be forgotten' byremoving their private or sensitive information encoded in machine learningmodels. However, it remains uncertain whether MU can be effectively applied toMultimodal Large Language Models (MLLMs), particularly in scenarios offorgetting the leaked visual data of concepts. To overcome the challenge, wepropose an efficient method, Single Image Unlearning (SIU), to unlearn thevisual recognition of a concept by fine-tuning a single associated image forfew steps. SIU consists of two key aspects: (i) Constructing Multifacetedfine-tuning data. We introduce four targets, based on which we constructfine-tuning data for the concepts to be forgotten; (ii) Jointly training loss.To synchronously forget the visual recognition of concepts and preserve theutility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergenceLoss combined with Cross Entropy loss. Alongside our method, we establishMMUBench, a new benchmark for MU in MLLMs and introduce a collection of metricsfor its evaluation. Experimental results on MMUBench show that SIU completelysurpasses the performance of existing methods. Furthermore, we surprisinglyfind that SIU can avoid invasive membership inference attacks and jailbreakattacks. To the best of our knowledge, we are the first to explore MU in MLLMs.We will release the code and benchmark in the near future.
  </details>
