# üîç MLLM Papers ¬∑ 2025-03-27

[![Total Papers](https://img.shields.io/badge/Papers-27-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `VLM` `MLM` `MLLM` `Multi-Modal Language Model` `Multimodal Language Model` `Vision Language Model`  
**Filter**: `None`

---

## üìö Paper List

- **[Video-R1: Reinforcing Video Reasoning in MLLMs](http://arxiv.org/abs/2503.21776v1)**  `arXiv:2503.21776`  `cs.CV`  
  _Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, et al._
  <details open><summary>Abstract</summary>
  Inspired by DeepSeek-R1's success in eliciting reasoning abilities throughrule-based reinforcement learning (RL), we introduce Video-R1 as the firstattempt to systematically explore the R1 paradigm for eliciting video reasoningwithin multimodal large language models (MLLMs). However, directly applying RLtraining with the GRPO algorithm to video reasoning presents two primarychallenges: (i) a lack of temporal modeling for video reasoning, and (ii) thescarcity of high-quality video-reasoning data. To address these issues, wefirst propose the T-GRPO algorithm, which encourages models to utilize temporalinformation in videos for reasoning. Additionally, instead of relying solely onvideo data, we incorporate high-quality image-reasoning data into the trainingprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold startand Video-R1-260k for RL training, both comprising image and video data.Experimental results demonstrate that Video-R1 achieves significantimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, aswell as on general video benchmarks including MVBench and TempCompass, etc.Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoningbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. Allcodes, models, data are released.
  </details>

- **[Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck](http://arxiv.org/abs/2503.21757v1)**  `arXiv:2503.21757`  `cs.AI` `cs.CV` `cs.LG`  
  _Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos_
  <details open><summary>Abstract</summary>
  In this work, we aim to compress the vision tokens of a Large Vision LanguageModel (LVLM) into a representation that is simultaneously suitable for (a)generative and (b) discriminative tasks, (c) is nearly lossless, and (d) isstorage-efficient. We propose a novel compression approach, called Fwd2Bot,that uses the LVLM itself to compress the visual information in a task-agnosticmanner. At the core of Fwd2bot there exists a "double-forward pass" trainingstrategy, whereby, during the first forward pass, the LLM (of the LVLM) createsa bottleneck by condensing the visual information into a small number ofsummary tokens. Then, using the same LLM, the second forward pass processes thelanguage instruction(s) alongside the summary tokens, used as a directreplacement for the image ones. The training signal is provided by two losses:an autoregressive one applied after the second pass that provides a directoptimization objective for compression, and a contrastive loss, applied afterthe first pass, that further boosts the representation strength, especially fordiscriminative tasks. The training is further enhanced by stage-specificadapters. We accompany the proposed method by an in-depth ablation study.Overall, Fwd2Bot results in highly-informative compressed representationssuitable for both generative and discriminative tasks. For generative tasks, weoffer a 2x higher compression rate without compromising the generativecapabilities, setting a new state-of-the-art result. For discriminative tasks,we set a new state-of-the-art on image retrieval and compositionality.
  </details>

- **[VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness](http://arxiv.org/abs/2503.21755v1)**  `arXiv:2503.21755`  `cs.CV`  
  _Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, et al._
  <details open><summary>Abstract</summary>
  Video generation has advanced significantly, evolving from producingunrealistic outputs to generating videos that appear visually convincing andtemporally coherent. To evaluate these video generative models, benchmarks suchas VBench have been developed to assess their faithfulness, measuring factorslike per-frame aesthetics, temporal consistency, and basic prompt adherence.However, these aspects mainly represent superficial faithfulness, which focuson whether the video appears visually convincing rather than whether it adheresto real-world principles. While recent models perform increasingly well onthese metrics, they still struggle to generate videos that are not justvisually plausible but fundamentally realistic. To achieve real "world models"through video generation, the next frontier lies in intrinsic faithfulness toensure that generated videos adhere to physical laws, commonsense reasoning,anatomical correctness, and compositional integrity. Achieving this level ofrealism is essential for applications such as AI-assisted filmmaking andsimulated world modeling. To bridge this gap, we introduce VBench-2.0, anext-generation benchmark designed to automatically evaluate video generativemodels for their intrinsic faithfulness. VBench-2.0 assesses five keydimensions: Human Fidelity, Controllability, Creativity, Physics, andCommonsense, each further broken down into fine-grained capabilities. Tailoredfor individual dimensions, our evaluation framework integrates generalists suchas state-of-the-art VLMs and LLMs, and specialists, including anomaly detectionmethods proposed for video generation. We conduct extensive annotations toensure alignment with human judgment. By pushing beyond superficialfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a newstandard for the next generation of video generative models in pursuit ofintrinsic faithfulness.
  </details>

- **[3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models](http://arxiv.org/abs/2503.21745v1)**  `arXiv:2503.21745`  `cs.CV`  
  _Yuhan Zhang, Mengchen Zhang, Tong Wu, Tengfei Wang, Gordon Wetzstein, Dahua Lin, et al._
  <details open><summary>Abstract</summary>
  3D generation is experiencing rapid advancements, while the development of 3Devaluation has not kept pace. How to keep automatic evaluation equitablyaligned with human perception has become a well-recognized challenge. Recentadvances in the field of language and image generation have explored humanpreferences and showcased respectable fitting ability. However, the 3D domainstill lacks such a comprehensive preference dataset over generative models. Tomitigate this absence, we develop 3DGen-Arena, an integrated platform in abattle manner. Then, we carefully design diverse text and image prompts andleverage the arena platform to gather human preferences from both public usersand expert annotators, resulting in a large-scale multi-dimension humanpreference dataset 3DGen-Bench. Using this dataset, we further train aCLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator,3DGen-Eval. These two models innovatively unify the quality evaluation oftext-to-3D and image-to-3D generation, and jointly form our automatedevaluation system with their respective strengths. Extensive experimentsdemonstrate the efficacy of our scoring model in predicting human preferences,exhibiting a superior correlation with human ranks compared to existingmetrics. We believe that our 3DGen-Bench dataset and automated evaluationsystem will foster a more equitable evaluation in the field of 3D generation,further promoting the development of 3D generative models and their downstreamapplications.
  </details>

- **[UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning](http://arxiv.org/abs/2503.21620v1)**  `arXiv:2503.21620`  `cs.AI`  
  _Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, et al._
  <details open><summary>Abstract</summary>
  The recent DeepSeek-R1 has showcased the emergence of reasoning capabilitiesin LLMs through reinforcement learning (RL) with rule-based rewards. Buildingon this idea, we are the first to explore how rule-based RL can enhance thereasoning capabilities of multimodal large language models (MLLMs) for graphicuser interface (GUI) action prediction tasks. To this end, we curate a smallyet high-quality dataset of 136 challenging tasks, encompassing five commonaction types on mobile devices. We also introduce a unified rule-based actionreward, enabling model optimization via policy-based algorithms such as GroupRelative Policy Optimization (GRPO). Experimental results demonstrate that ourproposed data-efficient model, UI-R1-3B, achieves substantial improvements onboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the IDbenchmark AndroidControl, the action type accuracy improves by 15%, whilegrounding accuracy increases by 10.3%, compared with the base model (i.e.Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our modelsurpasses the base model by 6.0% and achieves competitive performance withlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning(SFT) on 76K data. These results underscore the potential of rule-basedreinforcement learning to advance GUI understanding and control, paving the wayfor future research in this domain.
  </details>

- **[Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models](http://arxiv.org/abs/2503.21530v1)**  `arXiv:2503.21530`  `cs.AI` `cs.CL`  
  _Umer Butt, Stalin Veranasi, G√ºnter Neumann_
  <details open><summary>Abstract</summary>
  As the Information Retrieval (IR) field increasingly recognizes theimportance of inclusivity, addressing the needs of low-resource languagesremains a significant challenge. Transliteration between Urdu and its Romanizedform, Roman Urdu, remains underexplored despite the widespread use of bothscripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl datasetshowed promising results but suffered from poor domain adaptability and limitedevaluation. We propose a transformer-based approach using the m2m100multilingual translation model, enhanced with masked language modeling (MLM)pretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverseDakshina dataset. To address previous evaluation flaws, we introduce rigorousdataset splits and assess performance using BLEU, character-level BLEU, andCHRF. Our model achieves strong transliteration performance, with Char-BLEUscores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. Theseresults outperform both RNN baselines and GPT-4o Mini and demonstrate theeffectiveness of multilingual transfer learning for low-resourcetransliteration tasks.
  </details>

- **[Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving](http://arxiv.org/abs/2503.21505v1)**  `arXiv:2503.21505`  `cs.CL` `cs.CV`  
  _Yue Li, Meng Tian, Zhenyu Lin, Jiangtong Zhu, Dechang Zhu, Haiqiang Liu, et al._
  <details open><summary>Abstract</summary>
  Existing benchmarks for Vision-Language Model (VLM) on autonomous driving(AD) primarily assess interpretability through open-form visual questionanswering (QA) within coarse-grained tasks, which remain insufficient to assesscapabilities in complex driving scenarios. To this end, we introduce$\textbf{VLADBench}$, a challenging and fine-grained dataset featuringclose-form QAs that progress from static foundational knowledge and elements toadvanced reasoning for dynamic on-road situations. The elaborate$\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,General Element Recognition, Traffic Graph Generation, Target AttributeComprehension, and Ego Decision-Making and Planning. These domains are furtherbroken down into 11 secondary aspects and 29 tertiary tasks for a granularevaluation. A thorough assessment of general and domain-specific (DS) VLMs onthis benchmark reveals both their strengths and critical limitations in ADcontexts. To further exploit the cognitive and reasoning interactions among the5 domains for AD understanding, we start from a small-scale VLM and train theDS models on individual domain datasets (collected from 1.4M DS QAs acrosspublic sources). The experimental results demonstrate that the proposedbenchmark provides a crucial step toward a more comprehensive assessment ofVLMs in AD, paving the way for the development of more cognitivelysophisticated and reasoning-capable AD systems.
  </details>

- **[BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding](http://arxiv.org/abs/2503.21483v1)**  `arXiv:2503.21483`  `cs.CV`  
  _Shuming Liu, Chen Zhao, Tianqi Xu, Bernard Ghanem_
  <details open><summary>Abstract</summary>
  Large video-language models (VLMs) have demonstrated promising progress invarious video understanding tasks. However, their effectiveness in long-formvideo analysis is constrained by limited context windows. Traditionalapproaches, such as uniform frame sampling, often inevitably allocate resourcesto irrelevant content, diminishing their effectiveness in real-world scenarios.In this paper, we introduce BOLT, a method to BOost Large VLMs withoutadditional Training through a comprehensive study of frame selectionstrategies. First, to enable a more realistic evaluation of VLMs in long-formvideo understanding, we propose a multi-source retrieval evaluation setting.Our findings reveal that uniform sampling performs poorly in noisy contexts,underscoring the importance of selecting the right frames. Second, we exploreseveral frame selection strategies based on query-frame similarity and analyzetheir effectiveness at inference time. Our results show that inverse transformsampling yields the most significant performance improvement, increasingaccuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from58.9% to 63.4%. Our code is available at https://github.com/sming256/BOLT.
  </details>

- **[FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs](http://arxiv.org/abs/2503.21457v1)**  `arXiv:2503.21457`  `cs.CV`  
  _Xiaoqin Wang, Xusen Ma, Xianxu Hou, Meidan Ding, Yudong Li, Junliang Chen, et al._
  <details open><summary>Abstract</summary>
  Multimodal large language models (MLLMs) have demonstrated remarkablecapabilities in various tasks. However, effectively evaluating these MLLMs onface perception remains largely unexplored. To address this gap, we introduceFaceBench, a dataset featuring hierarchical multi-view and multi-levelattributes specifically designed to assess the comprehensive face perceptionabilities of MLLMs. Initially, we construct a hierarchical facial attributestructure, which encompasses five views with up to three levels of attributes,totaling over 210 attributes and 700 attribute values. Based on the structure,the proposed FaceBench consists of 49,919 visual question-answering (VQA) pairsfor evaluation and 23,841 pairs for fine-tuning. Moreover, we further develop arobust face perception MLLM baseline, Face-LLaVA, by training with our proposedface VQA data. Extensive experiments on various mainstream MLLMs and Face-LLaVAare conducted to test their face perception ability, with results also comparedagainst human performance. The results reveal that, the existing MLLMs are farfrom satisfactory in understanding the fine-grained facial attributes, whileour Face-LLaVA significantly outperforms existing open-source models with asmall amount of training data and is comparable to commercial ones like GPT-4oand Gemini. The dataset will be released athttps://github.com/CVI-SZU/FaceBench.
  </details>

- **[Graph-to-Vision: Multi-graph Understanding and Reasoning using Vision-Language Models](http://arxiv.org/abs/2503.21435v1)**  `arXiv:2503.21435`  `cs.AI`  
  _Ruizhou Li, Haiyun Jiang_
  <details open><summary>Abstract</summary>
  Graph Neural Networks (GNNs), as the dominant paradigm for graph-structuredlearning, have long faced dual challenges of exponentially escalatingcomputational complexity and inadequate cross-scenario generalizationcapability. With the rapid advancement of multimodal learning, Vision-LanguageModels (VLMs) have demonstrated exceptional cross-modal relational reasoningcapabilities and generalization capacities, thereby opening up novel pathwaysfor overcoming the inherent limitations of conventional graph learningparadigms. However, current research predominantly concentrates oninvestigating the single-graph reasoning capabilities of VLMs, whichfundamentally fails to address the critical requirement for coordinatedreasoning across multiple heterogeneous graph data in real-world applicationscenarios. To address these limitations, we propose the first multi-graph jointreasoning benchmark for VLMs. Our benchmark encompasses four graph categories:knowledge graphs, flowcharts, mind maps, and route maps,with each graph groupaccompanied by three progressively challenging instruction-response pairs.Leveraging this benchmark, we conducted comprehensive capability assessments ofstate-of-the-art VLMs and performed fine-tuning on open-source models. Thisstudy not only addresses the underexplored evaluation gap in multi-graphreasoning for VLMs but also empirically validates their generalizationsuperiority in graph-structured learning.
  </details>

- **[InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression](http://arxiv.org/abs/2503.21307v1)**  `arXiv:2503.21307`  `cs.AI` `cs.CV`  
  _Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, et al._
  <details open><summary>Abstract</summary>
  Most multimodal large language models (MLLMs) treat visual tokens as "asequence of text", integrating them with text tokens into a large languagemodel (LLM). However, a great quantity of visual tokens significantly increasesthe demand for computational resources and time. In this paper, we proposeInternVL-X, which outperforms the InternVL model in both performance andefficiency by incorporating three visual token compression methods. First, wepropose a novel vision-language projector, PVTC. This component integratesadjacent visual embeddings to form a local query and utilizes the transformedCLS token as a global query, then performs point-to-region cross-attentionthrough these local and global queries to more effectively convert visualfeatures. Second, we present a layer-wise visual token compression module,LVTC, which compresses tokens in the LLM shallow layers and then expands themthrough upsampling and residual connections in the deeper layers. Thissignificantly enhances the model computational efficiency. Futhermore, wepropose an efficient high resolution slicing method, RVTC, which dynamicallyadjusts the number of visual tokens based on image area or length filtering.RVTC greatly enhances training efficiency with only a slight reduction inperformance. By utilizing 20% or fewer visual tokens, InternVL-X achievesstate-of-the-art performance on 7 public MLLM benchmarks, and improves theaverage metric by 2.34% across 12 tasks.
  </details>

- **[Cultivating Game Sense for Yourself: Making VLMs Gaming Experts](http://arxiv.org/abs/2503.21263v1)**  `arXiv:2503.21263`  `cs.CL`  
  _Wenxuan Lu, Jiangyang He, Zhanqiu Zhang, Yiwen Guo, Tianning Zang_
  <details open><summary>Abstract</summary>
  Developing agents capable of fluid gameplay in first/third-person gameswithout API access remains a critical challenge in Artificial GeneralIntelligence (AGI). Recent efforts leverage Vision Language Models (VLMs) asdirect controllers, frequently pausing the game to analyze screens and planaction through language reasoning. However, this inefficient paradigmfundamentally restricts agents to basic and non-fluent interactions: relying onisolated VLM reasoning for each action makes it impossible to handle tasksrequiring high reactivity (e.g., FPS shooting) or dynamic adaptability (e.g.,ACT combat). To handle this, we propose a paradigm shift in gameplay agentdesign: instead of directly controlling gameplay, VLM develops specializedexecution modules tailored for tasks like shooting and combat. These moduleshandle real-time game interactions, elevating VLM to a high-level developer.Building upon this paradigm, we introduce GameSense, a gameplay agent frameworkwhere VLM develops task-specific game sense modules by observing task executionand leveraging vision tools and neural network training pipelines. Thesemodules encapsulate action-feedback logic, ranging from direct action rules toneural network-based decisions. Experiments demonstrate that our framework isthe first to achieve fluent gameplay in diverse genres, including ACT, FPS, andFlappy Bird, setting a new benchmark for game-playing agents.
  </details>

- **[LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models](http://arxiv.org/abs/2503.21227v1)**  `arXiv:2503.21227`  `cs.CL`  
  _Hengyuan Zhao, Ziqin Wang, Qixin Sun, Kaiyou Song, Yilin Li, Xiaolin Hu, et al._
  <details open><summary>Abstract</summary>
  Although applying Mixture of Experts to large language models for learningnew tasks is widely regarded as an effective strategy for continuous learning,there still remain two major challenges: (1) As the number of tasks grows,simple parameter expansion strategies can lead to excessively large models. (2)Modifying the parameters of the existing router results in the erosion ofpreviously acquired knowledge. In this paper, we present an innovativeframework named LLaVA-CMoE, which is a continuous Mixture of Experts (MoE)architecture without any replay data. Specifically, we have developed a methodcalled Probe-Guided Knowledge Extension (PGKE), which employs probe experts toassess whether additional knowledge is required for a specific layer. Thisapproach enables the model to adaptively expand its network parameters based ontask distribution, thereby significantly improving the efficiency of parameterexpansion. Additionally, we introduce a hierarchical routing algorithm calledProbabilistic Task Locator (PTL), where high-level routing captures inter-taskinformation and low-level routing focuses on intra-task details, ensuring thatnew task experts do not interfere with existing ones. Our experiments showsthat our efficient architecture has substantially improved model performance onthe Coin benchmark while maintaining a reasonable parameter count.
  </details>

- **[VoxRep: Enhancing 3D Spatial Understanding in 2D Vision-Language Models via Voxel Representation](http://arxiv.org/abs/2503.21214v1)**  `arXiv:2503.21214`  `cs.CV` `cs.CL`  
  _Alan Dao, Norapat Buppodom_
  <details open><summary>Abstract</summary>
  Comprehending 3D environments is vital for intelligent systems in domainslike robotics and autonomous navigation. Voxel grids offer a structuredrepresentation of 3D space, but extracting high-level semantic meaning remainschallenging. This paper proposes a novel approach utilizing a Vision-LanguageModel (VLM) to extract "voxel semantics"-object identity, color, andlocation-from voxel data. Critically, instead of employing complex 3D networks,our method processes the voxel space by systematically slicing it along aprimary axis (e.g., the Z-axis, analogous to CT scan slices). These 2D slicesare then formatted and sequentially fed into the image encoder of a standardVLM. The model learns to aggregate information across slices and correlatespatial patterns with semantic concepts provided by the language component.This slice-based strategy aims to leverage the power of pre-trained 2D VLMs forefficient 3D semantic understanding directly from voxel representations.
  </details>

- **[FakeReasoning: Towards Generalizable Forgery Detection and Reasoning](http://arxiv.org/abs/2503.21210v1)**  `arXiv:2503.21210`  `cs.CV`  
  _Yueying Gao, Dongliang Chang, Bingyao Yu, Haotian Qin, Lei Chen, Kongming Liang, et al._
  <details open><summary>Abstract</summary>
  Accurate and interpretable detection of AI-generated images is essential formitigating risks associated with AI misuse. However, the substantial domain gapamong generative models makes it challenging to develop a generalizable forgerydetection model. Moreover, since every pixel in an AI-generated image issynthesized, traditional saliency-based forgery explanation methods are notwell suited for this task. To address these challenges, we propose modelingAI-generated image detection and explanation as a Forgery Detection andReasoning task (FDR-Task), leveraging vision-language models (VLMs) to provideaccurate detection through structured and reliable reasoning over forgeryattributes. To facilitate this task, we introduce the Multi-Modal ForgeryReasoning dataset (MMFR-Dataset), a large-scale dataset containing 100K imagesacross 10 generative models, with 10 types of forgery reasoning annotations,enabling comprehensive evaluation of FDR-Task. Additionally, we proposeFakeReasoning, a forgery detection and reasoning framework with two keycomponents. First, Forgery-Aligned Contrastive Learning enhances VLMs'understanding of forgery-related semantics through both cross-modal andintra-modal contrastive learning between images and forgery attributereasoning. Second, a Classification Probability Mapper bridges the optimizationgap between forgery detection and language modeling by mapping the outputlogits of VLMs to calibrated binary classification probabilities. Experimentsacross multiple generative models demonstrate that FakeReasoning not onlyachieves robust generalization but also outperforms state-of-the-art methods onboth detection and reasoning tasks.
  </details>

- **[Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning](http://arxiv.org/abs/2503.20752v2)**  `arXiv:2503.20752`  `cs.AI` `cs.CV`  
  _Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, et al._
  <details open><summary>Abstract</summary>
  Visual reasoning abilities play a crucial role in understanding complexmultimodal data, advancing both domain-specific applications and artificialgeneral intelligence (AGI). Existing methods improve VLM reasoning viaChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotatedtraining data to enhance visual reasoning capabilities. However, this trainingparadigm may lead to overfitting and cognitive rigidity, restricting themodel's ability to transfer visual reasoning skills across domains and limitingits real-world applicability. To address these limitations, we proposeReason-RFT, a novel reinforcement fine-tuning framework that significantlyenhances generalization capabilities in visual reasoning tasks. Reason-RFTintroduces a two-phase training framework for visual reasoning: (1) SupervisedFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates thereasoning potential of Vision-Language Models (VLMs), followed by (2) GroupRelative Policy Optimization (GRPO)-based reinforcement learning that generatesmultiple reasoning-response pairs, significantly enhancing generalization invisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,we reconstructed a comprehensive dataset spanning visual counting, structureperception, and spatial transformation. Experimental results demonstrateReasoning-RFT's three key advantages: (1) Performance Enhancement: achievingstate-of-the-art results across multiple tasks, outperforming most mainstreamopen-source and proprietary models; (2) Generalization Superiority:consistently maintaining robust performance across diverse tasks and domains,outperforming alternative training paradigms; (3) Data Efficiency: excelling infew-shot learning scenarios while surpassing full-dataset SFT baselines.Project website: https://tanhuajie.github.io/ReasonRFT
  </details>

- **[RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models](http://arxiv.org/abs/2503.19654v2)**  `arXiv:2503.19654`  `cs.AI` `cs.CV` `cs.LG`  
  _Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen_
  <details open><summary>Abstract</summary>
  We introduce RGB-Th-Bench, the first benchmark designed to evaluate theability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.While VLMs have demonstrated remarkable progress in visual reasoning andmultimodal understanding, their evaluation has been predominantly limited toRGB-based benchmarks, leaving a critical gap in assessing their capabilities ininfrared vision tasks. Existing visible-infrared datasets are eithertask-specific or lack high-quality annotations necessary for rigorous modelevaluation. To address these limitations, RGB-Th-Bench provides a comprehensiveevaluation framework covering 14 distinct skill dimensions, with a total of1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracymetrics: a standard question-level accuracy and a stricter skill-levelaccuracy, which evaluates model robustness across multiple questions withineach skill dimension. This design ensures a thorough assessment of modelperformance, including resilience to adversarial and hallucinated responses. Weconduct extensive evaluations on 19 state-of-the-art VLMs, revealingsignificant performance gaps in RGB-Thermal understanding. Our results showthat even the strongest models struggle with thermal image comprehension, withperformance heavily constrained by their RGB-based capabilities. Additionally,the lack of large-scale application-specific and expert-annotatedthermal-caption-pair datasets in pre-training is an important reason of theobserved performance gap. RGB-Th-Bench highlights the urgent need for furtheradvancements in multimodal learning to bridge the gap between visible andthermal image understanding. The dataset is available through this link, andthe evaluation code will also be made publicly available.
  </details>

- **[MouseGPT: A Large-scale Vision-Language Model for Mouse Behavior Analysis](http://arxiv.org/abs/2503.10212v2)**  `arXiv:2503.10212`  `cs.CV`  
  _Teng Xu, Taotao Zhou, Youjia Wang, Peng Yang, Simin Tang, Kuixiang Shao, et al._
  <details open><summary>Abstract</summary>
  Analyzing animal behavior is crucial in advancing neuroscience, yetquantifying and deciphering its intricate dynamics remains a significantchallenge. Traditional machine vision approaches, despite their ability todetect spontaneous behaviors, fall short due to limited interpretability andreliance on manual labeling, which restricts the exploration of the fullbehavioral spectrum. Here, we introduce MouseGPT, a Vision-Language Model (VLM)that integrates visual cues with natural language to revolutionize mousebehavior analysis. Built upon our first-of-its-kind dataset - incorporatingpose dynamics and open-vocabulary behavioral annotations across over 42 millionframes of diverse psychiatric conditions - MouseGPT provides a novel,context-rich method for comprehensive behavior interpretation. Our holisticanalysis framework enables detailed behavior profiling, clustering, and novelbehavior discovery, offering deep insights without the need for labor -intensive manual annotation. Evaluations reveal that MouseGPT surpassesexisting models in precision, adaptability, and descriptive richness,positioning it as a transformative tool for ethology and for unraveling complexbehavioral dynamics in animal models.
  </details>

- **[Generalizable Prompt Learning of CLIP: A Brief Overview](http://arxiv.org/abs/2503.01263v2)**  `arXiv:2503.01263`  `cs.CV` `cs.CL`  
  _Fangming Cui, Yonggang Zhang, Xuan Wang, Xule Wang, Liang Xiao_
  <details open><summary>Abstract</summary>
  Existing vision-language models (VLMs) such as CLIP have showcased animpressive capability to generalize well across various downstream tasks. Thesemodels leverage the synergy between visual and textual information, enablingthem to understand and reason about the content present in images and text in aunified manner. This article provides a brief overview of CLIP based onfew-shot prompt learning, including experimental data and technicalcharacteristics of some methods. The purpose of this review is to provide areference for researchers who have just started their research in generalizableprompting of CLIP through few-shot training for classification across 15datasets and also to facilitate the integration of this field by researchers inother downstream tasks.
  </details>

- **[ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval](http://arxiv.org/abs/2502.15682v2)**  `arXiv:2502.15682`  `cs.CV`  
  _Guanqi Zhan, Yuanpei Liu, Kai Han, Weidi Xie, Andrew Zisserman_
  <details open><summary>Abstract</summary>
  The objective in this paper is to improve the performance of text-to-imageretrieval. To this end, we introduce a new framework that can boost theperformance of large-scale pre-trained vision-language models, so that they canbe used for text-to-image re-ranking. The approach, Enhanced Language-ImagePre-training (ELIP), uses the text query, via a simple MLP mapping network, topredict a set of visual prompts to condition the ViT image encoding. ELIP caneasily be applied to the commonly used CLIP, SigLIP and BLIP-2 networks. Totrain the architecture with limited computing resources, we develop a 'studentfriendly' best practice, involving global hard sample mining, and curation of alarge-scale dataset. On the evaluation side, we set up two newout-of-distribution (OOD) benchmarks, Occluded COCO and ImageNet-R, to assessthe zero-shot generalisation of the models to different domains. The resultsdemonstrate that ELIP significantly boosts CLIP/SigLIP/SigLIP-2 text-to-imageretrieval performance and outperforms BLIP-2 on several benchmarks, as well asproviding an easy means to adapt to OOD datasets.
  </details>

- **[Do Multimodal Large Language Models See Like Humans?](http://arxiv.org/abs/2412.09603v2)**  `arXiv:2412.09603`  `cs.CV`  
  _Jiaying Lin, Shuquan Ye, Rynson W. H. Lau_
  <details open><summary>Abstract</summary>
  Multimodal Large Language Models (MLLMs) have achieved impressive results onvarious vision tasks, leveraging recent advancements in large language models.However, a critical question remains unaddressed: do MLLMs perceive visualinformation similarly to humans? Current benchmarks lack the ability toevaluate MLLMs from this perspective. To address this challenge, we introduceHVSBench, a large-scale benchmark designed to assess the alignment betweenMLLMs and the human visual system (HVS) on fundamental vision tasks that mirrorhuman vision. HVSBench curated over 85K multimodal samples, spanning 13categories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,Free-Viewing, and Searching. Extensive experiments demonstrate theeffectiveness of our benchmark in providing a comprehensive evaluation ofMLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best modelsshow significant room for improvement, with most achieving only moderateresults. Our experiments reveal that HVSBench presents a new and significantchallenge for cutting-edge MLLMs. Diverse human participants attained strongperformance, significantly outperforming MLLMs, which further underscores thebenchmark's high quality. We believe that HVSBench will facilitate research onhuman-aligned and explainable MLLMs, marking a key step in understanding howMLLMs perceive and process visual information.
  </details>

- **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v2)**  `arXiv:2412.01095`  `cs.AI` `cs.CV` `cs.LG`  
  _Muchao Ye, Weiyang Liu, Pan He_
  <details open><summary>Abstract</summary>
  The rapid advancement of vision-language models (VLMs) has established a newparadigm in video anomaly detection (VAD): leveraging VLMs to simultaneouslydetect anomalies and provide comprehendible explanations for the decisions.Existing work in this direction often assumes the complex reasoning requiredfor VAD exceeds the capabilities of pretrained VLMs. Consequently, theseapproaches either incorporate specialized reasoning modules during inference orrely on instruction tuning datasets through additional training to adapt VLMsfor VAD. However, such strategies often incur substantial computational costsor data annotation overhead. To address these challenges in explainable VAD, weintroduce a verbalized learning framework named VERA that enables VLMs toperform VAD without model parameter modifications. Specifically, VERAautomatically decomposes the complex reasoning required for VAD intoreflections on simpler, more focused guiding questions capturing distinctabnormal patterns. It treats these reflective questions as learnable parametersand optimizes them through data-driven verbal interactions between learner andoptimizer VLMs, using coarsely labeled training data. During inference, VERAembeds the learned questions into model prompts to guide VLMs in generatingsegment-level anomaly scores, which are then refined into frame-level scoresvia the fusion of scene and temporal contexts. Experimental results onchallenging benchmarks demonstrate that the learned questions of VERA arehighly adaptable, significantly improving both detection performance andexplainability of VLMs for VAD.
  </details>

- **[Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding](http://arxiv.org/abs/2412.00493v2)**  `arXiv:2412.00493`  `cs.CV` `cs.CL`  
  _Duo Zheng, Shijia Huang, Liwei Wang_
  <details open><summary>Abstract</summary>
  The rapid advancement of Multimodal Large Language Models (MLLMs) hassignificantly impacted various multimodal tasks. However, these models facechallenges in tasks that require spatial understanding within 3D environments.Efforts to enhance MLLMs, such as incorporating point cloud features, have beenmade, yet a considerable gap remains between the models' learnedrepresentations and the inherent complexity of 3D scenes. This discrepancylargely stems from the training of MLLMs on predominantly 2D data, whichrestricts their effectiveness in comprehending 3D spaces. To address thisissue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,for 3D scene understanding. By treating 3D scenes as dynamic videos andincorporating 3D position encoding into these representations, our Video-3D LLMaligns video representations with real-world spatial contexts more accurately.In addition, we have implemented a maximum coverage sampling technique tooptimize the trade-off between computational cost and performance. Extensiveexperiments demonstrate that our model achieves state-of-the-art performance onseveral 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,Scan2Cap, ScanQA, and SQA3D.
  </details>

- **[GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI](http://arxiv.org/abs/2411.14522v2)**  `arXiv:2411.14522`  `cs.CV`  
  _Tianbin Li, Yanzhou Su, Wei Li, Bin Fu, Zhe Chen, Ziyan Huang, et al._
  <details open><summary>Abstract</summary>
  Despite significant advancements in general AI, its effectiveness in themedical domain is limited by the lack of specialized medical knowledge. Toaddress this, we formulate GMAI-VL-5.5M, a multimodal medical dataset createdby converting hundreds of specialized medical datasets with various annotationsinto high-quality image-text pairs. This dataset offers comprehensive taskcoverage, diverse modalities, and rich image-text data. Building upon thisdataset, we develop GMAI-VL, a general medical vision-language model, with athree-stage training strategy that enhances the integration of visual andtextual information. This approach significantly improves the model's abilityto process multimodal data, supporting accurate diagnoses and clinicaldecision-making. Experiments show that GMAI-VL achieves state-of-the-artperformance across various multimodal medical tasks, including visual questionanswering and medical image diagnosis.
  </details>

- **[ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom](http://arxiv.org/abs/2410.14138v2)**  `arXiv:2410.14138`  `cs.AI` `cs.CV`  
  _Jingqi Zhou, Sheng Wang, Jingwei Dong, Lei Li, Jiahui Gao, Jiyue Jiang, et al._
  <details open><summary>Abstract</summary>
  Large vision-language models (LVLMs) have witnessed significant progress onvisual understanding tasks. However, they often prioritize language knowledgeover image information on visual reasoning tasks, incurring performancedegradation. To tackle this issue, we first identify the drawbacks of existingsolutions (i.e., insufficient and irrelevant visual descriptions, and limitedmulti-modal capacities). We then decompose visual reasoning process into twostages: visual perception (i.e., eyesight) and textual reasoning (i.e.,wisdom), and introduce a novel visual reasoning framework named ProReason. Thisframework features multi-run proactive perception and decoupledvision-reasoning capabilities. Briefly, given a multi-modal question, ProReasoniterates proactive information collection and reasoning until the answer can beconcluded with necessary and sufficient visual descriptions. Notably, thedisassociation of capabilities allows seamless integration of existing largelanguage models (LLMs) to compensate for the reasoning deficits of LVLMs. Ourextensive experiments demonstrate that ProReason outperforms both existingmulti-step reasoning frameworks and passive peer methods on a wide range ofbenchmarks for both open-source and closed-source models. In addition, with theassistance of LLMs, ProReason achieves a performance improvement of up to 15%on MMMU benchmark. Our insights into existing solutions and the decoupledperspective for feasible integration of LLMs illuminate future research onvisual reasoning techniques, especially LLM-assisted ones.
  </details>

- **[OmniBench: Towards The Future of Universal Omni-Language Models](http://arxiv.org/abs/2409.15272v4)**  `arXiv:2409.15272`  `cs.AI` `cs.CL` `cs.CV`  
  _Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in multimodal large language models (MLLMs) have focusedon integrating multiple modalities, yet their ability to simultaneously processand reason across different inputs remains underexplored. We introduceOmniBench, a novel benchmark designed to evaluate models' ability to recognize,interpret, and reason across visual, acoustic, and textual inputssimultaneously. We define language models capable of such tri-modal processingas omni-language models (OLMs). OmniBench features high-quality humanannotations that require integrated understanding across all modalities. Ourevaluation reveals that: i) open-source OLMs show significant limitations ininstruction-following and reasoning in tri-modal contexts; and ii) mostbaseline models perform poorly (around 50% accuracy) even with textualalternatives to image/audio inputs. To address these limitations, we developOmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. Weadvocate for developing more robust tri-modal integration techniques andtraining strategies to enhance OLM performance. Codes and data could be foundat our repo (https://github.com/multimodal-art-projection/OmniBench).
  </details>

- **[Vision language models are blind: Failing to translate detailed visual features into words](http://arxiv.org/abs/2407.06581v6)**  `arXiv:2407.06581`  `cs.AI` `cs.CV`  
  _Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen_
  <details open><summary>Abstract</summary>
  While large language models with vision capabilities (VLMs), e.g., GPT-4o andGemini 1.5 Pro, score high on many vision-understanding benchmarks, they arestill struggling with low-level vision tasks that are easy to humans.Specifically, on BlindTest, our suite of 7 very simple tasks, includingidentifying (a) whether two circles overlap; (b) how many times two linesintersect; (c) which letter is being circled in a word; and (d) the number ofcircles in an Olympic-like logo, four state-of-the-art VLMs are only 58.07%accurate on average. Claude 3.5 Sonnet performs the best at 77.84% accuracy,far from the human expected accuracy of 100%. Across different imageresolutions and line widths, VLMs including slow-thinking models consistentlystruggle with those tasks that require precise spatial information whengeometric primitives overlap or are close. Yet, VLMs perform at near-100%accuracy when much more space is added to separate shapes and letters. Linearprobing experiments show that vision encoders contain sufficient visualinformation to solve BlindTest and that language models fail to decode thisinformation into correct answers. Code and data are at:https://vlmsareblind.github.io
  </details>
