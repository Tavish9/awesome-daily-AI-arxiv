# üîç MLLM Papers ¬∑ 2025-03-31

[![Total Papers](https://img.shields.io/badge/Papers-23-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `VLM` `MLM` `MLLM` `Multi-Modal Language Model` `Multimodal Language Model` `Vision Language Model`  
**Filter**: `None`

---

## üìö Paper List

- **[Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation](http://arxiv.org/abs/2503.24379v1)**  `arXiv:2503.24379`  `cs.CV` `cs.AI`  
  _Shengqiong Wu, Weicai Ye, Jiahao Wang, Quande Liu, Xintao Wang, Pengfei Wan, et al._
  <details open><summary>Abstract</summary>
  To address the bottleneck of accurate user intent interpretation within thecurrent video generation community, we present Any2Caption, a novel frameworkfor controllable video generation under any condition. The key idea is todecouple various condition interpretation steps from the video synthesis step.By leveraging modern multimodal large language models (MLLMs), Any2Captioninterprets diverse inputs--text, images, videos, and specialized cues such asregion, motion, and camera poses--into dense, structured captions that offerbackbone video generators with better guidance. We also introduce Any2CapIns, alarge-scale dataset with 337K instances and 407K conditions forany-condition-to-caption instruction tuning. Comprehensive evaluationsdemonstrate significant improvements of our system in controllability and videoquality across various aspects of existing video generation models. ProjectPage: https://sqwu.top/Any2Cap/
  </details>

- **[Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](http://arxiv.org/abs/2503.24376v1)**  `arXiv:2503.24376`  `cs.CL` `cs.CV` `cs.AI` `cs.LG`  
  _Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in Chain of Thought (COT) generation have significantlyimproved the reasoning capabilities of Large Language Models (LLMs), withreinforcement learning (RL) emerging as an effective post-training approach.Multimodal Large Language Models (MLLMs) inherit this reasoning potential butremain underexplored in tasks requiring both perception and logical reasoning.To address this, we introduce SEED-Bench-R1, a benchmark designed tosystematically evaluate post-training methods for MLLMs in video understanding.It includes intricate real-world videos and complex everyday planning tasks inthe format of multiple-choice questions, requiring sophisticated perception andreasoning. SEED-Bench-R1 assesses generalization through a three-levelhierarchy: in-distribution, cross-environment, and cross-environment-taskscenarios, equipped with a large-scale training dataset with easily verifiableground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RLwith supervised fine-tuning (SFT), demonstrating RL's data efficiency andsuperior performance on both in-distribution and out-of-distribution tasks,even outperforming SFT on general video understanding benchmarks likeLongVideoBench. Our detailed analysis reveals that RL enhances visualperception but often produces less logically coherent reasoning chains. Weidentify key limitations such as inconsistent reasoning and overlooked visualcues, and suggest future improvements in base model reasoning, reward modeling,and RL robustness against noisy signals.
  </details>

- **[H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding](http://arxiv.org/abs/2503.24008v1)**  `arXiv:2503.24008`  `cs.CV` `cs.AI`  
  _Qi Wu, Quanlong Zheng, Yanhao Zhang, Junlin Xie, Jinguo Luo, Kuo Wang, et al._
  <details open><summary>Abstract</summary>
  With the rapid development of multimodal models, the demand for assessingvideo understanding capabilities has been steadily increasing. However,existing benchmarks for evaluating video understanding exhibit significantlimitations in coverage, task diversity, and scene adaptability. Theseshortcomings hinder the accurate assessment of models' comprehensive videounderstanding capabilities. To tackle this challenge, we propose a hierarchicaland holistic video understanding (H2VU) benchmark designed to evaluate bothgeneral video and online streaming video comprehension. This benchmarkcontributes three key features:  Extended video duration: Spanning videos from brief 3-second clips tocomprehensive 1.5-hour recordings, thereby bridging the temporal gaps found incurrent benchmarks. Comprehensive assessment tasks: Beyond traditionalperceptual and reasoning tasks, we have introduced modules forcountercommonsense comprehension and trajectory state tracking. These additionstest the models' deep understanding capabilities beyond mere prior knowledge.Enriched video data: To keep pace with the rapid evolution of current AIagents, we have expanded first-person streaming video datasets. This expansionallows for the exploration of multimodal models' performance in understandingstreaming videos from a first-person perspective. Extensive results from H2VUreveal that existing multimodal large language models (MLLMs) possesssubstantial potential for improvement in our newly proposed evaluation tasks.We expect that H2VU will facilitate advancements in video understandingresearch by offering a comprehensive and in-depth analysis of MLLMs.
  </details>

- **[BeMERC: Behavior-Aware MLLM-based Framework for Multimodal Emotion Recognition in Conversation](http://arxiv.org/abs/2503.23990v1)**  `arXiv:2503.23990`  `cs.CL`  
  _Yumeng Fu, Junjie Wu, Zhongjie Wang, Meishan Zhang, Yulin Wu, Bingquan Liu_
  <details open><summary>Abstract</summary>
  Multimodal emotion recognition in conversation (MERC), the task ofidentifying the emotion label for each utterance in a conversation, is vitalfor developing empathetic machines. Current MLLM-based MERC studies focusmainly on capturing the speaker's textual or vocal characteristics, but ignorethe significance of video-derived behavior information. Different from text andaudio inputs, learning videos with rich facial expression, body language andposture, provides emotion trigger signals to the models for more accurateemotion predictions. In this paper, we propose a novel behavior-awareMLLM-based framework (BeMERC) to incorporate speaker's behaviors, includingsubtle facial micro-expression, body language and posture, into a vanillaMLLM-based MERC model, thereby facilitating the modeling of emotional dynamicsduring a conversation. Furthermore, BeMERC adopts a two-stage instructiontuning strategy to extend the model to the conversations scenario forend-to-end training of a MERC predictor. Experiments demonstrate that BeMERCachieves superior performance than the state-of-the-art methods on twobenchmark datasets, and also provides a detailed discussion on the significanceof video-derived behavior information in MERC.
  </details>

- **[AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference](http://arxiv.org/abs/2503.23956v1)**  `arXiv:2503.23956`  `cs.CV` `cs.AI`  
  _Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang_
  <details open><summary>Abstract</summary>
  Recent advancements in Large Visual Language Models (LVLMs) have gainedsignificant attention due to their remarkable reasoning capabilities andproficiency in generalization. However, processing a large number of visualtokens and generating long-context outputs impose substantial computationaloverhead, leading to excessive demands for key-value (KV) cache. To addressthis critical bottleneck, we propose AirCache, a novel KV cache compressionmethod aimed at accelerating LVLMs inference. This work systematicallyinvestigates the correlations between visual and textual tokens within theattention mechanisms of LVLMs. Our empirical analysis reveals considerableredundancy in cached visual tokens, wherein strategically eliminating thesetokens preserves model performance while significantly accelerating contextgeneration. Inspired by these findings, we introduce an elite observationwindow for assessing the importance of visual components in the KV cache,focusing on stable inter-modal relevancy modeling with enhancedmulti-perspective consistency. Additionally, we develop an adaptive layer-wisebudget allocation strategy that capitalizes on the strength and skewness oftoken importance distribution, showcasing superior efficiency compared touniform allocation. Comprehensive evaluations across multiple LVLMs andbenchmarks demonstrate that our method achieves comparable performance to thefull cache while retaining only 10% of visual KV cache, thereby reducingdecoding latency by 29% to 66% across various batch size and prompt length ofinputs. Notably, as cache retention rates decrease, our method exhibitsincreasing performance advantages over existing approaches.
  </details>

- **[HumanAesExpert: Advancing a Multi-Modality Foundation Model for Human Image Aesthetic Assessment](http://arxiv.org/abs/2503.23907v1)**  `arXiv:2503.23907`  `cs.CV` `cs.AI`  
  _Zhichao Liao, Xiaokun Liu, Wenyu Qin, Qingyu Li, Qiulin Wang, Pengfei Wan, et al._
  <details open><summary>Abstract</summary>
  Image Aesthetic Assessment (IAA) is a long-standing and challenging researchtask. However, its subset, Human Image Aesthetic Assessment (HIAA), has beenscarcely explored, even though HIAA is widely used in social media, AIworkflows, and related domains. To bridge this research gap, our work pioneersa holistic implementation framework tailored for HIAA. Specifically, weintroduce HumanBeauty, the first dataset purpose-built for HIAA, whichcomprises 108k high-quality human images with manual annotations. To achievecomprehensive and fine-grained HIAA, 50K human images are manually collectedthrough a rigorous curation process and annotated leveraging our trailblazing12-dimensional aesthetic standard, while the remaining 58K with overallaesthetic labels are systematically filtered from public datasets. Based on theHumanBeauty database, we propose HumanAesExpert, a powerful Vision LanguageModel for aesthetic evaluation of human images. We innovatively design anExpert head to incorporate human knowledge of aesthetic sub-dimensions whilejointly utilizing the Language Modeling (LM) and Regression head. This approachempowers our model to achieve superior proficiency in both overall andfine-grained HIAA. Furthermore, we introduce a MetaVoter, which aggregatesscores from all three heads, to effectively balance the capabilities of eachhead, thereby realizing improved assessment precision. Extensive experimentsdemonstrate that our HumanAesExpert models deliver significantly betterperformance in HIAA than other state-of-the-art models. Our datasets, models,and codes are publicly released to advance the HIAA community. Project webpage:https://humanaesexpert.github.io/HumanAesExpert/
  </details>

- **[Boosting MLLM Reasoning with Text-Debiased Hint-GRPO](http://arxiv.org/abs/2503.23905v1)**  `arXiv:2503.23905`  `cs.CV`  
  _Qihan Huang, Long Chan, Jinlong Liu, Wanggui He, Hao Jiang, Mingli Song, et al._
  <details open><summary>Abstract</summary>
  MLLM reasoning has drawn widespread research for its excellentproblem-solving capability. Current reasoning methods fall into two types: PRM,which supervises the intermediate reasoning steps, and ORM, which supervisesthe final results. Recently, DeepSeek-R1 has challenged the traditional viewthat PRM outperforms ORM, which demonstrates strong generalization performanceusing an ORM method (i.e., GRPO). However, current MLLM's GRPO algorithms stillstruggle to handle challenging and complex multimodal reasoning tasks (e.g.,mathematical reasoning). In this work, we reveal two problems that impede theperformance of GRPO on the MLLM: Low data utilization and Text-bias. Low datautilization refers to that GRPO cannot acquire positive rewards to update theMLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypassesimage condition and solely relies on text condition for generation after GRPOtraining. To tackle these problems, this work proposes Hint-GRPO that improvesdata utilization by adaptively providing hints for samples of varyingdifficulty, and text-bias calibration that mitigates text-bias by calibratingthe token prediction logits with image condition in test-time. Experimentresults on three base MLLMs across eleven datasets demonstrate that ourproposed methods advance the reasoning capability of original MLLM by a largemargin, exhibiting superior performance to existing MLLM reasoning methods. Ourcode is available at https://github.com/hqhQAQ/Hint-GRPO.
  </details>

- **[Communication-Efficient and Personalized Federated Foundation Model Fine-Tuning via Tri-Matrix Adaptation](http://arxiv.org/abs/2503.23869v1)**  `arXiv:2503.23869`  `cs.LG`  
  _Yongle Li, Bo Liu, Sheng Huang, ZHeng ZHang, Xiaotong Yuan, Richang Hong_
  <details open><summary>Abstract</summary>
  In federated learning, fine-tuning pre-trained foundation models posessignificant challenges, particularly regarding high communication cost andsuboptimal model performance due to data heterogeneity between the clients. Toaddress these issues, this paper introduces communication-efficient federatedLoRA adaption (CE-LoRA), a method that employs a tri-factorization low-rankadaptation approach with personalized model parameter aggregation. We firstpresents a novel LoRA parameter factorization by introducing a small-size densematrix, which can significantly reduce the communication cost and achievecomparable empirical performance than transferring the low-rank parametermatrix used by existing methods. Without violating data privacy, the serverconsiders the client similarity in both training dataset and model parameterspace, and learns personalized weights for model aggregation. Our experimentson various LLM and VLM fine-tuning tasks demonstrate that CE-LoRA not onlysignificantly reduces communication overhead but also improves performanceunder not independently and identically distributed data conditions. Inaddition, CE-LoRA improves data privacy protection, effectively mitigatinggradient-based data reconstruction attacks.
  </details>

- **[XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?](http://arxiv.org/abs/2503.23771v1)**  `arXiv:2503.23771`  `cs.CV`  
  _Fengxiang Wang, Hongzhen Wang, Mingshuo Chen, Di Wang, Yulin Wang, Zonghao Guo, et al._
  <details open><summary>Abstract</summary>
  The astonishing breakthrough of multimodal large language models (MLLMs) hasnecessitated new benchmarks to quantitatively assess their capabilities, revealtheir limitations, and indicate future research directions. However, this ischallenging in the context of remote sensing (RS), since the imagery featuresultra-high resolution that incorporates extremely complex semanticrelationships. Existing benchmarks usually adopt notably smaller image sizesthan real-world RS scenarios, suffer from limited annotation quality, andconsider insufficient dimensions of evaluation. To address these issues, wepresent XLRS-Bench: a comprehensive benchmark for evaluating the perception andreasoning capabilities of MLLMs in ultra-high-resolution RS scenarios.XLRS-Bench boasts the largest average image size (8500$\times$8500) observedthus far, with all evaluation samples meticulously annotated manually, assistedby a novel semi-automatic captioner on ultra-high-resolution RS images. On topof the XLRS-Bench, 16 sub-tasks are defined to evaluate MLLMs' 10 kinds ofperceptual capabilities and 6 kinds of reasoning capabilities, with a primaryemphasis on advanced cognitive processes that facilitate real-worlddecision-making and the capture of spatiotemporal changes. The results of bothgeneral and RS-focused MLLMs on XLRS-Bench indicate that further efforts areneeded for real-world RS applications. We have open-sourced XLRS-Bench tosupport further research in developing more powerful MLLMs for remote sensing.
  </details>

- **[Texture or Semantics? Vision-Language Models Get Lost in Font Recognition](http://arxiv.org/abs/2503.23768v1)**  `arXiv:2503.23768`  `cs.CL` `cs.CV`  
  _Zhecheng Li, Guoxian Song, Yujun Cai, Zhen Xiong, Junsong Yuan, Yiwei Wang_
  <details open><summary>Abstract</summary>
  Modern Vision-Language Models (VLMs) exhibit remarkable visual and linguisticcapabilities, achieving impressive performance in various tasks such as imagerecognition and object localization. However, their effectiveness infine-grained tasks remains an open question. In everyday scenarios, individualsencountering design materials, such as magazines, typography tutorials,research papers, or branding content, may wish to identify aestheticallypleasing fonts used in the text. Given their multimodal capabilities and freeaccessibility, many VLMs are often considered potential tools for fontrecognition. This raises a fundamental question: Do VLMs truly possess thecapability to recognize fonts? To investigate this, we introduce the FontRecognition Benchmark (FRB), a compact and well-structured dataset comprising15 commonly used fonts. FRB includes two versions: (i) an easy version, where10 sentences are rendered in different fonts, and (ii) a hard version, whereeach text sample consists of the names of the 15 fonts themselves, introducinga stroop effect that challenges model perception. Through extensive evaluationof various VLMs on font recognition tasks, we arrive at the following keyfindings: (i) Current VLMs exhibit limited font recognition capabilities, withmany state-of-the-art models failing to achieve satisfactory performance. (ii)Few-shot learning and Chain-of-Thought (CoT) prompting provide minimal benefitsin improving font recognition accuracy across different VLMs. (iii) Attentionanalysis sheds light on the inherent limitations of VLMs in capturing semanticfeatures.
  </details>

- **[STI-Bench: Are MLLMs Ready for Precise Spatial-Temporal World Understanding?](http://arxiv.org/abs/2503.23765v1)**  `arXiv:2503.23765`  `cs.CV`  
  _Yun Li, Yiming Zhang, Tao Lin, XiangRui Liu, Wenxiao Cai, Zheng Liu, et al._
  <details open><summary>Abstract</summary>
  The use of Multimodal Large Language Models (MLLMs) as an end-to-end solutionfor Embodied AI and Autonomous Driving has become a prevailing trend. WhileMLLMs have been extensively studied for visual semantic understanding tasks,their ability to perform precise and quantitative spatial-temporalunderstanding in real-world applications remains largely unexamined, leading touncertain prospects. To evaluate models' Spatial-Temporal Intelligence, weintroduce STI-Bench, a benchmark designed to evaluate MLLMs' spatial-temporalunderstanding through challenging tasks such as estimating and predicting theappearance, pose, displacement, and motion of objects. Our benchmarkencompasses a wide range of robot and vehicle operations across desktop,indoor, and outdoor scenarios. The extensive experiments reveals that thestate-of-the-art MLLMs still struggle in real-world spatial-temporalunderstanding, especially in tasks requiring precise distance estimation andmotion analysis.
  </details>

- **[AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization](http://arxiv.org/abs/2503.23733v1)**  `arXiv:2503.23733`  `cs.CL` `cs.CV`  
  _Yiyang Du, Xiaochen Wang, Chi Chen, Jiabo Ye, Yiru Wang, Peng Li, et al._
  <details open><summary>Abstract</summary>
  Recently, model merging methods have demonstrated powerful strengths incombining abilities on various tasks from multiple Large Language Models(LLMs). While previous model merging methods mainly focus on merginghomogeneous models with identical architecture, they meet challenges whendealing with Multimodal Large Language Models (MLLMs) with inherentheterogeneous property, including differences in model architecture and theasymmetry in the parameter space. In this work, we propose AdaMMS, a novelmodel merging method tailored for heterogeneous MLLMs. Our method tackles thechallenges in three steps: mapping, merging and searching. Specifically, wefirst design mapping function between models to apply model merging on MLLMswith different architecture. Then we apply linear interpolation on modelweights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally inthe hyper-parameter searching step, we propose an unsupervised hyper-parameterselection method for model merging. As the first model merging method capableof merging heterogeneous MLLMs without labeled data, extensive experiments onvarious model combinations demonstrated that AdaMMS outperforms previous modelmerging methods on various vision-language benchmarks.
  </details>

- **[KOFFVQA: An Objectively Evaluated Free-form VQA Benchmark for Large Vision-Language Models in the Korean Language](http://arxiv.org/abs/2503.23730v1)**  `arXiv:2503.23730`  `cs.CL` `cs.CV` `cs.AI`  
  _Yoonshik Kim, Jaeyoon Jung_
  <details open><summary>Abstract</summary>
  The recent emergence of Large Vision-Language Models(VLMs) has resulted in avariety of different benchmarks for evaluating such models. Despite this, weobserve that most existing evaluation methods suffer from the fact that theyeither require the model to choose from pre-determined responses, sacrificingopen-endedness, or evaluate responses using a judge model, resulting insubjective and unreliable evaluation. In addition, we observe a lack ofbenchmarks for VLMs in the Korean language, which are necessary as a separatemetric from more common English language benchmarks, as the performance ofgenerative language models can differ significantly based on the language beingused. Therefore, we present KOFFVQA, a general-purpose free-form visualquestion answering benchmark in the Korean language for the evaluation of VLMs.Our benchmark consists of 275 carefully crafted questions each paired with animage and grading criteria covering 10 different aspects of VLM performance.The grading criteria eliminate the problem of unreliability by allowing thejudge model to grade each response based on a pre-determined set of rules. Bydefining the evaluation criteria in an objective manner, even a smallopen-source model can be used to evaluate models on our benchmark reliably. Inaddition to evaluating a large number of existing VLMs on our benchmark, wealso experimentally verify that our method of using pre-existing gradingcriteria for evaluation is much more reliable than existing methods. Ourevaluation code is available at https://github.com/maum-ai/KOFFVQA
  </details>

- **[HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation](http://arxiv.org/abs/2503.23715v1)**  `arXiv:2503.23715`  `cs.CV`  
  _Kun Liu, Qi Liu, Xinchen Liu, Jie Li, Yongdong Zhang, Jiebo Luo, et al._
  <details open><summary>Abstract</summary>
  Text-to-video (T2V) generation has made tremendous progress in generatingcomplicated scenes based on texts. However, human-object interaction (HOI)often cannot be precisely generated by current T2V models due to the lack oflarge-scale videos with accurate captions for HOI. To address this issue, weintroduce HOIGen-1M, the first largescale dataset for HOI Generation,consisting of over one million high-quality videos collected from diversesources. In particular, to guarantee the high quality of videos, we firstdesign an efficient framework to automatically curate HOI videos using thepowerful multimodal large language models (MLLMs), and then the videos arefurther cleaned by human annotators. Moreover, to obtain accurate textualcaptions for HOI videos, we design a novel video description method based on aMixture-of-Multimodal-Experts (MoME) strategy that not only generatesexpressive captions but also eliminates the hallucination by individual MLLM.Furthermore, due to the lack of an evaluation framework for generated HOIvideos, we propose two new metrics to assess the quality of generated videos ina coarse-to-fine manner. Extensive experiments reveal that current T2V modelsstruggle to generate high-quality HOI videos and confirm that our HOIGen-1Mdataset is instrumental for improving HOI video generation. Project webpage isavailable at https://liuqi-creat.github.io/HOIGen.github.io.
  </details>

- **[Skip-Vision: Efficient and Scalable Acceleration of Vision-Language Models via Adaptive Token Skipping](http://arxiv.org/abs/2503.21817v2)**  `arXiv:2503.21817`  `cs.CV`  
  _Weili Zeng, Ziyuan Huang, Kaixiang Ji, Yichao Yan_
  <details open><summary>Abstract</summary>
  Transformer-based models have driven significant advancements in MultimodalLarge Language Models (MLLMs), yet their computational costs surge drasticallywhen scaling resolution, training data, and model parameters. A key bottleneckstems from the proliferation of visual tokens required for fine-grained imageunderstanding. We propose Skip-Vision, a unified framework addressing bothtraining and inference inefficiencies in vision-language models. On top ofconventional token compression approaches, our method introduces twocomplementary acceleration strategies. For training acceleration, we observethat Feed-Forward Network (FFN) computations on visual tokens induce marginalfeature updates. This motivates our Skip-FFN strategy, which bypasses FFNlayers for redundant visual tokens. For inference acceleration, we design aselective KV-cache removal mechanism that prunes the skipped key-value pairsduring decoding while preserving model performance. Experimental resultsdemonstrate that Skip-Vision reduces training time by up to 35\%, inferenceFLOPs by 75\%, and latency by 45\%, while achieving comparable or superiorperformance to existing methods. Our work provides a practical solution forscaling high-performance MLLMs with enhanced efficiency.
  </details>

- **[Mitigating Cache Noise in Test-Time Adaptation for Large Vision-Language Models](http://arxiv.org/abs/2503.18334v2)**  `arXiv:2503.18334`  `cs.CV`  
  _Haotian Zhai, Xinyu Chen, Can Zhang, Tianming Sha, Ruirui Li_
  <details open><summary>Abstract</summary>
  Test-time adaptation (TTA) of visual language models has recently attractedsignificant attention as a solution to the performance degradation caused bydistribution shifts in downstream tasks. However, existing cache-based TTAmethods have certain limitations. They mainly rely on the accuracy of cachedfeature labels, and the presence of noisy pseudo-labels can cause thesefeatures to deviate from their true distribution. This makes cache retrievalmethods based on similarity matching highly sensitive to outliers or extremesamples. Moreover, current methods lack effective mechanisms to model classdistributions, which limits their ability to fully exploit the potential ofcached information. To address these challenges, we introduce a comprehensiveand reliable caching mechanism and propose a novel zero-shot TTA method called"Cache, Residual, Gaussian" (CRG). This method not only employs learnableresidual parameters to better align positive and negative visual prototypeswith text prototypes, thereby optimizing the quality of cached features, butalso incorporates Gaussian Discriminant Analysis (GDA) to dynamically modelintra-class feature distributions, further mitigating the impact of noisyfeatures. Experimental results on 13 benchmarks demonstrate that CRGoutperforms state-of-the-art TTA methods, showcasing exceptional robustness andadaptability.
  </details>

- **[Will Pre-Training Ever End? A First Step Toward Next-Generation Foundation MLLMs via Self-Improving Systematic Cognition](http://arxiv.org/abs/2503.12303v5)**  `arXiv:2503.12303`  `cs.CV`  
  _Xiaoying Zhang, Da Peng, Yipeng Zhang, Zonghao Guo, Chengyue Wu, Chi Chen, et al._
  <details open><summary>Abstract</summary>
  Recent progress in (multimodal) large language models ((M)LLMs) has shiftedfocus from pre-training to inference-time compute scaling and post-trainingoptimization, driven by concerns over limited high-quality real-world data.However, these strategies alone are insufficient for advancing modelcapabilities. We hypothesize that effective model improvement requires a strongsynergy among pre-training, inference-time compute scaling, and post-trainingoptimization. In this paper, we validate this hypothesis in the context ofmultimodal pre-training for foundation MLLM construction. We introduceSelf-Improving cognition (SIcog), a self-learning framework for constructingnext-generation foundation MLLMs by imparting multimodal knowledge andenhancing their systematic cognitive capabilities through multimodalpre-training with self-generated data. Specifically, we introduceChain-of-Description, a step-by-step visual understanding method to improvecomprehensive perception, and integrate structured chain-of-thought (CoT)reasoning to support in-depth multimodal reasoning. SIcog first equips a basemodel with systematic perception and reasoning using minimal externalsupervision. The enhanced model then generates candidate image captions andCoT-style reasoning responses for unlabeled images and image-question pairsacross diverse tasks, which are curated through a self-consistency mechanism.These curated samples are subsequently used for large-scale multimodalpre-training, completing a self-learning cycle that strengthens the model'scognitive foundation. Extensive experiments demonstrate that SIcog producesnext-generation foundation MLLMs with substantially improved multimodalcognition, outperforming prevailing pre-training approaches. These findingsempirically establish SIcog as a promising framework for realizing a completeself-improving paradigm.
  </details>

- **[InPK: Infusing Prior Knowledge into Prompt for Vision-Language Models](http://arxiv.org/abs/2502.19777v2)**  `arXiv:2502.19777`  `cs.CV`  
  _Shuchang Zhou, Jiwei Wei, Shiyuan He, Yuyang Zhou, Chaoning Zhang, Jie Zou, et al._
  <details open><summary>Abstract</summary>
  Prompt tuning has become a popular strategy for adapting Vision-LanguageModels (VLMs) to zero/few-shot visual recognition tasks. Some promptingtechniques introduce prior knowledge due to its richness, but when learnabletokens are randomly initialized and disconnected from prior knowledge, theytend to overfit on seen classes and struggle with domain shifts for unseenones. To address this issue, we propose the InPK model, which infusesclass-specific prior knowledge into the learnable tokens during initialization,thus enabling the model to explicitly focus on class-relevant information.Furthermore, to mitigate the weakening of class information by multi-layerencoders, we continuously reinforce the interaction between learnable tokensand prior knowledge across multiple feature levels. This progressiveinteraction allows the learnable tokens to better capture the fine-graineddifferences and universal visual concepts within prior knowledge, enabling themodel to extract more discriminative and generalized text features. Even forunseen classes, the learned interaction allows the model to capture theircommon representations and infer their appropriate positions within theexisting semantic structure. Moreover, we introduce a learnable text-to-visionprojection layer to accommodate the text adjustments, ensuring better alignmentof visual-text semantics. Extensive experiments on 11 recognition datasets showthat InPK significantly outperforms state-of-the-art methods in multiplezero/few-shot image classification tasks.
  </details>

- **[Know "No'' Better: A Data-Driven Approach for Enhancing Negation Awareness in CLIP](http://arxiv.org/abs/2501.10913v2)**  `arXiv:2501.10913`  `cs.CL` `cs.CV`  
  _Junsung Park, Jungbeom Lee, Jongyoon Song, Sangwon Yu, Dahuin Jung, Sungroh Yoon_
  <details open><summary>Abstract</summary>
  While CLIP has significantly advanced multimodal understanding by bridgingvision and language, the inability to grasp negation - such as failing todifferentiate concepts like "parking" from "no parking" - poses substantialchallenges. By analyzing the data used in the public CLIP model's pre-training,we posit this limitation stems from a lack of negation-inclusive data. Toaddress this, we introduce data generation pipelines that employ a largelanguage model (LLM) and a multimodal LLM to produce negation-inclusivecaptions. Fine-tuning CLIP with data generated from our pipelines, we developNegationCLIP, which enhances negation awareness while preserving thegenerality. Moreover, to enable a comprehensive evaluation of negationunderstanding, we propose NegRefCOCOg-a benchmark tailored to test VLMs'ability to interpret negation across diverse expressions and positions within asentence. Experiments on various CLIP architectures validate the effectivenessof our data generation pipelines in enhancing CLIP's ability to perceivenegation accurately. Additionally, NegationCLIP's enhanced negation awarenesshas practical applications across various multimodal tasks, demonstrated byperformance gains in text-to-image generation and referring image segmentation.
  </details>

- **[EmoVerse: Exploring Multimodal Large Language Models for Sentiment and Emotion Understanding](http://arxiv.org/abs/2412.08049v3)**  `arXiv:2412.08049`  `cs.CL`  
  _Ao Li, Longwei Xu, Chen Ling, Jinghui Zhang, Pengwei Wang_
  <details open><summary>Abstract</summary>
  Sentiment and emotion understanding are essential to applications such ashuman-computer interaction and depression detection. While Multimodal LargeLanguage Models (MLLMs) demonstrate robust general capabilities, they faceconsiderable challenges in the field of affective computing, particularly indetecting subtle facial expressions and handling complex emotion-related tasks,such as emotion reason inference and understanding emotions in long-contextscenarios. Furthermore, there is a lack of a unified MLLM that can effectivelyhandle both sentiment and emotion-related tasks. To address these challenges,we explore multi-task training strategies for MLLMs in affective computing andintroduce Emotion Universe (EmoVerse), an MLLM designed to handle a broadspectrum of sentiment and emotion-related tasks. In addition, EmoVerse iscapable of deeply analyzing the underlying causes of emotional states. We alsointroduce the Affective Multitask (AMT) Dataset, which supports multimodalsentiment analysis, multimodal emotion recognition, facial expressionrecognition, emotion reason inference, and emotion cause-pair extraction tasks.Extensive experiments demonstrate that EmoVerse outperforms existing methods,achieving state-of-the-art results in sentiment and emotion-related tasks. Thecode is available at https://github.com/liaolea/EmoVerse.
  </details>

- **[MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models](http://arxiv.org/abs/2410.10139v2)**  `arXiv:2410.10139`  `cs.CL` `cs.CV` `cs.LG`  
  _Peng Xia, Siwei Han, Shi Qiu, Yiyang Zhou, Zhaoyang Wang, Wenhao Zheng, et al._
  <details open><summary>Abstract</summary>
  Interleaved multimodal comprehension and generation, enabling models toproduce and interpret both images and text in arbitrary sequences, have becomea pivotal area in multimodal learning. Despite significant advancements, theevaluation of this capability remains insufficient. Existing benchmarks sufferfrom limitations in data scale, scope, and evaluation depth, while currentevaluation metrics are often costly or biased, lacking in reliability forpractical applications. To address these challenges, we introduce MMIE, alarge-scale knowledge-intensive benchmark for evaluating interleaved multimodalcomprehension and generation in Large Vision-Language Models (LVLMs). MMIEcomprises 20K meticulously curated multimodal queries, spanning 3 categories,12 fields, and 102 subfields, including mathematics, coding, physics,literature, health, and arts. It supports both interleaved inputs and outputs,offering a mix of multiple-choice and open-ended question formats to evaluatediverse competencies. Moreover, we propose a reliable automated evaluationmetric, leveraging a scoring model fine-tuned with human-annotated data andsystematic evaluation criteria, aimed at reducing bias and improving evaluationaccuracy. Extensive experiments demonstrate the effectiveness of our benchmarkand metrics in providing a comprehensive evaluation of interleaved LVLMs.Specifically, we evaluate eight LVLMs, revealing that even the best models showsignificant room for improvement, with most achieving only moderate results. Webelieve MMIE will drive further advancements in the development of interleavedLVLMs. We publicly release our benchmark and code inhttps://mmie-bench.github.io/.
  </details>

- **[CASA: Class-Agnostic Shared Attributes in Vision-Language Models for Efficient Incremental Object Detection](http://arxiv.org/abs/2410.05804v3)**  `arXiv:2410.05804`  `cs.CV`  
  _Mingyi Guo, Yuyang Liu, Zhiyuan Yan, Zongying Lin, Peixi Peng, Yonghong Tian_
  <details open><summary>Abstract</summary>
  Incremental object detection is fundamentally challenged by catastrophicforgetting. A major factor contributing to this issue is background shift,where background categories in sequential tasks may overlap with eitherpreviously learned or future unseen classes. To address this, we propose anovel method called Class-Agnostic Shared Attribute Base (CASA) that encouragesthe model to learn category-agnostic attributes shared across incrementalclasses. Our approach leverages an LLM to generate candidate textualattributes, selects the most relevant ones based on the current training data,and records their importance in an assignment matrix. For subsequent tasks, theretained attributes are frozen, and new attributes are selected from theremaining candidates, ensuring both knowledge retention and adaptability.Extensive experiments on the COCO dataset demonstrate the state-of-the-artperformance of our method.
  </details>

- **[Cropper: Vision-Language Model for Image Cropping through In-Context Learning](http://arxiv.org/abs/2408.07790v2)**  `arXiv:2408.07790`  `cs.CV`  
  _Seung Hyun Lee, Jijun Jiang, Yiran Xu, Zhuofang Li, Junjie Ke, Yinxiao Li, et al._
  <details open><summary>Abstract</summary>
  The goal of image cropping is to identify visually appealing crops in animage. Conventional methods are trained on specific datasets and fail to adaptto new requirements. Recent breakthroughs in large vision-language models(VLMs) enable visual in-context learning without explicit training. However,downstream tasks with VLMs remain under explored. In this paper, we propose aneffective approach to leverage VLMs for image cropping. First, we propose anefficient prompt retrieval mechanism for image cropping to automate theselection of in-context examples. Second, we introduce an iterative refinementstrategy to iteratively enhance the predicted crops. The proposed framework, werefer to as Cropper, is applicable to a wide range of cropping tasks, includingfree-form cropping, subject-aware cropping, and aspect ratio-aware cropping.Extensive experiments demonstrate that Cropper significantly outperformsstate-of-the-art methods across several benchmarks.
  </details>
