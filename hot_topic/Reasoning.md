# üîç Reasoning Papers ¬∑ 2025-03-27

[![Total Papers](https://img.shields.io/badge/Papers-56-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Reasoning` `System 2` `Slow Thinking` `o1` `r1`  
**Filter**: `None`

---

## üìö Paper List

- **[Video-R1: Reinforcing Video Reasoning in MLLMs](http://arxiv.org/abs/2503.21776v1)**  `arXiv:2503.21776`  `cs.CV`  
  _Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, et al._
  <details open><summary>Abstract</summary>
  Inspired by DeepSeek-R1's success in eliciting reasoning abilities throughrule-based reinforcement learning (RL), we introduce Video-R1 as the firstattempt to systematically explore the R1 paradigm for eliciting video reasoningwithin multimodal large language models (MLLMs). However, directly applying RLtraining with the GRPO algorithm to video reasoning presents two primarychallenges: (i) a lack of temporal modeling for video reasoning, and (ii) thescarcity of high-quality video-reasoning data. To address these issues, wefirst propose the T-GRPO algorithm, which encourages models to utilize temporalinformation in videos for reasoning. Additionally, instead of relying solely onvideo data, we incorporate high-quality image-reasoning data into the trainingprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold startand Video-R1-260k for RL training, both comprising image and video data.Experimental results demonstrate that Video-R1 achieves significantimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, aswell as on general video benchmarks including MVBench and TempCompass, etc.Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoningbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. Allcodes, models, data are released.
  </details>

- **[VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness](http://arxiv.org/abs/2503.21755v1)**  `arXiv:2503.21755`  `cs.CV`  
  _Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, et al._
  <details open><summary>Abstract</summary>
  Video generation has advanced significantly, evolving from producingunrealistic outputs to generating videos that appear visually convincing andtemporally coherent. To evaluate these video generative models, benchmarks suchas VBench have been developed to assess their faithfulness, measuring factorslike per-frame aesthetics, temporal consistency, and basic prompt adherence.However, these aspects mainly represent superficial faithfulness, which focuson whether the video appears visually convincing rather than whether it adheresto real-world principles. While recent models perform increasingly well onthese metrics, they still struggle to generate videos that are not justvisually plausible but fundamentally realistic. To achieve real "world models"through video generation, the next frontier lies in intrinsic faithfulness toensure that generated videos adhere to physical laws, commonsense reasoning,anatomical correctness, and compositional integrity. Achieving this level ofrealism is essential for applications such as AI-assisted filmmaking andsimulated world modeling. To bridge this gap, we introduce VBench-2.0, anext-generation benchmark designed to automatically evaluate video generativemodels for their intrinsic faithfulness. VBench-2.0 assesses five keydimensions: Human Fidelity, Controllability, Creativity, Physics, andCommonsense, each further broken down into fine-grained capabilities. Tailoredfor individual dimensions, our evaluation framework integrates generalists suchas state-of-the-art VLMs and LLMs, and specialists, including anomaly detectionmethods proposed for video generation. We conduct extensive annotations toensure alignment with human judgment. By pushing beyond superficialfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a newstandard for the next generation of video generative models in pursuit ofintrinsic faithfulness.
  </details>

- **[LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis](http://arxiv.org/abs/2503.21749v1)**  `arXiv:2503.21749`  `cs.CV`  
  _Shitian Zhao, Qilong Wu, Xinyue Li, Bo Zhang, Ming Li, Qi Qin, et al._
  <details open><summary>Abstract</summary>
  We introduce LeX-Art, a comprehensive suite for high-quality text-imagesynthesis that systematically bridges the gap between prompt expressiveness andtext rendering fidelity. Our approach follows a data-centric paradigm,constructing a high-quality data synthesis pipeline based on Deepseek-R1 tocurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined1024$\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer,a robust prompt enrichment model, and train two text-to-image models, LeX-FLUXand LeX-Lumina, achieving state-of-the-art text rendering performance. Tosystematically evaluate visual text generation, we introduce LeX-Bench, abenchmark that assesses fidelity, aesthetics, and alignment, complemented byPairwise Normalized Edit Distance (PNED), a novel metric for robust textaccuracy evaluation. Experiments demonstrate significant improvements, withLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUXoutperforming baselines in color (+3.18%), positional (+4.45%), and fontaccuracy (+3.81%). Our codes, models, datasets, and demo are publiclyavailable.
  </details>

- **[ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation](http://arxiv.org/abs/2503.21729v1)**  `arXiv:2503.21729`  `cs.AI` `cs.CL`  
  _Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, et al._
  <details open><summary>Abstract</summary>
  Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but relyprimarily on parametric knowledge, limiting factual accuracy. While recentworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,they suffer from overthinking and lack robustness in reasoning, reducing theireffectiveness in question answering (QA) tasks. To address this, we proposeReaRAG, a factuality-enhanced reasoning model that explores diverse querieswithout excessive iterations. Our solution includes a novel data constructionframework with an upper bound on the reasoning chain length. Specifically, wefirst leverage an LRM to generate deliberate thinking, then select an actionfrom a predefined action space (Search and Finish). For Search action, a queryis executed against the RAG engine, where the result is returned as observationto guide reasoning steps later. This process iterates until a Finish action ischosen. Benefiting from ReaRAG's strong reasoning capabilities, our approachoutperforms existing baselines on multi-hop QA. Further analysis highlights itsstrong reflective ability to recognize errors and refine its reasoningtrajectory. Our study enhances LRMs' factuality while effectively integratingrobust reasoning for Retrieval-Augmented Generation (RAG).
  </details>

- **[CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?](http://arxiv.org/abs/2503.21717v1)**  `arXiv:2503.21717`  `cs.CL`  
  _Jiefu Ou, William Gantt Walden, Kate Sanders, Zhengping Jiang, Kaiser Sun, Jeffrey Cheng, et al._
  <details open><summary>Abstract</summary>
  A core part of scientific peer review involves providing expert critiquesthat directly assess the scientific claims a paper makes. While it is nowpossible to automatically generate plausible (if generic) reviews, ensuringthat these reviews are sound and grounded in the papers' claims remainschallenging. To facilitate LLM benchmarking on these challenges, we introduceCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions andreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts forweakness statements in the reviews and the paper claims that they dispute, aswell as fine-grained labels of the validity, objectivity, and type of theidentified weaknesses. We benchmark several LLMs on three claim-centric taskssupported by CLAIMCHECK, requiring models to (1) associate weaknesses with theclaims they dispute, (2) predict fine-grained labels for weaknesses and rewritethe weaknesses to enhance their specificity, and (3) verify a paper's claimswith grounded reasoning. Our experiments reveal that cutting-edge LLMs, whilecapable of predicting weakness labels in (2), continue to underperform relativeto human experts on all other tasks.
  </details>

- **[Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks](http://arxiv.org/abs/2503.21696v1)**  `arXiv:2503.21696`  `cs.CL` `cs.CV`  
  _Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, et al._
  <details open><summary>Abstract</summary>
  Recent advances in deep thinking models have demonstrated remarkablereasoning capabilities on mathematical and coding tasks. However, theireffectiveness in embodied domains which require continuous interaction withenvironments through image action interleaved trajectories remains largely-unexplored. We present Embodied Reasoner, a model that extends o1 stylereasoning to interactive embodied search tasks. Unlike mathematical reasoningthat relies primarily on logical deduction, embodied scenarios demand spatialunderstanding, temporal reasoning, and ongoing self-reflection based oninteraction history. To address these challenges, we synthesize 9.3k coherentObservation-Thought-Action trajectories containing 64k interactive images and90k diverse thinking processes (analysis, spatial reasoning, reflection,planning, and verification). We develop a three-stage training pipeline thatprogressively enhances the model's capabilities through imitation learning,self-exploration via rejection sampling, and self-correction through reflectiontuning. The evaluation shows that our model significantly outperforms thoseadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, andClaude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewerrepeated searches and logical inconsistencies, with particular advantages incomplex long-horizon tasks. Real-world environments also show our superioritywhile exhibiting fewer repeated searches and logical inconsistency cases.
  </details>

- **[UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning](http://arxiv.org/abs/2503.21620v1)**  `arXiv:2503.21620`  `cs.AI`  
  _Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, et al._
  <details open><summary>Abstract</summary>
  The recent DeepSeek-R1 has showcased the emergence of reasoning capabilitiesin LLMs through reinforcement learning (RL) with rule-based rewards. Buildingon this idea, we are the first to explore how rule-based RL can enhance thereasoning capabilities of multimodal large language models (MLLMs) for graphicuser interface (GUI) action prediction tasks. To this end, we curate a smallyet high-quality dataset of 136 challenging tasks, encompassing five commonaction types on mobile devices. We also introduce a unified rule-based actionreward, enabling model optimization via policy-based algorithms such as GroupRelative Policy Optimization (GRPO). Experimental results demonstrate that ourproposed data-efficient model, UI-R1-3B, achieves substantial improvements onboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the IDbenchmark AndroidControl, the action type accuracy improves by 15%, whilegrounding accuracy increases by 10.3%, compared with the base model (i.e.Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our modelsurpasses the base model by 6.0% and achieves competitive performance withlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning(SFT) on 76K data. These results underscore the potential of rule-basedreinforcement learning to advance GUI understanding and control, paving the wayfor future research in this domain.
  </details>

- **[A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond](http://arxiv.org/abs/2503.21614v1)**  `arXiv:2503.21614`  `cs.CL`  
  _Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, et al._
  <details open><summary>Abstract</summary>
  Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, havedemonstrated strong performance gains by scaling up the length ofChain-of-Thought (CoT) reasoning during inference. However, a growing concernlies in their tendency to produce excessively long reasoning traces, which areoften filled with redundant content (e.g., repeated definitions), over-analysisof simple problems, and superficial exploration of multiple reasoning paths forharder tasks. This inefficiency introduces significant challenges for training,inference, and real-world deployment (e.g., in agent-based systems), wheretoken economy is critical. In this survey, we provide a comprehensive overviewof recent efforts aimed at improving reasoning efficiency in LRMs, with aparticular focus on the unique challenges that arise in this new paradigm. Weidentify common patterns of inefficiency, examine methods proposed across theLRM lifecycle, i.e., from pretraining to inference, and discuss promisingfuture directions for research. To support ongoing development, we alsomaintain a real-time GitHub repository tracking recent progress in the field.We hope this survey serves as a foundation for further exploration and inspiresinnovation in this rapidly evolving area.
  </details>

- **[GenEdit: Compounding Operators and Continuous Improvement to Tackle Text-to-SQL in the Enterprise](http://arxiv.org/abs/2503.21602v1)**  `arXiv:2503.21602`  `cs.AI`  
  _Karime Maamari, Connor Landy, Amine Mhedhbi_
  <details open><summary>Abstract</summary>
  Recent advancements in Text-to-SQL, driven by large language models, aredemocratizing data access. Despite these advancements, enterprise deploymentsremain challenging due to the need to capture business-specific knowledge,handle complex queries, and meet expectations of continuous improvements. Toaddress these issues, we designed and implemented GenEdit: our Text-to-SQLgeneration system that improves with user feedback. GenEdit builds andmaintains a company-specific knowledge set, employs a pipeline of operatorsdecomposing SQL generation, and uses feedback to update its knowledge set toimprove future SQL generations.  We describe GenEdit's architecture made of two core modules: (i) decomposedSQL generation; and (ii) knowledge set edits based on user feedback. Forgeneration, GenEdit leverages compounding operators to improve knowledgeretrieval and to create a plan as chain-of-thought steps that guidesgeneration. GenEdit first retrieves relevant examples in an initial retrievalstage where original SQL queries are decomposed into sub-statements, clauses orsub-queries. It then also retrieves instructions and schema elements. Using theretrieved contextual information, GenEdit then generates step-by-step plan innatural language on how to produce the query. Finally, GenEdit uses the plan togenerate SQL, minimizing the need for model reasoning, which enhances complexSQL generation. If necessary, GenEdit regenerates the query based on syntacticand semantic errors. The knowledge set edits are recommended through aninteractive copilot, allowing users to iterate on their feedback and toregenerate SQL queries as needed. Each generation uses staged edits whichupdate the generation prompt. Once the feedback is submitted, it gets mergedafter passing regression testing and obtaining an approval, improving futuregenerations.
  </details>

- **[Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving](http://arxiv.org/abs/2503.21505v1)**  `arXiv:2503.21505`  `cs.CL` `cs.CV`  
  _Yue Li, Meng Tian, Zhenyu Lin, Jiangtong Zhu, Dechang Zhu, Haiqiang Liu, et al._
  <details open><summary>Abstract</summary>
  Existing benchmarks for Vision-Language Model (VLM) on autonomous driving(AD) primarily assess interpretability through open-form visual questionanswering (QA) within coarse-grained tasks, which remain insufficient to assesscapabilities in complex driving scenarios. To this end, we introduce$\textbf{VLADBench}$, a challenging and fine-grained dataset featuringclose-form QAs that progress from static foundational knowledge and elements toadvanced reasoning for dynamic on-road situations. The elaborate$\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,General Element Recognition, Traffic Graph Generation, Target AttributeComprehension, and Ego Decision-Making and Planning. These domains are furtherbroken down into 11 secondary aspects and 29 tertiary tasks for a granularevaluation. A thorough assessment of general and domain-specific (DS) VLMs onthis benchmark reveals both their strengths and critical limitations in ADcontexts. To further exploit the cognitive and reasoning interactions among the5 domains for AD understanding, we start from a small-scale VLM and train theDS models on individual domain datasets (collected from 1.4M DS QAs acrosspublic sources). The experimental results demonstrate that the proposedbenchmark provides a crucial step toward a more comprehensive assessment ofVLMs in AD, paving the way for the development of more cognitivelysophisticated and reasoning-capable AD systems.
  </details>

- **[OpenHuEval: Evaluating Large Language Model on Hungarian Specifics](http://arxiv.org/abs/2503.21500v1)**  `arXiv:2503.21500`  `cs.CL`  
  _Haote Yang, Xingjian Wei, Jiang Wu, No√©mi Ligeti-Nagy, Jiaxing Sun, Yinfan Wang, et al._
  <details open><summary>Abstract</summary>
  We introduce OpenHuEval, the first benchmark for LLMs focusing on theHungarian language and specifics. OpenHuEval is constructed from a vastcollection of Hungarian-specific materials sourced from multiple origins. Inthe construction, we incorporated the latest design principles for evaluatingLLMs, such as using real user queries from the internet, emphasizing theassessment of LLMs' generative capabilities, and employing LLM-as-judge toenhance the multidimensionality and accuracy of evaluations. Ultimately,OpenHuEval encompasses eight Hungarian-specific dimensions, featuring fivetasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive,in-depth, and scientifically accurate assessment of LLM performance in thecontext of the Hungarian language and its specifics. We evaluated currentmainstream LLMs, including both traditional LLMs and recently developed LargeReasoning Models. The results demonstrate the significant necessity forevaluation and model optimization tailored to the Hungarian language andspecifics. We also established the framework for analyzing the thinkingprocesses of LRMs with OpenHuEval, revealing intrinsic patterns and mechanismsof these models in non-English languages, with Hungarian serving as arepresentative example. We will release OpenHuEval athttps://github.com/opendatalab/OpenHuEval .
  </details>

- **[OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs](http://arxiv.org/abs/2503.21480v1)**  `arXiv:2503.21480`  `cs.CL`  
  _John Murzaku, Owen Rambow_
  <details open><summary>Abstract</summary>
  The use of omni-LLMs (large language models that accept any modality asinput), particularly for multimodal cognitive state tasks involving speech, isunderstudied. We present OmniVox, the first systematic evaluation of fouromni-LLMs on the zero-shot emotion recognition task. We evaluate on two widelyused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shotomni-LLMs outperform or are competitive with fine-tuned audio models. Alongsideour audio-only evaluation, we also evaluate omni-LLMs on text only and text andaudio. We present acoustic prompting, an audio-specific prompting strategy foromni-LLMs which focuses on acoustic feature analysis, conversation contextanalysis, and step-by-step reasoning. We compare our acoustic prompting tominimal prompting and full chain-of-thought prompting techniques. We perform acontext window analysis on IEMOCAP and MELD, and find that using context helps,especially on IEMOCAP. We conclude with an error analysis on the generatedacoustic reasoning outputs from the omni-LLMs.
  </details>

- **[Fine-Grained Behavior and Lane Constraints Guided Trajectory Prediction Method](http://arxiv.org/abs/2503.21477v1)**  `arXiv:2503.21477`  `cs.CV`  
  _Wenyi Xiong, Jian Chen, Ziheng Qi_
  <details open><summary>Abstract</summary>
  Trajectory prediction, as a critical component of autonomous driving systems,has attracted the attention of many researchers. Existing prediction algorithmsfocus on extracting more detailed scene features or selecting more reasonabletrajectory destinations. However, in the face of dynamic and evolving futuremovements of the target vehicle, these algorithms cannot provide a fine-grainedand continuous description of future behaviors and lane constraints, whichdegrades the prediction accuracy. To address this challenge, we present BLNet,a novel dualstream architecture that synergistically integrates behavioralintention recognition and lane constraint modeling through parallel attentionmechanisms. The framework generates fine-grained behavior state queries(capturing spatial-temporal movement patterns) and lane queries (encoding lanetopology constraints), supervised by two auxiliary losses, respectively.Subsequently, a two-stage decoder first produces trajectory proposals, thenperforms point-level refinement by jointly incorporating both the continuity ofpassed lanes and future motion features. Extensive experiments on two largedatasets, nuScenes and Argoverse, show that our network exhibits significantperformance gains over existing direct regression and goal-based algorithms.
  </details>

- **[Graph-to-Vision: Multi-graph Understanding and Reasoning using Vision-Language Models](http://arxiv.org/abs/2503.21435v1)**  `arXiv:2503.21435`  `cs.AI`  
  _Ruizhou Li, Haiyun Jiang_
  <details open><summary>Abstract</summary>
  Graph Neural Networks (GNNs), as the dominant paradigm for graph-structuredlearning, have long faced dual challenges of exponentially escalatingcomputational complexity and inadequate cross-scenario generalizationcapability. With the rapid advancement of multimodal learning, Vision-LanguageModels (VLMs) have demonstrated exceptional cross-modal relational reasoningcapabilities and generalization capacities, thereby opening up novel pathwaysfor overcoming the inherent limitations of conventional graph learningparadigms. However, current research predominantly concentrates oninvestigating the single-graph reasoning capabilities of VLMs, whichfundamentally fails to address the critical requirement for coordinatedreasoning across multiple heterogeneous graph data in real-world applicationscenarios. To address these limitations, we propose the first multi-graph jointreasoning benchmark for VLMs. Our benchmark encompasses four graph categories:knowledge graphs, flowcharts, mind maps, and route maps,with each graph groupaccompanied by three progressively challenging instruction-response pairs.Leveraging this benchmark, we conducted comprehensive capability assessments ofstate-of-the-art VLMs and performed fine-tuning on open-source models. Thisstudy not only addresses the underexplored evaluation gap in multi-graphreasoning for VLMs but also empirically validates their generalizationsuperiority in graph-structured learning.
  </details>

- **[Federated Intelligence: When Large AI Models Meet Federated Fine-Tuning and Collaborative Reasoning at the Network Edge](http://arxiv.org/abs/2503.21412v1)**  `arXiv:2503.21412`  `cs.AI`  
  _Wanli Ni, Haofeng Sun, Huiqing Ao, Hui Tian_
  <details open><summary>Abstract</summary>
  Large artificial intelligence (AI) models exhibit remarkable capabilities invarious application scenarios, but deploying them at the network edge posessignificant challenges due to issues such as data privacy, computationalresources, and latency. In this paper, we explore federated fine-tuning andcollaborative reasoning techniques to facilitate the implementation of large AImodels in resource-constrained wireless networks. Firstly, promisingapplications of large AI models within specific domains are discussed.Subsequently, federated fine-tuning methods are proposed to adapt large AImodels to specific tasks or environments at the network edge, effectivelyaddressing the challenges associated with communication overhead and enhancingcommunication efficiency. These methodologies follow clustered, hierarchical,and asynchronous paradigms to effectively tackle privacy issues and eliminatedata silos. Furthermore, to enhance operational efficiency and reduce latency,efficient frameworks for model collaborative reasoning are developed, whichinclude decentralized horizontal collaboration, cloud-edge-end verticalcollaboration, and multi-access collaboration. Next, simulation resultsdemonstrate the effectiveness of our proposed methods in reducing thefine-tuning loss of large AI models across various downstream tasks. Finally,several open challenges and research opportunities are outlined.
  </details>

- **[Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap](http://arxiv.org/abs/2503.21411v1)**  `arXiv:2503.21411`  `cs.AI`  
  _Tong Nie, Jian Sun, Wei Ma_
  <details open><summary>Abstract</summary>
  Modern transportation systems face pressing challenges due to increasingdemand, dynamic environments, and heterogeneous information integration. Therapid evolution of Large Language Models (LLMs) offers transformative potentialto address these challenges. Extensive knowledge and high-level capabilitiesderived from pretraining evolve the default role of LLMs as text generators tobecome versatile, knowledge-driven task solvers for intelligent transportationsystems. This survey first presents LLM4TR, a novel conceptual framework thatsystematically categorizes the roles of LLMs in transportation into foursynergetic dimensions: information processors, knowledge encoders, componentgenerators, and decision facilitators. Through a unified taxonomy, wesystematically elucidate how LLMs bridge fragmented data pipelines, enhancepredictive analytics, simulate human-like reasoning, and enable closed-loopinteractions across sensing, learning, modeling, and managing tasks intransportation systems. For each role, our review spans diverse applications,from traffic prediction and autonomous driving to safety analytics and urbanmobility optimization, highlighting how emergent capabilities of LLMs such asin-context learning and step-by-step reasoning can enhance the operation andmanagement of transportation systems. We further curate practical guidance,including available resources and computational guidelines, to supportreal-world deployment. By identifying challenges in existing LLM-basedsolutions, this survey charts a roadmap for advancing LLM-driven transportationresearch, positioning LLMs as central actors in the next generation ofcyber-physical-social mobility ecosystems. Online resources can be found in theproject page: https://github.com/tongnie/awesome-llm4tr.
  </details>

- **[Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models](http://arxiv.org/abs/2503.21380v1)**  `arXiv:2503.21380`  `cs.CL`  
  _Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, et al._
  <details open><summary>Abstract</summary>
  In recent years, the rapid development of large reasoning models has resultedin the saturation of existing benchmarks for evaluating mathematical reasoning,highlighting the urgent need for more challenging and rigorous evaluationframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-levelmathematical benchmark, designed to rigorously test the complex reasoningcapabilities of LLMs. OlymMATH features 200 meticulously curated problems, eachmanually verified and available in parallel English and Chinese versions. Theproblems are systematically organized into two distinct difficulty tiers: (1)AIME-level problems (easy) that establish a baseline for mathematical reasoningassessment, and (2) significantly more challenging problems (hard) designed topush the boundaries of current state-of-the-art models. In our benchmark, theseproblems span four core mathematical fields, each including a verifiablenumerical solution to enable objective, rule-based evaluation. Empiricalresults underscore the significant challenge presented by OlymMATH, withstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-minidemonstrating notably limited accuracy on the hard subset. Furthermore, thebenchmark facilitates comprehensive bilingual assessment of mathematicalreasoning abilities-a critical dimension that remains largely unaddressed inmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmarkat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.
  </details>

- **[ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback](http://arxiv.org/abs/2503.21332v1)**  `arXiv:2503.21332`  `cs.AI` `cs.CL`  
  _Taewon Yun, Jihwan Oh, Hyangsuk Min, Yuho Lee, Jihwan Bang, Jason Cai, et al._
  <details open><summary>Abstract</summary>
  Summarization refinement faces challenges when extending to multi-dimension.In this paper, we introduce ReFeed, a powerful summarization refinementpipeline that enhances multiple dimensions through reflective reasoning onfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-baseddataset optimized for training a lightweight model with reflective reasoning.Our experiments reveal how the number of dimensions, feedback exposure, andreasoning policy influence refinement performance, highlighting reflectivereasoning and simultaneously addressing multiple feedback is crucial tomitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisyfeedback and feedback order. Lastly, our finding emphasizes that creating datawith a proper goal and guideline constitutes a fundamental pillar of effectivereasoning. The dataset and model will be released.
  </details>

- **[R-PRM: Reasoning-Driven Process Reward Modeling](http://arxiv.org/abs/2503.21295v1)**  `arXiv:2503.21295`  `cs.CL`  
  _Shuaijie She, Junxiao Liu, Yifeng Liu, Jiajun Chen, Xin Huang, Shujian Huang_
  <details open><summary>Abstract</summary>
  Large language models (LLMs) inevitably make mistakes when performingstep-by-step mathematical reasoning. Process Reward Models (PRMs) have emergedas a promising solution by evaluating each reasoning step. However, existingPRMs typically output evaluation scores directly, limiting both learningefficiency and evaluation accuracy, which is further exacerbated by thescarcity of annotated data. To address these issues, we proposeReasoning-Driven Process Reward Modeling (R-PRM). First, we leverage strongerLLMs to generate seed data from limited annotations, effectively bootstrappingour model's reasoning capabilities and enabling comprehensive step-by-stepevaluation. Second, we further enhance performance through preferenceoptimization, without requiring additional annotated data. Third, we introduceinference-time scaling to fully harness the model's reasoning potential.Extensive experiments demonstrate R-PRM's effectiveness: on ProcessBench andPRMBench, it surpasses strong baselines by 11.9 and 8.5 points in F1 scores,respectively. When applied to guide mathematical reasoning, R-PRM achievesconsistent accuracy improvements of over 8.5 points across six challengingdatasets. Further analysis reveals that R-PRM exhibits more comprehensiveevaluation and stronger generalization capabilities, thereby highlighting itssignificant potential.
  </details>

- **[Cultivating Game Sense for Yourself: Making VLMs Gaming Experts](http://arxiv.org/abs/2503.21263v1)**  `arXiv:2503.21263`  `cs.CL`  
  _Wenxuan Lu, Jiangyang He, Zhanqiu Zhang, Yiwen Guo, Tianning Zang_
  <details open><summary>Abstract</summary>
  Developing agents capable of fluid gameplay in first/third-person gameswithout API access remains a critical challenge in Artificial GeneralIntelligence (AGI). Recent efforts leverage Vision Language Models (VLMs) asdirect controllers, frequently pausing the game to analyze screens and planaction through language reasoning. However, this inefficient paradigmfundamentally restricts agents to basic and non-fluent interactions: relying onisolated VLM reasoning for each action makes it impossible to handle tasksrequiring high reactivity (e.g., FPS shooting) or dynamic adaptability (e.g.,ACT combat). To handle this, we propose a paradigm shift in gameplay agentdesign: instead of directly controlling gameplay, VLM develops specializedexecution modules tailored for tasks like shooting and combat. These moduleshandle real-time game interactions, elevating VLM to a high-level developer.Building upon this paradigm, we introduce GameSense, a gameplay agent frameworkwhere VLM develops task-specific game sense modules by observing task executionand leveraging vision tools and neural network training pipelines. Thesemodules encapsulate action-feedback logic, ranging from direct action rules toneural network-based decisions. Experiments demonstrate that our framework isthe first to achieve fluent gameplay in diverse genres, including ACT, FPS, andFlappy Bird, setting a new benchmark for game-playing agents.
  </details>

- **[Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning](http://arxiv.org/abs/2503.21258v1)**  `arXiv:2503.21258`  `cs.AI` `cs.CV`  
  _Jizhou Han, Chenhao Ding, Yuhang He, Songlin Dong, Qiang Wang, Xinyuan Gao, et al._
  <details open><summary>Abstract</summary>
  Few-shot class-incremental Learning (FSCIL) enables models to learn newclasses from limited data while retaining performance on previously learnedclasses. Traditional FSCIL methods often require fine-tuning parameters withlimited new class data and suffer from a separation between learning newclasses and utilizing old knowledge. Inspired by the analogical learningmechanisms of the human brain, we propose a novel analogical generative method.Our approach includes the Brain-Inspired Analogical Generator (BiAG), whichderives new class weights from existing classes without parameter fine-tuningduring incremental stages. BiAG consists of three components: WeightSelf-Attention Module (WSA), Weight & Prototype Analogical Attention Module(WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theoryfor semantic conversion, WSA supplements new class weights, and WPAA computesanalogies to generate new class weights. Experiments on miniImageNet, CUB-200,and CIFAR-100 datasets demonstrate that our method achieves higher final andaverage accuracy compared to SOTA methods.
  </details>

- **[LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models](http://arxiv.org/abs/2503.21227v1)**  `arXiv:2503.21227`  `cs.CL`  
  _Hengyuan Zhao, Ziqin Wang, Qixin Sun, Kaiyou Song, Yilin Li, Xiaolin Hu, et al._
  <details open><summary>Abstract</summary>
  Although applying Mixture of Experts to large language models for learningnew tasks is widely regarded as an effective strategy for continuous learning,there still remain two major challenges: (1) As the number of tasks grows,simple parameter expansion strategies can lead to excessively large models. (2)Modifying the parameters of the existing router results in the erosion ofpreviously acquired knowledge. In this paper, we present an innovativeframework named LLaVA-CMoE, which is a continuous Mixture of Experts (MoE)architecture without any replay data. Specifically, we have developed a methodcalled Probe-Guided Knowledge Extension (PGKE), which employs probe experts toassess whether additional knowledge is required for a specific layer. Thisapproach enables the model to adaptively expand its network parameters based ontask distribution, thereby significantly improving the efficiency of parameterexpansion. Additionally, we introduce a hierarchical routing algorithm calledProbabilistic Task Locator (PTL), where high-level routing captures inter-taskinformation and low-level routing focuses on intra-task details, ensuring thatnew task experts do not interfere with existing ones. Our experiments showsthat our efficient architecture has substantially improved model performance onthe Coin benchmark while maintaining a reasonable parameter count.
  </details>

- **[Rethinking Graph Structure Learning in the Era of LLMs](http://arxiv.org/abs/2503.21223v1)**  `arXiv:2503.21223`  `cs.LG`  
  _Zhihan Zhang, Xunkai Li, Guang Zeng, Hongchao Qin, Ronghua Li, Guoren Wang_
  <details open><summary>Abstract</summary>
  Recently, the emergence of large language models (LLMs) has promptedresearchers to explore the integration of language descriptions into graphs,aiming to enhance model encoding capabilities from a data-centric perspective.This graph representation is called text-attributed graphs (TAGs). A review ofprior advancements highlights that graph structure learning (GSL) is a pivotaltechnique for improving data utility, making it highly relevant to efficientTAG learning. However, most GSL methods are tailored for traditional graphswithout textual information, underscoring the necessity of developing a new GSLparadigm. Despite clear motivations, it remains challenging: (1) How can wedefine a reasonable optimization objective for GSL in the era of LLMs,considering the massive parameters in LLM? (2) How can we design an efficientmodel architecture that enables seamless integration of LLM for thisoptimization objective? For Question 1, we reformulate existing GSLoptimization objectives as a tree optimization framework, shifting the focusfrom obtaining a well-trained edge predictor to a language-aware tree sampler.For Question 2, we propose decoupled and training-free model design principlesfor LLM integration, shifting the focus from computation-intensive fine-tuningto more efficient inference. Based on this, we propose Large Language and TreeAssistant (LLaTA), which leverages tree-based LLM in-context learning toenhance the understanding of topology and text, enabling reliable inference andgenerating improved graph structure. Extensive experiments on 10 TAG datasetsdemonstrate that LLaTA enjoys flexibility - incorporated with any backbone;scalability - outperforms other LLM-based GSL methods in terms of runningefficiency; effectiveness - achieves SOTA performance.
  </details>

- **[FakeReasoning: Towards Generalizable Forgery Detection and Reasoning](http://arxiv.org/abs/2503.21210v1)**  `arXiv:2503.21210`  `cs.CV`  
  _Yueying Gao, Dongliang Chang, Bingyao Yu, Haotian Qin, Lei Chen, Kongming Liang, et al._
  <details open><summary>Abstract</summary>
  Accurate and interpretable detection of AI-generated images is essential formitigating risks associated with AI misuse. However, the substantial domain gapamong generative models makes it challenging to develop a generalizable forgerydetection model. Moreover, since every pixel in an AI-generated image issynthesized, traditional saliency-based forgery explanation methods are notwell suited for this task. To address these challenges, we propose modelingAI-generated image detection and explanation as a Forgery Detection andReasoning task (FDR-Task), leveraging vision-language models (VLMs) to provideaccurate detection through structured and reliable reasoning over forgeryattributes. To facilitate this task, we introduce the Multi-Modal ForgeryReasoning dataset (MMFR-Dataset), a large-scale dataset containing 100K imagesacross 10 generative models, with 10 types of forgery reasoning annotations,enabling comprehensive evaluation of FDR-Task. Additionally, we proposeFakeReasoning, a forgery detection and reasoning framework with two keycomponents. First, Forgery-Aligned Contrastive Learning enhances VLMs'understanding of forgery-related semantics through both cross-modal andintra-modal contrastive learning between images and forgery attributereasoning. Second, a Classification Probability Mapper bridges the optimizationgap between forgery detection and language modeling by mapping the outputlogits of VLMs to calibrated binary classification probabilities. Experimentsacross multiple generative models demonstrate that FakeReasoning not onlyachieves robust generalization but also outperforms state-of-the-art methods onboth detection and reasoning tasks.
  </details>

- **[Embedding Domain-Specific Knowledge from LLMs into the Feature Engineering Pipeline](http://arxiv.org/abs/2503.21155v1)**  `arXiv:2503.21155`  `cs.LG`  
  _Jo√£o Eduardo Batista_
  <details open><summary>Abstract</summary>
  Feature engineering is mandatory in the machine learning pipeline to obtainrobust models. While evolutionary computation is well-known for its greatresults both in feature selection and feature construction, its methods arecomputationally expensive due to the large number of evaluations required toinduce the final model. Part of the reason why these algorithms require a largenumber of evaluations is their lack of domain-specific knowledge, resulting ina lot of random guessing during evolution. In this work, we propose using LargeLanguage Models (LLMs) as an initial feature construction step to add knowledgeto the dataset. By doing so, our results show that the evolution can convergefaster, saving us computational resources. The proposed approach only providesthe names of the features in the dataset and the target objective to the LLM,making it usable even when working with datasets containing private data. Whileconsistent improvements to test performance were only observed for one-third ofthe datasets (CSS, PM, and IM10), possibly due to problems being easilyexplored by LLMs, this approach only decreased the model performance in 1/77test cases. Additionally, this work introduces the M6GP feature engineeringalgorithm to symbolic regression, showing it can improve the results of therandom forest regressor and produce competitive results with its predecessor,M3GP.
  </details>

- **[EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues](http://arxiv.org/abs/2503.21080v1)**  `arXiv:2503.21080`  `cs.CL`  
  _Yuhan Liu, Yunbo Long_
  <details open><summary>Abstract</summary>
  While large language model (LLM)-based chatbots have been applied foreffective engagement in credit dialogues, their capacity for dynamic emotionalexpression remains limited. Current agents primarily rely on passive empathyrather than affective reasoning. For instance, when faced with persistentclient negativity, the agent should employ strategic emotional adaptation byexpressing measured anger to discourage counterproductive behavior and guidethe conversation toward resolution. This context-aware emotional modulation isessential for imitating the nuanced decision-making of human negotiators. Thispaper introduces an EQ-negotiator that combines emotion sensing frompre-trained language models (PLMs) with emotional reasoning based on GameTheory and Hidden Markov Models. It takes into account both the current andhistorical emotions of the client to better manage and address negativeemotions during interactions. By fine-tuning pre-trained language models (PLMs)on public emotion datasets and validating them on the credit dialogue datasets,our approach enables LLM-based agents to effectively capture shifts in clientemotions and dynamically adjust their response tone based on our emotiondecision policies in real-world financial negotiations. This EQ-negotiator canalso help credit agencies foster positive client relationships, enhancingsatisfaction in credit services.
  </details>

- **[What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning](http://arxiv.org/abs/2503.21055v1)**  `arXiv:2503.21055`  `cs.CV`  
  _Chi-Hsi Kung, Frangil Ramirez, Juhyung Ha, Yi-Ting Chen, David Crandall, Yi-Hsuan Tsai_
  <details open><summary>Abstract</summary>
  Understanding a procedural activity requires modeling both how action stepstransform the scene, and how evolving scene transformations can influence thesequence of action steps, even those that are accidental or erroneous. Existingwork has studied procedure-aware video representations by proposing novelapproaches such as modeling the temporal order of actions and has notexplicitly learned the state changes (scene transformations). In this work, westudy procedure-aware video representation learning by incorporatingstate-change descriptions generated by Large Language Models (LLMs) assupervision signals for video encoders. Moreover, we generate state-changecounterfactuals that simulate hypothesized failure outcomes, allowing models tolearn by imagining the unseen ``What if'' scenarios. This counterfactualreasoning facilitates the model's ability to understand the cause and effect ofeach step in an activity. To verify the procedure awareness of our model, weconduct extensive experiments on procedure-aware tasks, including temporalaction segmentation and error detection. Our results demonstrate theeffectiveness of the proposed state-change descriptions and theircounterfactuals and achieve significant improvements on multiple tasks. We willmake our source code and data publicly available soon.
  </details>

- **[Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning](http://arxiv.org/abs/2503.20752v2)**  `arXiv:2503.20752`  `cs.AI` `cs.CV`  
  _Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, et al._
  <details open><summary>Abstract</summary>
  Visual reasoning abilities play a crucial role in understanding complexmultimodal data, advancing both domain-specific applications and artificialgeneral intelligence (AGI). Existing methods improve VLM reasoning viaChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotatedtraining data to enhance visual reasoning capabilities. However, this trainingparadigm may lead to overfitting and cognitive rigidity, restricting themodel's ability to transfer visual reasoning skills across domains and limitingits real-world applicability. To address these limitations, we proposeReason-RFT, a novel reinforcement fine-tuning framework that significantlyenhances generalization capabilities in visual reasoning tasks. Reason-RFTintroduces a two-phase training framework for visual reasoning: (1) SupervisedFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates thereasoning potential of Vision-Language Models (VLMs), followed by (2) GroupRelative Policy Optimization (GRPO)-based reinforcement learning that generatesmultiple reasoning-response pairs, significantly enhancing generalization invisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,we reconstructed a comprehensive dataset spanning visual counting, structureperception, and spatial transformation. Experimental results demonstrateReasoning-RFT's three key advantages: (1) Performance Enhancement: achievingstate-of-the-art results across multiple tasks, outperforming most mainstreamopen-source and proprietary models; (2) Generalization Superiority:consistently maintaining robust performance across diverse tasks and domains,outperforming alternative training paradigms; (3) Data Efficiency: excelling infew-shot learning scenarios while surpassing full-dataset SFT baselines.Project website: https://tanhuajie.github.io/ReasonRFT
  </details>

- **[Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs](http://arxiv.org/abs/2503.20749v2)**  `arXiv:2503.20749`  `cs.CL`  
  _Yuxuan Lu, Jing Huang, Yan Han, Bennet Bei, Yaochen Xie, Dakuo Wang, et al._
  <details open><summary>Abstract</summary>
  Recent research shows that LLMs can simulate ``believable'' human behaviorsto power LLM agents via prompt-only methods. In this work, we focus onevaluating and improving LLM's objective ``accuracy'' rather than thesubjective ``believability'' in the web action generation task, leveraging alarge-scale, real-world dataset collected from online shopping human actions.We present the first comprehensive quantitative evaluation of state-of-the-artLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web actiongeneration. Our results show that fine-tuning LLMs on real-world behavioraldata substantially improves their ability to generate actions compared toprompt-only methods. Furthermore, incorporating synthesized reasoning tracesinto model training leads to additional performance gains, demonstrating thevalue of explicit rationale in behavior modeling. This work establishes a newbenchmark for evaluating LLMs in behavior simulation and offers actionableinsights into how real-world action data and reasoning augmentation can enhancethe fidelity of LLM agents.
  </details>

- **[RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models](http://arxiv.org/abs/2503.19654v2)**  `arXiv:2503.19654`  `cs.AI` `cs.CV` `cs.LG`  
  _Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen_
  <details open><summary>Abstract</summary>
  We introduce RGB-Th-Bench, the first benchmark designed to evaluate theability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.While VLMs have demonstrated remarkable progress in visual reasoning andmultimodal understanding, their evaluation has been predominantly limited toRGB-based benchmarks, leaving a critical gap in assessing their capabilities ininfrared vision tasks. Existing visible-infrared datasets are eithertask-specific or lack high-quality annotations necessary for rigorous modelevaluation. To address these limitations, RGB-Th-Bench provides a comprehensiveevaluation framework covering 14 distinct skill dimensions, with a total of1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracymetrics: a standard question-level accuracy and a stricter skill-levelaccuracy, which evaluates model robustness across multiple questions withineach skill dimension. This design ensures a thorough assessment of modelperformance, including resilience to adversarial and hallucinated responses. Weconduct extensive evaluations on 19 state-of-the-art VLMs, revealingsignificant performance gaps in RGB-Thermal understanding. Our results showthat even the strongest models struggle with thermal image comprehension, withperformance heavily constrained by their RGB-based capabilities. Additionally,the lack of large-scale application-specific and expert-annotatedthermal-caption-pair datasets in pre-training is an important reason of theobserved performance gap. RGB-Th-Bench highlights the urgent need for furtheradvancements in multimodal learning to bridge the gap between visible andthermal image understanding. The dataset is available through this link, andthe evaluation code will also be made publicly available.
  </details>

- **[ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning](http://arxiv.org/abs/2503.19470v2)**  `arXiv:2503.19470`  `cs.AI` `cs.CL`  
  _Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, et al._
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have shown remarkable capabilities in reasoning,exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integratingreasoning with external search processes remains challenging, especially forcomplex multi-hop questions requiring multiple retrieval steps. We proposeReSearch, a novel framework that trains LLMs to Reason with Search viareinforcement learning without using any supervised data on reasoning steps.Our approach treats search operations as integral components of the reasoningchain, where when and how to perform searches is guided by text-based thinking,and search results subsequently influence further reasoning. We train ReSearchon Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conductextensive experiments. Despite being trained on only one dataset, our modelsdemonstrate strong generalizability across various benchmarks. Analysis revealsthat ReSearch naturally elicits advanced reasoning capabilities such asreflection and self-correction during the reinforcement learning process.
  </details>

- **[AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning](http://arxiv.org/abs/2503.18769v2)**  `arXiv:2503.18769`  `cs.RO` `cs.CL`  
  _Alan Dao, Dinh Bach Vu, Bui Quang Huy_
  <details open><summary>Abstract</summary>
  This paper presents AlphaSpace, a novel methodology designed to enhance thespatial reasoning capabilities of language models for robotic manipulation in3D Cartesian space. AlphaSpace employs a hierarchical semantics-basedtokenization strategy that encodes spatial information at both coarse andfine-grained levels. Our approach represents objects with their attributes,positions, and height information through structured tokens, enabling precisespatial reasoning without relying on traditional vision-based embeddings. Thisapproach enables LLMs to accurately manipulate objects by positioning them atspecific (x, y, z) coordinates. Experimental results suggest that AlphaSpacedemonstrates promising potential for improving manipulation tasks, achieving atotal accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude3.5 Sonnet. These results demonstrate the potential of structured spatialencoding for manipulation tasks and warrant further exploration.
  </details>

- **[LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning](http://arxiv.org/abs/2503.17125v4)**  `arXiv:2503.17125`  `cs.AI` `cs.RO`  
  _Chan Kim, Seung-Woo Seo, Seong-Woo Kim_
  <details open><summary>Abstract</summary>
  Deep Reinforcement Learning (DRL) has demonstrated strong performance inrobotic control but remains susceptible to out-of-distribution (OOD) states,often resulting in unreliable actions and task failure. While previous methodshave focused on minimizing or preventing OOD occurrences, they largely neglectrecovery once an agent encounters such states. Although the latest research hasattempted to address this by guiding agents back to in-distribution states,their reliance on uncertainty estimation hinders scalability in complexenvironments. To overcome this limitation, we introduce Language Models forOut-of-Distribution Recovery (LaMOuR), which enables recovery learning withoutrelying on uncertainty estimation. LaMOuR generates dense reward codes thatguide the agent back to a state where it can successfully perform its originaltask, leveraging the capabilities of LVLMs in image description, logicalreasoning, and code generation. Experimental results show that LaMOuRsubstantially enhances recovery efficiency across diverse locomotion tasks andeven generalizes effectively to complex environments, including humanoidlocomotion and mobile manipulation, where existing methods struggle. The codeand supplementary materials are available at https://lamour-rl.github.io/.
  </details>

- **[Bias Evaluation and Mitigation in Retrieval-Augmented Medical Question-Answering Systems](http://arxiv.org/abs/2503.15454v3)**  `arXiv:2503.15454`  `cs.CL`  
  _Yuelyu Ji, Hang Zhang, Yanshan Wang_
  <details open><summary>Abstract</summary>
  Medical Question Answering systems based on Retrieval Augmented Generation ispromising for clinical decision support because they can integrate externalknowledge, thus reducing inaccuracies inherent in standalone large languagemodels (LLMs). However, these systems may unintentionally propagate or amplifybiases associated with sensitive demographic attributes like race, gender, andsocioeconomic factors. This study systematically evaluates demographic biaseswithin medical RAG pipelines across multiple QA benchmarks, including MedQA,MedMCQA, MMLU, and EquityMedQA. We quantify disparities in retrievalconsistency and answer correctness by generating and analyzing queriessensitive to demographic variations. We further implement and compare severalbias mitigation strategies to address identified biases, including Chain ofThought reasoning, Counterfactual filtering, Adversarial prompt refinement, andMajority Vote aggregation. Experimental results reveal significant demographicdisparities, highlighting that Majority Vote aggregation notably improvesaccuracy and fairness metrics. Our findings underscore the critical need forexplicitly fairness-aware retrieval methods and prompt engineering strategiesto develop truly equitable medical QA systems.
  </details>

- **[GR00T N1: An Open Foundation Model for Generalist Humanoid Robots](http://arxiv.org/abs/2503.14734v2)**  `arXiv:2503.14734`  `cs.AI` `cs.RO` `cs.LG`  
  _NVIDIA, :, Johan Bjorck, Fernando Casta√±eda, Nikita Cherniadev, Xingye Da, et al._
  <details open><summary>Abstract</summary>
  General-purpose robots need a versatile body and an intelligent mind. Recentadvancements in humanoid robots have shown great promise as a hardware platformfor building generalist autonomy in the human world. A robot foundation model,trained on massive and diverse data sources, is essential for enabling therobots to reason about novel situations, robustly handle real-worldvariability, and rapidly learn new tasks. To this end, we introduce GR00T N1,an open foundation model for humanoid robots. GR00T N1 is aVision-Language-Action (VLA) model with a dual-system architecture. Thevision-language module (System 2) interprets the environment through vision andlanguage instructions. The subsequent diffusion transformer module (System 1)generates fluid motor actions in real time. Both modules are tightly coupledand jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixtureof real-robot trajectories, human videos, and synthetically generated datasets.We show that our generalist robot model GR00T N1 outperforms thestate-of-the-art imitation learning baselines on standard simulation benchmarksacross multiple robot embodiments. Furthermore, we deploy our model on theFourier GR-1 humanoid robot for language-conditioned bimanual manipulationtasks, achieving strong performance with high data efficiency.
  </details>

- **[Cognitive-Mental-LLM: Evaluating Reasoning in Large Language Models for Mental Health Prediction via Online Text](http://arxiv.org/abs/2503.10095v2)**  `arXiv:2503.10095`  `cs.AI` `cs.CL`  
  _Avinash Patil, Amardeep Kour Gedhu_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have demonstrated potential in predicting mentalhealth outcomes from online text, yet traditional classification methods oftenlack interpretability and robustness. This study evaluates structured reasoningtechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), andTree-of-Thought (ToT)-to improve classification accuracy across multiple mentalhealth datasets sourced from Reddit. We analyze reasoning-driven promptingstrategies, including Zero-shot CoT and Few-shot CoT, using key performancemetrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Ourfindings indicate that reasoning-enhanced techniques improve classificationperformance over direct prediction, particularly in complex cases. Compared tobaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trainedtransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMssuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notablegains on datasets like Dreaddit (+0.52\% over M-LLM, +0.82\% over BERT) andSDCNL (+4.67\% over M-LLM, +2.17\% over BERT). However, performance declines inDepression Severity, and CSSRS predictions suggest dataset-specificlimitations, likely due to our using a more extensive test set. Among promptingstrategies, Few-shot CoT consistently outperforms others, reinforcing theeffectiveness of reasoning-driven LLMs. Nonetheless, dataset variabilityhighlights challenges in model reliability and interpretability. This studyprovides a comprehensive benchmark of reasoning-based LLM techniques for mentalhealth text classification. It offers insights into their potential forscalable clinical applications while identifying key challenges for futureimprovements.
  </details>

- **[Deep Cut-informed Graph Embedding and Clustering](http://arxiv.org/abs/2503.06635v2)**  `arXiv:2503.06635`  `cs.AI` `cs.LG`  
  _Zhiyuan Ning, Zaitian Wang, Ran Zhang, Ping Xu, Kunpeng Liu, Pengyang Wang, et al._
  <details open><summary>Abstract</summary>
  Graph clustering aims to divide the graph into different clusters. Therecently emerging deep graph clustering approaches are largely built on graphneural networks (GNN). However, GNN is designed for general graph encoding andthere is a common issue of representation collapse in existing GNN-based deepgraph clustering algorithms. We attribute two main reasons for such issues: (i)the inductive bias of GNN models: GNNs tend to generate similar representationsfor proximal nodes. Since graphs often contain a non-negligible amount ofinter-cluster links, the bias results in error message passing and leads tobiased clustering; (ii) the clustering guided loss function: most traditionalapproaches strive to make all samples closer to pre-learned cluster centers,which causes a degenerate solution assigning all data points to a single labelthus make all samples and less discriminative. To address these challenges, weinvestigate graph clustering from a graph cut perspective and propose aninnovative and non-GNN-based Deep Cut-informed Graph embedding and Clusteringframework, namely DCGC. This framework includes two modules: (i) cut-informedgraph encoding; (ii) self-supervised graph clustering via optimal transport.For the encoding module, we derive a cut-informed graph embedding objective tofuse graph structure and attributes by minimizing their joint normalized cut.For the clustering module, we utilize the optimal transport theory to obtainthe clustering assignments, which can balance the guidance of "proximity to thepre-learned cluster center". With the above two tailored designs, DCGC is moresuitable for the graph clustering task, which can effectively alleviate theproblem of representation collapse and achieve better performance. We conductextensive experiments to demonstrate that our method is simple but effectivecompared with benchmarks.
  </details>

- **[Generalizable Prompt Learning of CLIP: A Brief Overview](http://arxiv.org/abs/2503.01263v2)**  `arXiv:2503.01263`  `cs.CV` `cs.CL`  
  _Fangming Cui, Yonggang Zhang, Xuan Wang, Xule Wang, Liang Xiao_
  <details open><summary>Abstract</summary>
  Existing vision-language models (VLMs) such as CLIP have showcased animpressive capability to generalize well across various downstream tasks. Thesemodels leverage the synergy between visual and textual information, enablingthem to understand and reason about the content present in images and text in aunified manner. This article provides a brief overview of CLIP based onfew-shot prompt learning, including experimental data and technicalcharacteristics of some methods. The purpose of this review is to provide areference for researchers who have just started their research in generalizableprompting of CLIP through few-shot training for classification across 15datasets and also to facilitate the integration of this field by researchers inother downstream tasks.
  </details>

- **[R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs](http://arxiv.org/abs/2502.12767v4)**  `arXiv:2502.12767`  `cs.AI` `cs.CL`  
  _Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi_
  <details open><summary>Abstract</summary>
  Recent studies have combined Large Language Models (LLMs) with KnowledgeGraphs (KGs) to enhance reasoning, improving inference accuracy withoutadditional training while mitigating hallucination. However, existingframeworks are often rigid, struggling to adapt to KG or task changes. Theyalso rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.To address this, We introduce R2-KG, a plug-and-play, dual-agent framework thatseparates reasoning into two roles: an Operator (a low-capacity LLM) thatgathers evidence and a Supervisor (a high-capacity LLM) that makes finaljudgments. This design is cost-efficient for LLM inference while stillmaintaining strong reasoning accuracy. Additionally, R2-KG employs anAbstention mechanism, generating answers only when sufficient evidence iscollected from KG, which significantly enhances reliability. Experiments acrossmultiple KG-based reasoning tasks show that R2-KG consistently outperformsbaselines in both accuracy and reliability, regardless of the inherentcapability of LLMs used as the Operator. Further experiments reveal that thesingle-agent version of R2-KG, equipped with a strict self-consistencystrategy, achieves significantly higher-than-baseline reliability whilereducing inference cost. However, it also leads to a higher abstention rate incomplex KGs. Our findings establish R2-KG as a flexible and cost-effectivesolution for KG-based reasoning. It reduces reliance on high-capacity LLMswhile ensuring trustworthy inference. The code is available athttps://github.com/ekrxjwh2009/R2-KG/.
  </details>

- **[Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging -- An Open Recipe](http://arxiv.org/abs/2502.09056v3)**  `arXiv:2502.09056`  `cs.AI` `cs.CL`  
  _Kunat Pipatanakul, Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai_
  <details open><summary>Abstract</summary>
  This paper investigates data selection and model merging methodologies aimedat incorporating advanced reasoning capabilities such as those of DeepSeek R1into language-specific large language models (LLMs), with a particular focus onthe Thai LLM. Our goal is to enhance the reasoning capabilities oflanguage-specific LLMs while maintaining their target language abilities.DeepSeek R1 excels in reasoning but primarily benefits high-resource languagessuch as English and Chinese. However, low-resource languages remain underserveddue to the dominance of English-centric training data and model optimizations,which limit performance in these languages. This limitation results inunreliable code-switching and diminished effectiveness on tasks in low-resourcelanguages. Meanwhile, local and regional LLM initiatives have attempted tobridge this gap by developing language-specific LLMs that focus on improvinglocal linguistic fidelity. We demonstrate that, with only publicly availabledatasets and a computational budget of $120, it is possible to enhance thereasoning capabilities of language-specific LLMs to match the level of DeepSeekR1, without compromising their performance on target language tasks.
  </details>

- **[Typhoon T1: An Open Thai Reasoning Model](http://arxiv.org/abs/2502.09042v2)**  `arXiv:2502.09042`  `cs.AI` `cs.CL`  
  _Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai, Kunat Pipatanakul_
  <details open><summary>Abstract</summary>
  This paper introduces Typhoon T1, an open effort to develop an open Thaireasoning model. A reasoning model is a relatively new type of generative modelbuilt on top of large language models (LLMs). A reasoning model generates along chain of thought before arriving at a final answer, an approach found toimprove performance on complex tasks. However, details on developing such amodel are limited, especially for reasoning models that can generate traces ina low-resource language. Typhoon T1 presents an open effort that dives into thedetails of developing a reasoning model in a more cost-effective way byleveraging supervised fine-tuning using open datasets, instead of reinforcementlearning. This paper shares the details about synthetic data generation andtraining, as well as our dataset and model weights. Additionally, we provideinsights gained from developing a reasoning model that generalizes acrossdomains and is capable of generating reasoning traces in a low-resourcelanguage, using Thai as an example. We hope this open effort provides afoundation for further research in this field.
  </details>

- **[Enhancing LLM Character-Level Manipulation via Divide and Conquer](http://arxiv.org/abs/2502.08180v2)**  `arXiv:2502.08180`  `cs.AI` `cs.CL`  
  _Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Zhecheng Li, Yiwei Wang_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have demonstrated strong generalizationcapabilities across a wide range of natural language processing (NLP) tasks.However, they exhibit notable weaknesses in character-level stringmanipulation, struggling with fundamental operations such as characterdeletion, insertion, and substitution. These challenges stem primarily fromtokenization constraints, despite the critical role of such operations in datapreprocessing and code generation. Through systematic analysis, we derive twokey insights: (1) LLMs face significant difficulties in leveraging intrinsictoken knowledge for character-level reasoning, and (2) atomized word structurescan substantially enhance LLMs' ability to process token-level structuralinformation. Building on these insights, we propose Character-LevelManipulation via Divide and Conquer, a novel approach designed to bridge thegap between token-level processing and character-level manipulation. Our methoddecomposes complex operations into explicit character-level subtasks coupledwith controlled token reconstruction phases, leading to significantimprovements in accuracy. Without additional training, our method significantlyimproves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and$\texttt{Substitution}$ tasks. To support further research, we open-source ourimplementation and benchmarks.
  </details>

- **[Group Reasoning Emission Estimation Networks](http://arxiv.org/abs/2502.06874v2)**  `arXiv:2502.06874`  `cs.AI` `cs.CL` `cs.LG`  
  _Yanming Guo, Xiao Qian, Kevin Credit, Jin Ma_
  <details open><summary>Abstract</summary>
  Accurate greenhouse gas (GHG) emission reporting is critical for governments,businesses, and investors. However, adoption remains limited particularly amongsmall and medium enterprises due to high implementation costs, fragmentedemission factor databases, and a lack of robust sector classification methods.To address these challenges, we introduce Group Reasoning Emission EstimationNetworks (GREEN), an AI-driven carbon accounting framework that standardizesenterprise-level emission estimation, constructs a large-scale benchmarkdataset, and leverages a novel reasoning approach with large language models(LLMs). Specifically, we compile textual descriptions for 20,850 companies withvalidated North American Industry Classification System (NAICS) labels andalign these with an economic model of carbon intensity factors. By reframingsector classification as an information retrieval task, we fine-tuneSentence-BERT models using a contrastive learning loss. To overcome thelimitations of single-stage models in handling thousands of hierarchicalcategories, we propose a Group Reasoning method that ensembles LLM classifiersbased on the natural NAICS ontology, decomposing the task into multiplesub-classification steps. We theoretically prove that this approach reducesclassification uncertainty and computational complexity. Experiments on 1,114NAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%Top-10 accuracy), and case studies on 20 companies report a mean absolutepercentage error (MAPE) of 45.88%. The project is available at:https://huggingface.co/datasets/Yvnminc/ExioNAICS.
  </details>

- **[iTool: Boosting Tool Use of Large Language Models via Iterative Reinforced Fine-Tuning](http://arxiv.org/abs/2501.09766v3)**  `arXiv:2501.09766`  `cs.AI` `cs.CL` `cs.LG`  
  _Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, et al._
  <details open><summary>Abstract</summary>
  Augmenting large language models (LLMs) with external tools is known as apromising approach to enhancing their capabilities, especially for complextasks. Synthesizing tool-use data through real-world simulations is aneffective way to achieve it. Nevertheless, our investigation reveals that (1)training gains significantly decay as synthetic data increases. The modelstruggles to benefit from more synthetic data due to potential data diversityissues, resulting in poor performance in complex scenarios. Moreover, we findthat (2) this challenge primarily manifests as minor discrepancies between themodel's output and the ground truth response (termed as deficiency), such aserrors in parameter values that require complex reasoning from the context toresolve. To this end, we propose an iterative reinforced fine-tuning strategydesigned to alleviate these challenges. This strategy involves: (1) enhancingthe diversity of synthetic data through path exploration of Monte Carlo TreeSearch. (2) iteratively identifying deficiency-related data, constructingfine-grained preference pairs to pinpoint deficiencies, and then applyingpreference optimization to optimize these deficiencies. Our experiments showthat models trained using our method achieve about 12\% better performance thanbaseline models, outperforming larger open-source and closed-source models.
  </details>

- **[OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?](http://arxiv.org/abs/2501.05510v2)**  `arXiv:2501.05510`  `cs.AI` `cs.CV`  
  _Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, et al._
  <details open><summary>Abstract</summary>
  Temporal Awareness, the ability to reason dynamically based on the timestampwhen a question is raised, is the key distinction between offline and onlinevideo LLMs. Unlike offline models, which rely on complete videos for static,post hoc analysis, online models process video streams incrementally anddynamically adapt their responses based on the timestamp at which the questionis posed. Despite its significance, temporal awareness has not been adequatelyevaluated in existing benchmarks. To fill this gap, we present OVO-Bench(Online-VideO-Benchmark), a novel video benchmark that emphasizes theimportance of timestamps for advanced online video understanding capabilitybenchmarking. OVO-Bench evaluates the ability of video LLMs to reason andrespond to events occurring at specific timestamps under three distinctscenarios: (1) Backward tracing: trace back to past events to answer thequestion. (2) Real-time understanding: understand and respond to events as theyunfold at the current timestamp. (3) Forward active responding: delay theresponse until sufficient future information becomes available to answer thequestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videosand approximately human-curated 2,800 fine-grained meta-annotations withprecise timestamps. We combine automated generation pipelines with humancuration. With these high-quality samples, we further developed an evaluationpipeline to systematically query video LLMs along the video timeline.Evaluations of nine Video-LLMs reveal that, despite advancements on traditionalbenchmarks, current models struggle with online video understanding, showing asignificant gap compared to human agents. We hope OVO-Bench will drive progressin video LLMs and inspire future research in online video reasoning. Ourbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.
  </details>

- **[LongViTU: Instruction Tuning for Long-Form Video Understanding](http://arxiv.org/abs/2501.05037v2)**  `arXiv:2501.05037`  `cs.CV` `cs.LG`  
  _Rujie Wu, Xiaojian Ma, Hai Ci, Yue Fan, Yuxuan Wang, Haozhe Zhao, et al._
  <details open><summary>Abstract</summary>
  This paper introduces LongViTU, a large-scale (~121k QA pairs, ~900h videos),automatically generated dataset for long-form video understanding. We propose asystematic approach that organizes videos into a hierarchical tree structurefor QA generation and incorporates self-revision mechanisms to ensurehigh-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context(average certificate length of 4.6 minutes); 2) rich knowledge and condensedreasoning (commonsense, causality, planning, etc.)). We also offer explicittimestamp annotations of relevant events for each QA pair. We have conductedextensive human studies on LongViTU, and the results prove the quality of ourdataset. To better evaluate the challenges posed by LongViTU's emphasis onlong-term context and condensed reasoning, we manually curate a subset ofLongViTU into a benchmark. Evaluations using a state-of-the-art open-sourcemodel (LongVU), a proprietary model (Gemini-1.5-Pro), and human annotatorsyield GPT-4 scores of 49.9, 52.3, and 81.0, respectively, underscoring thesubstantial difficulty presented by LongViTU questions. Performing supervisedfine-tuning (SFT) of LongVU and LLaVA-Video on LongViTU data results in averageperformance gains of 2.5% and 3.7%, respectively, across a suite of long videounderstanding benchmarks (EgoSchema, VideoMME-Long, MLVU, LVBench).
  </details>

- **[Understanding the Logic of Direct Preference Alignment through Logic](http://arxiv.org/abs/2412.17696v2)**  `arXiv:2412.17696`  `cs.CL`  
  _Kyle Richardson, Vivek Srikumar, Ashish Sabharwal_
  <details open><summary>Abstract</summary>
  Recent direct preference alignment algorithms (DPA), such as DPO, have showngreat promise in aligning large language models to human preferences. Whilethis has motivated the development of many new variants of the original DPOloss, understanding the differences between these recent proposals, as well asdeveloping new DPA loss functions, remains difficult given the lack of atechnical and conceptual framework for reasoning about the underlying semanticsof these algorithms. In this paper, we attempt to remedy this by formalizingDPA losses in terms of discrete reasoning problems. Specifically, we ask: Givenan existing DPA loss, can we systematically derive a symbolic program thatcharacterizes its semantics? We propose a novel formalism for characterizingpreference losses for single model and reference model based approaches, andidentify symbolic forms for a number of commonly used DPA variants. Further, weshow how this formal view of preference learning sheds new light on both thesize and structure of the DPA loss landscape, making it possible to not onlyrigorously characterize the relationships between recent loss proposals butalso to systematically explore the landscape and derive new loss functions fromfirst principles. We hope our framework and findings will help provide usefulguidance to those working on human AI alignment.
  </details>

- **[Beyond [cls]: Exploring the true potential of Masked Image Modeling representations](http://arxiv.org/abs/2412.03215v2)**  `arXiv:2412.03215`  `cs.CV` `cs.LG`  
  _Marcin Przewiƒô≈∫likowski, Randall Balestriero, Wojciech Jasi≈Ñski, Marek ≈ömieja, Bartosz Zieli≈Ñski_
  <details open><summary>Abstract</summary>
  Masked Image Modeling (MIM) has emerged as a promising approach forSelf-Supervised Learning (SSL) of visual representations. However, theout-of-the-box performance of MIMs is typically inferior to competingapproaches. Most users cannot afford fine-tuning due to the need for largeamounts of data, high GPU consumption, and specialized user knowledge.Therefore, the practical use of MIM representations is limited. In this paperwe ask what is the reason for the poor out-of-the-box performance of MIMs. Isit due to weaker features produced by MIM models, or is it due to suboptimalusage? Through detailed analysis, we show that attention in MIMs is spreadalmost uniformly over many patches, leading to ineffective aggregation by the[cls] token. Based on this insight, we propose Selective Aggregation to bettercapture the rich semantic information retained in patch tokens, whichsignificantly improves the out-of-the-box performance of MIM.
  </details>

- **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v2)**  `arXiv:2412.01095`  `cs.AI` `cs.CV` `cs.LG`  
  _Muchao Ye, Weiyang Liu, Pan He_
  <details open><summary>Abstract</summary>
  The rapid advancement of vision-language models (VLMs) has established a newparadigm in video anomaly detection (VAD): leveraging VLMs to simultaneouslydetect anomalies and provide comprehendible explanations for the decisions.Existing work in this direction often assumes the complex reasoning requiredfor VAD exceeds the capabilities of pretrained VLMs. Consequently, theseapproaches either incorporate specialized reasoning modules during inference orrely on instruction tuning datasets through additional training to adapt VLMsfor VAD. However, such strategies often incur substantial computational costsor data annotation overhead. To address these challenges in explainable VAD, weintroduce a verbalized learning framework named VERA that enables VLMs toperform VAD without model parameter modifications. Specifically, VERAautomatically decomposes the complex reasoning required for VAD intoreflections on simpler, more focused guiding questions capturing distinctabnormal patterns. It treats these reflective questions as learnable parametersand optimizes them through data-driven verbal interactions between learner andoptimizer VLMs, using coarsely labeled training data. During inference, VERAembeds the learned questions into model prompts to guide VLMs in generatingsegment-level anomaly scores, which are then refined into frame-level scoresvia the fusion of scene and temporal contexts. Experimental results onchallenging benchmarks demonstrate that the learned questions of VERA arehighly adaptable, significantly improving both detection performance andexplainability of VLMs for VAD.
  </details>

- **[Not Just Object, But State: Compositional Incremental Learning without Forgetting](http://arxiv.org/abs/2411.01739v3)**  `arXiv:2411.01739`  `cs.CV`  
  _Yanyi Zhang, Binglin Qiu, Qi Jia, Yu Liu, Ran He_
  <details open><summary>Abstract</summary>
  Most incremental learners excessively prioritize coarse classes of objectswhile neglecting various kinds of states (e.g. color and material) attached tothe objects. As a result, they are limited in the ability to reasonfine-grained compositionality of state-object pairs. To remedy this limitation,we propose a novel task called Compositional Incremental Learning(composition-IL), enabling the model to recognize state-object compositions asa whole in an incremental learning fashion. Since the lack of suitablebenchmarks, we re-organize two existing datasets and make them tailored forcomposition-IL. Then, we propose a prompt-based Composition Incremental Learner(CompILer), to overcome the ambiguous composition boundary problem whichchallenges composition-IL largely. Specifically, we exploit multi-pool promptlearning, which is regularized by inter-pool prompt discrepancy and intra-poolprompt diversity. Besides, we devise object-injected state prompting by usingobject prompts to guide the selection of state prompts. Furthermore, we fusethe selected prompts by a generalized-mean strategy, to eliminate irrelevantinformation learned in the prompts. Extensive experiments on two datasetsexhibit state-of-the-art performance achieved by CompILer.
  </details>

- **[ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom](http://arxiv.org/abs/2410.14138v2)**  `arXiv:2410.14138`  `cs.AI` `cs.CV`  
  _Jingqi Zhou, Sheng Wang, Jingwei Dong, Lei Li, Jiahui Gao, Jiyue Jiang, et al._
  <details open><summary>Abstract</summary>
  Large vision-language models (LVLMs) have witnessed significant progress onvisual understanding tasks. However, they often prioritize language knowledgeover image information on visual reasoning tasks, incurring performancedegradation. To tackle this issue, we first identify the drawbacks of existingsolutions (i.e., insufficient and irrelevant visual descriptions, and limitedmulti-modal capacities). We then decompose visual reasoning process into twostages: visual perception (i.e., eyesight) and textual reasoning (i.e.,wisdom), and introduce a novel visual reasoning framework named ProReason. Thisframework features multi-run proactive perception and decoupledvision-reasoning capabilities. Briefly, given a multi-modal question, ProReasoniterates proactive information collection and reasoning until the answer can beconcluded with necessary and sufficient visual descriptions. Notably, thedisassociation of capabilities allows seamless integration of existing largelanguage models (LLMs) to compensate for the reasoning deficits of LVLMs. Ourextensive experiments demonstrate that ProReason outperforms both existingmulti-step reasoning frameworks and passive peer methods on a wide range ofbenchmarks for both open-source and closed-source models. In addition, with theassistance of LLMs, ProReason achieves a performance improvement of up to 15%on MMMU benchmark. Our insights into existing solutions and the decoupledperspective for feasible integration of LLMs illuminate future research onvisual reasoning techniques, especially LLM-assisted ones.
  </details>

- **[OmniBench: Towards The Future of Universal Omni-Language Models](http://arxiv.org/abs/2409.15272v4)**  `arXiv:2409.15272`  `cs.AI` `cs.CL` `cs.CV`  
  _Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in multimodal large language models (MLLMs) have focusedon integrating multiple modalities, yet their ability to simultaneously processand reason across different inputs remains underexplored. We introduceOmniBench, a novel benchmark designed to evaluate models' ability to recognize,interpret, and reason across visual, acoustic, and textual inputssimultaneously. We define language models capable of such tri-modal processingas omni-language models (OLMs). OmniBench features high-quality humanannotations that require integrated understanding across all modalities. Ourevaluation reveals that: i) open-source OLMs show significant limitations ininstruction-following and reasoning in tri-modal contexts; and ii) mostbaseline models perform poorly (around 50% accuracy) even with textualalternatives to image/audio inputs. To address these limitations, we developOmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. Weadvocate for developing more robust tri-modal integration techniques andtraining strategies to enhance OLM performance. Codes and data could be foundat our repo (https://github.com/multimodal-art-projection/OmniBench).
  </details>

- **[Vision language models are blind: Failing to translate detailed visual features into words](http://arxiv.org/abs/2407.06581v6)**  `arXiv:2407.06581`  `cs.AI` `cs.CV`  
  _Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen_
  <details open><summary>Abstract</summary>
  While large language models with vision capabilities (VLMs), e.g., GPT-4o andGemini 1.5 Pro, score high on many vision-understanding benchmarks, they arestill struggling with low-level vision tasks that are easy to humans.Specifically, on BlindTest, our suite of 7 very simple tasks, includingidentifying (a) whether two circles overlap; (b) how many times two linesintersect; (c) which letter is being circled in a word; and (d) the number ofcircles in an Olympic-like logo, four state-of-the-art VLMs are only 58.07%accurate on average. Claude 3.5 Sonnet performs the best at 77.84% accuracy,far from the human expected accuracy of 100%. Across different imageresolutions and line widths, VLMs including slow-thinking models consistentlystruggle with those tasks that require precise spatial information whengeometric primitives overlap or are close. Yet, VLMs perform at near-100%accuracy when much more space is added to separate shapes and letters. Linearprobing experiments show that vision encoders contain sufficient visualinformation to solve BlindTest and that language models fail to decode thisinformation into correct answers. Code and data are at:https://vlmsareblind.github.io
  </details>

- **[Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems](http://arxiv.org/abs/2404.14963v5)**  `arXiv:2404.14963`  `cs.AI` `cs.CL`  
  _Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du_
  <details open><summary>Abstract</summary>
  Chain-of-Thought (CoT) prompting has enhanced the performance of LargeLanguage Models (LLMs) across various reasoning tasks. However, CoT still fallsshort in dealing with complex math word problems, as it usually suffers fromthree pitfalls: semantic misunderstanding errors, calculation errors, andstep-missing errors. Prior studies involve addressing the calculation errorsand step-missing errors, but neglect the semantic misunderstanding errors,which is the major factor limiting the reasoning performance of LLMs. To thisend, we propose a simple-yet-effective method, namely Deeply Understanding theProblems (DUP), to improve the LLMs' math problem-solving ability by addressingsemantic misunderstanding errors. The core of our method is to encourage theLLMs to deeply understand the problems and extract the key problem-solvinginformation used for better reasoning. Extensive experiments on 10 diversereasoning benchmarks show that our DUP method consistently outperforms theother counterparts by a large margin. More encouragingly, DUP achieves a newSOTA result on the GSM8K benchmark, with an accuracy of 97.1% under thezero-shot setting.
  </details>

- **[MoReVQA: Exploring Modular Reasoning Models for Video Question Answering](http://arxiv.org/abs/2404.06511v2)**  `arXiv:2404.06511`  `cs.AI` `cs.CV` `cs.LG`  
  _Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid_
  <details open><summary>Abstract</summary>
  This paper addresses the task of video question answering (videoQA) via adecomposed multi-stage, modular reasoning framework. Previous modular methodshave shown promise with a single planning stage ungrounded in visual content.However, through a simple and effective baseline, we find that such systems canlead to brittle behavior in practice for challenging videoQA settings. Thus,unlike traditional single-stage planning methods, we propose a multi-stagesystem consisting of an event parser, a grounding stage, and a final reasoningstage in conjunction with an external memory. All stages are training-free, andperformed using few-shot prompting of large models, creating interpretableintermediate outputs at each stage. By decomposing the underlying planning andtask complexity, our method, MoReVQA, improves over prior work on standardvideoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) withstate-of-the-art results, and extensions to related tasks (grounded videoQA,paragraph captioning).
  </details>

- **[Contextual AD Narration with Interleaved Multimodal Sequence](http://arxiv.org/abs/2403.12922v2)**  `arXiv:2403.12922`  `cs.CV`  
  _Hanlin Wang, Zhan Tong, Kecheng Zheng, Yujun Shen, Limin Wang_
  <details open><summary>Abstract</summary>
  The Audio Description (AD) task aims to generate descriptions of visualelements for visually impaired individuals to help them access long-form videocontent, like movies. With video feature, text, character bank and contextinformation as inputs, the generated ADs are able to correspond to thecharacters by name and provide reasonable, contextual descriptions to helpaudience understand the storyline of movie. To achieve this goal, we propose toleverage pre-trained foundation models through a simple and unified frameworkto generate ADs with interleaved multimodal sequence as input, termed asUni-AD. To enhance the alignment of features across various modalities withfiner granularity, we introduce a simple and lightweight module that maps videofeatures into the textual feature space. Moreover, we also propose acharacter-refinement module to provide more precise information by identifyingthe main characters who play more significant roles in the video context. Withthese unique designs, we further incorporate contextual information and acontrastive loss into our architecture to generate smoother and morecontextually appropriate ADs. Experiments on multiple AD datasets show thatUni-AD performs well on AD generation, which demonstrates the effectiveness ofour approach. Our code is available at: https://github.com/ant-research/UniAD.
  </details>
