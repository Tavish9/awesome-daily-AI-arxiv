# üîç Reasoning Papers ¬∑ 2025-03-28

[![Total Papers](https://img.shields.io/badge/Papers-35-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Reasoning` `System 2` `Slow Thinking` `o1` `r1`  
**Filter**: `None`

---

## üìö Paper List

- **[Q-Insight: Understanding Image Quality via Visual Reinforcement Learning](http://arxiv.org/abs/2503.22679v1)**  `arXiv:2503.22679`  `cs.CV`  
  _Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, et al._
  <details open><summary>Abstract</summary>
  Image quality assessment (IQA) focuses on the perceptual visual quality ofimages, playing a crucial role in downstream tasks such as imagereconstruction, compression, and generation. The rapid advancement ofmulti-modal large language models (MLLMs) has significantly broadened the scopeof IQA, moving toward comprehensive image quality understanding thatincorporates content analysis, degradation perception, and comparison reasoningbeyond mere numerical scoring. Previous MLLM-based methods typically eithergenerate numerical scores lacking interpretability or heavily rely onsupervised fine-tuning (SFT) using large-scale annotated datasets to providedescriptive assessments, limiting their flexibility and applicability. In thispaper, we propose Q-Insight, a reinforcement learning-based model built upongroup relative policy optimization (GRPO), which demonstrates strong visualreasoning capability for image quality understanding while requiring only alimited amount of rating scores and degradation labels. By jointly optimizingscore regression and degradation perception tasks with carefully designedreward functions, our approach effectively exploits their mutual benefits forenhanced performance. Extensive experiments demonstrate that Q-Insightsubstantially outperforms existing state-of-the-art methods in both scoreregression and degradation perception tasks, while exhibiting impressivezero-shot generalization to comparison reasoning tasks. Code will be availableat https://github.com/lwq20020127/Q-Insight.
  </details>

- **[Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions](http://arxiv.org/abs/2503.22678v1)**  `arXiv:2503.22678`  `cs.CL`  
  _Mohammad Almansoori, Komal Kumar, Hisham Cholakkal_
  <details open><summary>Abstract</summary>
  In this work, we introduce MedAgentSim, an open-source simulated clinicalenvironment with doctor, patient, and measurement agents designed to evaluateand enhance LLM performance in dynamic diagnostic settings. Unlike priorapproaches, our framework requires doctor agents to actively engage withpatients through multi-turn conversations, requesting relevant medicalexaminations (e.g., temperature, blood pressure, ECG) and imaging results(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnosticprocess. Additionally, we incorporate self improvement mechanisms that allowmodels to iteratively refine their diagnostic strategies. We enhance LLMperformance in our simulated setting by integrating multi-agent discussions,chain-of-thought reasoning, and experience-based knowledge retrieval,facilitating progressive learning as doctor agents interact with more patients.We also introduce an evaluation benchmark for assessing the LLM's ability toengage in dynamic, context-aware diagnostic interactions. While MedAgentSim isfully automated, it also supports a user-controlled mode, enabling humaninteraction with either the doctor or patient agent. Comprehensive evaluationsin various simulated diagnostic scenarios demonstrate the effectiveness of ourapproach. Our code, simulation tool, and benchmark are available at\href{https://medagentsim.netlify.app/}.
  </details>

- **[QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?](http://arxiv.org/abs/2503.22674v1)**  `arXiv:2503.22674`  `cs.CL` `cs.AI` `cs.LG`  
  _Belinda Z. Li, Been Kim, Zi Wang_
  <details open><summary>Abstract</summary>
  Recently, a large amount of work has focused on improving large languagemodels' (LLMs') performance on reasoning benchmarks such as math and logic.However, past work has largely assumed that tasks are well-defined. In the realworld, queries to LLMs are often underspecified, only solvable throughacquiring missing information. We formalize this as a constraint satisfactionproblem (CSP) with missing variable assignments. Using a special case of thisformalism where only one necessary variable assignment is missing, we canrigorously evaluate an LLM's ability to identify the minimal necessary questionto ask and quantify axes of difficulty levels for each problem. We presentQuestBench, a set of underspecified reasoning tasks solvable by asking at mostone question, which includes: (1) Logic-Q: Logical reasoning tasks with onemissing proposition, (2) Planning-Q: PDDL planning problems with initial statesthat are partially-observed, (3) GSM-Q: Human-annotated grade school mathproblems with one missing variable assignment, and (4) GSME-Q: a version ofGSM-Q where word problems are translated into equations by human annotators.The LLM is tasked with selecting the correct clarification question(s) from alist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, theiraccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates thatthe ability to solve well-specified reasoning problems may not be sufficientfor success on our benchmark: models have difficulty identifying the rightquestion to ask, even when they can solve the fully specified version of theproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, evenwhen explicitly presented with the option to predict ``not sure.'' Thishighlights the need for deeper investigation into models' informationacquisition capabilities.
  </details>

- **[Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning](http://arxiv.org/abs/2503.22456v1)**  `arXiv:2503.22456`  `cs.AI` `cs.LG`  
  _Abdullah Vanlioglu_
  <details open><summary>Abstract</summary>
  We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach thatenhances the exploration-exploitation tradeoff by dynamically assigning weightsto generated outputs based on their advantage and entropy for ReinforcementLearning-based Large Language Model fine-tuning. EGSW integrates entropyregularization with advantage-based weighting to balance policy updates,enabling efficient exploration in high-dimensional state spaces. By employingtemperature-scaled softmax weighting over sequences, EGSW prioritizinghigh-reward, high-uncertainty steps while maintaining training stability.Although originally developed to improve Group Relative Policy Optimization(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable toother reinforcement learning (RL) algorithms and can be implemented in bothstep-wise and trajectory-wise settings. Empirical evaluations demonstrate thatEGSW enhances GRPO reasoning ability, yielding improvements in sampleefficiency. Future work will explore the application of EGSW to advanced RLmethodologies.
  </details>

- **[A Causal Framework to Measure and Mitigate Non-binary Treatment Discrimination](http://arxiv.org/abs/2503.22454v1)**  `arXiv:2503.22454`  `cs.AI` `cs.LG`  
  _Ayan Majumdar, Deborah D. Kanubala, Kavya Gupta, Isabel Valera_
  <details open><summary>Abstract</summary>
  Fairness studies of algorithmic decision-making systems often simplifycomplex decision processes, such as bail or loan approvals, into binaryclassification tasks. However, these approaches overlook that such decisionsare not inherently binary (e.g., approve or not approve bail or loan); theyalso involve non-binary treatment decisions (e.g., bail conditions or loanterms) that can influence the downstream outcomes (e.g., loan repayment orreoffending). In this paper, we argue that non-binary treatment decisions areintegral to the decision process and controlled by decision-makers and,therefore, should be central to fairness analyses in algorithmicdecision-making. We propose a causal framework that extends fairness analysesand explicitly distinguishes between decision-subjects' covariates and thetreatment decisions. This specification allows decision-makers to use ourframework to (i) measure treatment disparity and its downstream effects inhistorical data and, using counterfactual reasoning, (ii) mitigate the impactof past unfair treatment decisions when automating decision-making. We use ourframework to empirically analyze four widely used loan approval datasets toreveal potential disparity in non-binary treatment decisions and theirdiscriminatory impact on outcomes, highlighting the need to incorporatetreatment decisions in fairness assessments. Moreover, by intervening intreatment decisions, we show that our framework effectively mitigates treatmentdiscrimination from historical data to ensure fair risk score estimation and(non-binary) decision-making processes that benefit all stakeholders.
  </details>

- **[NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous Driving](http://arxiv.org/abs/2503.22436v1)**  `arXiv:2503.22436`  `cs.CV`  
  _Fuhao Li, Huan Jin, Bin Gao, Liaoyuan Fan, Lihui Jiang, Long Zeng_
  <details open><summary>Abstract</summary>
  Multi-view 3D visual grounding is critical for autonomous driving vehicles tointerpret natural languages and localize target objects in complexenvironments. However, existing datasets and methods suffer from coarse-grainedlanguage instructions, and inadequate integration of 3D geometric reasoningwith linguistic comprehension. To this end, we introduce NuGrounding, the firstlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.We present a Hierarchy of Grounding (HoG) method to construct NuGrounding togenerate hierarchical multi-level instructions, ensuring comprehensive coverageof human instruction patterns. To tackle this challenging dataset, we propose anovel paradigm that seamlessly combines instruction comprehension abilities ofmulti-modal LLMs (MLLMs) with precise localization abilities of specialistdetection models. Our approach introduces two decoupled task tokens and acontext query to aggregate 3D geometric information and semantic instructions,followed by a fusion decoder to refine spatial-semantic feature fusion forprecise localization. Extensive experiments demonstrate that our methodsignificantly outperforms the baselines adapted from representative 3D sceneunderstanding methods by a significant margin and achieves 0.59 in precisionand 0.64 in recall, with improvements of 50.8% and 54.7%.
  </details>

- **[Collapse and Collision Aware Grasping for Cluttered Shelf Picking](http://arxiv.org/abs/2503.22427v1)**  `arXiv:2503.22427`  `cs.RO`  
  _Abhinav Pathak, Rajkumar Muthusamy_
  <details open><summary>Abstract</summary>
  Efficient and safe retrieval of stacked objects in warehouse environments isa significant challenge due to complex spatial dependencies and structuralinter-dependencies. Traditional vision-based methods excel at objectlocalization but often lack the physical reasoning required to predict theconsequences of extraction, leading to unintended collisions and collapses.This paper proposes a collapse and collision aware grasp planner thatintegrates dynamic physics simulations for robotic decision-making. Using asingle image and depth map, an approximate 3D representation of the scene isreconstructed in a simulation environment, enabling the robot to evaluatedifferent retrieval strategies before execution. Two approaches 1)heuristic-based and 2) physics-based are proposed for both single-boxextraction and shelf clearance tasks. Extensive real-world experiments onstructured and unstructured box stacks, along with validation using datasetsfrom existing databases, show that our physics-aware method significantlyimproves efficiency and success rates compared to baseline heuristics.
  </details>

- **[VITAL: More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow](http://arxiv.org/abs/2503.22399v1)**  `arXiv:2503.22399`  `cs.CV`  
  _Ada Gorgun, Bernt Schiele, Jonas Fischer_
  <details open><summary>Abstract</summary>
  Neural networks are widely adopted to solve complex and challenging tasks.Especially in high-stakes decision-making, understanding their reasoningprocess is crucial, yet proves challenging for modern deep networks. Featurevisualization (FV) is a powerful tool to decode what information neurons areresponding to and hence to better understand the reasoning behind suchnetworks. In particular, in FV we generate human-understandable images thatreflect the information detected by neurons of interest. However, currentmethods often yield unrecognizable visualizations, exhibiting repetitivepatterns and visual artifacts that are hard to understand for a human. Toaddress these problems, we propose to guide FV through statistics of real imagefeatures combined with measures of relevant network flow to generateprototypical images. Our approach yields human-understandable visualizationsthat both qualitatively and quantitatively improve over state-of-the-art FVsacross various architectures. As such, it can be used to decode whichinformation the network uses, complementing mechanistic circuits that identifywhere it is encoded. Code is available at: https://github.com/adagorgun/VITAL
  </details>

- **[Negation: A Pink Elephant in the Large Language Models' Room?](http://arxiv.org/abs/2503.22395v1)**  `arXiv:2503.22395`  `cs.CL`  
  _Tereza Vrabcov√°, Marek Kadlƒç√≠k, Petr Sojka, Michal ≈†tef√°nik, Michal Spiegel_
  <details open><summary>Abstract</summary>
  Negations are key to determining sentence meaning, making them essential forlogical reasoning. Despite their importance, negations pose a substantialchallenge for large language models (LLMs) and remain underexplored.  We construct two multilingual natural language inference (NLI) datasets with\textit{paired} examples differing in negation. We investigate how model sizeand language impact its ability to handle negation correctly by evaluatingpopular LLMs.  Contrary to previous work, we show that increasing the model sizeconsistently improves the models' ability to handle negations. Furthermore, wefind that both the models' reasoning accuracy and robustness to negation arelanguage-dependent and that the length and explicitness of the premise have agreater impact on robustness than language.  Our datasets can facilitate further research and improvements of languagemodel reasoning in multilingual settings.
  </details>

- **[MASCOTS: Model-Agnostic Symbolic COunterfactual explanations for Time Series](http://arxiv.org/abs/2503.22389v1)**  `arXiv:2503.22389`  `cs.LG`  
  _Dawid P≈Çudowski, Francesco Spinnato, Piotr Wilczy≈Ñski, Krzysztof Kotowski, Evridiki Vasileia Ntagiou, Riccardo Guidotti, et al._
  <details open><summary>Abstract</summary>
  Counterfactual explanations provide an intuitive way to understand modeldecisions by identifying minimal changes required to alter an outcome. However,applying counterfactual methods to time series models remains challenging dueto temporal dependencies, high dimensionality, and the lack of an intuitivehuman-interpretable representation. We introduce MASCOTS, a method thatleverages the Bag-of-Receptive-Fields representation alongside symbolictransformations inspired by Symbolic Aggregate Approximation. By operating in asymbolic feature space, it enhances interpretability while preserving fidelityto the original data and model. Unlike existing approaches that either dependon model structure or autoencoder-based sampling, MASCOTS directly generatesmeaningful and diverse counterfactual observations in a model-agnostic manner,operating on both univariate and multivariate data. We evaluate MASCOTS onunivariate and multivariate benchmark datasets, demonstrating comparablevalidity, proximity, and plausibility to state-of-the-art methods, whilesignificantly improving interpretability and sparsity. Its symbolic natureallows for explanations that can be expressed visually, in natural language, orthrough semantic representations, making counterfactual reasoning moreaccessible and actionable.
  </details>

- **[Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors](http://arxiv.org/abs/2503.22388v1)**  `arXiv:2503.22388`  `cs.CL`  
  _Zhiyu Yang, Shuo Wang, Yukun Yan, Yang Deng_
  <details open><summary>Abstract</summary>
  LLMs are transforming software development, yet current code generation andcode repair benchmarks mainly assess syntactic and functional correctness insimple, single-error cases. LLMs' capabilities to autonomously find and fixruntime logical errors in complex data science code remain largely unexplored.To address this gap, we introduce DSDBench: the Data Science DebuggingBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hoperror tracing and multi-bug detection in data science code debugging. DSDBenchadapts datasets from existing data science task benchmarks, such as DABench andMatPlotBench, featuring realistic data science debugging tasks withautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes1,117 annotated samples with 741 cause-effect error pairs and runtime errormessages. Evaluations of state-of-the-art LLMs on DSDBench show significantperformance gaps, highlighting challenges in debugging logical runtime errorsin data science code. DSDBench offers a crucial resource to evaluate andimprove LLMs' debugging and reasoning capabilities, enabling more reliableAI-assisted data science in the future.DSDBench is publicly available athttps://github.com/KevinCL16/DSDBench.
  </details>

- **[CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models](http://arxiv.org/abs/2503.22342v1)**  `arXiv:2503.22342`  `cs.AI`  
  _Zhihang Lin, Mingbao Lin, Yuan Xie, Rongrong Ji_
  <details open><summary>Abstract</summary>
  This paper introduces Completion Pruning Policy Optimization (CPPO) toaccelerate the training of reasoning models based on Group Relative PolicyOptimization (GRPO). GRPO, while effective, incurs high training costs due tothe need for sampling multiple completions for each question. Our experimentand theoretical analysis reveals that the number of completions impacts modelaccuracy yet increases training time multiplicatively, and not all completionscontribute equally to policy training -- their contribution depends on theirrelative advantage. To address these issues, we propose CPPO, which prunescompletions with low absolute advantages, significantly reducing the numberneeded for gradient calculation and updates. Additionally, we introduce adynamic completion allocation strategy to maximize GPU utilization byincorporating additional questions, further enhancing training efficiency.Experimental results demonstrate that CPPO achieves up to $8.32\times$ speedupon GSM8K and $3.51\times$ on Math while preserving or even enhancing theaccuracy compared to the original GRPO. We release our code athttps://github.com/lzhxmu/CPPO.
  </details>

- **[Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback](http://arxiv.org/abs/2503.22230v1)**  `arXiv:2503.22230`  `cs.LG`  
  _Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, et al._
  <details open><summary>Abstract</summary>
  Reinforcement Learning from Human Feedback (RLHF) is crucial for aligninglarge language models with human preferences. While recent research has focusedon algorithmic improvements, the importance of prompt-data construction hasbeen overlooked. This paper addresses this gap by exploring data-drivenbottlenecks in RLHF performance scaling, particularly reward hacking anddecreasing response diversity. We introduce a hybrid reward system combiningreasoning task verifiers (RTV) and a generative reward model (GenRM) tomitigate reward hacking. We also propose a novel prompt-selection method,Pre-PPO, to maintain response diversity and enhance learning effectiveness.Additionally, we find that prioritizing mathematical and coding tasks early inRLHF training significantly improves performance. Experiments across two modelsizes validate our methods' effectiveness and scalability. Results show thatRTV is most resistant to reward hacking, followed by GenRM with ground truth,and then GenRM with SFT Best-of-N responses. Our strategies enable rapidcapture of subtle task-specific distinctions, leading to substantialimprovements in overall RLHF performance. This work highlights the importanceof careful data construction and provides practical methods to overcomeperformance barriers in RLHF.
  </details>

- **[Reasoning of Large Language Models over Knowledge Graphs with Super-Relations](http://arxiv.org/abs/2503.22166v1)**  `arXiv:2503.22166`  `cs.LG`  
  _Song Wang, Junhong Lin, Xiaojie Guo, Julian Shun, Jundong Li, Yada Zhu_
  <details open><summary>Abstract</summary>
  While large language models (LLMs) have made significant progress inprocessing and reasoning over knowledge graphs, current methods suffer from ahigh non-retrieval rate. This limitation reduces the accuracy of answeringquestions based on these graphs. Our analysis reveals that the combination ofgreedy search and forward reasoning is a major contributor to this issue. Toovercome these challenges, we introduce the concept of super-relations, whichenables both forward and backward reasoning by summarizing and connectingvarious relational paths within the graph. This holistic approach not onlyexpands the search space, but also significantly improves retrieval efficiency.In this paper, we propose the ReKnoS framework, which aims to Reason overKnowledge Graphs with Super-Relations. Our framework's key advantages includethe inclusion of multiple relation paths through super-relations, enhancedforward and backward reasoning capabilities, and increased efficiency inquerying LLMs. These enhancements collectively lead to a substantialimprovement in the successful retrieval rate and overall reasoning performance.We conduct extensive experiments on nine real-world datasets to evaluateReKnoS, and the results demonstrate the superior performance of ReKnoS overexisting state-of-the-art baselines, with an average accuracy gain of 2.92%.
  </details>

- **[Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models](http://arxiv.org/abs/2503.22165v1)**  `arXiv:2503.22165`  `cs.LG`  
  _Zhanke Zhou, Zhaocheng Zhu, Xuan Li, Mikhail Galkin, Xiao Feng, Sanmi Koyejo, et al._
  <details open><summary>Abstract</summary>
  Numerous applications of large language models (LLMs) rely on their abilityto perform step-by-step reasoning. However, the reasoning behavior of LLMsremains poorly understood, posing challenges to research, development, andsafety. To address this gap, we introduce landscape of thoughts-the firstvisualization tool for users to inspect the reasoning paths of chain-of-thoughtand its derivatives on any multi-choice dataset. Specifically, we represent thestates in a reasoning path as feature vectors that quantify their distances toall answer choices. These features are then visualized in two-dimensional plotsusing t-SNE. Qualitative and quantitative analysis with the landscape ofthoughts effectively distinguishes between strong and weak models, correct andincorrect answers, as well as different reasoning tasks. It also uncoversundesirable reasoning patterns, such as low consistency and high uncertainty.Additionally, users can adapt our tool to a model that predicts the propertythey observe. We showcase this advantage by adapting our tool to a lightweightverifier that evaluates the correctness of reasoning paths. The code ispublicly available at: https://github.com/tmlr-group/landscape-of-thoughts.
  </details>

- **[EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos](http://arxiv.org/abs/2503.22152v1)**  `arXiv:2503.22152`  `cs.CV` `cs.AI`  
  _Yuxuan Li, Vijay Veerabadran, Michael L. Iuzzolino, Brett D. Roads, Asli Celikyilmaz, Karl Ridgeway_
  <details open><summary>Abstract</summary>
  We introduce EgoToM, a new video question-answering benchmark that extendsTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToMmodel, we generate multi-choice video QA instances for the Ego4D dataset tobenchmark the ability to predict a camera wearer's goals, beliefs, and nextactions. We study the performance of both humans and state of the artmultimodal large language models (MLLMs) on these three interconnectedinference problems. Our evaluation shows that MLLMs achieve close tohuman-level accuracy on inferring goals from egocentric videos. However, MLLMs(including the largest ones we tested with over 100B parameters) fall short ofhuman performance when inferring the camera wearers' in-the-moment beliefstates and future actions that are most consistent with the unseen videofuture. We believe that our results will shape the future design of animportant class of egocentric digital assistants which are equipped with areasonable model of the user's internal mental states.
  </details>

- **[REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation](http://arxiv.org/abs/2503.22122v1)**  `arXiv:2503.22122`  `cs.CL` `cs.CV` `cs.RO` `cs.AI`  
  _Puzhen Yuan, Angyuan Ma, Yunchao Yao, Huaxiu Yao, Masayoshi Tomizuka, Mingyu Ding_
  <details open><summary>Abstract</summary>
  Vision-language models (VLMs) have demonstrated remarkable capabilities inrobotic planning, particularly for long-horizon tasks that require a holisticunderstanding of the environment for task decomposition. Existing methodstypically rely on prior environmental knowledge or carefully designedtask-specific prompts, making them struggle with dynamic scene changes orunexpected task conditions, e.g., a robot attempting to put a carrot in themicrowave but finds the door was closed. Such challenges underscore twocritical issues: adaptability and efficiency. To address them, in this work, wepropose an adaptive multi-agent planning framework, termed REMAC, that enablesefficient, scene-agnostic multi-robot long-horizon task planning and executionthrough continuous reflection and self-evolution. REMAC incorporates two keymodules: a self-reflection module performing pre-condition and post-conditionchecks in the loop to evaluate progress and refine plans, and a self-evolvementmodule dynamically adapting plans based on scene-specific reasoning. It offersseveral appealing benefits: 1) Robots can initially explore and reason aboutthe environment without complex prompt design. 2) Robots can keep reflecting onpotential planning errors and adapting the plan based on task-specificinsights. 3) After iterations, a robot can call another one to coordinate tasksin parallel, maximizing the task execution efficiency. To validate REMAC'seffectiveness, we build a multi-agent environment for long-horizon robotmanipulation and navigation based on RoboCasa, featuring 4 task categories with27 task styles and 50+ different objects. Based on it, we further benchmarkstate-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, andGrok3, demonstrating REMAC's superiority by boosting average success rates by40% and execution efficiency by 52.7% over the single robot baseline.
  </details>

- **[How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark](http://arxiv.org/abs/2503.22093v1)**  `arXiv:2503.22093`  `cs.CV` `cs.AI`  
  _Ximing Wen, Mallika Mainali, Anik Sen_
  <details open><summary>Abstract</summary>
  Vision Language Models (VLMs) have demonstrated strong reasoning capabilitiesin Visual Question Answering (VQA) tasks; However, their ability to performTheory of Mind (ToM) tasks such as accurately inferring human intentions,beliefs, and other mental states remains underexplored. In this work, wepropose an open-ended question framework to comprehensively evaluate VLMs'performance across diverse categories of ToM tasks. We curated and annotated abenchmark dataset composed of 30 images. We then assessed the performance offour VLMs of varying sizes on this dataset. Our experimental results show thatthe GPT-4 model outperformed all others, with only one smaller model,GPT-4o-mini, achieving comparable performance. Additionally, we observed thatVLMs often struggle to accurately infer intentions in complex scenarios such asbullying or cheating. Moreover, our findings also reveal that smaller modelscan sometimes infer correct intentions despite relying on incorrect visualcues.
  </details>

- **[Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A General Framework for Domain-Specific Large Language Model Adaptation](http://arxiv.org/abs/2503.22074v1)**  `arXiv:2503.22074`  `cs.CL` `cs.AI`  
  _Chuan-Wei Kuo, Siyu Chen, Chenqi Yan, Yu Yang Fredrik Liu_
  <details open><summary>Abstract</summary>
  Large language models (LLMs) hold great promise for specialized scientificdomains such as materials science, yet adapting them efficiently and accuratelyto domain-specific knowledge remains challenging due to limited data and highknowledge density. We propose a two-stage framework that combines structuredmodel compression with a scientific fine-tuning regimen to address thischallenge. In the compression stage, we decompose the LLM's weight matricesinto local low-rank "rank blocks" and arrange these blocks in a Penrose-likenon-periodic tiling pattern. Each block is then compacted via spectraltransformations (e.g., discrete cosine or Fourier transforms), and aKullback-Leibler (KL) divergence-based alignment loss preserves thedistributional similarity between the compressed model's representations andthose of the original full model. In the adaptation stage, the compressed modelis further tuned using a human-like scientific reading protocol: it processestechnical materials science documents section by section, engaging in astructured question-and-answer routine for each section. This section-wise Q&Afine-tuning strategy extracts explicit reasoning traces and gradually injectsdomain knowledge, while minimizing catastrophic forgetting of the model'sgeneral language capabilities. By balancing efficient compression with targetedadaptation, our two-stage approach enables precise specialization of LLMs tohigh-value domains under data-scarce conditions. We present this principled yetexploratory pipeline and outline its potential for advancing materials scienceknowledge integration, laying the groundwork for comprehensive empiricalevaluation in future work.
  </details>

- **[OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs](http://arxiv.org/abs/2503.21480v2)**  `arXiv:2503.21480`  `cs.CL`  
  _John Murzaku, Owen Rambow_
  <details open><summary>Abstract</summary>
  The use of omni-LLMs (large language models that accept any modality asinput), particularly for multimodal cognitive state tasks involving speech, isunderstudied. We present OmniVox, the first systematic evaluation of fouromni-LLMs on the zero-shot emotion recognition task. We evaluate on two widelyused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shotomni-LLMs outperform or are competitive with fine-tuned audio models. Alongsideour audio-only evaluation, we also evaluate omni-LLMs on text only and text andaudio. We present acoustic prompting, an audio-specific prompting strategy foromni-LLMs which focuses on acoustic feature analysis, conversation contextanalysis, and step-by-step reasoning. We compare our acoustic prompting tominimal prompting and full chain-of-thought prompting techniques. We perform acontext window analysis on IEMOCAP and MELD, and find that using context helps,especially on IEMOCAP. We conclude with an error analysis on the generatedacoustic reasoning outputs from the omni-LLMs.
  </details>

- **[EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues](http://arxiv.org/abs/2503.21080v2)**  `arXiv:2503.21080`  `cs.CL`  
  _Yuhan Liu, Yunbo Long_
  <details open><summary>Abstract</summary>
  While large language model (LLM)-based chatbots have been applied foreffective engagement in credit dialogues, their capacity for dynamic emotionalexpression remains limited. Current agents primarily rely on passive empathyrather than affective reasoning. For instance, when faced with persistentclient negativity, the agent should employ strategic emotional adaptation byexpressing measured anger to discourage counterproductive behavior and guidethe conversation toward resolution. This context-aware emotional modulation isessential for imitating the nuanced decision-making of human negotiators. Thispaper introduces an EQ-negotiator that combines emotion sensing frompre-trained language models (PLMs) with emotional reasoning based on GameTheory and Hidden Markov Models. It takes into account both the current andhistorical emotions of the client to better manage and address negativeemotions during interactions. By fine-tuning pre-trained language models (PLMs)on public emotion datasets and validating them on the credit dialogue datasets,our approach enables LLM-based agents to effectively capture shifts in clientemotions and dynamically adjust their response tone based on our emotiondecision policies in real-world financial negotiations. This EQ-negotiator canalso help credit agencies foster positive client relationships, enhancingsatisfaction in credit services.
  </details>

- **[DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts](http://arxiv.org/abs/2503.19498v2)**  `arXiv:2503.19498`  `cs.CL`  
  _Ling Zhong, Yujing Lu, Jing Yang, Weiming Li, Peng Wei, Yongheng Wang, et al._
  <details open><summary>Abstract</summary>
  Chart Question Answering (CQA) benchmarks are essential for evaluating thecapability of Multimodal Large Language Models (MLLMs) to interpret visualdata. However, current benchmarks focus primarily on the evaluation ofgeneral-purpose CQA but fail to adequately capture domain-specific challenges.We introduce DomainCQA, a systematic methodology for constructingdomain-specific CQA benchmarks, and demonstrate its effectiveness by developingAstroChart, a CQA benchmark in the field of astronomy. Our evaluation showsthat chart reasoning and combining chart information with domain knowledge fordeeper analysis and summarization, rather than domain-specific knowledge, posethe primary challenge for existing MLLMs, highlighting a critical gap incurrent benchmarks. By providing a scalable and rigorous framework, DomainCQAenables more precise assessment and improvement of MLLMs for domain-specificapplications.
  </details>

- **[Sun-Shine: A Large Language Model for Tibetan Culture](http://arxiv.org/abs/2503.18288v2)**  `arXiv:2503.18288`  `cs.CL`  
  _Cheng Huang, Fan Gao, Nyima Tashi, Yutong Liu, Xiangxiang Wang, Thupten Tsering, et al._
  <details open><summary>Abstract</summary>
  Tibetan, a minority language in China, features a highly intricategrammatical structure, characterized by four verb tenses and a tense systemwith frequent irregularities, contributing to its extensive inflectionaldiversity. Recently, advances in Large Language Models (LLMs) have transformedthe paradigm in many domains. Despite the success in other fields, current LLMsoften fall short in catering to the needs of domain experts like Tibetans, andthe potential of LLMs for Tibetan culture is under-explored. The intrinsicreasons are the immense and intricate nature of Tibetan culture as well as thenecessity for higher granularity and richness in knowledge. Simultaneously, thecomplexity and uniqueness of its grammatical structure, coupled with its statusas a minority ethnic language, contribute to data scarcity, which remains afundamental challenge. To alleviate these issues, we introduce Llama-Sunshine(Sun-Shine), the first large language model for Tibetan culture, which isexpert in various Tibetan language processing tasks. Sun-Shine incorporatesstate-of-the-art model architectures optimized for Tibetan's linguisticfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverseTibetan texts such as literature, religious scripts, news, and conversationaldata, which is also the first large-scale dataset for Tibetan culture. Thoughcomprehensive experiments, Sun-Shine not only demonstrates a higher level ofknowledge expertise for Tibetan culture but also gains preliminary embodiedintelligence capabilities in Tibetan language processing tasks, like languagemodeling, text classification, machine translation, and syntactic analysis.Moreover, it excels in low-resource scenarios, showcasing strong generalizationcapabilities.
  </details>

- **[Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering](http://arxiv.org/abs/2503.18172v2)**  `arXiv:2503.18172`  `cs.CL` `cs.AI`  
  _Zixin Chen, Sicheng Song, Kashun Shum, Yanna Lin, Rui Sheng, Huamin Qu_
  <details open><summary>Abstract</summary>
  Misleading chart visualizations, which intentionally manipulate datarepresentations to support specific claims, can distort perceptions and lead toincorrect conclusions. Despite decades of research, misleading visualizationsremain a widespread and pressing issue. Recent advances in multimodal largelanguage models (MLLMs) have demonstrated strong chart comprehensioncapabilities, yet no existing work has systematically evaluated their abilityto detect and interpret misleading charts. This paper introduces the MisleadingChart Question Answering (Misleading ChartQA) Benchmark, a large-scalemultimodal dataset designed to assess MLLMs in identifying and reasoning aboutmisleading charts. It contains over 3,000 curated examples, covering 21 typesof misleaders and 10 chart types. Each example includes standardized chartcode, CSV data, and multiple-choice questions with labeled explanations,validated through multi-round MLLM checks and exhausted expert human review. Webenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitationsin identifying visually deceptive practices. We also propose a novel pipelinethat detects and localizes misleaders, enhancing MLLMs' accuracy in misleadingchart interpretation. Our work establishes a foundation for advancingMLLM-driven misleading chart comprehension. We publicly release the sampledataset to support further research in this critical area.
  </details>

- **[LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning](http://arxiv.org/abs/2503.17125v5)**  `arXiv:2503.17125`  `cs.RO` `cs.AI`  
  _Chan Kim, Seung-Woo Seo, Seong-Woo Kim_
  <details open><summary>Abstract</summary>
  Deep Reinforcement Learning (DRL) has demonstrated strong performance inrobotic control but remains susceptible to out-of-distribution (OOD) states,often resulting in unreliable actions and task failure. While previous methodshave focused on minimizing or preventing OOD occurrences, they largely neglectrecovery once an agent encounters such states. Although the latest research hasattempted to address this by guiding agents back to in-distribution states,their reliance on uncertainty estimation hinders scalability in complexenvironments. To overcome this limitation, we introduce Language Models forOut-of-Distribution Recovery (LaMOuR), which enables recovery learning withoutrelying on uncertainty estimation. LaMOuR generates dense reward codes thatguide the agent back to a state where it can successfully perform its originaltask, leveraging the capabilities of LVLMs in image description, logicalreasoning, and code generation. Experimental results show that LaMOuRsubstantially enhances recovery efficiency across diverse locomotion tasks andeven generalizes effectively to complex environments, including humanoidlocomotion and mobile manipulation, where existing methods struggle. The codeand supplementary materials are available at https://lamour-rl.github.io/.
  </details>

- **[Can Language Models Follow Multiple Turns of Entangled Instructions?](http://arxiv.org/abs/2503.13222v2)**  `arXiv:2503.13222`  `cs.CL` `cs.AI`  
  _Chi Han_
  <details open><summary>Abstract</summary>
  Despite significant achievements in improving the instruction-followingcapabilities of large language models (LLMs), the ability to process multiplepotentially entangled or conflicting instructions remains a considerablechallenge. Real-world scenarios often require consistency across multipleinstructions over time, such as secret privacy, personal preferences, andprioritization, which demand sophisticated abilities to integrate multipleturns and carefully balance competing objectives when instructions intersect orconflict. This work presents a systematic investigation of LLMs' capabilitiesin handling multiple turns of instructions, covering three levels ofdifficulty: (1) retrieving information from instructions, (2) tracking andreasoning across turns, and (3) resolving conflicts among instructions. Weconstruct MultiTurnInstruct with around 1.1K high-quality multi-turnconversations through the human-in-the-loop approach and result in ninecapability categories, including statics and dynamics, reasoning, andmultitasking. Our finding reveals an intriguing trade-off between differentcapabilities. While GPT models demonstrate superior memorization, they showreduced effectiveness in privacy-protection tasks requiring selectiveinformation withholding. Larger models exhibit stronger reasoning capabilitiesbut still struggle with resolving conflicting instructions. Importantly, theseperformance gaps cannot be attributed solely to information loss, as modelsdemonstrate strong BLEU scores on memorization tasks but their attentionmechanisms fail to integrate multiple related instructions effectively. Thesefindings highlight critical areas for improvement in complex real-world tasksinvolving multi-turn instructions.
  </details>

- **[Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical Investigation](http://arxiv.org/abs/2503.12854v2)**  `arXiv:2503.12854`  `cs.CL`  
  _Songjun Tu, Jiahao Lin, Xiangyu Tian, Qichao Zhang, Linjing Li, Yuqian Fu, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in post-training methodologies for large language models(LLMs) have highlighted reinforcement learning (RL) as a critical component forenhancing reasoning. However, the substantial computational costs associatedwith RL-based approaches have led to growing interest in alternative paradigms,such as Direct Preference Optimization (DPO). In this study, we investigate theeffectiveness of DPO in facilitating self-improvement for LLMs throughiterative preference-based learning. We demonstrate that a single round of DPOwith coarse filtering significantly enhances mathematical reasoningperformance, particularly for strong base model. Furthermore, we design aniterative enhancement framework for both the generator and the reward model(RM), enabling their mutual improvement through online interaction acrossmultiple rounds of DPO. Finally, with simple verifiable rewards, our modelDPO-VP achieves RL-level performance with significantly lower computationaloverhead. These findings highlight DPO as a scalable and cost-effectivealternative to RL, offering a practical solution for enhancing LLM reasoning inresource-constrained situations.
  </details>

- **[Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](http://arxiv.org/abs/2503.12496v2)**  `arXiv:2503.12496`  `cs.CV`  
  _Tianyuan Qu, Longxiang Tang, Bohao Peng, Senqiao Yang, Bei Yu, Jiaya Jia_
  <details open><summary>Abstract</summary>
  The rise of Large Vision-Language Models (LVLMs) has significantly advancedvideo understanding. However, efficiently processing long videos remains achallenge due to the ``Sampling Dilemma'': low-density sampling risks missingcritical information, while high-density sampling introduces redundancy. Toaddress this issue, we introduce LSDBench, the first benchmark designed toevaluate LVLMs on long-video tasks by constructing high Necessary SamplingDensity (NSD) questions, where NSD represents the minimum sampling densityrequired to accurately answer a given question. LSDBench focuses on dense,short-duration actions to rigorously assess the sampling strategies employed byLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novelReasoning-Driven Hierarchical Sampling (RHS) framework, which combines globallocalization of question-relevant cues with local dense sampling for preciseinference. Additionally, we develop a lightweight Semantic-Guided FrameSelector to prioritize informative frames, enabling RHS to achieve comparableor superior performance with significantly fewer sampled frames. Together, ourLSDBench and RHS framework address the unique challenges of high-NSD long-videotasks, setting a new standard for evaluating and improving LVLMs in thisdomain. Our benchmark and evaluation codes has been released at:https://github.com/dvlab-research/LSDBench
  </details>

- **[Generalizable Prompt Learning of CLIP: A Brief Overview](http://arxiv.org/abs/2503.01263v3)**  `arXiv:2503.01263`  `cs.CL` `cs.CV`  
  _Fangming Cui, Yonggang Zhang, Xuan Wang, Xule Wang, Liang Xiao_
  <details open><summary>Abstract</summary>
  Existing vision-language models (VLMs) such as CLIP have showcased animpressive capability to generalize well across various downstream tasks. Thesemodels leverage the synergy between visual and textual information, enablingthem to understand and reason about the content present in images and text in aunified manner. This article provides a brief overview of CLIP based onfew-shot prompt learning, including experimental data and technicalcharacteristics of some methods. The purpose of this review is to provide areference for researchers who have just started their research in generalizableprompting of CLIP through few-shot training for classification across 15datasets and also to facilitate the integration of this field by researchers inother downstream tasks.
  </details>

- **[SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines](http://arxiv.org/abs/2502.14739v4)**  `arXiv:2502.14739`  `cs.CL`  
  _M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) have demonstrated remarkable proficiency inmainstream academic disciplines such as mathematics, physics, and computerscience. However, human knowledge encompasses over 200 specialized disciplines,far exceeding the scope of existing benchmarks. The capabilities of LLMs inmany of these specialized fields-particularly in light industry, agriculture,and service-oriented disciplines-remain inadequately evaluated. To address thisgap, we present SuperGPQA, a comprehensive benchmark that evaluatesgraduate-level knowledge and reasoning capabilities across 285 disciplines. Ourbenchmark employs a novel Human-LLM collaborative filtering mechanism toeliminate trivial or ambiguous questions through iterative refinement based onboth LLM responses and expert feedback. Our experimental results revealsignificant room for improvement in the performance of current state-of-the-artLLMs across diverse knowledge domains (e.g., the reasoning-focused modelDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlightingthe considerable gap between current model capabilities and artificial generalintelligence. Additionally, we present comprehensive insights from ourmanagement of a large-scale annotation process, involving over 80 expertannotators and an interactive Human-LLM collaborative system, offering valuablemethodological guidance for future research initiatives of comparable scope.
  </details>

- **[Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance](http://arxiv.org/abs/2502.08127v2)**  `arXiv:2502.08127`  `cs.CL`  
  _Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Han Yi, Jimin Huang, et al._
  <details open><summary>Abstract</summary>
  While large language models (LLMs) have shown strong general reasoningcapabilities, their effectiveness in financial reasoning, which is crucial forreal-world financial applications remains underexplored. In this study, weconduct a comprehensive evaluation of 24 state-of-the-art general andreasoning-focused LLMs across four complex financial reasoning tasks involvingfinancial text, tabular data, and equations. We assess key capabilities such asnumerical reasoning, tabular interpretation, financial terminologycomprehension, long-context understanding, and equation-based problem solving.Our analysis reveals that while data quality and pretraining contribute toperformance, general techniques like chain-of-thought (CoT) fine-tuning offerlimited gains in financial tasks. To address this, we propose twodomain-adapted models, Fino1-8B and Fino1-14B, trained with CoT fine-tuning andreinforcement learning using domain-specific reasoning paths. Our models aretrained on a carefully curated dataset integrating high-quality examples fromdiverse sources, covering financial reports, tables, equations, and structuredXBRL texts. Despite limited training data, they achieve an 7-9% performanceimprovement, outperforming several advanced LLMs, including GPT-o1,GPT-o3-mini, GPT-4.5, and comparable with DeepSeek models (V3 and R1),demonstrating strong practical value in resource, constrained scenarios. Ourfindings highlight the need for domain-specific adaptations in financialreasoning, and we release all datasets, models, and code for future research.
  </details>

- **[Visual Agentic AI for Spatial Reasoning with a Dynamic API](http://arxiv.org/abs/2502.06787v2)**  `arXiv:2502.06787`  `cs.CV`  
  _Damiano Marsili, Rohun Agrawal, Yisong Yue, Georgia Gkioxari_
  <details open><summary>Abstract</summary>
  Visual reasoning -- the ability to interpret the visual world -- is crucialfor embodied agents that operate within three-dimensional scenes. Progress inAI has led to vision and language models capable of answering questions fromimages. However, their performance declines when tasked with 3D spatialreasoning. To tackle the complexity of such reasoning problems, we introduce anagentic program synthesis approach where LLM agents collaboratively generate aPythonic API with new functions to solve common subproblems. Our methodovercomes limitations of prior approaches that rely on a static, human-definedAPI, allowing it to handle a wider range of queries. To assess AI capabilitiesfor 3D understanding, we introduce a new benchmark of queries involvingmultiple steps of grounding and inference. We show that our method outperformsprior zero-shot models for visual reasoning in 3D and empirically validate theeffectiveness of our agentic framework for 3D spatial reasoning tasks. Projectwebsite: https://glab-caltech.github.io/vadar/
  </details>

- **[Output Scouting: Auditing Large Language Models for Catastrophic Responses](http://arxiv.org/abs/2410.05305v2)**  `arXiv:2410.05305`  `cs.CL` `cs.AI`  
  _Andrew Bell, Joao Fonseca_
  <details open><summary>Abstract</summary>
  Recent high profile incidents in which the use of Large Language Models(LLMs) resulted in significant harm to individuals have brought about a growinginterest in AI safety. One reason LLM safety issues occur is that models oftenhave at least some non-zero probability of producing harmful outputs. In thiswork, we explore the following scenario: imagine an AI safety auditor issearching for catastrophic responses from an LLM (e.g. a "yes" responses to"can I fire an employee for being pregnant?"), and is able to query the model alimited number times (e.g. 1000 times). What is a strategy for querying themodel that would efficiently find those failure responses? To this end, wepropose output scouting: an approach that aims to generate semantically fluentoutputs to a given prompt matching any target probability distribution. We thenrun experiments using two LLMs and find numerous examples of catastrophicresponses. We conclude with a discussion that includes advice for practitionerswho are looking to implement LLM auditing for catastrophic responses. We alsorelease an open-source toolkit (https://github.com/joaopfonseca/outputscouting)that implements our auditing framework using the Hugging Face transformerslibrary.
  </details>

- **[Vocabulary-Free 3D Instance Segmentation with Vision and Language Assistant](http://arxiv.org/abs/2408.10652v2)**  `arXiv:2408.10652`  `cs.CV` `cs.AI`  
  _Guofeng Mei, Luigi Riz, Yiming Wang, Fabio Poiesi_
  <details open><summary>Abstract</summary>
  Most recent 3D instance segmentation methods are open vocabulary, offering agreater flexibility than closed-vocabulary methods. Yet, they are limited toreasoning within a specific set of concepts, \ie the vocabulary, prompted bythe user at test time. In essence, these models cannot reason in an open-endedfashion, i.e., answering "List the objects in the scene.''. We introduce thefirst method to address 3D instance segmentation in a setting that is void ofany vocabulary prior, namely a vocabulary-free setting. We leverage a largevision-language assistant and an open-vocabulary 2D instance segmenter todiscover and ground semantic categories on the posed images. To form 3Dinstance mask, we first partition the input point cloud into dense superpoints,which are then merged into 3D instance masks. We propose a novel superpointmerging strategy via spectral clustering, accounting for both mask coherenceand semantic coherence that are estimated from the 2D object instance masks. Weevaluate our method using ScanNet200 and Replica, outperforming existingmethods in both vocabulary-free and open-vocabulary settings. Code will be madeavailable. Project page: https://gfmei.github.io/PoVo
  </details>

- **[ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context](http://arxiv.org/abs/2403.02177v3)**  `arXiv:2403.02177`  `cs.CL`  
  _Zirui Wu, Yansong Feng_
  <details open><summary>Abstract</summary>
  Tables play a crucial role in conveying information in various domains. Wepropose a Plan-then-Reason framework to answer different types of user queriesover tables with sentence context. The framework first plans the reasoningpaths over the context, then assigns each step to program-based or textualreasoning to reach the final answer. This framework enhances the tablereasoning abilities for both in-context learning and fine-tuning methods.GPT-3.5-Turbo following Plan-then-Reason framework surpasses other promptingbaselines without self-consistency while using less API calls and in-contextdemonstrations. We also construct an instruction tuning set TrixInstruct toevaluate the effectiveness of fine-tuning with this framework. We presentProTrix model family by finetuning models on TrixInstruct. Our experiments showthat ProTrix family generalizes to diverse unseen tabular tasks with only 6ktraining instances. We further demonstrate that ProTrix can generate accurateand faithful explanations to answer complex free-form questions. Our workunderscores the importance of the planning and reasoning abilities towards amodel over tabular tasks with generalizability and interpretability. Weopen-source our dataset and models at https://github.com/WilliamZR/ProTrix.
  </details>
