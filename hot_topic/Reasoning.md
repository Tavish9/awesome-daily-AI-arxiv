# üîç Reasoning Papers ¬∑ 2025-03-31

[![Total Papers](https://img.shields.io/badge/Papers-42-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Reasoning` `System 2` `Slow Thinking` `o1` `r1`  
**Filter**: `None`

---

## üìö Paper List

- **[RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy](http://arxiv.org/abs/2503.24388v1)**  `arXiv:2503.24388`  `cs.LG` `cs.CL` `cs.AI`  
  _Zhonghan Zhao, Wenwei Zhang, Haian Huang, Kuikun Liu, Jianfei Gao, Gaoang Wang, et al._
  <details open><summary>Abstract</summary>
  Reasoning before action and imagining potential outcomes (i.e., world models)are essential for embodied agents operating in complex open-world environments.Yet, prior work either incorporates only one of these abilities in anend-to-end agent or integrates multiple specialized models into an agentsystem, limiting the learning efficiency and generalization of the policy.Thus, this paper makes the first attempt to synergize Reasoning and Imaginationin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-endmanner, we construct a data pipeline that progressively integrates and enrichesthe content of imagination and reasoning in the trajectories collected fromexisting agents. The joint learning of reasoning and next image generationexplicitly models the inherent correlation between reasoning, action, anddynamics of environments, and thus exhibits more than $17\times$ sampleefficiency improvements and generalization in comparison with previous works.During inference, RIG first reasons about the next action, produces potentialaction, and then predicts the action outcomes, which offers the agent a chanceto review and self-correct based on the imagination before taking real actions.Experimental results show that the synergy of reasoning and imagination notonly improves the robustness, generalization, and interoperability ofgeneralist policy but also enables test-time scaling to enhance overallperformance.
  </details>

- **[ACPBench Hard: Unrestrained Reasoning about Action, Change, and Planning](http://arxiv.org/abs/2503.24378v1)**  `arXiv:2503.24378`  `cs.AI`  
  _Harsha Kokel, Michael Katz, Kavitha Srinivas, Shirin Sohrabi_
  <details open><summary>Abstract</summary>
  The ACPBench dataset provides atomic reasoning tasks required for efficientplanning. The dataset is aimed at distilling the complex plan generation taskinto separate atomic reasoning tasks in their easiest possible form, boolean ormultiple-choice questions, where the model has to choose the right answer fromthe provided options. While the aim of ACPBench is to test the simplest form ofreasoning about action and change, when tasked with planning, a model does nottypically have options to choose from and thus the reasoning required forplanning dictates an open-ended, generative form for these tasks. To that end,we introduce ACPBench Hard, a generative version of ACPBench, with open-endedquestions which the model needs to answer. Models that perform well on thesetasks could in principle be integrated into a planner or be used directly as apolicy. We discuss the complexity of these tasks as well as the complexity ofvalidating the correctness of their answers and present validation algorithmsfor each task. Equipped with these validators, we test the performance of avariety of models on our tasks and find that for most of these tasks theperformance of even the largest models is still subpar. Our experiments showthat no model outperforms another in these tasks and with a few exceptions alltested language models score below 65%, indicating that even the currentfrontier language models have a long way to go before they can reliably reasonabout planning. In fact, even the so-called reasoning models struggle withsolving these reasoning tasks. ACPBench Hard collection is available at thefollowing link: https://ibm.github.io/ACPBench
  </details>

- **[Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models](http://arxiv.org/abs/2503.24377v1)**  `arXiv:2503.24377`  `cs.CL` `cs.AI`  
  _Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in Large Language Models (LLMs) have significantlyenhanced their ability to perform complex reasoning tasks, transitioning fromfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).While System 2 reasoning improves task accuracy, it often incurs substantialcomputational costs due to its slow thinking nature and inefficient orunnecessary reasoning behaviors. In contrast, System 1 reasoning iscomputationally efficient but leads to suboptimal performance. Consequently, itis critical to balance the trade-off between performance (benefits) andcomputational costs (budgets), giving rise to the concept of reasoning economy.In this survey, we provide a comprehensive analysis of reasoning economy inboth the post-training and test-time inference stages of LLMs, encompassing i)the cause of reasoning inefficiency, ii) behavior analysis of differentreasoning patterns, and iii) potential solutions to achieve reasoning economy.By offering actionable insights and highlighting open challenges, we aim toshed light on strategies for improving the reasoning economy of LLMs, therebyserving as a valuable resource for advancing research in this evolving area. Wealso provide a public repository to continually track developments in thisfast-evolving field.
  </details>

- **[Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](http://arxiv.org/abs/2503.24376v1)**  `arXiv:2503.24376`  `cs.LG` `cs.CV` `cs.CL` `cs.AI`  
  _Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in Chain of Thought (COT) generation have significantlyimproved the reasoning capabilities of Large Language Models (LLMs), withreinforcement learning (RL) emerging as an effective post-training approach.Multimodal Large Language Models (MLLMs) inherit this reasoning potential butremain underexplored in tasks requiring both perception and logical reasoning.To address this, we introduce SEED-Bench-R1, a benchmark designed tosystematically evaluate post-training methods for MLLMs in video understanding.It includes intricate real-world videos and complex everyday planning tasks inthe format of multiple-choice questions, requiring sophisticated perception andreasoning. SEED-Bench-R1 assesses generalization through a three-levelhierarchy: in-distribution, cross-environment, and cross-environment-taskscenarios, equipped with a large-scale training dataset with easily verifiableground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RLwith supervised fine-tuning (SFT), demonstrating RL's data efficiency andsuperior performance on both in-distribution and out-of-distribution tasks,even outperforming SFT on general video understanding benchmarks likeLongVideoBench. Our detailed analysis reveals that RL enhances visualperception but often produces less logically coherent reasoning chains. Weidentify key limitations such as inconsistent reasoning and overlooked visualcues, and suggest future improvements in base model reasoning, reward modeling,and RL robustness against noisy signals.
  </details>

- **[Effectively Controlling Reasoning Models through Thinking Intervention](http://arxiv.org/abs/2503.24370v1)**  `arXiv:2503.24370`  `cs.LG` `cs.CL` `cs.AI`  
  _Tong Wu, Chong Xiang, Jiachen T. Wang, Prateek Mittal_
  <details open><summary>Abstract</summary>
  Reasoning-enhanced large language models (LLMs) explicitly generateintermediate reasoning steps prior to generating final answers, helping themodel excel in complex problem-solving. In this paper, we demonstrate that thisemerging generation framework offers a unique opportunity for more fine-grainedcontrol over model behavior. We propose Thinking Intervention, a novel paradigmdesigned to explicitly guide the internal reasoning processes of LLMs bystrategically inserting or revising specific thinking tokens. We conductcomprehensive evaluations across multiple tasks, including instructionfollowing on IFEval, instruction hierarchy on SEP, and safety alignment onXSTest and SORRY-Bench. Our results demonstrate that Thinking Interventionsignificantly outperforms baseline prompting approaches, achieving up to 6.7%accuracy gains in instruction-following scenarios, 15.4% improvements inreasoning about instruction hierarchies, and a 40.0% increase in refusal ratesfor unsafe prompts using open-source DeepSeek R1 models. Overall, our workopens a promising new research avenue for controlling reasoning LLMs.
  </details>

- **[Query and Conquer: Execution-Guided SQL Generation](http://arxiv.org/abs/2503.24364v1)**  `arXiv:2503.24364`  `cs.CL`  
  _≈Åukasz Borchmann, Marek Wydmuch_
  <details open><summary>Abstract</summary>
  We propose a novel approach for generating complex outputs that significantlyimproves accuracy in text-to-SQL tasks. Our method leverages execution resultsto select the most semantically consistent query from multiple candidates,enabling smaller, cost-effective models to surpass computationally intensivereasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inferencecost by as much as 30 times. It integrates effortlessly with existing models,offering a practical and scalable pathway to state-of-the-art SQL generation.
  </details>

- **[Is analogy enough to draw novel adjective-noun inferences?](http://arxiv.org/abs/2503.24293v1)**  `arXiv:2503.24293`  `cs.CL`  
  _Hayley Ross, Kathryn Davidson, Najoung Kim_
  <details open><summary>Abstract</summary>
  Recent work (Ross et al., 2025, 2024) has argued that the ability of humansand LLMs respectively to generalize to novel adjective-noun combinations showsthat they each have access to a compositional mechanism to determine thephrase's meaning and derive inferences. We study whether these inferences caninstead be derived by analogy to known inferences, without need forcomposition. We investigate this by (1) building a model of analogicalreasoning using similarity over lexical items, and (2) asking humanparticipants to reason by analogy. While we find that this strategy works wellfor a large proportion of the dataset of Ross et al. (2025), there are novelcombinations for which both humans and LLMs derive convergent inferences butwhich are not well handled by analogy. We thus conclude that the mechanismhumans and LLMs use to generalize in these cases cannot be fully reduced toanalogy, and likely involves composition.
  </details>

- **[Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model](http://arxiv.org/abs/2503.24290v1)**  `arXiv:2503.24290`  `cs.LG` `cs.CL`  
  _Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, Heung-Yeung Shum_
  <details open><summary>Abstract</summary>
  We introduce Open-Reasoner-Zero, the first open source implementation oflarge-scale reasoning-oriented RL training focusing on scalability, simplicityand accessibility. Through extensive experiments, we demonstrate that aminimalist approach, vanilla PPO with GAE ($\lambda=1$, $\gamma=1$) andstraightforward rule-based rewards, without any KL regularization, issufficient to scale up both response length and benchmark performance, similarto the phenomenon observed in DeepSeek-R1-Zero. Using the same base model asDeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance onAIME2024, MATH500, and the GPQA Diamond benchmark while demonstratingremarkable efficiency -- requiring only a tenth of the training steps, comparedto DeepSeek-R1-Zero pipeline. In the spirit of open source, we release oursource code, parameter settings, training data, and model weights acrossvarious sizes.
  </details>

- **[Learning Velocity and Acceleration: Self-Supervised Motion Consistency for Pedestrian Trajectory Prediction](http://arxiv.org/abs/2503.24272v1)**  `arXiv:2503.24272`  `cs.LG` `cs.CV`  
  _Yizhou Huang, Yihua Cheng, Kezhi Wang_
  <details open><summary>Abstract</summary>
  Understanding human motion is crucial for accurate pedestrian trajectoryprediction. Conventional methods typically rely on supervised learning, whereground-truth labels are directly optimized against predicted trajectories. Thisamplifies the limitations caused by long-tailed data distributions, making itdifficult for the model to capture abnormal behaviors. In this work, we proposea self-supervised pedestrian trajectory prediction framework that explicitlymodels position, velocity, and acceleration. We leverage velocity andacceleration information to enhance position prediction through featureinjection and a self-supervised motion consistency mechanism. Our modelhierarchically injects velocity features into the position stream. Accelerationfeatures are injected into the velocity stream. This enables the model topredict position, velocity, and acceleration jointly. From the predictedposition, we compute corresponding pseudo velocity and acceleration, allowingthe model to learn from data-generated pseudo labels and thus achieveself-supervised learning. We further design a motion consistency evaluationstrategy grounded in physical principles; it selects the most reasonablepredicted motion trend by comparing it with historical dynamics and uses thistrend to guide and constrain trajectory generation. We conduct experiments onthe ETH-UCY and Stanford Drone datasets, demonstrating that our method achievesstate-of-the-art performance on both datasets.
  </details>

- **[FakeScope: Large Multimodal Expert Model for Transparent AI-Generated Image Forensics](http://arxiv.org/abs/2503.24267v1)**  `arXiv:2503.24267`  `cs.CV`  
  _Yixuan Li, Yu Tian, Yipo Huang, Wei Lu, Shiqi Wang, Weisi Lin, et al._
  <details open><summary>Abstract</summary>
  The rapid and unrestrained advancement of generative artificial intelligence(AI) presents a double-edged sword: while enabling unprecedented creativity, italso facilitates the generation of highly convincing deceptive content,undermining societal trust. As image generation techniques become increasinglysophisticated, detecting synthetic images is no longer just a binary task: itnecessitates interpretable, context-aware methodologies that enhancetrustworthiness and transparency. However, existing detection models primarilyfocus on classification, offering limited explanatory insights into imageauthenticity. In this work, we propose FakeScope, an expert multimodal model(LMM) tailored for AI-generated image forensics, which not only identifiesAI-synthetic images with high accuracy but also provides rich, interpretable,and query-driven forensic insights. We first construct FakeChain dataset thatcontains linguistic authenticity reasoning based on visual trace evidence,developed through a novel human-machine collaborative framework. Building uponit, we further present FakeInstruct, the largest multimodal instruction tuningdataset containing 2 million visual instructions tailored to enhance forensicawareness in LMMs. FakeScope achieves state-of-the-art performance in bothclosed-ended and open-ended forensic scenarios. It can distinguish syntheticimages with high accuracy while offering coherent and insightful explanations,free-form discussions on fine-grained forgery attributes, and actionableenhancement strategies. Notably, despite being trained exclusively onqualitative hard labels, FakeScope demonstrates remarkable zero-shotquantitative capability on detection, enabled by our proposed token-basedprobability estimation strategy. Furthermore, FakeScope exhibits stronggeneralization and in-the-wild ability, ensuring its applicability inreal-world scenarios.
  </details>

- **[What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models](http://arxiv.org/abs/2503.24235v1)**  `arXiv:2503.24235`  `cs.CL` `cs.AI`  
  _Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, et al._
  <details open><summary>Abstract</summary>
  As enthusiasm for scaling computation (data and parameters) in thepretraining era gradually diminished, test-time scaling (TTS), also referred toas ``test-time computing'' has emerged as a prominent research focus. Recentstudies demonstrate that TTS can further elicit the problem-solvingcapabilities of large language models (LLMs), enabling significantbreakthroughs not only in specialized reasoning tasks, such as mathematics andcoding, but also in general tasks like open-ended Q&A. However, despite theexplosion of recent efforts in this area, there remains an urgent need for acomprehensive survey offering a systemic understanding. To fill this gap, wepropose a unified, multidimensional framework structured along four coredimensions of TTS research: what to scale, how to scale, where to scale, andhow well to scale. Building upon this taxonomy, we conduct an extensive reviewof methods, application scenarios, and assessment aspects, and present anorganized decomposition that highlights the unique functional roles ofindividual techniques within the broader TTS landscape. From this analysis, wedistill the major developmental trajectories of TTS to date and offer hands-onguidelines for practical deployment. Furthermore, we identify several openchallenges and offer insights into promising future directions, includingfurther scaling, clarifying the functional essence of techniques, generalizingto more tasks, and more attributions.
  </details>

- **[All You Need is Sally-Anne: ToM in AI Strongly Supported After Surpassing Tests for 3-Year-Olds](http://arxiv.org/abs/2503.24215v1)**  `arXiv:2503.24215`  `cs.AI`  
  _Nitay Alon, Joseph Barnby, Reuth Mirsky, Stefan Sarkadi_
  <details open><summary>Abstract</summary>
  Theory of Mind (ToM) is a hallmark of human cognition, allowing individualsto reason about others' beliefs and intentions. Engineers behind recentadvances in Artificial Intelligence (AI) have claimed to demonstrate comparablecapabilities. This paper presents a model that surpasses traditional ToM testsdesigned for 3-year-old children, providing strong support for the presence ofToM in AI systems.
  </details>

- **[TwT: Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers' Guidance](http://arxiv.org/abs/2503.24198v1)**  `arXiv:2503.24198`  `cs.CL`  
  _Jingxian Xu, Mengyu Zhou, Weichang Liu, Hanbing Liu, Shi Han, Dongmei Zhang_
  <details open><summary>Abstract</summary>
  Large Language Models (LLMs) have made significant strides in problem-solvingby incorporating reasoning processes. However, this enhanced reasoningcapability results in an increased number of output tokens during inference,leading to higher computational costs. To address this challenge, we proposeTwT (Thinking without Tokens), a method that reduces inference-time coststhrough habitual reasoning distillation with multi-teachers' guidance, whilemaintaining high performance. Our approach introduces a Habitual ReasoningDistillation method, which internalizes explicit reasoning into the model'shabitual behavior through a Teacher-Guided compression strategy inspired byhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling(DCRS), a technique that generates a high-quality and diverse distillationdataset using multiple teacher models, making our method suitable forunsupervised scenarios. Experimental results demonstrate that TwT effectivelyreduces inference costs while preserving superior performance, achieving up toa 13.6% improvement in accuracy with fewer output tokens compared to otherdistillation methods, offering a highly practical solution for efficient LLMdeployment.
  </details>

- **[LLM4FS: Leveraging Large Language Models for Feature Selection and How to Improve It](http://arxiv.org/abs/2503.24157v1)**  `arXiv:2503.24157`  `cs.LG`  
  _Jianhao Li, Xianchao Xiu_
  <details open><summary>Abstract</summary>
  Recent advances in large language models (LLMs) have provided newopportunities for decision-making, particularly in the task of automatedfeature selection. In this paper, we first comprehensively evaluate LLM-basedfeature selection methods, covering the state-of-the-art DeepSeek-R1,GPT-o3-mini, and GPT-4.5. Then, we propose a novel hybrid strategy calledLLM4FS that integrates LLMs with traditional data-driven methods. Specifically,input data samples into LLMs, and directly call traditional data-driventechniques such as random forest and forward sequential selection. Notably, ouranalysis reveals that the hybrid strategy leverages the contextualunderstanding of LLMs and the high statistical reliability of traditionaldata-driven methods to achieve excellent feature selection performance, evensurpassing LLMs and traditional data-driven methods. Finally, we point out thelimitations of its application in decision-making.
  </details>

- **[Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to Embodied Cognition](http://arxiv.org/abs/2503.24110v1)**  `arXiv:2503.24110`  `cs.CL` `cs.AI`  
  _Fran√ßois Olivier, Zied Bouraoui_
  <details open><summary>Abstract</summary>
  Despite advances in embodied AI, agent reasoning systems still struggle tocapture the fundamental conceptual structures that humans naturally use tounderstand and interact with their environment. To address this, we propose anovel framework that bridges embodied cognition theory and agent systems byleveraging a formal characterization of image schemas, which are defined asrecurring patterns of sensorimotor experience that structure human cognition.By customizing LLMs to translate natural language descriptions into formalrepresentations based on these sensorimotor patterns, we will be able to createa neurosymbolic system that grounds the agent's understanding in fundamentalconceptual structures. We argue that such an approach enhances both efficiencyand interpretability while enabling more intuitive human-agent interactionsthrough shared embodied understanding.
  </details>

- **[Level the Level: Balancing Game Levels for Asymmetric Player Archetypes With Reinforcement Learning](http://arxiv.org/abs/2503.24099v1)**  `arXiv:2503.24099`  `cs.LG`  
  _Florian Rupp, Kai Eckert_
  <details open><summary>Abstract</summary>
  Balancing games, especially those with asymmetric multiplayer content,requires significant manual effort and extensive human playtesting duringdevelopment. For this reason, this work focuses on generating balanced levelstailored to asymmetric player archetypes, where the disparity in abilities isbalanced entirely through the level design. For instance, while one archetypemay have an advantage over another, both should have an equal chance ofwinning. We therefore conceptualize game balancing as a procedural contentgeneration problem and build on and extend a recently introduced method thatuses reinforcement learning to balance tile-based game levels. We evaluate themethod on four different player archetypes and demonstrate its ability tobalance a larger proportion of levels compared to two baseline approaches.Furthermore, our results indicate that as the disparity between playerarchetypes increases, the required number of training steps grows, while themodel's accuracy in achieving balance decreases.
  </details>

- **[H2VU-Benchmark: A Comprehensive Benchmark for Hierarchical Holistic Video Understanding](http://arxiv.org/abs/2503.24008v1)**  `arXiv:2503.24008`  `cs.CV` `cs.AI`  
  _Qi Wu, Quanlong Zheng, Yanhao Zhang, Junlin Xie, Jinguo Luo, Kuo Wang, et al._
  <details open><summary>Abstract</summary>
  With the rapid development of multimodal models, the demand for assessingvideo understanding capabilities has been steadily increasing. However,existing benchmarks for evaluating video understanding exhibit significantlimitations in coverage, task diversity, and scene adaptability. Theseshortcomings hinder the accurate assessment of models' comprehensive videounderstanding capabilities. To tackle this challenge, we propose a hierarchicaland holistic video understanding (H2VU) benchmark designed to evaluate bothgeneral video and online streaming video comprehension. This benchmarkcontributes three key features:  Extended video duration: Spanning videos from brief 3-second clips tocomprehensive 1.5-hour recordings, thereby bridging the temporal gaps found incurrent benchmarks. Comprehensive assessment tasks: Beyond traditionalperceptual and reasoning tasks, we have introduced modules forcountercommonsense comprehension and trajectory state tracking. These additionstest the models' deep understanding capabilities beyond mere prior knowledge.Enriched video data: To keep pace with the rapid evolution of current AIagents, we have expanded first-person streaming video datasets. This expansionallows for the exploration of multimodal models' performance in understandingstreaming videos from a first-person perspective. Extensive results from H2VUreveal that existing multimodal large language models (MLLMs) possesssubstantial potential for improvement in our newly proposed evaluation tasks.We expect that H2VU will facilitate advancements in video understandingresearch by offering a comprehensive and in-depth analysis of MLLMs.
  </details>

- **[AirCache: Activating Inter-modal Relevancy KV Cache Compression for Efficient Large Vision-Language Model Inference](http://arxiv.org/abs/2503.23956v1)**  `arXiv:2503.23956`  `cs.CV` `cs.AI`  
  _Kai Huang, Hao Zou, Bochen Wang, Ye Xi, Zhen Xie, Hao Wang_
  <details open><summary>Abstract</summary>
  Recent advancements in Large Visual Language Models (LVLMs) have gainedsignificant attention due to their remarkable reasoning capabilities andproficiency in generalization. However, processing a large number of visualtokens and generating long-context outputs impose substantial computationaloverhead, leading to excessive demands for key-value (KV) cache. To addressthis critical bottleneck, we propose AirCache, a novel KV cache compressionmethod aimed at accelerating LVLMs inference. This work systematicallyinvestigates the correlations between visual and textual tokens within theattention mechanisms of LVLMs. Our empirical analysis reveals considerableredundancy in cached visual tokens, wherein strategically eliminating thesetokens preserves model performance while significantly accelerating contextgeneration. Inspired by these findings, we introduce an elite observationwindow for assessing the importance of visual components in the KV cache,focusing on stable inter-modal relevancy modeling with enhancedmulti-perspective consistency. Additionally, we develop an adaptive layer-wisebudget allocation strategy that capitalizes on the strength and skewness oftoken importance distribution, showcasing superior efficiency compared touniform allocation. Comprehensive evaluations across multiple LVLMs andbenchmarks demonstrate that our method achieves comparable performance to thefull cache while retaining only 10% of visual KV cache, thereby reducingdecoding latency by 29% to 66% across various batch size and prompt length ofinputs. Notably, as cache retention rates decrease, our method exhibitsincreasing performance advantages over existing approaches.
  </details>

- **[Green MLOps to Green GenOps: An Empirical Study of Energy Consumption in Discriminative and Generative AI Operations](http://arxiv.org/abs/2503.23934v1)**  `arXiv:2503.23934`  `cs.LG` `cs.AI`  
  _Adri√°n S√°nchez-Momp√≥, Ioannis Mavromatis, Peizheng Li, Konstantinos Katsaros, Aftab Khan_
  <details open><summary>Abstract</summary>
  This study presents an empirical investigation into the energy consumption ofDiscriminative and Generative AI models within real-world MLOps pipelines. ForDiscriminative models, we examine various architectures and hyperparametersduring training and inference and identify energy-efficient practices. ForGenerative AI, Large Language Models (LLMs) are assessed, focusing primarily onenergy consumption across different model sizes and varying service requests.Our study employs software-based power measurements, ensuring ease ofreplication across diverse configurations, models, and datasets. We analysemultiple models and hardware setups to uncover correlations among variousmetrics, identifying key contributors to energy consumption. The resultsindicate that for Discriminative models, optimising architectures,hyperparameters, and hardware can significantly reduce energy consumptionwithout sacrificing performance. For LLMs, energy efficiency depends onbalancing model size, reasoning complexity, and request-handling capacity, aslarger models do not necessarily consume more energy when utilisation remainslow. This analysis provides practical guidelines for designing green andsustainable ML operations, emphasising energy consumption and carbon footprintreductions while maintaining performance. This paper can serve as a benchmarkfor accurately estimating total energy use across different types of AI models.
  </details>

- **[Entropy-Based Adaptive Weighting for Self-Training](http://arxiv.org/abs/2503.23913v1)**  `arXiv:2503.23913`  `cs.CL`  
  _Xiaoxuan Wang, Yihe Deng, Mingyu Derek Ma, Wei Wang_
  <details open><summary>Abstract</summary>
  The mathematical problem-solving capabilities of large language models havebecome a focal point of research, with growing interests in leveragingself-generated reasoning paths as a promising way to refine and enhance thesemodels. These paths capture step-by-step logical processes while requiring onlythe correct answer for supervision. The self-training method has been shown tobe effective in reasoning tasks while eliminating the need for external modelsand manual annotations. However, optimizing the use of self-generated data formodel training remains an open challenge. In this work, we proposeEntropy-Based Adaptive Weighting for Self-Training (EAST), an adaptiveweighting strategy designed to prioritize uncertain data during self-training.Specifically, EAST employs a mapping function with a tunable parameter thatcontrols the sharpness of the weighting, assigning higher weights to data wherethe model exhibits greater uncertainty. This approach guides the model to focuson more informative and challenging examples, thereby enhancing its reasoningability. We evaluate our approach on GSM8K and MATH benchmarks. Empiricalresults show that, while the vanilla method yields virtually no improvement(0%) on MATH, EAST achieves around a 1% gain over backbone model. On GSM8K,EAST attains a further 1-2% performance boost compared to the vanilla method.
  </details>

- **[Boosting MLLM Reasoning with Text-Debiased Hint-GRPO](http://arxiv.org/abs/2503.23905v1)**  `arXiv:2503.23905`  `cs.CV`  
  _Qihan Huang, Long Chan, Jinlong Liu, Wanggui He, Hao Jiang, Mingli Song, et al._
  <details open><summary>Abstract</summary>
  MLLM reasoning has drawn widespread research for its excellentproblem-solving capability. Current reasoning methods fall into two types: PRM,which supervises the intermediate reasoning steps, and ORM, which supervisesthe final results. Recently, DeepSeek-R1 has challenged the traditional viewthat PRM outperforms ORM, which demonstrates strong generalization performanceusing an ORM method (i.e., GRPO). However, current MLLM's GRPO algorithms stillstruggle to handle challenging and complex multimodal reasoning tasks (e.g.,mathematical reasoning). In this work, we reveal two problems that impede theperformance of GRPO on the MLLM: Low data utilization and Text-bias. Low datautilization refers to that GRPO cannot acquire positive rewards to update theMLLM on difficult samples, and text-bias is a phenomenon that the MLLM bypassesimage condition and solely relies on text condition for generation after GRPOtraining. To tackle these problems, this work proposes Hint-GRPO that improvesdata utilization by adaptively providing hints for samples of varyingdifficulty, and text-bias calibration that mitigates text-bias by calibratingthe token prediction logits with image condition in test-time. Experimentresults on three base MLLMs across eleven datasets demonstrate that ourproposed methods advance the reasoning capability of original MLLM by a largemargin, exhibiting superior performance to existing MLLM reasoning methods. Ourcode is available at https://github.com/hqhQAQ/Hint-GRPO.
  </details>

- **[Expanding RL with Verifiable Rewards Across Diverse Domains](http://arxiv.org/abs/2503.23829v1)**  `arXiv:2503.23829`  `cs.CL`  
  _Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, et al._
  <details open><summary>Abstract</summary>
  Reinforcement learning (RL) with verifiable rewards (RLVR) has shownpromising results in mathematical reasoning and coding tasks wherewell-structured reference answers are available. However, its applicability tobroader domains remains underexplored. In this work, we study the extension ofRLVR to more diverse domains such as medicine, chemistry, psychology, andeconomics. We observe high agreement in binary judgments across different largelanguage models (LLMs) when objective reference answers exist, which challengesthe necessity of large-scale annotation for training domain-specific rewardmodels. To address the limitations of binary rewards when handling unstructuredreference answers, we further incorporate model-based soft scoring into RLVR toimprove its flexibility. Our experiments show that a distilled generativereward model can serve as an effective cross-domain verifier, providingreliable reward signals for RL without requiring domain-specific annotations.By fine-tuning a base 7B model using various RL algorithms against our rewardmodel, we obtain policies that outperform state-of-the-art open-source alignedLLMs such as Qwen2.5-72B-Instruct and DeepSeek-R1-Distill-Qwen-32B by a largemargin, across domains in free-form answer settings. This also strengthensRLVR's robustness and scalability, highlighting its potential for real-worldapplications with noisy or weak labels.
  </details>

- **[When Counterfactual Reasoning Fails: Chaos and Real-World Complexity](http://arxiv.org/abs/2503.23820v1)**  `arXiv:2503.23820`  `cs.LG` `cs.AI`  
  _Yahya Aalaila, Gerrit Gro√ümann, Sumantrak Mukherjee, Jonas Wahl, Sebastian Vollmer_
  <details open><summary>Abstract</summary>
  Counterfactual reasoning, a cornerstone of human cognition anddecision-making, is often seen as the 'holy grail' of causal learning, withapplications ranging from interpreting machine learning models to promotingalgorithmic fairness. While counterfactual reasoning has been extensivelystudied in contexts where the underlying causal model is well-defined,real-world causal modeling is often hindered by model and parameteruncertainty, observational noise, and chaotic behavior. The reliability ofcounterfactual analysis in such settings remains largely unexplored. In thiswork, we investigate the limitations of counterfactual reasoning within theframework of Structural Causal Models. Specifically, we empirically investigate\emph{counterfactual sequence estimation} and highlight cases where it becomesincreasingly unreliable. We find that realistic assumptions, such as lowdegrees of model uncertainty or chaotic dynamics, can result incounterintuitive outcomes, including dramatic deviations between predicted andtrue counterfactual trajectories. This work urges caution when applyingcounterfactual reasoning in settings characterized by chaos and uncertainty.Furthermore, it raises the question of whether certain systems may posefundamental limitations on the ability to answer counterfactual questions abouttheir behavior.
  </details>

- **[An extension of linear self-attention for in-context learning](http://arxiv.org/abs/2503.23814v1)**  `arXiv:2503.23814`  `cs.LG`  
  _Katsuyuki Hagiwara_
  <details open><summary>Abstract</summary>
  In-context learning is a remarkable property of transformers and has been thefocus of recent research. An attention mechanism is a key component intransformers, in which an attention matrix encodes relationships between wordsin a sentence and is used as weights for words in a sentence. This mechanism iseffective for capturing language representations. However, it is questionablewhether naive self-attention is suitable for in-context learning in generaltasks, since the computation implemented by self-attention is somewhatrestrictive in terms of matrix multiplication. In fact, we may need appropriateinput form designs when considering heuristic implementations of computationalalgorithms. In this paper, in case of linear self-attention, we extend it byintroducing a bias matrix in addition to a weight matrix for an input. Despitethe simple extension, the extended linear self-attention can output anyconstant matrix, input matrix and multiplications of two or three matrices inthe input. Note that the second property implies that it can be a skipconnection. Therefore, flexible matrix manipulations can be implemented byconnecting the extended linear self-attention components. As an example ofimplementation using the extended linear self-attention, we show a heuristicconstruction of a batch-type gradient descent of ridge regression under areasonable input form.
  </details>

- **[DebFlow: Automating Agent Creation via Agent Debate](http://arxiv.org/abs/2503.23781v1)**  `arXiv:2503.23781`  `cs.AI`  
  _Jinwei Su, Yinghui Xia, Ronghua Shi, Jianhui Wang, Jianuo Huang, Yijin Wang, et al._
  <details open><summary>Abstract</summary>
  Large language models (LLMs) have demonstrated strong potential andimpressive performance in automating the generation and optimization ofworkflows. However, existing approaches are marked by limited reasoningcapabilities, high computational demands, and significant resourcerequirements. To address these issues, we propose DebFlow, a framework thatemploys a debate mechanism to optimize workflows and integrates reflexion toimprove based on previous experiences. We evaluated our method across sixbenchmark datasets, including HotpotQA, MATH, and ALFWorld. Our approachachieved a 3\% average performance improvement over the latest baselines,demonstrating its effectiveness in diverse problem domains. In particular,during training, our framework reduces resource consumption by 37\% compared tothe state-of-the-art baselines. Additionally, we performed ablation studies.Removing the Debate component resulted in a 4\% performance drop across twobenchmark datasets, significantly greater than the 2\% drop observed when theReflection component was removed. These findings strongly demonstrate thecritical role of Debate in enhancing framework performance, while alsohighlighting the auxiliary contribution of reflexion to overall optimization.
  </details>

- **[WinoWhat: A Parallel Corpus of Paraphrased WinoGrande Sentences with Common Sense Categorization](http://arxiv.org/abs/2503.23779v1)**  `arXiv:2503.23779`  `cs.CL` `cs.AI`  
  _Ine Gevers, Victor De Marez, Luna De Bruyne, Walter Daelemans_
  <details open><summary>Abstract</summary>
  In this study, we take a closer look at how Winograd schema challenges can beused to evaluate common sense reasoning in LLMs. Specifically, we evaluategenerative models of different sizes on the popular WinoGrande benchmark. Werelease WinoWhat, a new corpus, in which each instance of the WinoGrandevalidation set is paraphrased. Additionally, we evaluate the performance on thechallenge across five common sense knowledge categories, giving morefine-grained insights on what types of knowledge are more challenging for LLMs.Surprisingly, all models perform significantly worse on WinoWhat, implying thatLLM reasoning capabilities are overestimated on WinoGrande. To verify whetherthis is an effect of benchmark memorization, we match benchmark instances toLLM trainingdata and create two test-suites. We observe that memorization has aminimal effect on model performance on WinoGrande.
  </details>

- **[XLRS-Bench: Could Your Multimodal LLMs Understand Extremely Large Ultra-High-Resolution Remote Sensing Imagery?](http://arxiv.org/abs/2503.23771v1)**  `arXiv:2503.23771`  `cs.CV`  
  _Fengxiang Wang, Hongzhen Wang, Mingshuo Chen, Di Wang, Yulin Wang, Zonghao Guo, et al._
  <details open><summary>Abstract</summary>
  The astonishing breakthrough of multimodal large language models (MLLMs) hasnecessitated new benchmarks to quantitatively assess their capabilities, revealtheir limitations, and indicate future research directions. However, this ischallenging in the context of remote sensing (RS), since the imagery featuresultra-high resolution that incorporates extremely complex semanticrelationships. Existing benchmarks usually adopt notably smaller image sizesthan real-world RS scenarios, suffer from limited annotation quality, andconsider insufficient dimensions of evaluation. To address these issues, wepresent XLRS-Bench: a comprehensive benchmark for evaluating the perception andreasoning capabilities of MLLMs in ultra-high-resolution RS scenarios.XLRS-Bench boasts the largest average image size (8500$\times$8500) observedthus far, with all evaluation samples meticulously annotated manually, assistedby a novel semi-automatic captioner on ultra-high-resolution RS images. On topof the XLRS-Bench, 16 sub-tasks are defined to evaluate MLLMs' 10 kinds ofperceptual capabilities and 6 kinds of reasoning capabilities, with a primaryemphasis on advanced cognitive processes that facilitate real-worlddecision-making and the capture of spatiotemporal changes. The results of bothgeneral and RS-focused MLLMs on XLRS-Bench indicate that further efforts areneeded for real-world RS applications. We have open-sourced XLRS-Bench tosupport further research in developing more powerful MLLMs for remote sensing.
  </details>

- **[DeepDubber-V1: Towards High Quality and Dialogue, Narration, Monologue Adaptive Movie Dubbing Via Multi-Modal Chain-of-Thoughts Reasoning Guidance](http://arxiv.org/abs/2503.23660v1)**  `arXiv:2503.23660`  `cs.CV`  
  _Junjie Zheng, Zihao Chen, Chaofan Ding, Xinhan Di_
  <details open><summary>Abstract</summary>
  Current movie dubbing technology can generate the desired voice from a givenspeech prompt, ensuring good synchronization between speech and visuals whileaccurately conveying the intended emotions. However, in movie dubbing, keyaspects such as adapting to different dubbing styles, handling dialogue,narration, and monologue effectively, and understanding subtle details like theage and gender of speakers, have not been well studied. To address thischallenge, we propose a framework of multi-modal large language model. First,it utilizes multimodal Chain-of-Thought (CoT) reasoning methods on visualinputs to understand dubbing styles and fine-grained attributes. Second, itgenerates high-quality dubbing through large speech generation models, guidedby multimodal conditions. Additionally, we have developed a movie dubbingdataset with CoT annotations. The evaluation results demonstrate a performanceimprovement over state-of-the-art methods across multiple datasets. Inparticular, for the evaluation metrics, the SPK-SIM and EMO-SIM increases from82.48% to 89.74%, 66.24% to 78.88% for dubbing setting 2.0 on V2C Animationdataset, LSE-D and MCD-SL decreases from 14.79 to 14.63, 5.24 to 4.74 fordubbing setting 2.0 on Grid dataset, SPK-SIM increases from 64.03 to 83.42 andWER decreases from 52.69% to 23.20% for initial reasoning setting on proposedCoT-Movie-Dubbing dataset in the comparison with the state-of-the art models.
  </details>

- **[Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning](http://arxiv.org/abs/2503.22456v2)**  `arXiv:2503.22456`  `cs.LG` `cs.AI`  
  _Abdullah Vanlioglu_
  <details open><summary>Abstract</summary>
  We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach thatenhances the exploration-exploitation tradeoff by dynamically assigning weightsto generated outputs based on their advantage and entropy for ReinforcementLearning-based Large Language Model fine-tuning. EGSW integrates entropyregularization with advantage-based weighting to balance policy updates,enabling efficient exploration in high-dimensional state spaces. By employingtemperature-scaled softmax weighting over sequences, EGSW prioritizinghigh-reward, high-uncertainty steps while maintaining training stability.Although originally developed to improve Group Relative Policy Optimization(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable toother reinforcement learning (RL) algorithms and can be implemented in bothstep-wise and trajectory-wise settings. Empirical evaluations demonstrate thatEGSW enhances GRPO reasoning ability, yielding improvements in sampleefficiency. Future work will explore the application of EGSW to advanced RLmethodologies.
  </details>

- **[Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback](http://arxiv.org/abs/2503.22230v2)**  `arXiv:2503.22230`  `cs.LG`  
  _Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, et al._
  <details open><summary>Abstract</summary>
  Reinforcement Learning from Human Feedback (RLHF) is crucial for aligninglarge language models with human preferences. While recent research has focusedon algorithmic improvements, the importance of prompt-data construction hasbeen overlooked. This paper addresses this gap by exploring data-drivenbottlenecks in RLHF performance scaling, particularly reward hacking anddecreasing response diversity. We introduce a hybrid reward system combiningreasoning task verifiers (RTV) and a generative reward model (GenRM) tomitigate reward hacking. We also propose a novel prompt-selection method,Pre-PPO, to maintain response diversity and enhance learning effectiveness.Additionally, we find that prioritizing mathematical and coding tasks early inRLHF training significantly improves performance. Experiments across two modelsizes validate our methods' effectiveness and scalability. Results show thatRTV is most resistant to reward hacking, followed by GenRM with ground truth,and then GenRM with SFT Best-of-N responses. Our strategies enable rapidcapture of subtle task-specific distinctions, leading to substantialimprovements in overall RLHF performance. This work highlights the importanceof careful data construction and provides practical methods to overcomeperformance barriers in RLHF.
  </details>

- **[EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues](http://arxiv.org/abs/2503.21080v3)**  `arXiv:2503.21080`  `cs.CL`  
  _Yuhan Liu, Yunbo Long_
  <details open><summary>Abstract</summary>
  While large language model (LLM)-based chatbots have been applied foreffective engagement in credit dialogues, their capacity for dynamic emotionalexpression remains limited. Current agents primarily rely on passive empathyrather than affective reasoning. For instance, when faced with persistentclient negativity, the agent should employ strategic emotional adaptation byexpressing measured anger to discourage counterproductive behavior and guidethe conversation toward resolution. This context-aware emotional modulation isessential for imitating the nuanced decision-making of human negotiators. Thispaper introduces an EQ-negotiator that combines emotion sensing frompre-trained language models (PLMs) with emotional reasoning based on GameTheory and Hidden Markov Models. It takes into account both the current andhistorical emotions of the client to better manage and address negativeemotions during interactions. By fine-tuning pre-trained language models (PLMs)on public emotion datasets and validating them on the credit dialogue datasets,our approach enables LLM-based agents to effectively capture shifts in clientemotions and dynamically adjust their response tone based on our emotiondecision policies in real-world financial negotiations. This EQ-negotiator canalso help credit agencies foster positive client relationships, enhancingsatisfaction in credit services.
  </details>

- **[Will Pre-Training Ever End? A First Step Toward Next-Generation Foundation MLLMs via Self-Improving Systematic Cognition](http://arxiv.org/abs/2503.12303v5)**  `arXiv:2503.12303`  `cs.CV`  
  _Xiaoying Zhang, Da Peng, Yipeng Zhang, Zonghao Guo, Chengyue Wu, Chi Chen, et al._
  <details open><summary>Abstract</summary>
  Recent progress in (multimodal) large language models ((M)LLMs) has shiftedfocus from pre-training to inference-time compute scaling and post-trainingoptimization, driven by concerns over limited high-quality real-world data.However, these strategies alone are insufficient for advancing modelcapabilities. We hypothesize that effective model improvement requires a strongsynergy among pre-training, inference-time compute scaling, and post-trainingoptimization. In this paper, we validate this hypothesis in the context ofmultimodal pre-training for foundation MLLM construction. We introduceSelf-Improving cognition (SIcog), a self-learning framework for constructingnext-generation foundation MLLMs by imparting multimodal knowledge andenhancing their systematic cognitive capabilities through multimodalpre-training with self-generated data. Specifically, we introduceChain-of-Description, a step-by-step visual understanding method to improvecomprehensive perception, and integrate structured chain-of-thought (CoT)reasoning to support in-depth multimodal reasoning. SIcog first equips a basemodel with systematic perception and reasoning using minimal externalsupervision. The enhanced model then generates candidate image captions andCoT-style reasoning responses for unlabeled images and image-question pairsacross diverse tasks, which are curated through a self-consistency mechanism.These curated samples are subsequently used for large-scale multimodalpre-training, completing a self-learning cycle that strengthens the model'scognitive foundation. Extensive experiments demonstrate that SIcog producesnext-generation foundation MLLMs with substantially improved multimodalcognition, outperforming prevailing pre-training approaches. These findingsempirically establish SIcog as a promising framework for realizing a completeself-improving paradigm.
  </details>

- **[Quantifying the Capability Boundary of DeepSeek Models: An Application-Driven Performance Analysis](http://arxiv.org/abs/2502.11164v4)**  `arXiv:2502.11164`  `cs.LG` `cs.AI`  
  _Kaikai Zhao, Zhaoxiang Liu, Xuejiao Lei, Jiaojiao Zhao, Zhenhong Long, Zipeng Wang, et al._
  <details open><summary>Abstract</summary>
  DeepSeek-R1, known for its low training cost and exceptional reasoningcapabilities, has achieved state-of-the-art performance on various benchmarks.However, detailed evaluations for DeepSeek Series models from the perspectiveof real-world applications are lacking, making it challenging for users toselect the most suitable DeepSeek models for their specific needs. To addressthis gap, we conduct a systematic evaluation of the DeepSeek-V3, DeepSeek-R1,DeepSeek-R1-Distill-Qwen series, DeepSeek-R1-Distill-Llama series, theircorresponding 4-bit quantized models, and the reasoning model QwQ-32B using theenhanced A-Eval benchmark, A-Eval-2.0. Through a comparative analysis oforiginal instruction-tuned models and their distilled counterparts, weinvestigate how reasoning enhancements impact performance across diversepractical tasks. To assist users in model selection, we quantify the capabilityboundary of DeepSeek models through performance tier classifications. Based onthe quantification results, we develop a model selection handbook that clearlyillustrates the relation among models, their capabilities and practicalapplications. This handbook enables users to select the most cost-effectivemodels without efforts, ensuring optimal performance and resource efficiency inreal-world applications. It should be noted that, despite our efforts toestablish a comprehensive, objective, and authoritative evaluation benchmark,the selection of test samples, characteristics of data distribution, and thesetting of evaluation criteria may inevitably introduce certain biases into theevaluation results. We will continuously optimize the evaluation benchmarks andperiodically update this paper to provide more comprehensive and accurateevaluation results. Please refer to the latest version of the paper for themost current results and conclusions.
  </details>

- **[PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models](http://arxiv.org/abs/2502.01584v3)**  `arXiv:2502.01584`  `cs.LG` `cs.AI`  
  _Zixuan Wu, Francesca Lucchetti, Aleksander Boruch-Gruszecki, Jingmiao Zhao, Carolyn Jane Anderson, Joydeep Biswas, et al._
  <details open><summary>Abstract</summary>
  Existing benchmarks for frontier models often test specialized, "PhD-level"knowledge that is difficult for non-experts to grasp. In contrast, we present abenchmark with 594 problems based on the NPR Sunday Puzzle Challenge thatrequires only general knowledge. Our benchmark is challenging for both humansand models; however correct solutions are easy to verify, and models' mistakesare easy to spot. As LLMs are more widely deployed in society, we believe it isuseful to develop benchmarks for frontier models that humans can understandwithout the need for deep domain expertise.  Our work reveals capability gaps that are not evident in existing benchmarks:OpenAI o1 significantly outperforms other reasoning models on our benchmark,despite being on par with other models when tested on benchmarks that testspecialized knowledge. Furthermore, our analysis of reasoning outputs uncoversnew kinds of failures. DeepSeek R1, for instance, often concedes with "I giveup" before providing an answer that it knows is wrong. R1 can also beremarkably "uncertain" in its output and in rare cases, it does not "finishthinking," which suggests the need for techniques to "wrap up" before thecontext window limit is reached. We also quantify the effectiveness ofreasoning longer to identify the point beyond which more reasoning is unlikelyto improve accuracy on our benchmark.
  </details>

- **[EmoVerse: Exploring Multimodal Large Language Models for Sentiment and Emotion Understanding](http://arxiv.org/abs/2412.08049v3)**  `arXiv:2412.08049`  `cs.CL`  
  _Ao Li, Longwei Xu, Chen Ling, Jinghui Zhang, Pengwei Wang_
  <details open><summary>Abstract</summary>
  Sentiment and emotion understanding are essential to applications such ashuman-computer interaction and depression detection. While Multimodal LargeLanguage Models (MLLMs) demonstrate robust general capabilities, they faceconsiderable challenges in the field of affective computing, particularly indetecting subtle facial expressions and handling complex emotion-related tasks,such as emotion reason inference and understanding emotions in long-contextscenarios. Furthermore, there is a lack of a unified MLLM that can effectivelyhandle both sentiment and emotion-related tasks. To address these challenges,we explore multi-task training strategies for MLLMs in affective computing andintroduce Emotion Universe (EmoVerse), an MLLM designed to handle a broadspectrum of sentiment and emotion-related tasks. In addition, EmoVerse iscapable of deeply analyzing the underlying causes of emotional states. We alsointroduce the Affective Multitask (AMT) Dataset, which supports multimodalsentiment analysis, multimodal emotion recognition, facial expressionrecognition, emotion reason inference, and emotion cause-pair extraction tasks.Extensive experiments demonstrate that EmoVerse outperforms existing methods,achieving state-of-the-art results in sentiment and emotion-related tasks. Thecode is available at https://github.com/liaolea/EmoVerse.
  </details>

- **[CALMM-Drive: Confidence-Aware Autonomous Driving with Large Multimodal Model](http://arxiv.org/abs/2412.04209v2)**  `arXiv:2412.04209`  `cs.RO`  
  _Ruoyu Yao, Yubin Wang, Haichao Liu, Rui Yang, Zengqi Peng, Lei Zhu, et al._
  <details open><summary>Abstract</summary>
  Decision-making and motion planning constitute critical components forensuring the safety and efficiency of autonomous vehicles (AVs). Existingmethodologies typically adopt two paradigms: decision then planning orgeneration then scoring. However, the former architecture often suffers fromdecision-planning misalignment that incurs risky situations. Meanwhile, thelatter struggles to balance short-term operational metrics (e.g., immediatemotion smoothness) with long-term tactical goals (e.g., route efficiency),resulting in myopic or overly conservative behaviors. To address these issues,we introduce CALMM-Drive, a novel Confidence-Aware Large Multimodal Model (LMM)empowered Autonomous Driving framework. Our approach integrates drivingtask-oriented Chain-of-Thought (CoT) reasoning coupled with Top-K confidenceelicitation, which facilitates high-level reasoning to generate multiplecandidate decisions with their confidence levels. Furthermore, we propose anovel planning module that integrates a diffusion model for trajectorygeneration and a hierarchical refinement process to find the optimaltrajectory. This framework enables the selection over trajectory candidatesaccounting for both low-level solution quality and high-level tacticalconfidence, which avoids the risks within one-shot decisions and overcomes thelimitations in short-sighted scoring mechanisms. Comprehensive evaluations innuPlan closed-loop simulation environments demonstrate the competitiveperformance of CALMM-Drive across both common and long-tail benchmarks,showcasing a significant advancement in the integration of uncertainty inLMM-empowered AVs. The code will be released upon acceptance.
  </details>

- **[HyperGLM: HyperGraph for Video Scene Graph Generation and Anticipation](http://arxiv.org/abs/2411.18042v2)**  `arXiv:2411.18042`  `cs.CV`  
  _Trong-Thuan Nguyen, Pha Nguyen, Jackson Cothren, Alper Yilmaz, Khoa Luu_
  <details open><summary>Abstract</summary>
  Multimodal LLMs have advanced vision-language tasks but still struggle withunderstanding video scenes. To bridge this gap, Video Scene Graph Generation(VidSGG) has emerged to capture multi-object relationships across video frames.However, prior methods rely on pairwise connections, limiting their ability tohandle complex multi-object interactions and reasoning. To this end, we proposeMultimodal LLMs on a Scene HyperGraph (HyperGLM), promoting reasoning aboutmulti-way interactions and higher-order relationships. Our approach uniquelyintegrates entity scene graphs, which capture spatial relationships betweenobjects, with a procedural graph that models their causal transitions, forminga unified HyperGraph. Significantly, HyperGLM enables reasoning by injectingthis unified HyperGraph into LLMs. Additionally, we introduce a new Video SceneGraph Reasoning (VSGR) dataset featuring 1.9M frames from third-person,egocentric, and drone views and supports five tasks: Scene Graph Generation,Scene Graph Anticipation, Video Question Answering, Video Captioning, andRelation Reasoning. Empirically, HyperGLM consistently outperformsstate-of-the-art methods across five tasks, effectively modeling and reasoningcomplex relationships in diverse video scenes.
  </details>

- **[Reversible Decoupling Network for Single Image Reflection Removal](http://arxiv.org/abs/2410.08063v2)**  `arXiv:2410.08063`  `cs.CV`  
  _Hao Zhao, Mingjia Li, Qiming Hu, Xiaojie Guo_
  <details open><summary>Abstract</summary>
  Recent deep-learning-based approaches to single-image reflection removal haveshown promising advances, primarily for two reasons: 1) the utilization ofrecognition-pretrained features as inputs, and 2) the design of dual-streaminteraction networks. However, according to the Information Bottleneckprinciple, high-level semantic clues tend to be compressed or discarded duringlayer-by-layer propagation. Additionally, interactions in dual-stream networksfollow a fixed pattern across different layers, limiting overall performance.To address these limitations, we propose a novel architecture called ReversibleDecoupling Network (RDNet), which employs a reversible encoder to securevaluable information while flexibly decoupling transmission- andreflection-relevant features during the forward pass. Furthermore, we customizea transmission-rate-aware prompt generator to dynamically calibrate features,further boosting performance. Extensive experiments demonstrate the superiorityof RDNet over existing SOTA methods on five widely-adopted benchmark datasets.RDNet achieves the best performance in the NTIRE 2025 Single Image ReflectionRemoval in the Wild Challenge in both fidelity and perceptual comparison. Ourcode is available at https://github.com/lime-j/RDNet
  </details>

- **[ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery](http://arxiv.org/abs/2410.05080v3)**  `arXiv:2410.05080`  `cs.LG` `cs.CL` `cs.AI`  
  _Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, et al._
  <details open><summary>Abstract</summary>
  The advancements of large language models (LLMs) have piqued growing interestin developing LLM-based language agents to automate scientific discoveryend-to-end, which has sparked both excitement and skepticism about their truecapabilities. In this work, we call for rigorous assessment of agents onindividual tasks in a scientific workflow before making bold claims onend-to-end automation. To this end, we present ScienceAgentBench, a newbenchmark for evaluating language agents for data-driven scientific discovery.To ensure the scientific authenticity and real-world relevance of ourbenchmark, we extract 102 tasks from 44 peer-reviewed publications in fourdisciplines and engage nine subject matter experts to validate them. We unifythe target output for every task to a self-contained Python program file andemploy an array of evaluation metrics to examine the generated programs,execution results, and costs. Each task goes through multiple rounds of manualvalidation by annotators and subject matter experts to ensure its annotationquality and scientific plausibility. We also propose two effective strategiesto mitigate data contamination concerns. Using ScienceAgentBench, we evaluatefive open-weight and proprietary LLMs, each with three frameworks: directprompting, OpenHands CodeAct, and self-debug. Given three attempts for eachtask, the best-performing agent can only solve 32.4% of the tasks independentlyand 34.3% with expert-provided knowledge. In addition, we evaluate OpenAIo1-preview with direct prompting and self-debug, which can boost theperformance to 42.2%, demonstrating the effectiveness of increasinginference-time compute but with more than 10 times the cost of other LLMs.Still, our results underscore the limitations of current language agents ingenerating code for data-driven discovery, let alone end-to-end automation forscientific research.
  </details>

- **[Fast and Accurate Task Planning using Neuro-Symbolic Language Models and Multi-level Goal Decomposition](http://arxiv.org/abs/2409.19250v2)**  `arXiv:2409.19250`  `cs.RO`  
  _Minseo Kwon, Yaesol Kim, Young J. Kim_
  <details open><summary>Abstract</summary>
  In robotic task planning, symbolic planners using rule-based representationslike PDDL are effective but struggle with long-sequential tasks in complicatedenvironments due to exponentially increasing search space. Meanwhile, LLM-basedapproaches, which are grounded in artificial neural networks, offer fasterinference and commonsense reasoning but suffer from lower success rates. Toaddress the limitations of the current symbolic (slow speed) or LLM-basedapproaches (low accuracy), we propose a novel neuro-symbolic task planner thatdecomposes complex tasks into subgoals using LLM and carries out task planningfor each subgoal using either symbolic or MCTS-based LLM planners, depending onthe subgoal complexity. This decomposition reduces planning time and improvessuccess rates by narrowing the search space and enabling LLMs to focus on moremanageable tasks. Our method significantly reduces planning time whilemaintaining high success rates across task planning domains, as well asreal-world and simulated robotics environments. More details are available athttp://graphics.ewha.ac.kr/LLMTAMP/.
  </details>

- **[MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty](http://arxiv.org/abs/2408.06816v2)**  `arXiv:2408.06816`  `cs.CL` `cs.AI`  
  _Yongjin Yang, Haneul Yoo, Hwaran Lee_
  <details open><summary>Abstract</summary>
  Despite the massive advancements in large language models (LLMs), they stillsuffer from producing plausible but incorrect responses. To improve thereliability of LLMs, recent research has focused on uncertainty quantificationto predict whether a response is correct or not. However, most uncertaintyquantification methods have been evaluated on single-labeled questions, whichremoves data uncertainty: the irreducible randomness often present in userqueries, which can arise from factors like multiple possible answers. Thislimitation may cause uncertainty quantification results to be unreliable inpractical settings. In this paper, we investigate previous uncertaintyquantification methods under the presence of data uncertainty. Ourcontributions are two-fold: 1) proposing a new Multi-Answer Question Answeringdataset, MAQA, consisting of world knowledge, mathematical reasoning, andcommonsense reasoning tasks to evaluate uncertainty quantification regardingdata uncertainty, and 2) assessing 5 uncertainty quantification methods ofdiverse white- and black-box LLMs. Our findings show that previous methodsrelatively struggle compared to single-answer settings, though this variesdepending on the task. Moreover, we observe that entropy- and consistency-basedmethods effectively estimate model uncertainty, even in the presence of datauncertainty. We believe these observations will guide future work onuncertainty quantification in more realistic settings.
  </details>

- **[TransXNet: Learning Both Global and Local Dynamics with a Dual Dynamic Token Mixer for Visual Recognition](http://arxiv.org/abs/2310.19380v4)**  `arXiv:2310.19380`  `cs.CV`  
  _Meng Lou, Shu Zhang, Hong-Yu Zhou, Sibei Yang, Chuan Wu, Yizhou Yu_
  <details open><summary>Abstract</summary>
  Recent studies have integrated convolutions into transformers to introduceinductive bias and improve generalization performance. However, the staticnature of conventional convolution prevents it from dynamically adapting toinput variations, resulting in a representation discrepancy between convolutionand self-attention as the latter computes attention maps dynamically.Furthermore, when stacking token mixers that consist of convolution andself-attention to form a deep network, the static nature of convolution hindersthe fusion of features previously generated by self-attention into convolutionkernels. These two limitations result in a sub-optimal representation capacityof the entire network. To find a solution, we propose a lightweight DualDynamic Token Mixer (D-Mixer) to simultaneously learn global and local dynamicsvia computing input-dependent global and local aggregation weights. D-Mixerworks by applying an efficient global attention module and an input-dependentdepthwise convolution separately on evenly split feature segments, endowing thenetwork with strong inductive bias and an enlarged receptive field. We useD-Mixer as the basic building block to design TransXNet, a novel hybridCNN-Transformer vision backbone network that delivers compelling performance.In the ImageNet-1K classification, TransXNet-T surpasses Swin-T by 0.3% intop-1 accuracy while requiring less than half of the computational cost.Furthermore, TransXNet-S and TransXNet-B exhibit excellent model scalability,achieving top-1 accuracy of 83.8% and 84.6% respectively, with reasonablecomputational costs. Additionally, our proposed network architecturedemonstrates strong generalization capabilities in various dense predictiontasks, outperforming other state-of-the-art networks while having lowercomputational costs. Code is publicly available athttps://github.com/LMMMEng/TransXNet.
  </details>
