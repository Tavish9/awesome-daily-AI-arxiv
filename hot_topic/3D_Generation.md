# üîç 3D_Generation Papers ¬∑ 2025-03-27

[![Total Papers](https://img.shields.io/badge/Papers-11-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Video Generation` `Scene Generation` `Content Generation`  
**Filter**: `2D`

---

## üìö Paper List

- **[VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models](http://arxiv.org/abs/2503.21781v1)**  `arXiv:2503.21781`  `cs.CV`  
  _Chi-Pin Huang, Yen-Siang Wu, Hung-Kai Chung, Kai-Po Chang, Fu-En Yang, Yu-Chiang Frank Wang_
  <details open><summary>Abstract</summary>
  Customized text-to-video generation aims to produce high-quality videos thatincorporate user-specified subject identities or motion patterns. However,existing methods mainly focus on personalizing a single concept, either subjectidentity or motion pattern, limiting their effectiveness for multiple subjectswith the desired motion patterns. To tackle this challenge, we propose aunified framework VideoMage for video customization over both multiple subjectsand their interactive motions. VideoMage employs subject and motion LoRAs tocapture personalized content from user-provided images and videos, along withan appearance-agnostic motion learning approach to disentangle motion patternsfrom visual appearance. Furthermore, we develop a spatial-temporal compositionscheme to guide interactions among subjects within the desired motion patterns.Extensive experiments demonstrate that VideoMage outperforms existing methods,generating coherent, user-controlled videos with consistent subject identitiesand interactions.
  </details>

- **[Exploring the Evolution of Physics Cognition in Video Generation: A Survey](http://arxiv.org/abs/2503.21765v1)**  `arXiv:2503.21765`  `cs.CV`  
  _Minghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, Pengxiang Ding, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in video generation have witnessed significant progress,especially with the rapid advancement of diffusion models. Despite this, theirdeficiencies in physical cognition have gradually received widespread attention- generated content often violates the fundamental laws of physics, fallinginto the dilemma of ''visual realism but physical absurdity". Researchers beganto increasingly recognize the importance of physical fidelity in videogeneration and attempted to integrate heuristic physical cognition such asmotion representations and physical knowledge into generative systems tosimulate real-world dynamic scenarios. Considering the lack of a systematicoverview in this field, this survey aims to provide a comprehensive summary ofarchitecture designs and their applications to fill this gap. Specifically, wediscuss and organize the evolutionary process of physical cognition in videogeneration from a cognitive science perspective, while proposing a three-tiertaxonomy: 1) basic schema perception for generation, 2) passive cognition ofphysical knowledge for generation, and 3) active cognition for worldsimulation, encompassing state-of-the-art methods, classical paradigms, andbenchmarks. Subsequently, we emphasize the inherent key challenges in thisdomain and delineate potential pathways for future research, contributing toadvancing the frontiers of discussion in both academia and industry. Throughstructured review and interdisciplinary analysis, this survey aims to providedirectional guidance for developing interpretable, controllable, and physicallyconsistent video generation paradigms, thereby propelling generative modelsfrom the stage of ''visual mimicry'' towards a new phase of ''human-likephysical comprehension''.
  </details>

- **[VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness](http://arxiv.org/abs/2503.21755v1)**  `arXiv:2503.21755`  `cs.CV`  
  _Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, et al._
  <details open><summary>Abstract</summary>
  Video generation has advanced significantly, evolving from producingunrealistic outputs to generating videos that appear visually convincing andtemporally coherent. To evaluate these video generative models, benchmarks suchas VBench have been developed to assess their faithfulness, measuring factorslike per-frame aesthetics, temporal consistency, and basic prompt adherence.However, these aspects mainly represent superficial faithfulness, which focuson whether the video appears visually convincing rather than whether it adheresto real-world principles. While recent models perform increasingly well onthese metrics, they still struggle to generate videos that are not justvisually plausible but fundamentally realistic. To achieve real "world models"through video generation, the next frontier lies in intrinsic faithfulness toensure that generated videos adhere to physical laws, commonsense reasoning,anatomical correctness, and compositional integrity. Achieving this level ofrealism is essential for applications such as AI-assisted filmmaking andsimulated world modeling. To bridge this gap, we introduce VBench-2.0, anext-generation benchmark designed to automatically evaluate video generativemodels for their intrinsic faithfulness. VBench-2.0 assesses five keydimensions: Human Fidelity, Controllability, Creativity, Physics, andCommonsense, each further broken down into fine-grained capabilities. Tailoredfor individual dimensions, our evaluation framework integrates generalists suchas state-of-the-art VLMs and LLMs, and specialists, including anomaly detectionmethods proposed for video generation. We conduct extensive annotations toensure alignment with human judgment. By pushing beyond superficialfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a newstandard for the next generation of video generative models in pursuit ofintrinsic faithfulness.
  </details>

- **[Audio-driven Gesture Generation via Deviation Feature in the Latent Space](http://arxiv.org/abs/2503.21616v1)**  `arXiv:2503.21616`  `cs.CV`  
  _Jiahui Chen, Yang Huan, Runhua Shi, Chanfan Ding, Xiaoqi Mo, Siyu Xiong, et al._
  <details open><summary>Abstract</summary>
  Gestures are essential for enhancing co-speech communication, offering visualemphasis and complementing verbal interactions. While prior work hasconcentrated on point-level motion or fully supervised data-driven methods, wefocus on co-speech gestures, advocating for weakly supervised learning andpixel-level motion deviations. We introduce a weakly supervised framework thatlearns latent representation deviations, tailored for co-speech gesture videogeneration. Our approach employs a diffusion model to integrate latent motionfeatures, enabling more precise and nuanced gesture representation. Byleveraging weakly supervised deviations in latent space, we effectivelygenerate hand gestures and mouth movements, crucial for realistic videoproduction. Experiments show our method significantly improves video quality,surpassing current state-of-the-art techniques.
  </details>

- **[The Procedural Content Generation Benchmark: An Open-source Testbed for Generative Challenges in Games](http://arxiv.org/abs/2503.21474v1)**  `arXiv:2503.21474`  `cs.AI` `cs.LG`  
  _Ahmed Khalifa, Roberto Gallotta, Matthew Barthet, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis_
  <details open><summary>Abstract</summary>
  This paper introduces the Procedural Content Generation Benchmark forevaluating generative algorithms on different game content creation tasks. Thebenchmark comes with 12 game-related problems with multiple variants on eachproblem. Problems vary from creating levels of different kinds to creating rulesets for simple arcade games. Each problem has its own content representation,control parameters, and evaluation metrics for quality, diversity, andcontrollability. This benchmark is intended as a first step towards astandardized way of comparing generative algorithms. We use the benchmark toscore three baseline algorithms: a random generator, an evolution strategy, anda genetic algorithm. Results show that some problems are easier to solve thanothers, as well as the impact the chosen objective has on quality, diversity,and controllability of the generated artifacts.
  </details>

- **[ChatAnyone: Stylized Real-time Portrait Video Generation with Hierarchical Motion Diffusion Model](http://arxiv.org/abs/2503.21144v1)**  `arXiv:2503.21144`  `cs.CV`  
  _Jinwei Qi, Chaonan Ji, Sheng Xu, Peng Zhang, Bang Zhang, Liefeng Bo_
  <details open><summary>Abstract</summary>
  Real-time interactive video-chat portraits have been increasingly recognizedas the future trend, particularly due to the remarkable progress made in textand voice chat technologies. However, existing methods primarily focus onreal-time generation of head movements, but struggle to produce synchronizedbody motions that match these head actions. Additionally, achievingfine-grained control over the speaking style and nuances of facial expressionsremains a challenge. To address these limitations, we introduce a novelframework for stylized real-time portrait video generation, enabling expressiveand flexible video chat that extends from talking head to upper-bodyinteraction. Our approach consists of the following two stages. The first stageinvolves efficient hierarchical motion diffusion models, that take bothexplicit and implicit motion representations into account based on audioinputs, which can generate a diverse range of facial expressions with stylisticcontrol and synchronization between head and body movements. The second stageaims to generate portrait video featuring upper-body movements, including handgestures. We inject explicit hand control signals into the generator to producemore detailed hand movements, and further perform face refinement to enhancethe overall realism and expressiveness of the portrait video. Additionally, ourapproach supports efficient and continuous generation of upper-body portraitvideo in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supportinginteractive video-chat in real-time. Experimental results demonstrate thecapability of our approach to produce portrait videos with rich expressivenessand natural upper-body movements.
  </details>

- **[Training-free Diffusion Acceleration with Bottleneck Sampling](http://arxiv.org/abs/2503.18940v2)**  `arXiv:2503.18940`  `cs.CV`  
  _Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, et al._
  <details open><summary>Abstract</summary>
  Diffusion models have demonstrated remarkable capabilities in visual contentgeneration but remain challenging to deploy due to their high computationalcost during inference. This computational burden primarily arises from thequadratic complexity of self-attention with respect to image or videoresolution. While existing acceleration methods often compromise output qualityor necessitate costly retraining, we observe that most diffusion models arepre-trained at lower resolutions, presenting an opportunity to exploit theselow-resolution priors for more efficient inference without degradingperformance. In this work, we introduce Bottleneck Sampling, a training-freeframework that leverages low-resolution priors to reduce computational overheadwhile preserving output fidelity. Bottleneck Sampling follows a high-low-highdenoising workflow: it performs high-resolution denoising in the initial andfinal stages while operating at lower resolutions in intermediate steps. Tomitigate aliasing and blurring artifacts, we further refine the resolutiontransition points and adaptively shift the denoising timesteps at each stage.We evaluate Bottleneck Sampling on both image and video generation tasks, whereextensive experiments demonstrate that it accelerates inference by up to3$\times$ for image generation and 2.5$\times$ for video generation, all whilemaintaining output quality comparable to the standard full-resolution samplingprocess across multiple evaluation metrics.
  </details>

- **[ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos](http://arxiv.org/abs/2503.16400v2)**  `arXiv:2503.16400`  `cs.LG`  
  _Haolin Yang, Feilong Tang, Ming Hu, Yulong Li, Yexin Liu, Zelin Peng, et al._
  <details open><summary>Abstract</summary>
  Video diffusion models (VDMs) facilitate the generation of high-qualityvideos, with current research predominantly concentrated on scaling effortsduring training through improvements in data quality, computational resources,and model complexity. However, inference-time scaling has received lessattention, with most approaches restricting models to a single generationattempt. Recent studies have uncovered the existence of "golden noises" thatcan enhance video quality during generation. Building on this, we find thatguiding the scaling inference-time search of VDMs to identify better noisecandidates not only evaluates the quality of the frames generated in thecurrent step but also preserves the high-level object features by referencingthe anchor frame from previous multi-chunks, thereby delivering long-termvalue. Our analysis reveals that diffusion models inherently possess flexibleadjustments of computation by varying denoising steps, and even a one-stepdenoising approach, when guided by a reward signal, yields significantlong-term benefits. Based on the observation, we proposeScalingNoise, aplug-and-play inference-time search strategy that identifies golden initialnoises for the diffusion sampling process to improve global content consistencyand visual diversity. Specifically, we perform one-step denoising to convertinitial noises into a clip and subsequently evaluate its long-term value,leveraging a reward model anchored by previously generated content. Moreover,to preserve diversity, we sample candidates from a tilted noise distributionthat up-weights promising noises. In this way, ScalingNoise significantlyreduces noise-induced errors, ensuring more coherent and spatiotemporallyconsistent video generation. Extensive experiments on benchmark datasetsdemonstrate that the proposed ScalingNoise effectively improves long videogeneration.
  </details>

- **[Rethinking Video Tokenization: A Conditioned Diffusion-based Approach](http://arxiv.org/abs/2503.03708v3)**  `arXiv:2503.03708`  `cs.AI` `cs.CV`  
  _Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, et al._
  <details open><summary>Abstract</summary>
  Existing video tokenizers typically use the traditional VariationalAutoencoder (VAE) architecture for video compression and reconstruction.However, to achieve good performance, its training process often relies oncomplex multi-stage training tricks that go beyond basic reconstruction lossand KL regularization. Among these tricks, the most challenging is the precisetuning of adversarial training with additional Generative Adversarial Networks(GANs) in the final stage, which can hinder stable convergence. In contrast toGANs, diffusion models offer more stable training processes and can generatehigher-quality results. Inspired by these advantages, we propose CDT, a novelConditioned Diffusion-based video Tokenizer, that replaces the GAN-baseddecoder with a conditional causal diffusion model. The encoder compressesspatio-temporal information into compact latents, while the decoderreconstructs videos through a reverse diffusion process conditioned on theselatents. During inference, we incorporate a feature cache mechanism to generatevideos of arbitrary length while maintaining temporal continuity and adoptsampling acceleration technique to enhance efficiency. Trained using only abasic MSE diffusion loss for reconstruction, along with KL term and LPIPSperceptual loss from scratch, extensive experiments demonstrate that CDTachieves state-of-the-art performance in video reconstruction tasks with just asingle-step sampling. Even a scaled-down version of CDT (3$\times$ inferencespeedup) still performs comparably with top baselines. Moreover, the latentvideo generation model trained with CDT also exhibits superior performance. Thesource code and pretrained weights are available athttps://github.com/ali-vilab/CDT.
  </details>

- **[VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors](http://arxiv.org/abs/2503.01107v2)**  `arXiv:2503.01107`  `cs.CV`  
  _Juil Koo, Paul Guerrero, Chun-Hao Paul Huang, Duygu Ceylan, Minhyuk Sung_
  <details open><summary>Abstract</summary>
  Generative methods for image and video editing use generative models aspriors to perform edits despite incomplete information, such as changing thecomposition of 3D objects shown in a single image. Recent methods have shownpromising composition editing results in the image setting, but in the videosetting, editing methods have focused on editing object's appearance andmotion, or camera motion, and as a result, methods to edit object compositionin videos are still missing. We propose \name as a method for editing 3D objectcompositions in videos of static scenes with camera motion. Our approach allowsediting the 3D position of a 3D object across all frames of a video in atemporally consistent manner. This is achieved by lifting intermediate featuresof a generative model to a 3D reconstruction that is shared between all frames,editing the reconstruction, and projecting the features on the editedreconstruction back to each frame. To the best of our knowledge, this is thefirst generative approach to edit object compositions in videos. Our approachis simple and training-free, while outperforming state-of-the-art image editingbaselines.
  </details>

- **[TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models](http://arxiv.org/abs/2502.06608v3)**  `arXiv:2502.06608`  `cs.AI` `cs.CV`  
  _Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in diffusion techniques have propelled image and videogeneration to unprecedented levels of quality, significantly accelerating thedeployment and application of generative AI. However, 3D shape generationtechnology has so far lagged behind, constrained by limitations in 3D datascale, complexity of 3D data processing, and insufficient exploration ofadvanced techniques in the 3D domain. Current approaches to 3D shape generationface substantial challenges in terms of output quality, generalizationcapability, and alignment with input conditions. We present TripoSG, a newstreamlined shape diffusion paradigm capable of generating high-fidelity 3Dmeshes with precise correspondence to input images. Specifically, we propose:1) A large-scale rectified flow transformer for 3D shape generation, achievingstate-of-the-art fidelity through training on extensive, high-quality data. 2)A hybrid supervised training strategy combining SDF, normal, and eikonal lossesfor 3D VAE, achieving high-quality 3D reconstruction performance. 3) A dataprocessing pipeline to generate 2 million high-quality 3D samples, highlightingthe crucial rules for data quality and quantity in training 3D generativemodels. Through comprehensive experiments, we have validated the effectivenessof each component in our new framework. The seamless integration of these partshas enabled TripoSG to achieve state-of-the-art performance in 3D shapegeneration. The resulting 3D shapes exhibit enhanced detail due tohigh-resolution capabilities and demonstrate exceptional fidelity to inputimages. Moreover, TripoSG demonstrates improved versatility in generating 3Dmodels from diverse image styles and contents, showcasing strong generalizationcapabilities. To foster progress and innovation in the field of 3D generation,we will make our model publicly available.
  </details>
