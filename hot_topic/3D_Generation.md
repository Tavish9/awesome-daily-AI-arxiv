# üîç 3D_Generation Papers ¬∑ 2025-03-28

[![Total Papers](https://img.shields.io/badge/Papers-8-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Video Generation` `Scene Generation` `Content Generation`  
**Filter**: `2D`

---

## üìö Paper List

- **[Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model](http://arxiv.org/abs/2503.22622v1)**  `arXiv:2503.22622`  `cs.CV`  
  _Jangho Park, Taesung Kwon, Jong Chul Ye_
  <details open><summary>Abstract</summary>
  Recently, multi-view or 4D video generation has emerged as a significantresearch topic. Nonetheless, recent approaches to 4D generation still strugglewith fundamental limitations, as they primarily rely on harnessing multiplevideo diffusion models with additional training or compute-intensive trainingof a full 4D diffusion model with limited real-world 4D data and largecomputational costs. To address these challenges, here we propose the firsttraining-free 4D video generation method that leverages the off-the-shelf videodiffusion models to generate multi-view videos from a single input video. Ourapproach consists of two key steps: (1) By designating the edge frames in thespatio-temporal sampling grid as key frames, we first synthesize them using avideo diffusion model, leveraging a depth-based warping technique for guidance.This approach ensures structural consistency across the generated frames,preserving spatial and temporal coherence. (2) We then interpolate theremaining frames using a video diffusion model, constructing a fully populatedand temporally coherent sampling grid while preserving spatial and temporalconsistency. Through this approach, we extend a single video into a multi-viewvideo along novel camera trajectories while maintaining spatio-temporalconsistency. Our method is training-free and fully utilizes an off-the-shelfvideo diffusion model, offering a practical and effective solution formulti-view video generation.
  </details>

- **[Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving Simulation Environments](http://arxiv.org/abs/2503.22496v1)**  `arXiv:2503.22496`  `cs.CV` `cs.RO`  
  _Luke Rowe, Roger Girgis, Anthony Gosselin, Liam Paull, Christopher Pal, Felix Heide_
  <details open><summary>Abstract</summary>
  We introduce Scenario Dreamer, a fully data-driven generative simulator forautonomous vehicle planning that generates both the initial traffic scene -comprising a lane graph and agent bounding boxes - and closed-loop agentbehaviours. Existing methods for generating driving simulation environmentsencode the initial traffic scene as a rasterized image and, as such, requireparameter-heavy networks that perform unnecessary computation due to many emptypixels in the rasterized scene. Moreover, we find that existing methods thatemploy rule-based agent behaviours lack diversity and realism. Scenario Dreamerinstead employs a novel vectorized latent diffusion model for initial scenegeneration that directly operates on the vectorized scene elements and anautoregressive Transformer for data-driven agent behaviour simulation. ScenarioDreamer additionally supports scene extrapolation via diffusion inpainting,enabling the generation of unbounded simulation environments. Extensiveexperiments show that Scenario Dreamer outperforms existing generativesimulators in realism and efficiency: the vectorized scene-generation basemodel achieves superior generation quality with around 2x fewer parameters, 6xlower generation latency, and 10x fewer GPU training hours compared to thestrongest baseline. We confirm its practical utility by showing thatreinforcement learning planning agents are more challenged in Scenario Dreamerenvironments than traditional non-generative simulation environments,especially on long and adversarial driving environments.
  </details>

- **[Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs](http://arxiv.org/abs/2503.22362v1)**  `arXiv:2503.22362`  `cs.CL`  
  _Yuan He, Bailan He, Zifeng Ding, Alisia Lupidi, Yuqicheng Zhu, Shuo Chen, et al._
  <details open><summary>Abstract</summary>
  Understanding and mitigating hallucinations in Large Language Models (LLMs)is crucial for ensuring reliable content generation. While previous researchhas primarily focused on "when" LLMs hallucinate, our work explains "why" anddirectly links model behaviour to the pre-training data that forms their priorknowledge. Specifically, we demonstrate that an asymmetry exists in therecognition of logically equivalent facts, which can be attributed to frequencydiscrepancies of entities appearing as subjects versus objects. Given that mostpre-training datasets are inaccessible, we leverage the fully open-source OLMoseries by indexing its Dolma dataset to estimate entity frequencies. Usingrelational facts (represented as triples) from Wikidata5M, we construct probingdatasets to isolate this effect. Our experiments reveal that facts with ahigh-frequency subject and a low-frequency object are better recognised thantheir inverse, despite their logical equivalence. The pattern reverses inlow-to-high frequency settings, and no statistically significant asymmetryemerges when both entities are high-frequency. These findings highlight theinfluential role of pre-training data in shaping model predictions and provideinsights for inferring the characteristics of pre-training data in closed orpartially closed LLMs.
  </details>

- **[EchoFlow: A Foundation Model for Cardiac Ultrasound Image and Video Generation](http://arxiv.org/abs/2503.22357v1)**  `arXiv:2503.22357`  `cs.CV`  
  _Hadrien Reynaud, Alberto Gomez, Paul Leeson, Qingjie Meng, Bernhard Kainz_
  <details open><summary>Abstract</summary>
  Advances in deep learning have significantly enhanced medical image analysis,yet the availability of large-scale medical datasets remains constrained bypatient privacy concerns. We present EchoFlow, a novel framework designed togenerate high-quality, privacy-preserving synthetic echocardiogram images andvideos. EchoFlow comprises four key components: an adversarial variationalautoencoder for defining an efficient latent representation of cardiacultrasound images, a latent image flow matching model for generating accuratelatent echocardiogram images, a latent re-identification model to ensureprivacy by filtering images anatomically, and a latent video flow matchingmodel for animating latent images into realistic echocardiogram videosconditioned on ejection fraction. We rigorously evaluate our synthetic datasetson the clinically relevant task of ejection fraction regression anddemonstrate, for the first time, that downstream models trained exclusively onEchoFlow-generated synthetic datasets achieve performance parity with modelstrained on real datasets. We release our models and synthetic datasets,enabling broader, privacy-compliant research in medical ultrasound imaging athttps://huggingface.co/spaces/HReynaud/EchoFlow.
  </details>

- **[The Procedural Content Generation Benchmark: An Open-source Testbed for Generative Challenges in Games](http://arxiv.org/abs/2503.21474v2)**  `arXiv:2503.21474`  `cs.AI` `cs.LG`  
  _Ahmed Khalifa, Roberto Gallotta, Matthew Barthet, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis_
  <details open><summary>Abstract</summary>
  This paper introduces the Procedural Content Generation Benchmark forevaluating generative algorithms on different game content creation tasks. Thebenchmark comes with 12 game-related problems with multiple variants on eachproblem. Problems vary from creating levels of different kinds to creating rulesets for simple arcade games. Each problem has its own content representation,control parameters, and evaluation metrics for quality, diversity, andcontrollability. This benchmark is intended as a first step towards astandardized way of comparing generative algorithms. We use the benchmark toscore three baseline algorithms: a random generator, an evolution strategy, anda genetic algorithm. Results show that some problems are easier to solve thanothers, as well as the impact the chosen objective has on quality, diversity,and controllability of the generated artifacts.
  </details>

- **[VidTwin: Video VAE with Decoupled Structure and Dynamics](http://arxiv.org/abs/2412.17726v2)**  `arXiv:2412.17726`  `cs.CV` `cs.AI` `cs.LG`  
  _Yuchi Wang, Junliang Guo, Xinyi Xie, Tianyu He, Xu Sun, Jiang Bian_
  <details open><summary>Abstract</summary>
  Recent advancements in video autoencoders (Video AEs) have significantlyimproved the quality and efficiency of video generation. In this paper, wepropose a novel and compact video autoencoder, VidTwin, that decouples videointo two distinct latent spaces: Structure latent vectors, which captureoverall content and global movement, and Dynamics latent vectors, whichrepresent fine-grained details and rapid movements. Specifically, our approachleverages an Encoder-Decoder backbone, augmented with two submodules forextracting these latent spaces, respectively. The first submodule employs aQ-Former to extract low-frequency motion trends, followed by downsamplingblocks to remove redundant content details. The second averages the latentvectors along the spatial dimension to capture rapid motion. Extensiveexperiments show that VidTwin achieves a high compression rate of 0.20% withhigh reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), andperforms efficiently and effectively in downstream generative tasks. Moreover,our model demonstrates explainability and scalability, paving the way forfuture research in video latent representation and generation. Check ourproject page for more details: https://vidtwin.github.io/.
  </details>

- **[Can video generation replace cinematographers? Research on the cinematic language of generated video](http://arxiv.org/abs/2412.12223v2)**  `arXiv:2412.12223`  `cs.CV` `cs.AI`  
  _Xiaozhe Li, Kai WU, Siyi Yang, YiZhan Qu, Guohua. Zhang, Zhiyu Chen, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in text-to-video (T2V) generation have leverageddiffusion models to enhance visual coherence in videos synthesized from textualdescriptions. However, existing research primarily focuses on object motion,often overlooking cinematic language, which is crucial for conveying emotionand narrative pacing in cinematography. To address this, we propose a threefoldapproach to improve cinematic control in T2V models. First, we introduce ameticulously annotated cinematic language dataset with twenty subcategories,covering shot framing, shot angles, and camera movements, enabling models tolearn diverse cinematic styles. Second, we present CameraDiff, which employsLoRA for precise and stable cinematic control, ensuring flexible shotgeneration. Third, we propose CameraCLIP, designed to evaluate cinematicalignment and guide multi-shot composition. Building on CameraCLIP, weintroduce CLIPLoRA, a CLIP-guided dynamic LoRA composition method thatadaptively fuses multiple pre-trained cinematic LoRAs, enabling smoothtransitions and seamless style blending. Experimental results demonstrate thatCameraDiff ensures stable and precise cinematic control, CameraCLIP achieves anR@1 score of 0.83, and CLIPLoRA significantly enhances multi-shot compositionwithin a single video, bridging the gap between automated video generation andprofessional cinematography.\textsuperscript{1}
  </details>

- **[Towards Stabilized and Efficient Diffusion Transformers through Long-Skip-Connections with Spectral Constraints](http://arxiv.org/abs/2411.17616v3)**  `arXiv:2411.17616`  `cs.CV`  
  _Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Xiaoye Qu, Tianlong Chen, Yu Cheng_
  <details open><summary>Abstract</summary>
  Diffusion Transformers (DiT) have emerged as a powerful architecture forimage and video generation, offering superior quality and scalability. However,their practical application suffers from inherent dynamic feature instability,leading to error amplification during cached inference. Through systematicanalysis, we identify the absence of long-range feature preservation mechanismsas the root cause of unstable feature propagation and perturbation sensitivity.To this end, we propose Skip-DiT, a novel DiT variant enhanced withLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.Theoretical spectral norm and visualization analysis demonstrate how LSCsstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamicfeature enable an efficient statical caching mechanism that reuses deepfeatures across timesteps while updating shallow components. Extensiveexperiments across image and video generation tasks demonstrate that Skip-DiTachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2times inference acceleration without quality loss and high fidelity to originaloutput, outperforming existing DiT caching methods across various quantitativemetrics. Our findings establish long-skip connections as critical architecturalcomponents for training stable and efficient diffusion transformers.
  </details>
