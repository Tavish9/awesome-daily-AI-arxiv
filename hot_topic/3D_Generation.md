# üîç 3D_Generation Papers ¬∑ 2025-03-31

[![Total Papers](https://img.shields.io/badge/Papers-9-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Video Generation` `Scene Generation` `Content Generation`  
**Filter**: `2D`

---

## üìö Paper List

- **[Consistent Subject Generation via Contrastive Instantiated Concepts](http://arxiv.org/abs/2503.24387v1)**  `arXiv:2503.24387`  `cs.CV`  
  _Lee Hsin-Ying, Kelvin C. K. Chan, Ming-Hsuan Yang_
  <details open><summary>Abstract</summary>
  While text-to-image generative models can synthesize diverse and faithfulcontents, subject variation across multiple creations limits the application inlong content generation. Existing approaches require time-consuming tuning,references for all subjects, or access to other creations. We introduceContrastive Concept Instantiation (CoCoIns) to effectively synthesizeconsistent subjects across multiple independent creations. The frameworkconsists of a generative model and a mapping network, which transforms inputlatent codes into pseudo-words associated with certain instances of concepts.Users can generate consistent subjects with the same latent codes. To constructsuch associations, we propose a contrastive learning approach that trains thenetwork to differentiate the combination of prompts and latent codes. Extensiveevaluations of human faces with a single subject show that CoCoIns performscomparably to existing methods while maintaining higher flexibility. We alsodemonstrate the potential of extending CoCoIns to multiple subjects and otherobject categories.
  </details>

- **[Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation](http://arxiv.org/abs/2503.24379v1)**  `arXiv:2503.24379`  `cs.CV` `cs.AI`  
  _Shengqiong Wu, Weicai Ye, Jiahao Wang, Quande Liu, Xintao Wang, Pengfei Wan, et al._
  <details open><summary>Abstract</summary>
  To address the bottleneck of accurate user intent interpretation within thecurrent video generation community, we present Any2Caption, a novel frameworkfor controllable video generation under any condition. The key idea is todecouple various condition interpretation steps from the video synthesis step.By leveraging modern multimodal large language models (MLLMs), Any2Captioninterprets diverse inputs--text, images, videos, and specialized cues such asregion, motion, and camera poses--into dense, structured captions that offerbackbone video generators with better guidance. We also introduce Any2CapIns, alarge-scale dataset with 337K instances and 407K conditions forany-condition-to-caption instruction tuning. Comprehensive evaluationsdemonstrate significant improvements of our system in controllability and videoquality across various aspects of existing video generation models. ProjectPage: https://sqwu.top/Any2Cap/
  </details>

- **[Level the Level: Balancing Game Levels for Asymmetric Player Archetypes With Reinforcement Learning](http://arxiv.org/abs/2503.24099v1)**  `arXiv:2503.24099`  `cs.LG`  
  _Florian Rupp, Kai Eckert_
  <details open><summary>Abstract</summary>
  Balancing games, especially those with asymmetric multiplayer content,requires significant manual effort and extensive human playtesting duringdevelopment. For this reason, this work focuses on generating balanced levelstailored to asymmetric player archetypes, where the disparity in abilities isbalanced entirely through the level design. For instance, while one archetypemay have an advantage over another, both should have an equal chance ofwinning. We therefore conceptualize game balancing as a procedural contentgeneration problem and build on and extend a recently introduced method thatuses reinforcement learning to balance tile-based game levels. We evaluate themethod on four different player archetypes and demonstrate its ability tobalance a larger proportion of levels compared to two baseline approaches.Furthermore, our results indicate that as the disparity between playerarchetypes increases, the required number of training steps grows, while themodel's accuracy in achieving balance decreases.
  </details>

- **[JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation](http://arxiv.org/abs/2503.23951v1)**  `arXiv:2503.23951`  `cs.CV`  
  _Fangda Chen, Shanshan Zhao, Chuanfu Xu, Long Lan_
  <details open><summary>Abstract</summary>
  Recent text-to-video advancements have enabled coherent video synthesis fromprompts and expanded to fine-grained control over appearance and motion.However, existing methods either suffer from concept interference due tofeature domain mismatch caused by naive decoupled optimizations or exhibitappearance contamination induced by spatial feature leakage resulting from theentanglement of motion and appearance in reference video reconstructions. Inthis paper, we propose JointTuner, a novel adaptive joint training framework,to alleviate these issues. Specifically, we develop Adaptive LoRA, whichincorporates a context-aware gating mechanism, and integrate the gated LoRAcomponents into the spatial and temporal Transformers within the diffusionmodel. These components enable simultaneous optimization of appearance andmotion, eliminating concept interference. In addition, we introduce theAppearance-independent Temporal Loss, which decouples motion patterns fromintrinsic appearance in reference video reconstructions through anappearance-agnostic noise prediction task. The key innovation lies in addingframe-wise offset noise to the ground-truth Gaussian noise, perturbing itsdistribution, thereby disrupting spatial attributes associated with frameswhile preserving temporal coherence. Furthermore, we construct a benchmarkcomprising 90 appearance-motion customized combinations and 10 multi-typeautomatic metrics across four dimensions, facilitating a more comprehensiveevaluation for this customization task. Extensive experiments demonstrate thesuperior performance of our method compared to current advanced approaches.
  </details>

- **[On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video Generation for Mobile Devices](http://arxiv.org/abs/2503.23796v1)**  `arXiv:2503.23796`  `cs.CV`  
  _Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, Seulki Lee_
  <details open><summary>Abstract</summary>
  We present On-device Sora, the first model training-free solution fordiffusion-based on-device text-to-video generation that operates efficiently onsmartphone-grade devices. To address the challenges of diffusion-basedtext-to-video generation on computation- and memory-limited mobile devices, theproposed On-device Sora applies three novel techniques to pre-trained videogenerative models. First, Linear Proportional Leap (LPL) reduces the excessivedenoising steps required in video diffusion through an efficient leap-basedapproach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensivetoken-processing computation in attention layers by merging consecutive tokensalong the temporal dimension. Third, Concurrent Inference with Dynamic Loading(CI-DL) dynamically partitions large models into smaller blocks and loads theminto memory for concurrent model inference, effectively addressing thechallenges of limited device memory. We implement On-device Sora on the iPhone15 Pro, and the experimental evaluations show that it is capable of generatinghigh-quality videos on the device, comparable to those produced by high-endGPUs. These results show that On-device Sora enables efficient and high-qualityvideo generation on resource-constrained mobile devices. We envision theproposed On-device Sora as a significant first step toward democratizingstate-of-the-art generative technologies, enabling video generation oncommodity mobile and embedded devices without resource-intensive re-trainingfor model optimization (compression). The code implementation is available at aGitHub repository(https://github.com/eai-lab/On-device-Sora).
  </details>

- **[HOIGen-1M: A Large-scale Dataset for Human-Object Interaction Video Generation](http://arxiv.org/abs/2503.23715v1)**  `arXiv:2503.23715`  `cs.CV`  
  _Kun Liu, Qi Liu, Xinchen Liu, Jie Li, Yongdong Zhang, Jiebo Luo, et al._
  <details open><summary>Abstract</summary>
  Text-to-video (T2V) generation has made tremendous progress in generatingcomplicated scenes based on texts. However, human-object interaction (HOI)often cannot be precisely generated by current T2V models due to the lack oflarge-scale videos with accurate captions for HOI. To address this issue, weintroduce HOIGen-1M, the first largescale dataset for HOI Generation,consisting of over one million high-quality videos collected from diversesources. In particular, to guarantee the high quality of videos, we firstdesign an efficient framework to automatically curate HOI videos using thepowerful multimodal large language models (MLLMs), and then the videos arefurther cleaned by human annotators. Moreover, to obtain accurate textualcaptions for HOI videos, we design a novel video description method based on aMixture-of-Multimodal-Experts (MoME) strategy that not only generatesexpressive captions but also eliminates the hallucination by individual MLLM.Furthermore, due to the lack of an evaluation framework for generated HOIvideos, we propose two new metrics to assess the quality of generated videos ina coarse-to-fine manner. Extensive experiments reveal that current T2V modelsstruggle to generate high-quality HOI videos and confirm that our HOIGen-1Mdataset is instrumental for improving HOI video generation. Project webpage isavailable at https://liuqi-creat.github.io/HOIGen.github.io.
  </details>

- **[Context-Independent OCR with Multimodal LLMs: Effects of Image Resolution and Visual Complexity](http://arxiv.org/abs/2503.23667v1)**  `arXiv:2503.23667`  `cs.CV`  
  _Kotaro Inoue_
  <details open><summary>Abstract</summary>
  Due to their high versatility in tasks such as image captioning, documentanalysis, and automated content generation, multimodal Large Language Models(LLMs) have attracted significant attention across various industrial fields.In particular, they have been shown to surpass specialized models in OpticalCharacter Recognition (OCR). Nevertheless, their performance under differentimage conditions remains insufficiently investigated, and individual characterrecognition is not guaranteed due to their reliance on contextual cues. In thiswork, we examine a context-independent OCR task using single-character imageswith diverse visual complexities to determine the conditions for accuraterecognition. Our findings reveal that multimodal LLMs can match conventionalOCR methods at about 300 ppi, yet their performance deteriorates significantlybelow 150 ppi. Additionally, we observe a very weak correlation between visualcomplexity and misrecognitions, whereas a conventional OCR-specific modelexhibits no correlation. These results suggest that image resolution and visualcomplexity may play an important role in the reliable application of multimodalLLMs to OCR tasks that require precise character-level accuracy.
  </details>

- **[On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video Generation for Mobile Devices](http://arxiv.org/abs/2502.04363v2)**  `arXiv:2502.04363`  `cs.CV`  
  _Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, Seulki Lee_
  <details open><summary>Abstract</summary>
  We present On-device Sora, the first model training-free solution fordiffusion-based on-device text-to-video generation that operates efficiently onsmartphone-grade devices. To address the challenges of diffusion-basedtext-to-video generation on computation- and memory-limited mobile devices, theproposed On-device Sora applies three novel techniques to pre-trained videogenerative models. First, Linear Proportional Leap (LPL) reduces the excessivedenoising steps required in video diffusion through an efficient leap-basedapproach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensivetoken-processing computation in attention layers by merging consecutive tokensalong the temporal dimension. Third, Concurrent Inference with Dynamic Loading(CI-DL) dynamically partitions large models into smaller blocks and loads theminto memory for concurrent model inference, effectively addressing thechallenges of limited device memory. We implement On-device Sora on the iPhone15 Pro, and the experimental evaluations show that it is capable of generatinghigh-quality videos on the device, comparable to those produced by high-endGPUs. These results show that On-device Sora enables efficient and high-qualityvideo generation on resource-constrained mobile devices. We envision theproposed On-device Sora as a significant first step toward democratizingstate-of-the-art generative technologies, enabling video generation oncommodity mobile and embedded devices without resource-intensive re-trainingfor model optimization (compression). The code implementation is available at aGitHub repository(https://github.com/eai-lab/On-device-Sora).
  </details>

- **[MovieBench: A Hierarchical Movie Level Dataset for Long Video Generation](http://arxiv.org/abs/2411.15262v2)**  `arXiv:2411.15262`  `cs.CV`  
  _Weijia Wu, Mingyu Liu, Zeyu Zhu, Xi Xia, Haoen Feng, Wen Wang, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in video generation models, like Stable Video Diffusion,show promising results, but primarily focus on short, single-scene videos.These models struggle with generating long videos that involve multiple scenes,coherent narratives, and consistent characters. Furthermore, there is nopublicly available dataset tailored for the analysis, evaluation, and trainingof long video generation models. In this paper, we present MovieBench: AHierarchical Movie-Level Dataset for Long Video Generation, which addressesthese challenges by providing unique contributions: (1) movie-length videosfeaturing rich, coherent storylines and multi-scene narratives, (2) consistencyof character appearance and audio across scenes, and (3) hierarchical datastructure contains high-level movie information and detailed shot-leveldescriptions. Experiments demonstrate that MovieBench brings some new insightsand challenges, such as maintaining character ID consistency across multiplescenes for various characters. The dataset will be public and continuouslymaintained, aiming to advance the field of long video generation. Data can befound at: https://weijiawu.github.io/MovieBench/.
  </details>
