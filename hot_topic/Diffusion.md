# üîç Diffusion Papers ¬∑ 2025-03-31

[![Total Papers](https://img.shields.io/badge/Papers-21-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Diffusion` `DDPM` `DDIM`  
**Filter**: `None`

---

## üìö Paper List

- **[InstructRestore: Region-Customized Image Restoration with Human Instructions](http://arxiv.org/abs/2503.24357v1)**  `arXiv:2503.24357`  `cs.CV`  
  _Shuaizheng Liu, Jianqi Ma, Lingchen Sun, Xiangtao Kong, Lei Zhang_
  <details open><summary>Abstract</summary>
  Despite the significant progress in diffusion prior-based image restoration,most existing methods apply uniform processing to the entire image, lacking thecapability to perform region-customized image restoration according to userinstructions. In this work, we propose a new framework, namely InstructRestore,to perform region-adjustable image restoration following human instructions. Toachieve this, we first develop a data generation engine to produce trainingtriplets, each consisting of a high-quality image, the target regiondescription, and the corresponding region mask. With this engine and carefuldata screening, we construct a comprehensive dataset comprising 536,945triplets to support the training and evaluation of this task. We then examinehow to integrate the low-quality image features under the ControlNetarchitecture to adjust the degree of image details enhancement. Consequently,we develop a ControlNet-like model to identify the target region and allocatedifferent integration scales to the target and surrounding regions, enablingregion-customized image restoration that aligns with user instructions.Experimental results demonstrate that our proposed InstructRestore approachenables effective human-instructed image restoration, such as images with bokeheffects and user-instructed local enhancement. Our work advances theinvestigation of interactive image restoration and enhancement techniques.Data, code, and models will be found athttps://github.com/shuaizhengliu/InstructRestore.git.
  </details>

- **[ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion](http://arxiv.org/abs/2503.24354v1)**  `arXiv:2503.24354`  `cs.LG` `cs.CV` `cs.CL` `cs.AI`  
  _Rana Muhammad Shahroz Khan, Dongwen Tang, Pingzhi Li, Kai Wang, Tianlong Chen_
  <details open><summary>Abstract</summary>
  Parameter generation has emerged as a novel paradigm for neural networkdevelopment, offering an alternative to traditional neural network training bysynthesizing high-quality model weights directly. In the context of Low-RankAdaptation (LoRA) for evolving ($\textit{i.e.}$, constantly updated) largelanguage models (LLMs), this approach promises efficient adaptation withoutcostly retraining. However, existing methods face critical limitations insimultaneously achieving scalability and controllability. In this paper, weintroduce $\texttt{ORAL}$, a novel $\textbf{conditional recurrent diffusion}$framework that addresses these challenges. $\texttt{ORAL}$ incorporates a novelconditioning mechanism that integrates model architecture and textual taskspecifications, enabling the generation of task-specific LoRA parameters thatcan seamlessly transfer across evolving foundation models. Our approachsuccessfully scales to billions-of-parameter LLMs and maintainscontrollability. Through extensive experiments across seven language tasks,four vision tasks, and three multimodal tasks using five pre-trained LLMs, wedemonstrate that $\texttt{ORAL}$ generates high-quality LoRA parameters thatachieve comparable or superior performance to vanilla trained counterparts.
  </details>

- **[Visual Acoustic Fields](http://arxiv.org/abs/2503.24270v1)**  `arXiv:2503.24270`  `cs.CV` `cs.AI`  
  _Yuelei Li, Hyunjin Kim, Fangneng Zhan, Ri-Zhao Qiu, Mazeyu Ji, Xiaojun Shan, et al._
  <details open><summary>Abstract</summary>
  Objects produce different sounds when hit, and humans can intuitively inferhow an object might sound based on its appearance and material properties.Inspired by this intuition, we propose Visual Acoustic Fields, a framework thatbridges hitting sounds and visual signals within a 3D space using 3D GaussianSplatting (3DGS). Our approach features two key modules: sound generation andsound localization. The sound generation module leverages a conditionaldiffusion model, which takes multiscale features rendered from afeature-augmented 3DGS to generate realistic hitting sounds. Meanwhile, thesound localization module enables querying the 3D scene, represented by thefeature-augmented 3DGS, to localize hitting positions based on the soundsources. To support this framework, we introduce a novel pipeline forcollecting scene-level visual-sound sample pairs, achieving alignment betweencaptured images, impact locations, and corresponding sounds. To the best of ourknowledge, this is the first dataset to connect visual and acoustic signals ina 3D context. Extensive experiments on our dataset demonstrate theeffectiveness of Visual Acoustic Fields in generating plausible impact soundsand accurately localizing impact sources. Our project page is athttps://yuelei0428.github.io/projects/Visual-Acoustic-Fields/.
  </details>

- **[DenseFormer: Learning Dense Depth Map from Sparse Depth and Image via Conditional Diffusion Model](http://arxiv.org/abs/2503.23993v1)**  `arXiv:2503.23993`  `cs.CV` `cs.AI`  
  _Ming Yuan, Sichao Wang, Chuang Zhang, Lei He, Qing Xu, Jianqiang Wang_
  <details open><summary>Abstract</summary>
  The depth completion task is a critical problem in autonomous driving,involving the generation of dense depth maps from sparse depth maps and RGBimages. Most existing methods employ a spatial propagation network toiteratively refine the depth map after obtaining an initial dense depth. Inthis paper, we propose DenseFormer, a novel method that integrates thediffusion model into the depth completion task. By incorporating the denoisingmechanism of the diffusion model, DenseFormer generates the dense depth map byprogressively refining an initial random depth distribution through multipleiterations. We propose a feature extraction module that leverages a featurepyramid structure, along with multi-layer deformable attention, to effectivelyextract and integrate features from sparse depth maps and RGB images, whichserve as the guiding condition for the diffusion process. Additionally, thispaper presents a depth refinement module that applies multi-step iterativerefinement across various ranges to the dense depth results generated by thediffusion process. The module utilizes image features enriched with multi-scaleinformation and sparse depth input to further enhance the accuracy of thepredicted depth map. Extensive experiments on the KITTI outdoor scene datasetdemonstrate that DenseFormer outperforms classical depth completion methods.
  </details>

- **[JointTuner: Appearance-Motion Adaptive Joint Training for Customized Video Generation](http://arxiv.org/abs/2503.23951v1)**  `arXiv:2503.23951`  `cs.CV`  
  _Fangda Chen, Shanshan Zhao, Chuanfu Xu, Long Lan_
  <details open><summary>Abstract</summary>
  Recent text-to-video advancements have enabled coherent video synthesis fromprompts and expanded to fine-grained control over appearance and motion.However, existing methods either suffer from concept interference due tofeature domain mismatch caused by naive decoupled optimizations or exhibitappearance contamination induced by spatial feature leakage resulting from theentanglement of motion and appearance in reference video reconstructions. Inthis paper, we propose JointTuner, a novel adaptive joint training framework,to alleviate these issues. Specifically, we develop Adaptive LoRA, whichincorporates a context-aware gating mechanism, and integrate the gated LoRAcomponents into the spatial and temporal Transformers within the diffusionmodel. These components enable simultaneous optimization of appearance andmotion, eliminating concept interference. In addition, we introduce theAppearance-independent Temporal Loss, which decouples motion patterns fromintrinsic appearance in reference video reconstructions through anappearance-agnostic noise prediction task. The key innovation lies in addingframe-wise offset noise to the ground-truth Gaussian noise, perturbing itsdistribution, thereby disrupting spatial attributes associated with frameswhile preserving temporal coherence. Furthermore, we construct a benchmarkcomprising 90 appearance-motion customized combinations and 10 multi-typeautomatic metrics across four dimensions, facilitating a more comprehensiveevaluation for this customization task. Extensive experiments demonstrate thesuperior performance of our method compared to current advanced approaches.
  </details>

- **[Training-Free Text-Guided Image Editing with Visual Autoregressive Model](http://arxiv.org/abs/2503.23897v1)**  `arXiv:2503.23897`  `cs.CV` `cs.AI`  
  _Yufei Wang, Lanqing Guo, Zhihao Li, Jiaxing Huang, Pichao Wang, Bihan Wen, et al._
  <details open><summary>Abstract</summary>
  Text-guided image editing is an essential task that enables users to modifyimages through natural language descriptions. Recent advances in diffusionmodels and rectified flows have significantly improved editing quality,primarily relying on inversion techniques to extract structured noise frominput images. However, inaccuracies in inversion can propagate errors, leadingto unintended modifications and compromising fidelity. Moreover, even withperfect inversion, the entanglement between textual prompts and image featuresoften results in global changes when only local edits are intended. To addressthese challenges, we propose a novel text-guided image editing framework basedon VAR (Visual AutoRegressive modeling), which eliminates the need for explicitinversion while ensuring precise and controlled modifications. Our methodintroduces a caching mechanism that stores token indices and probabilitydistributions from the original image, capturing the relationship between thesource prompt and the image. Using this cache, we design an adaptivefine-grained masking strategy that dynamically identifies and constrainsmodifications to relevant regions, preventing unintended changes. A tokenreassembling approach further refines the editing process, enhancing diversity,fidelity, and control. Our framework operates in a training-free manner andachieves high-fidelity editing with faster inference speeds, processing a 1Kresolution image in as fast as 1.2 seconds. Extensive experiments demonstratethat our method achieves performance comparable to, or even surpassing,existing diffusion- and rectified flow-based approaches in both quantitativemetrics and visual quality. The code will be released.
  </details>

- **[MuseFace: Text-driven Face Editing via Diffusion-based Mask Generation Approach](http://arxiv.org/abs/2503.23888v1)**  `arXiv:2503.23888`  `cs.CV` `cs.AI`  
  _Xin Zhang, Siting Huang, Xiangyang Luo, Yifan Xie, Weijiang Yu, Heng Chang, et al._
  <details open><summary>Abstract</summary>
  Face editing modifies the appearance of face, which plays a key role incustomization and enhancement of personal images. Although much work haveachieved remarkable success in text-driven face editing, they still facesignificant challenges as none of them simultaneously fulfill thecharacteristics of diversity, controllability and flexibility. To address thischallenge, we propose MuseFace, a text-driven face editing framework, whichrelies solely on text prompt to enable face editing. Specifically, MuseFaceintegrates a Text-to-Mask diffusion model and a semantic-aware face editingmodel, capable of directly generating fine-grained semantic masks from text andperforming face editing. The Text-to-Mask diffusion model provides\textit{diversity} and \textit{flexibility} to the framework, while thesemantic-aware face editing model ensures \textit{controllability} of theframework. Our framework can create fine-grained semantic masks, making preciseface editing possible, and significantly enhancing the controllability andflexibility of face editing models. Extensive experiments demonstrate thatMuseFace achieves superior high-fidelity performance.
  </details>

- **[ExScene: Free-View 3D Scene Reconstruction with Gaussian Splatting from a Single Image](http://arxiv.org/abs/2503.23881v1)**  `arXiv:2503.23881`  `cs.CV`  
  _Tianyi Gong, Boyan Li, Yifei Zhong, Fangxin Wang_
  <details open><summary>Abstract</summary>
  The increasing demand for augmented and virtual reality applications hashighlighted the importance of crafting immersive 3D scenes from a simplesingle-view image. However, due to the partial priors provided by single-viewinput, existing methods are often limited to reconstruct low-consistency 3Dscenes with narrow fields of view from single-view input. These limitationsmake them less capable of generalizing to reconstruct immersive scenes. Toaddress this problem, we propose ExScene, a two-stage pipeline to reconstructan immersive 3D scene from any given single-view image. ExScene designs a novelmultimodal diffusion model to generate a high-fidelity and globally consistentpanoramic image. We then develop a panoramic depth estimation approach tocalculate geometric information from panorama, and we combine geometricinformation with high-fidelity panoramic image to train an initial 3D GaussianSplatting (3DGS) model. Following this, we introduce a GS refinement techniquewith 2D stable video diffusion priors. We add camera trajectory consistency andcolor-geometric priors into the denoising process of diffusion to improve colorand spatial consistency across image sequences. These refined sequences arethen used to fine-tune the initial 3DGS model, leading to better reconstructionquality. Experimental results demonstrate that our ExScene achieves consistentand immersive scene reconstruction using only single-view input, significantlysurpassing state-of-the-art baselines.
  </details>

- **[On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video Generation for Mobile Devices](http://arxiv.org/abs/2503.23796v1)**  `arXiv:2503.23796`  `cs.CV`  
  _Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, Seulki Lee_
  <details open><summary>Abstract</summary>
  We present On-device Sora, the first model training-free solution fordiffusion-based on-device text-to-video generation that operates efficiently onsmartphone-grade devices. To address the challenges of diffusion-basedtext-to-video generation on computation- and memory-limited mobile devices, theproposed On-device Sora applies three novel techniques to pre-trained videogenerative models. First, Linear Proportional Leap (LPL) reduces the excessivedenoising steps required in video diffusion through an efficient leap-basedapproach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensivetoken-processing computation in attention layers by merging consecutive tokensalong the temporal dimension. Third, Concurrent Inference with Dynamic Loading(CI-DL) dynamically partitions large models into smaller blocks and loads theminto memory for concurrent model inference, effectively addressing thechallenges of limited device memory. We implement On-device Sora on the iPhone15 Pro, and the experimental evaluations show that it is capable of generatinghigh-quality videos on the device, comparable to those produced by high-endGPUs. These results show that On-device Sora enables efficient and high-qualityvideo generation on resource-constrained mobile devices. We envision theproposed On-device Sora as a significant first step toward democratizingstate-of-the-art generative technologies, enabling video generation oncommodity mobile and embedded devices without resource-intensive re-trainingfor model optimization (compression). The code implementation is available at aGitHub repository(https://github.com/eai-lab/On-device-Sora).
  </details>

- **[Effective Cloud Removal for Remote Sensing Images by an Improved Mean-Reverting Denoising Model with Elucidated Design Space](http://arxiv.org/abs/2503.23717v1)**  `arXiv:2503.23717`  `cs.CV`  
  _Yi Liu, Wengen Li, Jihong Guan, Shuigeng Zhou, Yichao Zhang_
  <details open><summary>Abstract</summary>
  Cloud removal (CR) remains a challenging task in remote sensing imageprocessing. Although diffusion models (DM) exhibit strong generativecapabilities, their direct applications to CR are suboptimal, as they generatecloudless images from random noise, ignoring inherent information in cloudyinputs. To overcome this drawback, we develop a new CR model EMRDM based onmean-reverting diffusion models (MRDMs) to establish a direct diffusion processbetween cloudy and cloudless images. Compared to current MRDMs, EMRDM offers amodular framework with updatable modules and an elucidated design space, basedon a reformulated forward process and a new ordinary differential equation(ODE)-based backward process. Leveraging our framework, we redesign key MRDMmodules to boost CR performance, including restructuring the denoiser via apreconditioning technique, reorganizing the training process, and improving thesampling process by introducing deterministic and stochastic samplers. Toachieve multi-temporal CR, we further develop a denoising network forsimultaneously denoising sequential images. Experiments on mono-temporal andmulti-temporal datasets demonstrate the superior performance of EMRDM. Our codeis available at https://github.com/Ly403/EMRDM.
  </details>

- **[Expanding-and-Shrinking Binary Neural Networks](http://arxiv.org/abs/2503.23709v1)**  `arXiv:2503.23709`  `cs.CV`  
  _Xulong Shi, Caiyi Sun, Zhi Qi, Liu Hao, Xiaodong Yang_
  <details open><summary>Abstract</summary>
  While binary neural networks (BNNs) offer significant benefits in terms ofspeed, memory and energy, they encounter substantial accuracy degradation inchallenging tasks compared to their real-valued counterparts. Due to thebinarization of weights and activations, the possible values of each entry inthe feature maps generated by BNNs are strongly constrained. To tackle thislimitation, we propose the expanding-and-shrinking operation, which enhancesbinary feature maps with negligible increase of computation complexity, therebystrengthening the representation capacity. Extensive experiments conducted onmultiple benchmarks reveal that our approach generalizes well across diverseapplications ranging from image classification, object detection to generativediffusion model, while also achieving remarkable improvement over variousleading binarization algorithms based on different architectures including bothCNNs and Transformers.
  </details>

- **[Grasping a Handful: Sequential Multi-Object Dexterous Grasp Generation](http://arxiv.org/abs/2503.22370v2)**  `arXiv:2503.22370`  `cs.LG` `cs.RO`  
  _Haofei Lu, Yifei Dong, Zehang Weng, Jens Lundell, Danica Kragic_
  <details open><summary>Abstract</summary>
  We introduce the sequential multi-object robotic grasp sampling algorithmSeqGrasp that can robustly synthesize stable grasps on diverse objects usingthe robotic hand's partial Degrees of Freedom (DoF). We use SeqGrasp toconstruct the large-scale Allegro Hand sequential grasping dataset SeqDatasetand use it for training the diffusion-based sequential grasp generatorSeqDiffuser. We experimentally evaluate SeqGrasp and SeqDiffuser against thestate-of-the-art non-sequential multi-object grasp generation method MultiGraspin simulation and on a real robot. The experimental results demonstrate thatSeqGrasp and SeqDiffuser reach an 8.71%-43.33% higher grasp success rate thanMultiGrasp. Furthermore, SeqDiffuser is approximately 1000 times faster atgenerating grasps than SeqGrasp and MultiGrasp.
  </details>

- **[MagicDistillation: Weak-to-Strong Video Distillation for Large-Scale Few-Step Synthesis](http://arxiv.org/abs/2503.13319v2)**  `arXiv:2503.13319`  `cs.CV`  
  _Shitong Shao, Hongwei Yi, Hanzhong Guo, Tian Ye, Daquan Zhou, Michael Lingelbach, et al._
  <details open><summary>Abstract</summary>
  Recently, open-source video diffusion models (VDMs), such as WanX, Magic141and HunyuanVideo, have been scaled to over 10 billion parameters. Theselarge-scale VDMs have demonstrated significant improvements over smaller-scaleVDMs across multiple dimensions, including enhanced visual quality and morenatural motion dynamics. However, these models face two major limitations: (1)High inference overhead: Large-scale VDMs require approximately 10 minutes tosynthesize a 28-step video on a single H100 GPU. (2) Limited in portrait videosynthesis: Models like WanX-I2V and HunyuanVideo-I2V often produce unnaturalfacial expressions and movements in portrait videos. To address thesechallenges, we propose MagicDistillation, a novel framework designed to reduceinference overhead while ensuring the generalization of VDMs for portrait videosynthesis. Specifically, we primarily use sufficiently high-quality talkingvideo to fine-tune Magic141, which is dedicated to portrait video synthesis. Wethen employ LoRA to effectively and efficiently fine-tune the fake DiT withinthe step distillation framework known as distribution matching distillation(DMD). Following this, we apply weak-to-strong (W2S) distribution matching andminimize the discrepancy between the fake data distribution and the groundtruth distribution, thereby improving the visual fidelity and motion dynamicsof the synthesized videos. Experimental results on portrait video synthesisdemonstrate the effectiveness of MagicDistillation, as our method surpassesEuler, LCM, and DMD baselines in both FID/FVD metrics and VBench. Moreover,MagicDistillation, requiring only 4 steps, also outperforms WanX-I2V (14B) andHunyuanVideo-I2V (13B) on visualization and VBench. Our project page ishttps://magicdistillation.github.io/MagicDistillation/.
  </details>

- **[On-device Sora: Enabling Training-Free Diffusion-based Text-to-Video Generation for Mobile Devices](http://arxiv.org/abs/2502.04363v2)**  `arXiv:2502.04363`  `cs.CV`  
  _Bosung Kim, Kyuhwan Lee, Isu Jeong, Jungmin Cheon, Yeojin Lee, Seulki Lee_
  <details open><summary>Abstract</summary>
  We present On-device Sora, the first model training-free solution fordiffusion-based on-device text-to-video generation that operates efficiently onsmartphone-grade devices. To address the challenges of diffusion-basedtext-to-video generation on computation- and memory-limited mobile devices, theproposed On-device Sora applies three novel techniques to pre-trained videogenerative models. First, Linear Proportional Leap (LPL) reduces the excessivedenoising steps required in video diffusion through an efficient leap-basedapproach. Second, Temporal Dimension Token Merging (TDTM) minimizes intensivetoken-processing computation in attention layers by merging consecutive tokensalong the temporal dimension. Third, Concurrent Inference with Dynamic Loading(CI-DL) dynamically partitions large models into smaller blocks and loads theminto memory for concurrent model inference, effectively addressing thechallenges of limited device memory. We implement On-device Sora on the iPhone15 Pro, and the experimental evaluations show that it is capable of generatinghigh-quality videos on the device, comparable to those produced by high-endGPUs. These results show that On-device Sora enables efficient and high-qualityvideo generation on resource-constrained mobile devices. We envision theproposed On-device Sora as a significant first step toward democratizingstate-of-the-art generative technologies, enabling video generation oncommodity mobile and embedded devices without resource-intensive re-trainingfor model optimization (compression). The code implementation is available at aGitHub repository(https://github.com/eai-lab/On-device-Sora).
  </details>

- **[Singular Value Scaling: Efficient Generative Model Compression via Pruned Weights Refinement](http://arxiv.org/abs/2412.17387v3)**  `arXiv:2412.17387`  `cs.CV` `cs.AI`  
  _Hyeonjin Kim, Jaejun Yoo_
  <details open><summary>Abstract</summary>
  While pruning methods effectively maintain model performance without extratraining costs, they often focus solely on preserving crucial connections,overlooking the impact of pruned weights on subsequent fine-tuning ordistillation, leading to inefficiencies. Moreover, most compression techniquesfor generative models have been developed primarily for GANs, tailored tospecific architectures like StyleGAN, and research into compressing Diffusionmodels has just begun. Even more, these methods are often applicable only toGANs or Diffusion models, highlighting the need for approaches that work acrossboth model types. In this paper, we introduce Singular Value Scaling (SVS), aversatile technique for refining pruned weights, applicable to both modeltypes. Our analysis reveals that pruned weights often exhibit dominant singularvectors, hindering fine-tuning efficiency and leading to suboptimal performancecompared to random initialization. Our method enhances weight initialization byminimizing the disparities between singular values of pruned weights, therebyimproving the fine-tuning process. This approach not only guides the compressedmodel toward superior solutions but also significantly speeds up fine-tuning.Extensive experiments on StyleGAN2, StyleGAN3 and DDPM demonstrate that SVSimproves compression performance across model types without additional trainingcosts. Our code is available at:https://github.com/LAIT-CVLab/Singular-Value-Scaling.
  </details>

- **[CALMM-Drive: Confidence-Aware Autonomous Driving with Large Multimodal Model](http://arxiv.org/abs/2412.04209v2)**  `arXiv:2412.04209`  `cs.RO`  
  _Ruoyu Yao, Yubin Wang, Haichao Liu, Rui Yang, Zengqi Peng, Lei Zhu, et al._
  <details open><summary>Abstract</summary>
  Decision-making and motion planning constitute critical components forensuring the safety and efficiency of autonomous vehicles (AVs). Existingmethodologies typically adopt two paradigms: decision then planning orgeneration then scoring. However, the former architecture often suffers fromdecision-planning misalignment that incurs risky situations. Meanwhile, thelatter struggles to balance short-term operational metrics (e.g., immediatemotion smoothness) with long-term tactical goals (e.g., route efficiency),resulting in myopic or overly conservative behaviors. To address these issues,we introduce CALMM-Drive, a novel Confidence-Aware Large Multimodal Model (LMM)empowered Autonomous Driving framework. Our approach integrates drivingtask-oriented Chain-of-Thought (CoT) reasoning coupled with Top-K confidenceelicitation, which facilitates high-level reasoning to generate multiplecandidate decisions with their confidence levels. Furthermore, we propose anovel planning module that integrates a diffusion model for trajectorygeneration and a hierarchical refinement process to find the optimaltrajectory. This framework enables the selection over trajectory candidatesaccounting for both low-level solution quality and high-level tacticalconfidence, which avoids the risks within one-shot decisions and overcomes thelimitations in short-sighted scoring mechanisms. Comprehensive evaluations innuPlan closed-loop simulation environments demonstrate the competitiveperformance of CALMM-Drive across both common and long-tail benchmarks,showcasing a significant advancement in the integration of uncertainty inLMM-empowered AVs. The code will be released upon acceptance.
  </details>

- **[Controllable Human Image Generation with Personalized Multi-Garments](http://arxiv.org/abs/2411.16801v2)**  `arXiv:2411.16801`  `cs.CV`  
  _Yisol Choi, Sangkyung Kwak, Sihyun Yu, Hyungwon Choi, Jinwoo Shin_
  <details open><summary>Abstract</summary>
  We present BootComp, a novel framework based on text-to-image diffusionmodels for controllable human image generation with multiple referencegarments. Here, the main bottleneck is data acquisition for training:collecting a large-scale dataset of high-quality reference garment images perhuman subject is quite challenging, i.e., ideally, one needs to manually gatherevery single garment photograph worn by each human. To address this, we proposea data generation pipeline to construct a large synthetic dataset, consistingof human and multiple-garment pairs, by introducing a model to extract anyreference garment images from each human image. To ensure data quality, we alsopropose a filtering strategy to remove undesirable generated data based onmeasuring perceptual similarities between the garment presented in human imageand extracted garment. Finally, by utilizing the constructed synthetic dataset,we train a diffusion model having two parallel denoising paths that usemultiple garment images as conditions to generate human images while preservingtheir fine-grained details. We further show the wide-applicability of ourframework by adapting it to different types of reference-based generation inthe fashion domain, including virtual try-on, and controllable human imagegeneration with other conditions, e.g., pose, face, etc.
  </details>

- **[MovieBench: A Hierarchical Movie Level Dataset for Long Video Generation](http://arxiv.org/abs/2411.15262v2)**  `arXiv:2411.15262`  `cs.CV`  
  _Weijia Wu, Mingyu Liu, Zeyu Zhu, Xi Xia, Haoen Feng, Wen Wang, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in video generation models, like Stable Video Diffusion,show promising results, but primarily focus on short, single-scene videos.These models struggle with generating long videos that involve multiple scenes,coherent narratives, and consistent characters. Furthermore, there is nopublicly available dataset tailored for the analysis, evaluation, and trainingof long video generation models. In this paper, we present MovieBench: AHierarchical Movie-Level Dataset for Long Video Generation, which addressesthese challenges by providing unique contributions: (1) movie-length videosfeaturing rich, coherent storylines and multi-scene narratives, (2) consistencyof character appearance and audio across scenes, and (3) hierarchical datastructure contains high-level movie information and detailed shot-leveldescriptions. Experiments demonstrate that MovieBench brings some new insightsand challenges, such as maintaining character ID consistency across multiplescenes for various characters. The dataset will be public and continuouslymaintained, aiming to advance the field of long video generation. Data can befound at: https://weijiawu.github.io/MovieBench/.
  </details>

- **[DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models](http://arxiv.org/abs/2410.08207v2)**  `arXiv:2410.08207`  `cs.LG` `cs.CV`  
  _Xiaoxiao He, Ligong Han, Quan Dao, Song Wen, Minhao Bai, Di Liu, et al._
  <details open><summary>Abstract</summary>
  Discrete diffusion models have achieved success in tasks like imagegeneration and masked language modeling but face limitations in controlledcontent editing. We introduce DICE (Discrete Inversion for ControllableEditing), the first approach to enable precise inversion for discrete diffusionmodels, including multinomial diffusion and masked generative models. Byrecording noise sequences and masking patterns during the reverse diffusionprocess, DICE enables accurate reconstruction and flexible editing of discretedata without the need for predefined masks or attention manipulation. Wedemonstrate the effectiveness of DICE across both image and text domains,evaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our resultsshow that DICE preserves high data fidelity while enhancing editingcapabilities, offering new opportunities for fine-grained content manipulationin discrete spaces.
  </details>

- **[MultiBooth: Towards Generating All Your Concepts in an Image from Text](http://arxiv.org/abs/2404.14239v3)**  `arXiv:2404.14239`  `cs.CV`  
  _Chenyang Zhu, Kai Li, Yue Ma, Chunming He, Xiu Li_
  <details open><summary>Abstract</summary>
  This paper introduces MultiBooth, a novel and efficient technique formulti-concept customization in image generation from text. Despite thesignificant advancements in customized generation methods, particularly withthe success of diffusion models, existing methods often struggle withmulti-concept scenarios due to low concept fidelity and high inference cost.MultiBooth addresses these issues by dividing the multi-concept generationprocess into two phases: a single-concept learning phase and a multi-conceptintegration phase. During the single-concept learning phase, we employ amulti-modal image encoder and an efficient concept encoding technique to learna concise and discriminative representation for each concept. In themulti-concept integration phase, we use bounding boxes to define the generationarea for each concept within the cross-attention map. This method enables thecreation of individual concepts within their specified regions, therebyfacilitating the formation of multi-concept images. This strategy not onlyimproves concept fidelity but also reduces additional inference cost.MultiBooth surpasses various baselines in both qualitative and quantitativeevaluations, showcasing its superior performance and computational efficiency.Project Page: https://multibooth.github.io/
  </details>

- **[Enhancing Object Coherence in Layout-to-Image Synthesis](http://arxiv.org/abs/2311.10522v7)**  `arXiv:2311.10522`  `cs.CV` `cs.AI`  
  _Yibin Wang, Changhai Zhou, Honghui Xu_
  <details open><summary>Abstract</summary>
  Layout-to-image synthesis is an emerging technique in conditional imagegeneration. It aims to generate complex scenes, where users require finecontrol over the layout of the objects in a scene. However, it remainschallenging to control the object coherence, including semantic coherence(e.g., the cat looks at the flowers or not) and physical coherence (e.g., thehand and the racket should not be misaligned). In this paper, we propose anovel diffusion model with effective global semantic fusion (GSF) andself-similarity feature enhancement modules to guide the object coherence forthis task. For semantic coherence, we argue that the image caption containsrich information for defining the semantic relationship within the objects inthe images. Instead of simply employing cross-attention between captions andlatent images, which addresses the highly relevant layout restriction andsemantic coherence requirement separately and thus leads to unsatisfyingresults shown in our experiments, we develop GSF to fuse the supervision fromthe layout restriction and semantic coherence requirement and exploit it toguide the image synthesis process. Moreover, to improve the physical coherence,we develop a Self-similarity Coherence Attention (SCA) module to explicitlyintegrate local contextual physical coherence relation into each pixel'sgeneration process. Specifically, we adopt a self-similarity map to encode thephysical coherence restrictions and employ it to extract coherent features fromtext embedding. Through visualization of our self-similarity map, we explorethe essence of SCA, revealing that its effectiveness is not only in capturingreliable physical coherence patterns but also in enhancing complex texturegeneration. Extensive experiments demonstrate the superiority of our proposedmethod.
  </details>
