# üîç Diffusion Papers ¬∑ 2025-03-27

[![Total Papers](https://img.shields.io/badge/Papers-36-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Diffusion` `DDPM` `DDIM`  
**Filter**: `None`

---

## üìö Paper List

- **[VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models](http://arxiv.org/abs/2503.21781v1)**  `arXiv:2503.21781`  `cs.CV`  
  _Chi-Pin Huang, Yen-Siang Wu, Hung-Kai Chung, Kai-Po Chang, Fu-En Yang, Yu-Chiang Frank Wang_
  <details open><summary>Abstract</summary>
  Customized text-to-video generation aims to produce high-quality videos thatincorporate user-specified subject identities or motion patterns. However,existing methods mainly focus on personalizing a single concept, either subjectidentity or motion pattern, limiting their effectiveness for multiple subjectswith the desired motion patterns. To tackle this challenge, we propose aunified framework VideoMage for video customization over both multiple subjectsand their interactive motions. VideoMage employs subject and motion LoRAs tocapture personalized content from user-provided images and videos, along withan appearance-agnostic motion learning approach to disentangle motion patternsfrom visual appearance. Furthermore, we develop a spatial-temporal compositionscheme to guide interactions among subjects within the desired motion patterns.Extensive experiments demonstrate that VideoMage outperforms existing methods,generating coherent, user-controlled videos with consistent subject identitiesand interactions.
  </details>

- **[Optimal Stepsize for Diffusion Sampling](http://arxiv.org/abs/2503.21774v1)**  `arXiv:2503.21774`  `cs.CV`  
  _Jianning Pei, Han Hu, Shuyang Gu_
  <details open><summary>Abstract</summary>
  Diffusion models achieve remarkable generation quality but suffer fromcomputational intensive sampling due to suboptimal step discretization. Whileexisting works focus on optimizing denoising directions, we address theprincipled design of stepsize schedules. This paper proposes Optimal StepsizeDistillation, a dynamic programming framework that extracts theoreticallyoptimal schedules by distilling knowledge from reference trajectories. Byreformulating stepsize optimization as recursive error minimization, our methodguarantees global discretization bounds through optimal substructureexploitation. Crucially, the distilled schedules demonstrate strong robustnessacross architectures, ODE solvers, and noise schedules. Experiments show 10xaccelerated text-to-image generation while preserving 99.4% performance onGenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.
  </details>

- **[StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion](http://arxiv.org/abs/2503.21775v1)**  `arXiv:2503.21775`  `cs.AI` `cs.CV` `cs.LG` `cs.CL`  
  _Ziyu Guo, Young Yoon Lee, Joseph Liu, Yizhak Ben-Shabat, Victor Zordan, Mubbasir Kapadia_
  <details open><summary>Abstract</summary>
  We present StyleMotif, a novel Stylized Motion Latent Diffusion model,generating motion conditioned on both content and style from multiplemodalities. Unlike existing approaches that either focus on generating diversemotion content or transferring style from sequences, StyleMotif seamlesslysynthesizes motion across a wide range of content while incorporating stylisticcues from multi-modal inputs, including motion, text, image, video, and audio.To achieve this, we introduce a style-content cross fusion mechanism and aligna style encoder with a pre-trained multi-modal model, ensuring that thegenerated motion accurately captures the reference style while preservingrealism. Extensive experiments demonstrate that our framework surpassesexisting methods in stylized motion generation and exhibits emergentcapabilities for multi-modal motion stylization, enabling more nuanced motionsynthesis. Source code and pre-trained models will be released upon acceptance.Project Page: https://stylemotif.github.io
  </details>

- **[Exploring the Evolution of Physics Cognition in Video Generation: A Survey](http://arxiv.org/abs/2503.21765v1)**  `arXiv:2503.21765`  `cs.CV`  
  _Minghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, Pengxiang Ding, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in video generation have witnessed significant progress,especially with the rapid advancement of diffusion models. Despite this, theirdeficiencies in physical cognition have gradually received widespread attention- generated content often violates the fundamental laws of physics, fallinginto the dilemma of ''visual realism but physical absurdity". Researchers beganto increasingly recognize the importance of physical fidelity in videogeneration and attempted to integrate heuristic physical cognition such asmotion representations and physical knowledge into generative systems tosimulate real-world dynamic scenarios. Considering the lack of a systematicoverview in this field, this survey aims to provide a comprehensive summary ofarchitecture designs and their applications to fill this gap. Specifically, wediscuss and organize the evolutionary process of physical cognition in videogeneration from a cognitive science perspective, while proposing a three-tiertaxonomy: 1) basic schema perception for generation, 2) passive cognition ofphysical knowledge for generation, and 3) active cognition for worldsimulation, encompassing state-of-the-art methods, classical paradigms, andbenchmarks. Subsequently, we emphasize the inherent key challenges in thisdomain and delineate potential pathways for future research, contributing toadvancing the frontiers of discussion in both academia and industry. Throughstructured review and interdisciplinary analysis, this survey aims to providedirectional guidance for developing interpretable, controllable, and physicallyconsistent video generation paradigms, thereby propelling generative modelsfrom the stage of ''visual mimicry'' towards a new phase of ''human-likephysical comprehension''.
  </details>

- **[A Unified Framework for Diffusion Bridge Problems: Flow Matching and Schr√∂dinger Matching into One](http://arxiv.org/abs/2503.21756v1)**  `arXiv:2503.21756`  `cs.LG`  
  _Minyoung Kim_
  <details open><summary>Abstract</summary>
  The bridge problem is to find an SDE (or sometimes an ODE) that bridges twogiven distributions. The application areas of the bridge problem are enormous,among which the recent generative modeling (e.g., conditional or unconditionalimage generation) is the most popular. Also the famous Schr\"{o}dinger bridgeproblem, a widely known problem for a century, is a special instance of thebridge problem. Two most popular algorithms to tackle the bridge problems inthe deep learning era are: (conditional) flow matching and iterative fittingalgorithms, where the former confined to ODE solutions, and the latterspecifically for the Schr\"{o}dinger bridge problem. The main contribution ofthis article is in two folds: i) We provide concise reviews of these algorithmswith technical details to some extent; ii) We propose a novel unifiedperspective and framework that subsumes these seemingly unrelated algorithms(and their variants) into one. In particular, we show that our unifiedframework can instantiate the Flow Matching (FM) algorithm, the (mini-batch)optimal transport FM algorithm, the (mini-batch) Schr\"{o}dinger bridge FMalgorithm, and the deep Schr\"{o}dinger bridge matching (DSBM) algorithm as itsspecial cases. We believe that this unified framework will be useful forviewing the bridge problems in a more general and flexible perspective, and inturn can help researchers and practitioners to develop new bridge algorithms intheir fields.
  </details>

- **[Audio-driven Gesture Generation via Deviation Feature in the Latent Space](http://arxiv.org/abs/2503.21616v1)**  `arXiv:2503.21616`  `cs.CV`  
  _Jiahui Chen, Yang Huan, Runhua Shi, Chanfan Ding, Xiaoqi Mo, Siyu Xiong, et al._
  <details open><summary>Abstract</summary>
  Gestures are essential for enhancing co-speech communication, offering visualemphasis and complementing verbal interactions. While prior work hasconcentrated on point-level motion or fully supervised data-driven methods, wefocus on co-speech gestures, advocating for weakly supervised learning andpixel-level motion deviations. We introduce a weakly supervised framework thatlearns latent representation deviations, tailored for co-speech gesture videogeneration. Our approach employs a diffusion model to integrate latent motionfeatures, enabling more precise and nuanced gesture representation. Byleveraging weakly supervised deviations in latent space, we effectivelygenerate hand gestures and mouth movements, crucial for realistic videoproduction. Experiments show our method significantly improves video quality,surpassing current state-of-the-art techniques.
  </details>

- **[Critical Iterative Denoising: A Discrete Generative Model Applied to Graphs](http://arxiv.org/abs/2503.21592v1)**  `arXiv:2503.21592`  `cs.AI` `cs.LG`  
  _Yoann Boget, Alexandros Kalousis_
  <details open><summary>Abstract</summary>
  Discrete Diffusion and Flow Matching models have significantly advancedgenerative modeling for discrete structures, including graphs. However, thetime dependencies in the noising process of these models lead to erroraccumulation and propagation during the backward process. This issue,particularly pronounced in mask diffusion, is a known limitation in sequencemodeling and, as we demonstrate, also impacts discrete diffusion models forgraphs.  To address this problem, we propose a novel framework called IterativeDenoising, which simplifies discrete diffusion and circumvents the issue byassuming conditional independence across time. Additionally, we enhance ourmodel by incorporating a Critic, which during generation selectively retains orcorrupts elements in an instance based on their likelihood under the datadistribution. Our empirical evaluations demonstrate that the proposed methodsignificantly outperforms existing discrete diffusion baselines in graphgeneration tasks.
  </details>

- **[AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion](http://arxiv.org/abs/2503.21581v1)**  `arXiv:2503.21581`  `cs.AI` `cs.CV`  
  _Liuyue Xie, Jiancong Guo, Ozan Cakmakci, Andre Araujo, Laszlo A. Jeni, Zhiheng Jia_
  <details open><summary>Abstract</summary>
  Accurate camera calibration is a fundamental task for 3D perception,especially when dealing with real-world, in-the-wild environments where complexoptical distortions are common. Existing methods often rely on pre-rectifiedimages or calibration patterns, which limits their applicability andflexibility. In this work, we introduce a novel framework that addresses thesechallenges by jointly modeling camera intrinsic and extrinsic parameters usinga generic ray camera model. Unlike previous approaches, AlignDiff shifts focusfrom semantic to geometric features, enabling more accurate modeling of localdistortions. We propose AlignDiff, a diffusion model conditioned on geometricpriors, enabling the simultaneous estimation of camera distortions and scenegeometry. To enhance distortion prediction, we incorporate edge-awareattention, focusing the model on geometric features around image edges, ratherthan semantic content. Furthermore, to enhance generalizability to real-worldcaptures, we incorporate a large database of ray-traced lenses containing overthree thousand samples. This database characterizes the distortion inherent ina diverse variety of lens forms. Our experiments demonstrate that the proposedmethod significantly reduces the angular error of estimated ray bundles by ~8.2degrees and overall calibration accuracy, outperforming existing approaches onchallenging, real-world datasets.
  </details>

- **[LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing](http://arxiv.org/abs/2503.21541v1)**  `arXiv:2503.21541`  `cs.AI` `cs.CV`  
  _Achint Soni, Meet Soni, Sirisha Rambhatla_
  <details open><summary>Abstract</summary>
  Text-guided image editing aims to modify specific regions of an imageaccording to natural language instructions while maintaining the generalstructure and the background fidelity. Existing methods utilize masks derivedfrom cross-attention maps generated from diffusion models to identify thetarget regions for modification. However, since cross-attention mechanismsfocus on semantic relevance, they struggle to maintain the image integrity. Asa result, these methods often lack spatial consistency, leading to editingartifacts and distortions. In this work, we address these limitations andintroduce LOCATEdit, which enhances cross-attention maps through a graph-basedapproach utilizing self-attention-derived patch relationships to maintainsmooth, coherent attention across image regions, ensuring that alterations arelimited to the designated items while retaining the surrounding structure.\method consistently and substantially outperforms existing baselines onPIE-Bench, demonstrating its state-of-the-art performance and effectiveness onvarious editing tasks. Code can be found onhttps://github.com/LOCATEdit/LOCATEdit/
  </details>

- **[Invert2Restore: Zero-Shot Degradation-Blind Image Restoration](http://arxiv.org/abs/2503.21486v1)**  `arXiv:2503.21486`  `cs.CV`  
  _Hamadi Chihaoui, Paolo Favaro_
  <details open><summary>Abstract</summary>
  Two of the main challenges of image restoration in real-world scenarios arethe accurate characterization of an image prior and the precise modeling of theimage degradation operator. Pre-trained diffusion models have been verysuccessfully used as image priors in zero-shot image restoration methods.However, how to best handle the degradation operator is still an open problem.In real-world data, methods that rely on specific parametric assumptions aboutthe degradation model often face limitations in their applicability. To addressthis, we introduce Invert2Restore, a zero-shot, training-free method thatoperates in both fully blind and partially blind settings -- requiring no priorknowledge of the degradation model or only partial knowledge of its parametricform without known parameters. Despite this, Invert2Restore achieveshigh-fidelity results and generalizes well across various types of imagedegradation. It leverages a pre-trained diffusion model as a deterministicmapping between normal samples and undistorted image samples. The key insightis that the input noise mapped by a diffusion model to a degraded image lies ina low-probability density region of the standard normal distribution. Thus, wecan restore the degraded image by carefully guiding its input noise toward ahigher-density region. We experimentally validate Invert2Restore across severalimage restoration tasks, demonstrating that it achieves state-of-the-artperformance in scenarios where the degradation operator is either unknown orpartially known.
  </details>

- **[Towards Generating Realistic 3D Semantic Training Data for Autonomous Driving](http://arxiv.org/abs/2503.21449v1)**  `arXiv:2503.21449`  `cs.CV`  
  _Lucas Nunes, Rodrigo Marcuzzi, Jens Behley, Cyrill Stachniss_
  <details open><summary>Abstract</summary>
  Semantic scene understanding is crucial for robotics and computer visionapplications. In autonomous driving, 3D semantic segmentation plays animportant role for enabling safe navigation. Despite significant advances inthe field, the complexity of collecting and annotating 3D data is a bottleneckin this developments. To overcome that data annotation limitation, syntheticsimulated data has been used to generate annotated data on demand. There isstill however a domain gap between real and simulated data. More recently,diffusion models have been in the spotlight, enabling close-to-real datasynthesis. Those generative models have been recently applied to the 3D datadomain for generating scene-scale data with semantic annotations. Still, thosemethods either rely on image projection or decoupled models trained withdifferent resolutions in a coarse-to-fine manner. Such intermediaryrepresentations impact the generated data quality due to errors added in thosetransformations. In this work, we propose a novel approach able to generate 3Dsemantic scene-scale data without relying on any projection or decoupledtrained multi-resolution models, achieving more realistic semantic scene datageneration compared to previous state-of-the-art methods. Besides improving 3Dsemantic scene-scale data synthesis, we thoroughly evaluate the use of thesynthetic scene samples as labeled data to train a semantic segmentationnetwork. In our experiments, we show that using the synthetic annotated datagenerated by our method as training data together with the real semanticsegmentation labels, leads to an improvement in the semantic segmentation modelperformance. Our results show the potential of generated scene-scale pointclouds to generate more training data to extend existing datasets, reducing thedata annotation effort. Our code is available athttps://github.com/PRBonn/3DiSS.
  </details>

- **[Diffusion Image Prior](http://arxiv.org/abs/2503.21410v1)**  `arXiv:2503.21410`  `cs.CV`  
  _Hamadi Chihaoui, Paolo Favaro_
  <details open><summary>Abstract</summary>
  Zero-shot image restoration (IR) methods based on pretrained diffusion modelshave recently achieved significant success. These methods typically require atleast a parametric form of the degradation model. However, in real-worldscenarios, the degradation may be too complex to define explicitly. To handlethis general case, we introduce the Diffusion Image Prior (DIIP). We takeinspiration from the Deep Image Prior (DIP)[16], since it can be used to removeartifacts without the need for an explicit degradation model. However, incontrast to DIP, we find that pretrained diffusion models offer a much strongerprior, despite being trained without knowledge from corrupted data. We showthat, the optimization process in DIIP first reconstructs a clean version ofthe image before eventually overfitting to the degraded input, but it does sofor a broader range of degradations than DIP. In light of this result, wepropose a blind image restoration (IR) method based on early stopping, whichdoes not require prior knowledge of the degradation model. We validate DIIP onvarious degradation-blind IR tasks, including JPEG artifact removal, waterdropremoval, denoising and super-resolution with state-of-the-art results.
  </details>

- **[HORT: Monocular Hand-held Objects Reconstruction with Transformers](http://arxiv.org/abs/2503.21313v1)**  `arXiv:2503.21313`  `cs.CV`  
  _Zerui Chen, Rolandos Alexandros Potamias, Shizhe Chen, Cordelia Schmid_
  <details open><summary>Abstract</summary>
  Reconstructing hand-held objects in 3D from monocular images remains asignificant challenge in computer vision. Most existing approaches rely onimplicit 3D representations, which produce overly smooth reconstructions andare time-consuming to generate explicit 3D shapes. While more recent methodsdirectly reconstruct point clouds with diffusion models, the multi-stepdenoising makes high-resolution reconstruction inefficient. To address theselimitations, we propose a transformer-based model to efficiently reconstructdense 3D point clouds of hand-held objects. Our method follows a coarse-to-finestrategy, first generating a sparse point cloud from the image andprogressively refining it into a dense representation using pixel-aligned imagefeatures. To enhance reconstruction accuracy, we integrate image features with3D hand geometry to jointly predict the object point cloud and its poserelative to the hand. Our model is trained end-to-end for optimal performance.Experimental results on both synthetic and real datasets demonstrate that ourmethod achieves state-of-the-art accuracy with much faster inference speed,while generalizing well to in-the-wild images.
  </details>

- **[GenFusion: Closing the Loop between Reconstruction and Generation via Videos](http://arxiv.org/abs/2503.21219v1)**  `arXiv:2503.21219`  `cs.AI` `cs.CV`  
  _Sibo Wu, Congrong Xu, Binbin Huang, Andreas Geiger, Anpei Chen_
  <details open><summary>Abstract</summary>
  Recently, 3D reconstruction and generation have demonstrated impressive novelview synthesis results, achieving high fidelity and efficiency. However, anotable conditioning gap can be observed between these two fields, e.g.,scalable 3D scene reconstruction often requires densely captured views, whereas3D generation typically relies on a single or no input view, whichsignificantly limits their applications. We found that the source of thisphenomenon lies in the misalignment between 3D constraints and generativepriors. To address this problem, we propose a reconstruction-driven videodiffusion model that learns to condition video frames on artifact-prone RGB-Drenderings. Moreover, we propose a cyclical fusion pipeline that iterativelyadds restoration frames from the generative model to the training set, enablingprogressive expansion and addressing the viewpoint saturation limitations seenin previous reconstruction and generation pipelines. Our evaluation, includingview synthesis from sparse view and masked input, validates the effectivenessof our approach.
  </details>

- **[ChatAnyone: Stylized Real-time Portrait Video Generation with Hierarchical Motion Diffusion Model](http://arxiv.org/abs/2503.21144v1)**  `arXiv:2503.21144`  `cs.CV`  
  _Jinwei Qi, Chaonan Ji, Sheng Xu, Peng Zhang, Bang Zhang, Liefeng Bo_
  <details open><summary>Abstract</summary>
  Real-time interactive video-chat portraits have been increasingly recognizedas the future trend, particularly due to the remarkable progress made in textand voice chat technologies. However, existing methods primarily focus onreal-time generation of head movements, but struggle to produce synchronizedbody motions that match these head actions. Additionally, achievingfine-grained control over the speaking style and nuances of facial expressionsremains a challenge. To address these limitations, we introduce a novelframework for stylized real-time portrait video generation, enabling expressiveand flexible video chat that extends from talking head to upper-bodyinteraction. Our approach consists of the following two stages. The first stageinvolves efficient hierarchical motion diffusion models, that take bothexplicit and implicit motion representations into account based on audioinputs, which can generate a diverse range of facial expressions with stylisticcontrol and synchronization between head and body movements. The second stageaims to generate portrait video featuring upper-body movements, including handgestures. We inject explicit hand control signals into the generator to producemore detailed hand movements, and further perform face refinement to enhancethe overall realism and expressiveness of the portrait video. Additionally, ourapproach supports efficient and continuous generation of upper-body portraitvideo in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supportinginteractive video-chat in real-time. Experimental results demonstrate thecapability of our approach to produce portrait videos with rich expressivenessand natural upper-body movements.
  </details>

- **[StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency](http://arxiv.org/abs/2503.21104v1)**  `arXiv:2503.21104`  `cs.CV`  
  _Yuyin Chen, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Yifei Zhan, et al._
  <details open><summary>Abstract</summary>
  Urban scene reconstruction requires modeling both static infrastructure anddynamic elements while supporting diverse environmental conditions. We present\textbf{StyledStreets}, a multi-style street simulator that achievesinstruction-driven scene editing with guaranteed spatial and temporalconsistency. Building on a state-of-the-art Gaussian Splatting framework forstreet scenarios enhanced by our proposed pose optimization and multi-viewtraining, our method enables photorealistic style transfers across seasons,weather conditions, and camera setups through three key innovations: First, ahybrid embedding scheme disentangles persistent scene geometry from transientstyle attributes, allowing realistic environmental edits while preservingstructural integrity. Second, uncertainty-aware rendering mitigates supervisionnoise from diffusion priors, enabling robust training across extreme stylevariations. Third, a unified parametric model prevents geometric drift throughregularized updates, maintaining multi-view consistency across sevenvehicle-mounted cameras.  Our framework preserves the original scene's motion patterns and geometricrelationships. Qualitative results demonstrate plausible transitions betweendiverse conditions (snow, sandstorm, night), while quantitative evaluationsshow state-of-the-art geometric accuracy under style transfers. The approachestablishes new capabilities for urban simulation, with applications inautonomous vehicle testing and augmented reality systems requiring reliableenvironmental consistency. Codes will be publicly available upon publication.
  </details>

- **[Can Video Diffusion Model Reconstruct 4D Geometry?](http://arxiv.org/abs/2503.21082v1)**  `arXiv:2503.21082`  `cs.CV`  
  _Jinjie Mai, Wenxuan Zhu, Haozhe Liu, Bing Li, Cheng Zheng, J√ºrgen Schmidhuber, et al._
  <details open><summary>Abstract</summary>
  Reconstructing dynamic 3D scenes (i.e., 4D geometry) from monocular video isan important yet challenging problem. Conventional multiview geometry-basedapproaches often struggle with dynamic motion, whereas recent learning-basedmethods either require specialized 4D representation or sophisticatedoptimization. In this paper, we present Sora3R, a novel framework that tapsinto the rich spatiotemporal priors of large-scale video diffusion models todirectly infer 4D pointmaps from casual videos. Sora3R follows a two-stagepipeline: (1) we adapt a pointmap VAE from a pretrained video VAE, ensuringcompatibility between the geometry and video latent spaces; (2) we finetune adiffusion backbone in combined video and pointmap latent space to generatecoherent 4D pointmaps for every frame. Sora3R operates in a fully feedforwardmanner, requiring no external modules (e.g., depth, optical flow, orsegmentation) or iterative global alignment. Extensive experiments demonstratethat Sora3R reliably recovers both camera poses and detailed scene geometry,achieving performance on par with state-of-the-art methods for dynamic 4Dreconstruction across diverse scenarios.
  </details>

- **[Efficient Multi-Instance Generation with Janus-Pro-Dirven Prompt Parsing](http://arxiv.org/abs/2503.21069v1)**  `arXiv:2503.21069`  `cs.CV`  
  _Fan Qi, Yu Duan, Changsheng Xu_
  <details open><summary>Abstract</summary>
  Recent advances in text-guided diffusion models have revolutionizedconditional image generation, yet they struggle to synthesize complex sceneswith multiple objects due to imprecise spatial grounding and limitedscalability. We address these challenges through two key modules: 1)Janus-Pro-driven Prompt Parsing, a prompt-layout parsing module that bridgestext understanding and layout generation via a compact 1B-parameterarchitecture, and 2) MIGLoRA, a parameter-efficient plug-in integratingLow-Rank Adaptation (LoRA) into UNet (SD1.5) and DiT (SD3) backbones. MIGLoRAis capable of preserving the base model's parameters and ensuring plug-and-playadaptability, minimizing architectural intrusion while enabling efficientfine-tuning. To support a comprehensive evaluation, we create DescripBox andDescripBox-1024, benchmarks that span diverse scenes and resolutions. Theproposed method achieves state-of-the-art performance on COCO and LVISbenchmarks while maintaining parameter efficiency, demonstrating superiorlayout fidelity and scalability for open-world synthesis.
  </details>

- **[MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D Generation](http://arxiv.org/abs/2503.20519v2)**  `arXiv:2503.20519`  `cs.CV`  
  _Jinnan Chen, Lingting Zhu, Zeyu Hu, Shengju Qian, Yugang Chen, Xin Wang, et al._
  <details open><summary>Abstract</summary>
  Recent advances in auto-regressive transformers have revolutionizedgenerative modeling across different domains, from language processing tovisual generation, demonstrating remarkable capabilities. However, applyingthese advances to 3D generation presents three key challenges: the unorderednature of 3D data conflicts with sequential next-token prediction paradigm,conventional vector quantization approaches incur substantial compression losswhen applied to 3D meshes, and the lack of efficient scaling strategies forhigher resolution latent prediction. To address these challenges, we introduceMAR-3D, which integrates a pyramid variational autoencoder with a cascadedmasked auto-regressive transformer (Cascaded MAR) for progressive latentupscaling in the continuous space. Our architecture employs random maskingduring training and auto-regressive denoising in random order during inference,naturally accommodating the unordered property of 3D latent tokens.Additionally, we propose a cascaded training strategy with conditionaugmentation that enables efficiently up-scale the latent token resolution withfast convergence. Extensive experiments demonstrate that MAR-3D not onlyachieves superior performance and generalization capabilities compared toexisting methods but also exhibits enhanced scaling capabilities compared tojoint distribution modeling approaches (e.g., diffusion transformers).
  </details>

- **[Consistency Trajectory Matching for One-Step Generative Super-Resolution](http://arxiv.org/abs/2503.20349v2)**  `arXiv:2503.20349`  `cs.CV`  
  _Weiyi You, Mingyang Zhang, Leheng Zhang, Xingyu Zhou, Kexuan Shi, Shuhang Gu_
  <details open><summary>Abstract</summary>
  Current diffusion-based super-resolution (SR) approaches achieve commendableperformance at the cost of high inference overhead. Therefore, distillationtechniques are utilized to accelerate the multi-step teacher model intoone-step student model. Nevertheless, these methods significantly raisetraining costs and constrain the performance of the student model by theteacher model. To overcome these tough challenges, we propose ConsistencyTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategythat is able to generate photo-realistic SR results in one step. Concretely, wefirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)trajectory to establish a deterministic mapping from low-resolution (LR) imageswith noise to high-resolution (HR) images. Then we apply the ConsistencyTraining (CT) strategy to directly learn the mapping in one step, eliminatingthe necessity of pre-trained diffusion model. To further enhance theperformance and better leverage the ground-truth during the training process,we aim to align the distribution of SR results more closely with that of thenatural images. To this end, we propose to minimize the discrepancy betweentheir respective PF-ODE trajectories from the LR image distribution by ourmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting inimproved realism of our recovered HR images. Comprehensive experimental resultsdemonstrate that the proposed methods can attain comparable or even superiorcapabilities on both synthetic and real datasets while maintaining minimalinference latency.
  </details>

- **[Training-free Diffusion Acceleration with Bottleneck Sampling](http://arxiv.org/abs/2503.18940v2)**  `arXiv:2503.18940`  `cs.CV`  
  _Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, et al._
  <details open><summary>Abstract</summary>
  Diffusion models have demonstrated remarkable capabilities in visual contentgeneration but remain challenging to deploy due to their high computationalcost during inference. This computational burden primarily arises from thequadratic complexity of self-attention with respect to image or videoresolution. While existing acceleration methods often compromise output qualityor necessitate costly retraining, we observe that most diffusion models arepre-trained at lower resolutions, presenting an opportunity to exploit theselow-resolution priors for more efficient inference without degradingperformance. In this work, we introduce Bottleneck Sampling, a training-freeframework that leverages low-resolution priors to reduce computational overheadwhile preserving output fidelity. Bottleneck Sampling follows a high-low-highdenoising workflow: it performs high-resolution denoising in the initial andfinal stages while operating at lower resolutions in intermediate steps. Tomitigate aliasing and blurring artifacts, we further refine the resolutiontransition points and adaptively shift the denoising timesteps at each stage.We evaluate Bottleneck Sampling on both image and video generation tasks, whereextensive experiments demonstrate that it accelerates inference by up to3$\times$ for image generation and 2.5$\times$ for video generation, all whilemaintaining output quality comparable to the standard full-resolution samplingprocess across multiple evaluation metrics.
  </details>

- **[MotionDiff: Training-free Zero-shot Interactive Motion Editing via Flow-assisted Multi-view Diffusion](http://arxiv.org/abs/2503.17695v2)**  `arXiv:2503.17695`  `cs.CV`  
  _Yikun Ma, Yiqing Li, Jiawei Wu, Xing Luo, Zhi Jin_
  <details open><summary>Abstract</summary>
  Generative models have made remarkable advancements and are capable ofproducing high-quality content. However, performing controllable editing withgenerative models remains challenging, due to their inherent uncertainty inoutputs. This challenge is praticularly pronounced in motion editing, whichinvolves the processing of spatial information. While some physics-basedgenerative methods have attempted to implement motion editing, they typicallyoperate on single-view images with simple motions, such as translation anddragging. These methods struggle to handle complex rotation and stretchingmotions and ensure multi-view consistency, often necessitatingresource-intensive retraining. To address these challenges, we proposeMotionDiff, a training-free zero-shot diffusion method that leverages opticalflow for complex multi-view motion editing. Specifically, given a static scene,users can interactively select objects of interest to add motion priors. Theproposed Point Kinematic Model (PKM) then estimates corresponding multi-viewoptical flows during the Multi-view Flow Estimation Stage (MFES). Subsequently,these optical flows are utilized to generate multi-view motion results throughdecoupled motion representation in the Multi-view Motion Diffusion Stage(MMDS). Extensive experiments demonstrate that MotionDiff outperforms otherphysics-based generative motion editing methods in achieving high-qualitymulti-view consistent motion results. Notably, MotionDiff does not requireretraining, enabling users to conveniently adapt it for various down-streamtasks.
  </details>

- **[ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos](http://arxiv.org/abs/2503.16400v2)**  `arXiv:2503.16400`  `cs.LG`  
  _Haolin Yang, Feilong Tang, Ming Hu, Yulong Li, Yexin Liu, Zelin Peng, et al._
  <details open><summary>Abstract</summary>
  Video diffusion models (VDMs) facilitate the generation of high-qualityvideos, with current research predominantly concentrated on scaling effortsduring training through improvements in data quality, computational resources,and model complexity. However, inference-time scaling has received lessattention, with most approaches restricting models to a single generationattempt. Recent studies have uncovered the existence of "golden noises" thatcan enhance video quality during generation. Building on this, we find thatguiding the scaling inference-time search of VDMs to identify better noisecandidates not only evaluates the quality of the frames generated in thecurrent step but also preserves the high-level object features by referencingthe anchor frame from previous multi-chunks, thereby delivering long-termvalue. Our analysis reveals that diffusion models inherently possess flexibleadjustments of computation by varying denoising steps, and even a one-stepdenoising approach, when guided by a reward signal, yields significantlong-term benefits. Based on the observation, we proposeScalingNoise, aplug-and-play inference-time search strategy that identifies golden initialnoises for the diffusion sampling process to improve global content consistencyand visual diversity. Specifically, we perform one-step denoising to convertinitial noises into a clip and subsequently evaluate its long-term value,leveraging a reward model anchored by previously generated content. Moreover,to preserve diversity, we sample candidates from a tilted noise distributionthat up-weights promising noises. In this way, ScalingNoise significantlyreduces noise-induced errors, ensuring more coherent and spatiotemporallyconsistent video generation. Extensive experiments on benchmark datasetsdemonstrate that the proposed ScalingNoise effectively improves long videogeneration.
  </details>

- **[GR00T N1: An Open Foundation Model for Generalist Humanoid Robots](http://arxiv.org/abs/2503.14734v2)**  `arXiv:2503.14734`  `cs.AI` `cs.RO` `cs.LG`  
  _NVIDIA, :, Johan Bjorck, Fernando Casta√±eda, Nikita Cherniadev, Xingye Da, et al._
  <details open><summary>Abstract</summary>
  General-purpose robots need a versatile body and an intelligent mind. Recentadvancements in humanoid robots have shown great promise as a hardware platformfor building generalist autonomy in the human world. A robot foundation model,trained on massive and diverse data sources, is essential for enabling therobots to reason about novel situations, robustly handle real-worldvariability, and rapidly learn new tasks. To this end, we introduce GR00T N1,an open foundation model for humanoid robots. GR00T N1 is aVision-Language-Action (VLA) model with a dual-system architecture. Thevision-language module (System 2) interprets the environment through vision andlanguage instructions. The subsequent diffusion transformer module (System 1)generates fluid motor actions in real time. Both modules are tightly coupledand jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixtureof real-robot trajectories, human videos, and synthetically generated datasets.We show that our generalist robot model GR00T N1 outperforms thestate-of-the-art imitation learning baselines on standard simulation benchmarksacross multiple robot embodiments. Furthermore, we deploy our model on theFourier GR-1 humanoid robot for language-conditioned bimanual manipulationtasks, achieving strong performance with high data efficiency.
  </details>

- **[DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection](http://arxiv.org/abs/2503.13985v2)**  `arXiv:2503.13985`  `cs.AI` `cs.CV` `cs.LG`  
  _Jaewoo Song, Daemin Park, Kanghyun Baek, Sangyub Lee, Jooyoung Choi, Eunji Kim, et al._
  <details open><summary>Abstract</summary>
  Developing effective visual inspection models remains challenging due to thescarcity of defect data. While image generation models have been used tosynthesize defect images, producing highly realistic defects remains difficult.We propose DefectFill, a novel method for realistic defect generation thatrequires only a few reference defect images. It leverages a fine-tunedinpainting diffusion model, optimized with our custom loss functionsincorporating defect, object, and attention terms. It enables precise captureof detailed, localized defect features and their seamless integration intodefect-free objects. Additionally, our Low-Fidelity Selection method furtherenhances the defect sample quality. Experiments show that DefectFill generateshigh-quality defect images, enabling visual inspection models to achievestate-of-the-art performance on the MVTec AD dataset.
  </details>

- **[Towards Better Alignment: Training Diffusion Models with Reinforcement Learning Against Sparse Rewards](http://arxiv.org/abs/2503.11240v2)**  `arXiv:2503.11240`  `cs.CV` `cs.LG`  
  _Zijing Hu, Fengda Zhang, Long Chen, Kun Kuang, Jiahui Li, Kaifeng Gao, et al._
  <details open><summary>Abstract</summary>
  Diffusion models have achieved remarkable success in text-to-imagegeneration. However, their practical applications are hindered by themisalignment between generated images and corresponding text prompts. To tacklethis issue, reinforcement learning (RL) has been considered for diffusion modelfine-tuning. Yet, RL's effectiveness is limited by the challenge of sparsereward, where feedback is only available at the end of the generation process.This makes it difficult to identify which actions during the denoising processcontribute positively to the final generated image, potentially leading toineffective or unnecessary denoising policies. To this end, this paper presentsa novel RL-based framework that addresses the sparse reward problem whentraining diffusion models. Our framework, named $\text{B}^2\text{-DiffuRL}$,employs two strategies: \textbf{B}ackward progressive training and\textbf{B}ranch-based sampling. For one thing, backward progressive trainingfocuses initially on the final timesteps of denoising process and graduallyextends the training interval to earlier timesteps, easing the learningdifficulty from sparse rewards. For another, we perform branch-based samplingfor each training interval. By comparing the samples within the same branch, wecan identify how much the policies of the current training interval contributeto the final image, which helps to learn effective policies instead ofunnecessary ones. $\text{B}^2\text{-DiffuRL}$ is compatible with existingoptimization algorithms. Extensive experiments demonstrate the effectiveness of$\text{B}^2\text{-DiffuRL}$ in improving prompt-image alignment and maintainingdiversity in generated images. The code for this work is available.
  </details>

- **[Rethinking Video Tokenization: A Conditioned Diffusion-based Approach](http://arxiv.org/abs/2503.03708v3)**  `arXiv:2503.03708`  `cs.AI` `cs.CV`  
  _Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, et al._
  <details open><summary>Abstract</summary>
  Existing video tokenizers typically use the traditional VariationalAutoencoder (VAE) architecture for video compression and reconstruction.However, to achieve good performance, its training process often relies oncomplex multi-stage training tricks that go beyond basic reconstruction lossand KL regularization. Among these tricks, the most challenging is the precisetuning of adversarial training with additional Generative Adversarial Networks(GANs) in the final stage, which can hinder stable convergence. In contrast toGANs, diffusion models offer more stable training processes and can generatehigher-quality results. Inspired by these advantages, we propose CDT, a novelConditioned Diffusion-based video Tokenizer, that replaces the GAN-baseddecoder with a conditional causal diffusion model. The encoder compressesspatio-temporal information into compact latents, while the decoderreconstructs videos through a reverse diffusion process conditioned on theselatents. During inference, we incorporate a feature cache mechanism to generatevideos of arbitrary length while maintaining temporal continuity and adoptsampling acceleration technique to enhance efficiency. Trained using only abasic MSE diffusion loss for reconstruction, along with KL term and LPIPSperceptual loss from scratch, extensive experiments demonstrate that CDTachieves state-of-the-art performance in video reconstruction tasks with just asingle-step sampling. Even a scaled-down version of CDT (3$\times$ inferencespeedup) still performs comparably with top baselines. Moreover, the latentvideo generation model trained with CDT also exhibits superior performance. Thesource code and pretrained weights are available athttps://github.com/ali-vilab/CDT.
  </details>

- **[TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models](http://arxiv.org/abs/2502.06608v3)**  `arXiv:2502.06608`  `cs.AI` `cs.CV`  
  _Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in diffusion techniques have propelled image and videogeneration to unprecedented levels of quality, significantly accelerating thedeployment and application of generative AI. However, 3D shape generationtechnology has so far lagged behind, constrained by limitations in 3D datascale, complexity of 3D data processing, and insufficient exploration ofadvanced techniques in the 3D domain. Current approaches to 3D shape generationface substantial challenges in terms of output quality, generalizationcapability, and alignment with input conditions. We present TripoSG, a newstreamlined shape diffusion paradigm capable of generating high-fidelity 3Dmeshes with precise correspondence to input images. Specifically, we propose:1) A large-scale rectified flow transformer for 3D shape generation, achievingstate-of-the-art fidelity through training on extensive, high-quality data. 2)A hybrid supervised training strategy combining SDF, normal, and eikonal lossesfor 3D VAE, achieving high-quality 3D reconstruction performance. 3) A dataprocessing pipeline to generate 2 million high-quality 3D samples, highlightingthe crucial rules for data quality and quantity in training 3D generativemodels. Through comprehensive experiments, we have validated the effectivenessof each component in our new framework. The seamless integration of these partshas enabled TripoSG to achieve state-of-the-art performance in 3D shapegeneration. The resulting 3D shapes exhibit enhanced detail due tohigh-resolution capabilities and demonstrate exceptional fidelity to inputimages. Moreover, TripoSG demonstrates improved versatility in generating 3Dmodels from diverse image styles and contents, showcasing strong generalizationcapabilities. To foster progress and innovation in the field of 3D generation,we will make our model publicly available.
  </details>

- **[TREAD: Token Routing for Efficient Architecture-agnostic Diffusion Training](http://arxiv.org/abs/2501.04765v2)**  `arXiv:2501.04765`  `cs.AI` `cs.CV`  
  _Felix Krause, Timy Phan, Ming Gui, Stefan Andreas Baumann, Vincent Tao Hu, Bj√∂rn Ommer_
  <details open><summary>Abstract</summary>
  Diffusion models have emerged as the mainstream approach for visualgeneration. However, these models typically suffer from sample inefficiency andhigh training costs. Consequently, methods for efficient finetuning, inferenceand personalization were quickly adopted by the community. However, trainingthese models in the first place remains very costly. While several recentapproaches - including masking, distillation, and architectural modifications -have been proposed to improve training efficiency, each of these methods comeswith a tradeoff: they achieve enhanced performance at the expense of increasedcomputational cost or vice versa. In contrast, this work aims to improvetraining efficiency as well as generative performance at the same time throughroutes that act as a transport mechanism for randomly selected tokens fromearly layers to deeper layers of the model. Our method is not limited to thecommon transformer-based model - it can also be applied to state-space modelsand achieves this without architectural modifications or additional parameters.Finally, we show that TREAD reduces computational cost and simultaneouslyboosts model performance on the standard ImageNet-256 benchmark inclass-conditional synthesis. Both of these benefits multiply to a convergencespeedup of 14x at 400K training iterations compared to DiT and 37x compared tothe best benchmark performance of DiT at 7M training iterations. Furthermore,we achieve a competitive FID of 2.09 in a guided and 3.93 in an unguidedsetting, which improves upon the DiT, without architectural changes.
  </details>

- **[SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis](http://arxiv.org/abs/2412.20104v4)**  `arXiv:2412.20104`  `cs.RO` `cs.AI` `cs.CV` `cs.LG`  
  _Wenkun He, Yun Liu, Ruitao Liu, Li Yi_
  <details open><summary>Abstract</summary>
  Synthesizing realistic human-object interaction motions is a critical problemin VR/AR and human animation. Unlike the commonly studied scenarios involving asingle human or hand interacting with one object, we address a more genericmulti-body setting with arbitrary numbers of humans, hands, and objects. Thiscomplexity introduces significant challenges in synchronizing motions due tothe high correlations and mutual influences among bodies. To address thesechallenges, we introduce SyncDiff, a novel method for multi-body interactionsynthesis using a synchronized motion diffusion strategy. SyncDiff employs asingle diffusion model to capture the joint distribution of multi-body motions.To enhance motion fidelity, we propose a frequency-domain motion decompositionscheme. Additionally, we introduce a new set of alignment scores to emphasizethe synchronization of different body motions. SyncDiff jointly optimizes bothdata sample likelihood and alignment likelihood through an explicitsynchronization strategy. Extensive experiments across four datasets withvarious multi-body configurations demonstrate the superiority of SyncDiff overexisting state-of-the-art motion synthesis methods.
  </details>

- **[Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers](http://arxiv.org/abs/2412.16822v2)**  `arXiv:2412.16822`  `cs.AI` `cs.CV` `cs.LG`  
  _Haoran You, Connelly Barnes, Yuqian Zhou, Yan Kang, Zhenbang Du, Wei Zhou, et al._
  <details open><summary>Abstract</summary>
  Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) imagegeneration quality but suffer from high latency and memory inefficiency, makingthem difficult to deploy on resource-constrained devices. One major efficiencybottleneck is that existing DiTs apply equal computation across all regions ofan image. However, not all image tokens are equally important, and certainlocalized areas require more computation, such as objects. To address this, wepropose DiffCR, a dynamic DiT inference framework with differentiablecompression ratios, which automatically learns to dynamically route computationacross layers and timesteps for each image token, resulting in efficient DiTs.Specifically, DiffCR integrates three features: (1) A token-level routingscheme where each DiT layer includes a router that is fine-tuned jointly withmodel weights to predict token importance scores. In this way, unimportanttokens bypass the entire layer's computation; (2) A layer-wise differentiableratio mechanism where different DiT layers automatically learn varyingcompression ratios from a zero initialization, resulting in large compressionratios in redundant layers while others remain less compressed or evenuncompressed; (3) A timestep-wise differentiable ratio mechanism where eachdenoising timestep learns its own compression ratio. The resulting patternshows higher ratios for noisier timesteps and lower ratios as the image becomesclearer. Extensive experiments on text-to-image and inpainting tasks show thatDiffCR effectively captures dynamism across token, layer, and timestep axes,achieving superior trade-offs between generation quality and efficiencycompared to prior works. The project website is available athttps://www.haoranyou.com/diffcr.
  </details>

- **[Video Motion Transfer with Diffusion Transformers](http://arxiv.org/abs/2412.07776v2)**  `arXiv:2412.07776`  `cs.AI` `cs.CV` `cs.LG`  
  _Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr, Fabio Pizzati_
  <details open><summary>Abstract</summary>
  We propose DiTFlow, a method for transferring the motion of a reference videoto a newly synthesized one, designed specifically for Diffusion Transformers(DiT). We first process the reference video with a pre-trained DiT to analyzecross-frame attention maps and extract a patch-wise motion signal called theAttention Motion Flow (AMF). We guide the latent denoising process in anoptimization-based, training-free, manner by optimizing latents with our AMFloss to generate videos reproducing the motion of the reference one. We alsoapply our optimization strategy to transformer positional embeddings, grantingus a boost in zero-shot motion transfer capabilities. We evaluate DiTFlowagainst recently published methods, outperforming all across multiple metricsand human evaluation.
  </details>

- **[Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](http://arxiv.org/abs/2412.03044v2)**  `arXiv:2412.03044`  `cs.CV`  
  _Xiaofeng Tan, Hongsong Wang, Xin Geng, Liang Wang_
  <details open><summary>Abstract</summary>
  Video anomaly detection (VAD) is a vital yet complex open-set task incomputer vision, commonly tackled through reconstruction-based methods.However, these methods struggle with two key limitations: (1) insufficientrobustness in open-set scenarios, where unseen normal motions are frequentlymisclassified as anomalies, and (2) an overemphasis on, but restricted capacityfor, local motion reconstruction, which are inherently difficult to captureaccurately due to their diversity. To overcome these challenges, we introduce anovel frequency-guided diffusion model with perturbation training. First, weenhance robustness by training a generator to produce perturbed samples, whichare similar to normal samples and target the weakness of the reconstructionmodel. This training paradigm expands the reconstruction domain of the model,improving its generalization to unseen normal motions. Second, to address theoveremphasis on motion details, we employ the 2D Discrete Cosine Transform(DCT) to separate high-frequency (local) and low-frequency (global) motioncomponents. By guiding the diffusion model with observed high-frequencyinformation, we prioritize the reconstruction of low-frequency components,enabling more accurate and robust anomaly detection. Extensive experiments onfive widely used VAD datasets demonstrate that our approach surpassesstate-of-the-art methods, underscoring its effectiveness in open-set scenariosand diverse motion contexts. Our project website ishttps://xiaofeng-tan.github.io/projects/FG-Diff/index.html.
  </details>

- **[VIRES: Video Instance Repainting via Sketch and Text Guided Generation](http://arxiv.org/abs/2411.16199v5)**  `arXiv:2411.16199`  `cs.CV`  
  _Shuchen Weng, Haojie Zheng, Peixuan Zhan, Yuchen Hong, Han Jiang, Si Li, et al._
  <details open><summary>Abstract</summary>
  We introduce VIRES, a video instance repainting method with sketch and textguidance, enabling video instance repainting, replacement, generation, andremoval. Existing approaches struggle with temporal consistency and accuratealignment with the provided sketch sequence. VIRES leverages the generativepriors of text-to-video models to maintain temporal consistency and producevisually pleasing results. We propose the Sequential ControlNet with thestandardized self-scaling, which effectively extracts structure layouts andadaptively captures high-contrast sketch details. We further augment thediffusion transformer backbone with the sketch attention to interpret andinject fine-grained sketch semantics. A sketch-aware encoder ensures thatrepainted results are aligned with the provided sketch sequence. Additionally,we contribute the VireSet, a dataset with detailed annotations tailored fortraining and evaluating video instance editing methods. Experimental resultsdemonstrate the effectiveness of VIRES, which outperforms state-of-the-artmethods in visual quality, temporal consistency, condition alignment, and humanratings. Project page: https://hjzheng.net/projects/VIRES/
  </details>

- **[Rethinking Training for De-biasing Text-to-Image Generation: Unlocking the Potential of Stable Diffusion](http://arxiv.org/abs/2408.12692v2)**  `arXiv:2408.12692`  `cs.AI`  
  _Eunji Kim, Siwon Kim, Minjun Park, Rahim Entezari, Sungroh Yoon_
  <details open><summary>Abstract</summary>
  Recent advancements in text-to-image models, such as Stable Diffusion, showsignificant demographic biases. Existing de-biasing techniques rely heavily onadditional training, which imposes high computational costs and risks ofcompromising core image generation functionality. This hinders them from beingwidely adopted to real-world applications. In this paper, we explore StableDiffusion's overlooked potential to reduce bias without requiring additionaltraining. Through our analysis, we uncover that initial noises associated withminority attributes form "minority regions" rather than scattered. We viewthese "minority regions" as opportunities in SD to reduce bias. To unlock thepotential, we propose a novel de-biasing method called 'weak guidance,'carefully designed to guide a random noise to the minority regions withoutcompromising semantic integrity. Through analysis and experiments on variousversions of SD, we demonstrate that our proposed approach effectively reducesbias without additional training, achieving both efficiency and preservation ofcore image generation functionality.
  </details>

- **[Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation](http://arxiv.org/abs/2407.03006v2)**  `arXiv:2407.03006`  `cs.CV`  
  _Xiang Gao, Zhengbo Xu, Junhan Zhao, Jiaying Liu_
  <details open><summary>Abstract</summary>
  Recently, large-scale text-to-image (T2I) diffusion models have emerged as apowerful tool for image-to-image translation (I2I), allowing open-domain imagetranslation via user-provided text prompts. This paper proposesfrequency-controlled diffusion model (FCDiffusion), an end-to-enddiffusion-based framework that contributes a novel solution to text-guided I2Ifrom a frequency-domain perspective. At the heart of our framework is afeature-space frequency-domain filtering module based on Discrete CosineTransform, which filters the latent features of the source image in the DCTdomain, yielding filtered image features bearing different DCT spectral bandsas different control signals to the pre-trained Latent Diffusion Model. Wereveal that control signals of different DCT spectral bands bridge the sourceimage and the T2I generated image in different correlations (e.g., style,structure, layout, contour, etc.), and thus enable versatile I2I applicationsemphasizing different I2I correlations, including style-guided contentcreation, image semantic manipulation, image scene translation, and image styletranslation. Different from related approaches, FCDiffusion establishes aunified text-guided I2I framework suitable for diverse image translation taskssimply by switching among different frequency control branches at inferencetime. The effectiveness and superiority of our method for text-guided I2I aredemonstrated with extensive experiments both qualitatively and quantitatively.Our project is publicly available at:https://xianggao1102.github.io/FCDiffusion/.
  </details>
