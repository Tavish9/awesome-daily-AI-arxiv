# üîç Diffusion Papers ¬∑ 2025-03-28

[![Total Papers](https://img.shields.io/badge/Papers-28-2688EB)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)]()

---

## üìå Filter by Category
**Keywords**: `Diffusion` `DDPM` `DDIM`  
**Filter**: `None`

---

## üìö Paper List

- **[DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness](http://arxiv.org/abs/2503.22677v1)**  `arXiv:2503.22677`  `cs.CV` `cs.AI` `cs.LG`  
  _Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi_
  <details open><summary>Abstract</summary>
  Most 3D object generators focus on aesthetic quality, often neglectingphysical constraints necessary in applications. One such constraint is that the3D object should be self-supporting, i.e., remains balanced under gravity.Prior approaches to generating stable 3D objects used differentiable physicssimulators to optimize geometry at test-time, which is slow, unstable, andprone to local optima. Inspired by the literature on aligning generative modelsto external feedback, we propose Direct Simulation Optimization (DSO), aframework to use the feedback from a (non-differentiable) simulator to increasethe likelihood that the 3D generator outputs stable 3D objects directly. Weconstruct a dataset of 3D objects labeled with a stability score obtained fromthe physics simulator. We can then fine-tune the 3D generator using thestability score as the alignment metric, via direct preference optimization(DPO) or direct reward optimization (DRO), a novel objective, which weintroduce, to align diffusion models without requiring pairwise preferences.Our experiments show that the fine-tuned feed-forward generator, using eitherDPO or DRO objective, is much faster and more likely to produce stable objectsthan test-time optimization. Notably, the DSO framework works even without anyground-truth 3D objects for training, allowing the 3D generator to self-improveby automatically collecting simulation feedback on its own outputs.
  </details>

- **[Empirical Analysis of Sim-and-Real Cotraining Of Diffusion Policies For Planar Pushing from Pixels](http://arxiv.org/abs/2503.22634v1)**  `arXiv:2503.22634`  `cs.RO` `cs.AI`  
  _Adam Wei, Abhinav Agarwal, Boyuan Chen, Rohan Bosworth, Nicholas Pfaff, Russ Tedrake_
  <details open><summary>Abstract</summary>
  In imitation learning for robotics, cotraining with demonstration datagenerated both in simulation and on real hardware has emerged as a powerfulrecipe to overcome the sim2real gap. This work seeks to elucidate basicprinciples of this sim-and-real cotraining to help inform simulation design,sim-and-real dataset creation, and policy training. Focusing narrowly on thecanonical task of planar pushing from camera inputs enabled us to be thoroughin our study. These experiments confirm that cotraining with simulated data\emph{can} dramatically improve performance in real, especially when real datais limited. Performance gains scale with simulated data, but eventuallyplateau; real-world data increases this performance ceiling. The results alsosuggest that reducing the domain gap in physics may be more important thanvisual fidelity for non-prehensile manipulation tasks. Perhaps surprisingly,having some visual domain gap actually helps the cotrained policy -- binaryprobes reveal that high-performing policies learn to distinguish simulateddomains from real. We conclude by investigating this nuance and mechanisms thatfacilitate positive transfer between sim-and-real. In total, our experimentsspan over 40 real-world policies (evaluated on 800+ trials) and 200 simulatedpolicies (evaluated on 40,000+ trials).
  </details>

- **[Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model](http://arxiv.org/abs/2503.22622v1)**  `arXiv:2503.22622`  `cs.CV`  
  _Jangho Park, Taesung Kwon, Jong Chul Ye_
  <details open><summary>Abstract</summary>
  Recently, multi-view or 4D video generation has emerged as a significantresearch topic. Nonetheless, recent approaches to 4D generation still strugglewith fundamental limitations, as they primarily rely on harnessing multiplevideo diffusion models with additional training or compute-intensive trainingof a full 4D diffusion model with limited real-world 4D data and largecomputational costs. To address these challenges, here we propose the firsttraining-free 4D video generation method that leverages the off-the-shelf videodiffusion models to generate multi-view videos from a single input video. Ourapproach consists of two key steps: (1) By designating the edge frames in thespatio-temporal sampling grid as key frames, we first synthesize them using avideo diffusion model, leveraging a depth-based warping technique for guidance.This approach ensures structural consistency across the generated frames,preserving spatial and temporal coherence. (2) We then interpolate theremaining frames using a video diffusion model, constructing a fully populatedand temporally coherent sampling grid while preserving spatial and temporalconsistency. Through this approach, we extend a single video into a multi-viewvideo along novel camera trajectories while maintaining spatio-temporalconsistency. Our method is training-free and fully utilizes an off-the-shelfvideo diffusion model, offering a practical and effective solution formulti-view video generation.
  </details>

- **[Generative Latent Neural PDE Solver using Flow Matching](http://arxiv.org/abs/2503.22600v1)**  `arXiv:2503.22600`  `cs.AI` `cs.LG`  
  _Zijie Li, Anthony Zhou, Amir Barati Farimani_
  <details open><summary>Abstract</summary>
  Autoregressive next-step prediction models have become the de-facto standardfor building data-driven neural solvers to forecast time-dependent partialdifferential equations (PDEs). Denoise training that is closely related todiffusion probabilistic model has been shown to enhance the temporal stabilityof neural solvers, while its stochastic inference mechanism enables ensemblepredictions and uncertainty quantification. In principle, such traininginvolves sampling a series of discretized diffusion timesteps during bothtraining and inference, inevitably increasing computational overhead. Inaddition, most diffusion models apply isotropic Gaussian noise on structured,uniform grids, limiting their adaptability to irregular domains. We propose alatent diffusion model for PDE simulation that embeds the PDE state in alower-dimensional latent space, which significantly reduces computationalcosts. Our framework uses an autoencoder to map different types of meshes ontoa unified structured latent grid, capturing complex geometries. By analyzingcommon diffusion paths, we propose to use a coarsely sampled noise schedulefrom flow matching for both training and testing. Numerical experiments showthat the proposed model outperforms several deterministic baselines in bothaccuracy and long-term stability, highlighting the potential of diffusion-basedapproaches for robust data-driven PDE learning.
  </details>

- **[Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction in Transformers through Token Correlation](http://arxiv.org/abs/2503.22547v1)**  `arXiv:2503.22547`  `cs.CL` `cs.LG`  
  _Zhuo-Yang Song, Zeyu Li, Qing-Hong Cao, Ming-xing Luo, Hua Xing Zhu_
  <details open><summary>Abstract</summary>
  The geometric evolution of token representations in large language models(LLMs) presents a fundamental paradox: while human language inherentlyorganizes semantic information in low-dimensional spaces ($\sim 10^1$dimensions), modern LLMs employ high-dimensional embeddings ($\sim 10^3$dimensions) processed through Transformer architectures. To resolve thisparadox, this work bridges this conceptual gap by developing a geometricframework that tracks token dynamics across Transformers layers. Throughlayer-wise analysis of intrinsic dimensions across multiple architectures, wereveal an expansion-contraction pattern where tokens diffuse to a "workingspace" and then progressively project onto lower-dimensional submanifolds. Ourfinding implies a negative correlation between the working space dimension andparameter-sensitive performance of the LLMs, and indicates that effectivemodels tend to compress tokens into approximately 10-dimensional submanifolds,closely resembling human semantic spaces. This work not only advances LLMinterpretability by reframing Transformers layers as projectors that mediatebetween high-dimensional computation and low-dimensional semantics, but alsoprovides practical tools for model diagnostics that do not rely ontask-specific evaluations.
  </details>

- **[LIM: Large Interpolator Model for Dynamic Reconstruction](http://arxiv.org/abs/2503.22537v1)**  `arXiv:2503.22537`  `cs.CV` `cs.AI`  
  _Remy Sabathier, Niloy J. Mitra, David Novotny_
  <details open><summary>Abstract</summary>
  Reconstructing dynamic assets from video data is central to many in computervision and graphics tasks. Existing 4D reconstruction approaches are limited bycategory-specific models or slow optimization-based methods. Inspired by therecent Large Reconstruction Model (LRM), we present the Large InterpolationModel (LIM), a transformer-based feed-forward solution, guided by a novelcausal consistency loss, for interpolating implicit 3D representations acrosstime. Given implicit 3D representations at times $t_0$ and $t_1$, LIM producesa deformed shape at any continuous time $t\in[t_0,t_1]$, deliveringhigh-quality interpolated frames in seconds. Furthermore, LIM allows explicitmesh tracking across time, producing a consistently uv-textured mesh sequenceready for integration into existing production pipelines. We also use LIM, inconjunction with a diffusion-based multiview generator, to produce dynamic 4Dreconstructions from monocular videos. We evaluate LIM on various dynamicdatasets, benchmarking against image-space interpolation methods (e.g., FiLM)and direct triplane linear interpolation, and demonstrate clear advantages. Insummary, LIM is the first feed-forward model capable of high-speed tracked 4Dasset reconstruction across diverse categories.
  </details>

- **[Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving Simulation Environments](http://arxiv.org/abs/2503.22496v1)**  `arXiv:2503.22496`  `cs.CV` `cs.RO`  
  _Luke Rowe, Roger Girgis, Anthony Gosselin, Liam Paull, Christopher Pal, Felix Heide_
  <details open><summary>Abstract</summary>
  We introduce Scenario Dreamer, a fully data-driven generative simulator forautonomous vehicle planning that generates both the initial traffic scene -comprising a lane graph and agent bounding boxes - and closed-loop agentbehaviours. Existing methods for generating driving simulation environmentsencode the initial traffic scene as a rasterized image and, as such, requireparameter-heavy networks that perform unnecessary computation due to many emptypixels in the rasterized scene. Moreover, we find that existing methods thatemploy rule-based agent behaviours lack diversity and realism. Scenario Dreamerinstead employs a novel vectorized latent diffusion model for initial scenegeneration that directly operates on the vectorized scene elements and anautoregressive Transformer for data-driven agent behaviour simulation. ScenarioDreamer additionally supports scene extrapolation via diffusion inpainting,enabling the generation of unbounded simulation environments. Extensiveexperiments show that Scenario Dreamer outperforms existing generativesimulators in realism and efficiency: the vectorized scene-generation basemodel achieves superior generation quality with around 2x fewer parameters, 6xlower generation latency, and 10x fewer GPU training hours compared to thestrongest baseline. We confirm its practical utility by showing thatreinforcement learning planning agents are more challenged in Scenario Dreamerenvironments than traditional non-generative simulation environments,especially on long and adversarial driving environments.
  </details>

- **[On-site estimation of battery electrochemical parameters via transfer learning based physics-informed neural network approach](http://arxiv.org/abs/2503.22396v1)**  `arXiv:2503.22396`  `cs.AI` `cs.LG`  
  _Josu Yeregui, Iker Lopetegi, Sergio Fernandez, Erik Garayalde, Unai Iraola_
  <details open><summary>Abstract</summary>
  This paper presents a novel physical parameter estimation framework foron-site model characterization, using a two-phase modelling strategy withPhysics-Informed Neural Networks (PINNs) and transfer learning (TL). In thefirst phase, a PINN is trained using only the physical principles of the singleparticle model (SPM) equations. In the second phase, the majority of the PINNparameters are frozen, while critical electrochemical parameters are set astrainable and adjusted using real-world voltage profile data. The proposedapproach significantly reduces computational costs, making it suitable forreal-time implementation on Battery Management Systems (BMS). Additionally, asthe initial phase does not require field data, the model is easy to deploy withminimal setup requirements. With the proposed methodology, we have been able toeffectively estimate relevant electrochemical parameters with operating data.This has been proved estimating diffusivities and active material volumefractions with charge data in different degradation conditions. The methodologyis experimentally validated in a Raspberry Pi device using data from a standardcharge profile with a 3.89\% relative accuracy estimating the active materialvolume fractions of a NMC cell with 82.09\% of its nominal capacity.
  </details>

- **[Grasping a Handful: Sequential Multi-Object Dexterous Grasp Generation](http://arxiv.org/abs/2503.22370v1)**  `arXiv:2503.22370`  `cs.RO` `cs.LG`  
  _Haofei Lu, Yifei Dong, Zehang Weng, Jens Lundell, Danica Kragic_
  <details open><summary>Abstract</summary>
  We introduce the sequential multi-object robotic grasp sampling algorithmSeqGrasp that can robustly synthesize stable grasps on diverse objects usingthe robotic hand's partial Degrees of Freedom (DoF). We use SeqGrasp toconstruct the large-scale Allegro Hand sequential grasping dataset SeqDatasetand use it for training the diffusion-based sequential grasp generatorSeqDiffuser. We experimentally evaluate SeqGrasp and SeqDiffuser against thestate-of-the-art non-sequential multi-object grasp generation method MultiGraspin simulation and on a real robot. The experimental results demonstrate thatSeqGrasp and SeqDiffuser reach an 8.71%-43.33% higher grasp success rate thanMultiGrasp. Furthermore, SeqDiffuser is approximately 1000 times faster atgenerating grasps than SeqGrasp and MultiGrasp.
  </details>

- **[Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization](http://arxiv.org/abs/2503.22352v1)**  `arXiv:2503.22352`  `cs.CV`  
  _Barƒ±≈ü Batuhan Topal, Umut √ñzyurt, Zafer Doƒüan Budak, Ramazan Gokberk Cinbis_
  <details open><summary>Abstract</summary>
  Recent advancements in text-to-image generative models, particularly latentdiffusion models (LDMs), have demonstrated remarkable capabilities insynthesizing high-quality images from textual prompts. However, achievingidentity personalization-ensuring that a model consistently generatessubject-specific outputs from limited reference images-remains a fundamentalchallenge. To address this, we introduce Meta-Low-Rank Adaptation (Meta-LoRA),a novel framework that leverages meta-learning to encode domain-specific priorsinto LoRA-based identity personalization. Our method introduces a structuredthree-layer LoRA architecture that separates identity-agnostic knowledge fromidentity-specific adaptation. In the first stage, the LoRA Meta-Down layers aremeta-trained across multiple subjects, learning a shared manifold that capturesgeneral identity-related features. In the second stage, only the LoRA-Mid andLoRA-Up layers are optimized to specialize on a given subject, significantlyreducing adaptation time while improving identity fidelity. To evaluate ourapproach, we introduce Meta-PHD, a new benchmark dataset for identitypersonalization, and compare Meta-LoRA against state-of-the-art methods. Ourresults demonstrate that Meta-LoRA achieves superior identity retention,computational efficiency, and adaptability across diverse identity conditions.The code, model weights, and dataset will be released publicly upon acceptance.
  </details>

- **[GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion](http://arxiv.org/abs/2503.22349v1)**  `arXiv:2503.22349`  `cs.CV`  
  _Li-Heng Chen, Zi-Xin Zou, Chang Liu, Tianjiao Jing, Yan-Pei Cao, Shi-Sheng Huang, et al._
  <details open><summary>Abstract</summary>
  Accurate surface reconstruction from unposed images is crucial for efficient3D object or scene creation. However, it remains challenging, particularly forthe joint camera pose estimation. Previous approaches have achieved impressivepose-free surface reconstruction results in dense-view settings, but couldeasily fail for sparse-view scenarios without sufficient visual overlap. Inthis paper, we propose a new technique for pose-free surface reconstruction,which follows triplane-based signed distance field (SDF) learning butregularizes the learning by explicit points sampled from ray-based diffusion ofcamera pose estimation. Our key contribution is a novel Geometric ConsistentRay Diffusion model (GCRayDiffusion), where we represent camera poses as neuralbundle rays and regress the distribution of noisy rays via a diffusion model.More importantly, we further condition the denoising process of RGRayDiffusionusing the triplane-based SDF of the entire scene, which provides effective 3Dconsistent regularization to achieve multi-view consistent camera poseestimation. Finally, we incorporate RGRayDiffusion into the triplane-based SDFlearning by introducing on-surface geometric regularization from the samplingpoints of the neural bundle rays, which leads to highly accurate pose-freesurface reconstruction results even for sparse-view inputs. Extensiveevaluations on public datasets show that our GCRayDiffusion achieves moreaccurate camera pose estimation than previous approaches, with geometricallymore consistent surface reconstruction results, especially given sparse-viewinputs.
  </details>

- **[Semantix: An Energy Guided Sampler for Semantic Style Transfer](http://arxiv.org/abs/2503.22344v1)**  `arXiv:2503.22344`  `cs.CV`  
  _Huiang He, Minghui Hu, Chuanxia Zheng, Chaoyue Wang, Tat-Jen Cham_
  <details open><summary>Abstract</summary>
  Recent advances in style and appearance transfer are impressive, but mostmethods isolate global style and local appearance transfer, neglecting semanticcorrespondence. Additionally, image and video tasks are typically handled inisolation, with little focus on integrating them for video transfer. To addressthese limitations, we introduce a novel task, Semantic Style Transfer, whichinvolves transferring style and appearance features from a reference image to atarget visual content based on semantic correspondence. We subsequently proposea training-free method, Semantix an energy-guided sampler designed for SemanticStyle Transfer that simultaneously guides both style and appearance transferbased on semantic understanding capacity of pre-trained diffusion models.Additionally, as a sampler, Semantix be seamlessly applied to both image andvideo models, enabling semantic style transfer to be generic across variousvisual media. Specifically, once inverting both reference and context images orvideos to noise space by SDEs, Semantix utilizes a meticulously crafted energyfunction to guide the sampling process, including three key components: StyleFeature Guidance, Spatial Feature Guidance and Semantic Distance as aregularisation term. Experimental results demonstrate that Semantix not onlyeffectively accomplishes the task of semantic style transfer across images andvideos, but also surpasses existing state-of-the-art solutions in both fields.The project website is available at https://huiang-he.github.io/semantix/
  </details>

- **[Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion](http://arxiv.org/abs/2503.22262v1)**  `arXiv:2503.22262`  `cs.CV`  
  _Songsong Yu, Yuxin Chen, Zhongang Qi, Zeke Xie, Yifan Wang, Lijun Wang, et al._
  <details open><summary>Abstract</summary>
  With the rapid proliferation of 3D devices and the shortage of 3D content,stereo conversion is attracting increasing attention. Recent works introducepretrained Diffusion Models (DMs) into this task. However, due to the scarcityof large-scale training data and comprehensive benchmarks, the optimalmethodologies for employing DMs in stereo conversion and the accurateevaluation of stereo effects remain largely unexplored. In this work, weintroduce the Mono2Stereo dataset, providing high-quality training data andbenchmark to support in-depth exploration of stereo conversion. With thisdataset, we conduct an empirical study that yields two primary findings. 1) Thedifferences between the left and right views are subtle, yet existing metricsconsider overall pixels, failing to concentrate on regions critical to stereoeffects. 2) Mainstream methods adopt either one-stage left-to-right generationor warp-and-inpaint pipeline, facing challenges of degraded stereo effect andimage distortion respectively. Based on these findings, we introduce a newevaluation metric, Stereo Intersection-over-Union, which prioritizes disparityand achieves a high correlation with human judgments on stereo effect.Moreover, we propose a strong baseline model, harmonizing the stereo effect andimage quality simultaneously, and notably surpassing current mainstreammethods. Our code and data will be open-sourced to promote further research instereo conversion. Our models are available at mono2stereo-bench.github.io.
  </details>

- **[Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance](http://arxiv.org/abs/2503.22225v1)**  `arXiv:2503.22225`  `cs.CV`  
  _Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang_
  <details open><summary>Abstract</summary>
  Pre-trained conditional diffusion models have demonstrated remarkablepotential in image editing. However, they often face challenges with temporalconsistency, particularly in the talking head domain, where continuous changesin facial expressions intensify the level of difficulty. These issues stem fromthe independent editing of individual images and the inherent loss of temporalcontinuity during the editing process. In this paper, we introduce Follow YourMotion (FYM), a generic framework for maintaining temporal consistency inportrait editing. Specifically, given portrait images rendered by a pre-trained3D Gaussian Splatting model, we first develop a diffusion model thatintuitively and inherently learns motion trajectory changes at different scalesand pixel coordinates, from the first frame to each subsequent frame. Thisapproach ensures that temporally inconsistent edited avatars inherit the motioninformation from the rendered avatars. Secondly, to maintain fine-grainedexpression temporal consistency in talking head editing, we propose a dynamicre-weighted attention mechanism. This mechanism assigns higher weightcoefficients to landmark points in space and dynamically updates these weightsbased on landmark loss, achieving more consistent and refined facialexpressions. Extensive experiments demonstrate that our method outperformsexisting approaches in terms of temporal consistency and can be used tooptimize and compensate for temporally inconsistent outputs in a range ofapplications, such as text-driven editing, relighting, and various otherapplications.
  </details>

- **[High-Fidelity Diffusion Face Swapping with ID-Constrained Facial Conditioning](http://arxiv.org/abs/2503.22179v1)**  `arXiv:2503.22179`  `cs.CV`  
  _Dailan He, Xiahong Wang, Shulun Wang, Guanglu Song, Bingqi Ma, Hao Shao, et al._
  <details open><summary>Abstract</summary>
  Face swapping aims to seamlessly transfer a source facial identity onto atarget while preserving target attributes such as pose and expression.Diffusion models, known for their superior generative capabilities, haverecently shown promise in advancing face-swapping quality. This paper addressestwo key challenges in diffusion-based face swapping: the prioritizedpreservation of identity over target attributes and the inherent conflictbetween identity and attribute conditioning. To tackle these issues, weintroduce an identity-constrained attribute-tuning framework for face swappingthat first ensures identity preservation and then fine-tunes for attributealignment, achieved through a decoupled condition injection. We further enhancefidelity by incorporating identity and adversarial losses in a post-trainingrefinement stage. Our proposed identity-constrained diffusion-basedface-swapping model outperforms existing methods in both qualitative andquantitative evaluations, demonstrating superior identity similarity andattribute consistency, achieving a new state-of-the-art performance inhigh-fidelity face swapping.
  </details>

- **[Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis](http://arxiv.org/abs/2503.22168v1)**  `arXiv:2503.22168`  `cs.CV`  
  _Woojung Han, Yeonkyung Lee, Chanyoung Kim, Kwanghyun Park, Seong Jae Hwang_
  <details open><summary>Abstract</summary>
  Diffusion-based text-to-image (T2I) models have recently excelled inhigh-quality image generation, particularly in a training-free manner, enablingcost-effective adaptability and generalization across diverse tasks. However,while the existing methods have been continuously focusing on severalchallenges, such as "missing objects" and "mismatched attributes," anothercritical issue of "mislocated objects" remains where generated spatialpositions fail to align with text prompts. Surprisingly, ensuring suchseemingly basic functionality remains challenging in popular T2I models due tothe inherent difficulty of imposing explicit spatial guidance via text forms.To address this, we propose STORM (Spatial Transport Optimization byRepositioning Attention Map), a novel training-free approach for spatiallycoherent T2I synthesis. STORM employs Spatial Transport Optimization (STO),rooted in optimal transport theory, to dynamically adjust object attention mapsfor precise spatial adherence, supported by a Spatial Transport (ST) Costfunction that enhances spatial understanding. Our analysis shows thatintegrating spatial awareness is most effective in the early denoising stages,while later phases refine details. Extensive experiments demonstrate that STORMsurpasses existing methods, effectively mitigating mislocated objects whileimproving missing and mismatched attributes, setting a new benchmark forspatial alignment in T2I synthesis.
  </details>

- **[LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing](http://arxiv.org/abs/2503.21541v2)**  `arXiv:2503.21541`  `cs.CV` `cs.AI`  
  _Achint Soni, Meet Soni, Sirisha Rambhatla_
  <details open><summary>Abstract</summary>
  Text-guided image editing aims to modify specific regions of an imageaccording to natural language instructions while maintaining the generalstructure and the background fidelity. Existing methods utilize masks derivedfrom cross-attention maps generated from diffusion models to identify thetarget regions for modification. However, since cross-attention mechanismsfocus on semantic relevance, they struggle to maintain the image integrity. Asa result, these methods often lack spatial consistency, leading to editingartifacts and distortions. In this work, we address these limitations andintroduce LOCATEdit, which enhances cross-attention maps through a graph-basedapproach utilizing self-attention-derived patch relationships to maintainsmooth, coherent attention across image regions, ensuring that alterations arelimited to the designated items while retaining the surrounding structure.LOCATEdit consistently and substantially outperforms existing baselines onPIE-Bench, demonstrating its state-of-the-art performance and effectiveness onvarious editing tasks. Code can be found onhttps://github.com/LOCATEdit/LOCATEdit/
  </details>

- **[Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2503.18352v2)**  `arXiv:2503.18352`  `cs.CV`  
  _Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, Di Huang_
  <details open><summary>Abstract</summary>
  In this paper, we present Diffusion-4K, a novel framework for directultra-high-resolution image synthesis using text-to-image diffusion models. Thecore advancements include: (1) Aesthetic-4K Benchmark: addressing the absenceof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,a comprehensive benchmark for ultra-high-resolution image generation. Wecurated a high-quality 4K dataset with carefully selected images and captionsgenerated by GPT-4o. Additionally, we introduce GLCM Score and CompressionRatio metrics to evaluate fine details, combined with holistic measures such asFID, Aesthetics and CLIPScore for a comprehensive assessment ofultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose awavelet-based fine-tuning approach for direct training with photorealistic 4Kimages, applicable to various latent diffusion models, demonstrating itseffectiveness in synthesizing highly detailed 4K images. Consequently,Diffusion-4K achieves impressive performance in high-quality image synthesisand text prompt adherence, especially when powered by modern large-scalediffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental resultsfrom our benchmark demonstrate the superiority of Diffusion-4K inultra-high-resolution image synthesis.
  </details>

- **[UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models](http://arxiv.org/abs/2503.17221v2)**  `arXiv:2503.17221`  `cs.CV`  
  _Fanghua Yu, Jinjin Gu, Jinfan Hu, Zheyuan Li, Chao Dong_
  <details open><summary>Abstract</summary>
  We introduce UniCon, a novel architecture designed to enhance control andefficiency in training adapters for large-scale diffusion models. Unlikeexisting methods that rely on bidirectional interaction between the diffusionmodel and control adapter, UniCon implements a unidirectional flow from thediffusion network to the adapter, allowing the adapter alone to generate thefinal output. UniCon reduces computational demands by eliminating the need forthe diffusion model to compute and store gradients during adapter training. Ourresults indicate that UniCon reduces GPU memory usage by one-third andincreases training speed by 2.3 times, while maintaining the same adapterparameter size. Additionally, without requiring extra computational resources,UniCon enables the training of adapters with double the parameter volume ofexisting ControlNets. In a series of image conditional generation tasks, UniConhas demonstrated precise responsiveness to control inputs and exceptionalgeneration capabilities.
  </details>

- **[Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding](http://arxiv.org/abs/2503.16707v2)**  `arXiv:2503.16707`  `cs.CV`  
  _Jinlong Li, Cristiano Saltori, Fabio Poiesi, Nicu Sebe_
  <details open><summary>Abstract</summary>
  The lack of a large-scale 3D-text corpus has led recent works to distillopen-vocabulary knowledge from vision-language models (VLMs). However, thesemethods typically rely on a single VLM to align the feature spaces of 3D modelswithin a common language space, which limits the potential of 3D models toleverage the diverse spatial and semantic capabilities encapsulated in variousfoundation models. In this paper, we propose Cross-modal and Uncertainty-awareAgglomeration for Open-vocabulary 3D Scene Understanding dubbed CUA-O3D, thefirst model to integrate multiple foundation models-such as CLIP, DINOv2, andStable Diffusion-into 3D scene understanding. We further introduce adeterministic uncertainty estimation to adaptively distill and harmonize theheterogeneous 2D feature embeddings from these models. Our method addresses twokey challenges: (1) incorporating semantic priors from VLMs alongside thegeometric knowledge of spatially-aware vision foundation models, and (2) usinga novel deterministic uncertainty estimation to capture model-specificuncertainties across diverse semantic and geometric sensitivities, helping toreconcile heterogeneous representations during training. Extensive experimentson ScanNetV2 and Matterport3D demonstrate that our method not only advancesopen-vocabulary segmentation but also achieves robust cross-domain alignmentand competitive spatial perception capabilities. The code will be available at:https://github.com/TyroneLi/CUA_O3D.
  </details>

- **[Patch-Depth Fusion: Dichotomous Image Segmentation via Fine-Grained Patch Strategy and Depth Integrity-Prior](http://arxiv.org/abs/2503.06100v3)**  `arXiv:2503.06100`  `cs.CV`  
  _Xianjie Liu, Keren Fu, Qijun Zhao_
  <details open><summary>Abstract</summary>
  Dichotomous Image Segmentation (DIS) is a high-precision object segmentationtask for high-resolution natural images. The current mainstream methods focuson the optimization of local details but overlook the fundamental challenge ofmodeling the integrity of objects. We have found that the depth integrity-priorimplicit in the the pseudo-depth maps generated by Depth Anything Model v2 andthe local detail features of image patches can jointly address the abovedilemmas. Based on the above findings, we have designed a novel Patch-DepthFusion Network (PDFNet) for high-precision dichotomous image segmentation. Thecore of PDFNet consists of three aspects. Firstly, the object perception isenhanced through multi-modal input fusion. By utilizing the patch fine-grainedstrategy, coupled with patch selection and enhancement, the sensitivity todetails is improved. Secondly, by leveraging the depth integrity-priordistributed in the depth maps, we propose an integrity-prior loss to enhancethe uniformity of the segmentation results in the depth maps. Finally, weutilize the features of the shared encoder and, through a simple depthrefinement decoder, improve the ability of the shared encoder to capture subtledepth-related information in the images. Experiments on the DIS-5K dataset showthat PDFNet significantly outperforms state-of-the-art non-diffusion methods.Due to the incorporation of the depth integrity-prior, PDFNet achieves or evensurpassing the performance of the latest diffusion-based methods while usingless than 11% of the parameters of diffusion-based methods. The source code athttps://github.com/Tennine2077/PDFNet
  </details>

- **[DoubleDiffusion: Combining Heat Diffusion with Denoising Diffusion for Generative Learning on 3D Meshes](http://arxiv.org/abs/2501.03397v4)**  `arXiv:2501.03397`  `cs.CV`  
  _Xuyang Wang, Ziang Cheng, Zhenyu Li, Jiayu Yang, Haorui Ji, Pan Ji, et al._
  <details open><summary>Abstract</summary>
  This paper proposes DoubleDiffusion, a novel framework that combines heatdissipation diffusion and denoising diffusion for direct generative learning on3D mesh surfaces. Our approach addresses the challenges of generatingcontinuous signal distributions residing on a curve manifold surface. Unlikeprevious methods that rely on unrolling 3D meshes into 2D or adopting fieldrepresentations, DoubleDiffusion leverages the Laplacian-Beltrami operator toprocess features respecting the mesh structure. This combination enableseffective geometry-aware signal diffusion across the underlying geometry. Asshown in Fig.1, we demonstrate that DoubleDiffusion has the ability to generateRGB signal distributions on complex 3D mesh surfaces and achieves per-categoryshape-conditioned texture generation across different shape geometry. Our workcontributes a new direction in diffusion-based generative modeling on 3Dsurfaces, with potential applications in the field of 3D asset generation.
  </details>

- **[LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis](http://arxiv.org/abs/2412.15214v2)**  `arXiv:2412.15214`  `cs.CV`  
  _Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, et al._
  <details open><summary>Abstract</summary>
  The intuitive nature of drag-based interaction has led to its growingadoption for controlling object trajectories in image-to-video synthesis.Still, existing methods that perform dragging in the 2D space usually faceambiguity when handling out-of-plane movements. In this work, we augment theinteraction with a new dimension, i.e., the depth dimension, such that usersare allowed to assign a relative depth for each point on the trajectory. Thatway, our new interaction paradigm not only inherits the convenience from 2Ddragging, but facilitates trajectory control in the 3D space, broadening thescope of creativity. We propose a pioneering method for 3D trajectory controlin image-to-video synthesis by abstracting object masks into a few clusterpoints. These points, accompanied by the depth information and the instanceinformation, are finally fed into a video diffusion model as the controlsignal. Extensive experiments validate the effectiveness of our approach,dubbed LeviTor, in precisely manipulating the object movements when producingphoto-realistic videos from static images. Our code is available at:https://github.com/ant-research/LeviTor.
  </details>

- **[Can video generation replace cinematographers? Research on the cinematic language of generated video](http://arxiv.org/abs/2412.12223v2)**  `arXiv:2412.12223`  `cs.CV` `cs.AI`  
  _Xiaozhe Li, Kai WU, Siyi Yang, YiZhan Qu, Guohua. Zhang, Zhiyu Chen, et al._
  <details open><summary>Abstract</summary>
  Recent advancements in text-to-video (T2V) generation have leverageddiffusion models to enhance visual coherence in videos synthesized from textualdescriptions. However, existing research primarily focuses on object motion,often overlooking cinematic language, which is crucial for conveying emotionand narrative pacing in cinematography. To address this, we propose a threefoldapproach to improve cinematic control in T2V models. First, we introduce ameticulously annotated cinematic language dataset with twenty subcategories,covering shot framing, shot angles, and camera movements, enabling models tolearn diverse cinematic styles. Second, we present CameraDiff, which employsLoRA for precise and stable cinematic control, ensuring flexible shotgeneration. Third, we propose CameraCLIP, designed to evaluate cinematicalignment and guide multi-shot composition. Building on CameraCLIP, weintroduce CLIPLoRA, a CLIP-guided dynamic LoRA composition method thatadaptively fuses multiple pre-trained cinematic LoRAs, enabling smoothtransitions and seamless style blending. Experimental results demonstrate thatCameraDiff ensures stable and precise cinematic control, CameraCLIP achieves anR@1 score of 0.83, and CLIPLoRA significantly enhances multi-shot compositionwithin a single video, bridging the gap between automated video generation andprofessional cinematography.\textsuperscript{1}
  </details>

- **[LVMark: Robust Watermark for Latent Video Diffusion Models](http://arxiv.org/abs/2412.09122v3)**  `arXiv:2412.09122`  `cs.CV`  
  _MinHyuk Jang, Youngdong Jang, JaeHyeok Lee, Feng Yang, Gyeongrok Oh, Jongheon Jeong, et al._
  <details open><summary>Abstract</summary>
  Rapid advancements in video diffusion models have enabled the creation ofrealistic videos, raising concerns about unauthorized use and driving thedemand for techniques to protect model ownership. Existing watermarkingmethods, while effective for image diffusion models, do not account fortemporal consistency, leading to degraded video quality and reduced robustnessagainst video distortions. To address this issue, we introduce LVMark, a novelwatermarking method for video diffusion models. We propose a new watermarkdecoder tailored for generated videos by learning the consistency betweenadjacent frames. It ensures accurate message decoding, even under maliciousattacks, by combining the low-frequency components of the 3D wavelet domainwith the RGB features of the video. Additionally, our approach minimizes videoquality degradation by embedding watermark messages in layers with minimalimpact on visual appearance using an importance-based weight modulationstrategy. We optimize both the watermark decoder and the latent decoder ofdiffusion model, effectively balancing the trade-off between visual quality andbit accuracy. Our experiments show that our method embeds invisible watermarksinto video diffusion models, ensuring robust decoding accuracy with 512-bitcapacity, even under video distortions.
  </details>

- **[Towards Stabilized and Efficient Diffusion Transformers through Long-Skip-Connections with Spectral Constraints](http://arxiv.org/abs/2411.17616v3)**  `arXiv:2411.17616`  `cs.CV`  
  _Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Xiaoye Qu, Tianlong Chen, Yu Cheng_
  <details open><summary>Abstract</summary>
  Diffusion Transformers (DiT) have emerged as a powerful architecture forimage and video generation, offering superior quality and scalability. However,their practical application suffers from inherent dynamic feature instability,leading to error amplification during cached inference. Through systematicanalysis, we identify the absence of long-range feature preservation mechanismsas the root cause of unstable feature propagation and perturbation sensitivity.To this end, we propose Skip-DiT, a novel DiT variant enhanced withLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.Theoretical spectral norm and visualization analysis demonstrate how LSCsstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamicfeature enable an efficient statical caching mechanism that reuses deepfeatures across timesteps while updating shallow components. Extensiveexperiments across image and video generation tasks demonstrate that Skip-DiTachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2times inference acceleration without quality loss and high fidelity to originaloutput, outperforming existing DiT caching methods across various quantitativemetrics. Our findings establish long-skip connections as critical architecturalcomponents for training stable and efficient diffusion transformers.
  </details>

- **[JoyType: A Robust Design for Multilingual Visual Text Creation](http://arxiv.org/abs/2409.17524v2)**  `arXiv:2409.17524`  `cs.CV`  
  _Chao Li, Chen Jiang, Xiaolong Liu, Jun Zhao, Guoxin Wang_
  <details open><summary>Abstract</summary>
  Generating images with accurately represented text, especially in non-Latinlanguages, poses a significant challenge for diffusion models. Existingapproaches, such as the integration of hint condition diagrams via auxiliarynetworks (e.g., ControlNet), have made strides towards addressing this issue.However, diffusion models often fall short in tasks requiring controlled textgeneration, such as specifying particular fonts or producing text in smallfonts. In this paper, we introduce a novel approach for multilingual visualtext creation, named JoyType, designed to maintain the font style of textduring the image generation process. Our methodology begins with assembling atraining dataset, JoyType-1M, comprising 1 million pairs of data. Each pairincludes an image, its description, and glyph instructions corresponding to thefont style within the image. We then developed a text control network, FontControlNet, tasked with extracting font style information to steer the imagegeneration. To further enhance our model's ability to maintain font style,notably in generating small-font text, we incorporated a multi-layer OCR-awareloss into the diffusion process. This enhancement allows JoyType to direct textrendering using low-level descriptors. Our evaluations, based on both visualand accuracy metrics, demonstrate that JoyType significantly outperformsexisting state-of-the-art methods. Additionally, JoyType can function as aplugin, facilitating the creation of varied image styles in conjunction withother stable diffusion models on HuggingFace and CivitAI. Our project isopen-sourced on https://jdh-algo.github.io/JoyType/.
  </details>

- **[Dynamics-Guided Diffusion Model for Sensor-less Robot Manipulator Design](http://arxiv.org/abs/2402.15038v2)**  `arXiv:2402.15038`  `cs.RO` `cs.AI` `cs.LG`  
  _Xiaomeng Xu, Huy Ha, Shuran Song_
  <details open><summary>Abstract</summary>
  We present Dynamics-Guided Diffusion Model (DGDM), a data-driven frameworkfor generating task-specific manipulator designs without task-specifictraining. Given object shapes and task specifications, DGDM generatessensor-less manipulator designs that can blindly manipulate objects towardsdesired motions and poses using an open-loop parallel motion. This framework 1)flexibly represents manipulation tasks as interaction profiles, 2) representsthe design space using a geometric diffusion model, and 3) efficiently searchesthis design space using the gradients provided by a dynamics network trainedwithout any task information. We evaluate DGDM on various manipulation tasksranging from shifting/rotating objects to converging objects to a specificpose. Our generated designs outperform optimization-based and unguideddiffusion baselines relatively by 31.5% and 45.3% on average success rate. Withthe ability to generate a new design within 0.8s, DGDM facilitates rapid designiteration and enhances the adoption of data-driven approaches for robotmechanism design. Qualitative results are best viewed on our project websitehttps://dgdm-robot.github.io/.
  </details>
