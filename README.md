# üöÄ Daily AI arXiv Digest

[![Total Papers](https://img.shields.io/badge/paper_today-297+-red)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)](https://github.com/Tavish9/awesome-daily-AI-arxiv/commits/main/)
[![arXiv API](https://img.shields.io/badge/powered_by-arXiv_API-009688)](https://arxiv.org/help/api)
[![License](https://img.shields.io/badge/license-CC_BY--SA_4.0-3989c9)](LICENSE)


üìå ‚Äã**Tracking Breakthroughs in**: `AI` ‚Ä¢ `NLP` ‚Ä¢ `CV` ‚Ä¢ `ML` ‚Ä¢ `Robotics`  
‚è∞ ‚Äã**Update Schedule**: [UTC 02:00](https://time.is/UTC) | [GMT+8 10:00](https://time.is/China)

## üåü Today's Highlights

- üî• Hot Topic
  - [LLM](hot_topic/LLM.md)
  - [Benchmark](hot_topic/Benchmark.md)
  - [Reasoning](hot_topic/Reasoning.md)
  - [Diffusion](hot_topic/Diffusion.md)
  - [3D_Reconstruction](hot_topic/3D_Reconstruction.md)
  - [MLLM](hot_topic/MLLM.md)
  - [3D_Generation](hot_topic/3D_Generation.md)
  - [Embodied_AI](hot_topic/Embodied_AI.md)
- üí´ Active Platform
  - [Huggingface](https://huggingface.co/papers)
  - [LlamaFactory](https://www.llamafactory.cn/daily-paper/)
  - [X (Twitter)](https://x.com/arxiv_daily)
  - [Paper Reading](https://paperreading.club/)
  - [Paper Digest](https://www.paperdigest.org/arxiv/)
  

## üìå Full Archive

| Category                                                                                | Count |
| --------------------------------------------------------------------------------------- | ----- |
| [Artificial Intelligence üß†](#artificial-intelligence-) | 17    |
| [Computation and Language üí¨](#computation-and-language-) | 46    |
| [Computer Vision and Pattern Recognition üì∏](#computer-vision-and-pattern-recognition-) | 132   |
| [Machine Learning üìä](#machine-learning-) | 74    |
| [Multiagent Systems üåê](#multiagent-systems-) | 1     |
| [Robotics ü§ñ](#robotics-) | 27    |

### Artificial Intelligence üß†

<details open><summary>Click to Collapse</summary>

- **[QuestBench: Can LLMs ask the right question to acquire information in reasoning tasks?](http://arxiv.org/abs/2503.22674v1)**  `arXiv:2503.22674`  
  _Belinda Z. Li, Been Kim, Zi Wang_
  <details><summary>Abstract</summary>
  Recently, a large amount of work has focused on improving large languagemodels' (LLMs') performance on reasoning benchmarks such as math and logic.However, past work has largely assumed that tasks are well-defined. In the realworld, queries to LLMs are often underspecified, only solvable throughacquiring missing information. We formalize this as a constraint satisfactionproblem (CSP) with missing variable assignments. Using a special case of thisformalism where only one necessary variable assignment is missing, we canrigorously evaluate an LLM's ability to identify the minimal necessary questionto ask and quantify axes of difficulty levels for each problem. We presentQuestBench, a set of underspecified reasoning tasks solvable by asking at mostone question, which includes: (1) Logic-Q: Logical reasoning tasks with onemissing proposition, (2) Planning-Q: PDDL planning problems with initial statesthat are partially-observed, (3) GSM-Q: Human-annotated grade school mathproblems with one missing variable assignment, and (4) GSME-Q: a version ofGSM-Q where word problems are translated into equations by human annotators.The LLM is tasked with selecting the correct clarification question(s) from alist of options. While state-of-the-art models excel at GSM-Q and GSME-Q, theiraccuracy is only 40-50% on Logic-Q and Planning-Q. Analysis demonstrates thatthe ability to solve well-specified reasoning problems may not be sufficientfor success on our benchmark: models have difficulty identifying the rightquestion to ask, even when they can solve the fully specified version of theproblem. Furthermore, in the Planning-Q domain, LLMs tend not to hedge, evenwhen explicitly presented with the option to predict ``not sure.'' Thishighlights the need for deeper investigation into models' informationacquisition capabilities.
  </details>

- **[ActionStudio: A Lightweight Framework for Data and Training of Action Models](http://arxiv.org/abs/2503.22673v1)**  `arXiv:2503.22673`  
  _Jianguo Zhang, Thai Hoang, Ming Zhu, Zuxin Liu, Shiyu Wang, Tulika Awalgaonkar, et al._
  <details><summary>Abstract</summary>
  Action models are essential for enabling autonomous agents to perform complextasks. However, training large action models remains challenging due to thediversity of agent environments and the complexity of agentic data. Despitegrowing interest, existing infrastructure provides limited support forscalable, agent-specific fine-tuning. We present ActionStudio, a lightweightand extensible data and training framework designed for action models.ActionStudio unifies heterogeneous agent trajectories through a standardizedformat, supports diverse training paradigms including LoRA, full fine-tuning,and distributed setups, and integrates robust preprocessing and verificationtools. We validate its effectiveness across both public and realistic industrybenchmarks, demonstrating strong performance and practical scalability. Weopen-sourced code and data at https://github.com/SalesforceAIResearch/xLAM tofacilitate research in the community.
  </details>

- **[Unicorn: Text-Only Data Synthesis for Vision Language Model Training](http://arxiv.org/abs/2503.22655v1)**  `arXiv:2503.22655`  
  _Xiaomin Yu, Pengxiang Ding, Wenjie Zhang, Siteng Huang, Songyang Gao, Chengwei Qin, et al._
  <details><summary>Abstract</summary>
  Training vision-language models (VLMs) typically requires large-scale,high-quality image-text pairs, but collecting or synthesizing such data iscostly. In contrast, text data is abundant and inexpensive, prompting thequestion: can high-quality multimodal training data be synthesized purely fromtext? To tackle this, we propose a cross-integrated three-stage multimodal datasynthesis framework, which generates two datasets: Unicorn-1.2M andUnicorn-471K-Instruction. In Stage 1: Diverse Caption Data Synthesis, weconstruct 1.2M semantically diverse high-quality captions by expanding sparsecaption seeds using large language models (LLMs). In Stage 2:Instruction-Tuning Data Generation, we further process 471K captions intomulti-turn instruction-tuning tasks to support complex reasoning. Finally, inStage 3: Modality Representation Transfer, these textual captionsrepresentations are transformed into visual representations, resulting indiverse synthetic image representations. This three-stage process enables us toconstruct Unicorn-1.2M for pretraining and Unicorn-471K-Instruction forinstruction-tuning, without relying on real images. By eliminating thedependency on real images while maintaining data quality and diversity, ourframework offers a cost-effective and scalable solution for VLMs training. Codeis available at https://github.com/Yu-xm/Unicorn.git.
  </details>

- **[CPPO: Accelerating the Training of Group Relative Policy Optimization-Based Reasoning Models](http://arxiv.org/abs/2503.22342v1)**  `arXiv:2503.22342`  
  _Zhihang Lin, Mingbao Lin, Yuan Xie, Rongrong Ji_
  <details><summary>Abstract</summary>
  This paper introduces Completion Pruning Policy Optimization (CPPO) toaccelerate the training of reasoning models based on Group Relative PolicyOptimization (GRPO). GRPO, while effective, incurs high training costs due tothe need for sampling multiple completions for each question. Our experimentand theoretical analysis reveals that the number of completions impacts modelaccuracy yet increases training time multiplicatively, and not all completionscontribute equally to policy training -- their contribution depends on theirrelative advantage. To address these issues, we propose CPPO, which prunescompletions with low absolute advantages, significantly reducing the numberneeded for gradient calculation and updates. Additionally, we introduce adynamic completion allocation strategy to maximize GPU utilization byincorporating additional questions, further enhancing training efficiency.Experimental results demonstrate that CPPO achieves up to $8.32\times$ speedupon GSM8K and $3.51\times$ on Math while preserving or even enhancing theaccuracy compared to the original GRPO. We release our code athttps://github.com/lzhxmu/CPPO.
  </details>

- **[Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs](http://arxiv.org/abs/2503.22241v1)**  `arXiv:2503.22241`  
  _Ziye Chen, Yiqun Duan, Riheng Zhu, Zhenbang Sun, Mingming Gong_
  <details><summary>Abstract</summary>
  Personalized multiple clustering aims to generate diverse partitions of adataset based on different user-specific aspects, rather than a singleclustering. It has recently drawn research interest for accommodating varyinguser preferences. Recent approaches primarily use CLIP embeddings with proxylearning to extract representations biased toward user clustering preferences.However, CLIP primarily focuses on coarse image-text alignment, lacking a deepcontextual understanding of user interests. To overcome these limitations, wepropose an agent-centric personalized clustering framework that leveragesmulti-modal large language models (MLLMs) as agents to comprehensively traversea relational graph to search for clusters based on user interests. Due to theadvanced reasoning mechanism of MLLMs, the obtained clusters align more closelywith user-defined criteria than those obtained from CLIP-based representations.To reduce computational overhead, we shorten the agents' traversal path byconstructing a relational graph using user-interest-biased embeddings extractedby MLLMs. A large number of weakly connected edges can be filtered out based onembedding similarity, facilitating an efficient traversal search for agents.Experimental results show that the proposed method achieves NMI scores of0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively,largely improving the SOTA model by over 140%.
  </details>

- **[Sharpe Ratio-Guided Active Learning for Preference Optimization in RLHF](http://arxiv.org/abs/2503.22137v1)**  `arXiv:2503.22137`  
  _Syrine Belakaria, Joshua Kazdan, Charles Marx, Chris Cundy, Willie Neiswanger, Sanmi Koyejo, et al._
  <details><summary>Abstract</summary>
  Reinforcement learning from human feedback (RLHF) has become a cornerstone ofthe training and alignment pipeline for large language models (LLMs). Recentadvances, such as direct preference optimization (DPO), have simplified thepreference learning step. However, collecting preference data remains achallenging and costly process, often requiring expert annotation. This costcan be mitigated by carefully selecting the data points presented forannotation. In this work, we propose an active learning approach to efficientlyselect prompt and preference pairs using a risk assessment strategy based onthe Sharpe Ratio. To address the challenge of unknown preferences prior toannotation, our method evaluates the gradients of all potential preferenceannotations to assess their impact on model updates. These gradient-basedevaluations enable risk assessment of data points regardless of the annotationoutcome. By leveraging the DPO loss derivations, we derive a closed-formexpression for computing these Sharpe ratios on a per-tuple basis, ensuring ourapproach remains both tractable and computationally efficient. We alsointroduce two variants of our method, each making different assumptions aboutprior information. Experimental results demonstrate that our method outperformsthe baseline by up to 5% in win rates against the chosen completion withlimited human preference data across several language models and real-worlddatasets.
  </details>

- **[Multi-Task Semantic Communications via Large Models](http://arxiv.org/abs/2503.22064v1)**  `arXiv:2503.22064`  
  _Wanli Ni, Zhijin Qin, Haofeng Sun, Xiaoming Tao, Zhu Han_
  <details><summary>Abstract</summary>
  Artificial intelligence (AI) promises to revolutionize the design,optimization and management of next-generation communication systems. In thisarticle, we explore the integration of large AI models (LAMs) into semanticcommunications (SemCom) by leveraging their multi-modal data processing andgeneration capabilities. Although LAMs bring unprecedented abilities to extractsemantics from raw data, this integration entails multifaceted challengesincluding high resource demands, model complexity, and the need foradaptability across diverse modalities and tasks. To overcome these challenges,we propose a LAM-based multi-task SemCom (MTSC) architecture, which includes anadaptive model compression strategy and a federated split fine-tuning approachto facilitate the efficient deployment of LAM-based semantic models inresource-limited networks. Furthermore, a retrieval-augmented generation schemeis implemented to synthesize the most recent local and global knowledge basesto enhance the accuracy of semantic extraction and content generation, therebyimproving the inference performance. Finally, simulation results demonstratethe efficacy of the proposed LAM-based MTSC architecture, highlighting theperformance enhancements across various downstream tasks under varying channelconditions.
  </details>

- **[The Procedural Content Generation Benchmark: An Open-source Testbed for Generative Challenges in Games](http://arxiv.org/abs/2503.21474v2)**  `arXiv:2503.21474`  
  _Ahmed Khalifa, Roberto Gallotta, Matthew Barthet, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis_
  <details><summary>Abstract</summary>
  This paper introduces the Procedural Content Generation Benchmark forevaluating generative algorithms on different game content creation tasks. Thebenchmark comes with 12 game-related problems with multiple variants on eachproblem. Problems vary from creating levels of different kinds to creating rulesets for simple arcade games. Each problem has its own content representation,control parameters, and evaluation metrics for quality, diversity, andcontrollability. This benchmark is intended as a first step towards astandardized way of comparing generative algorithms. We use the benchmark toscore three baseline algorithms: a random generator, an evolution strategy, anda genetic algorithm. Results show that some problems are easier to solve thanothers, as well as the impact the chosen objective has on quality, diversity,and controllability of the generated artifacts.
  </details>

- **[Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In & Out Learning](http://arxiv.org/abs/2503.21419v2)**  `arXiv:2503.21419`  
  _Yupei Li, Manuel Milling, Bj√∂rn W. Schuller_
  <details><summary>Abstract</summary>
  Artificial Intelligence (AI) has achieved new levels of performance andspread in public usage with the rise of deep neural networks (DNNs). Initiallyinspired by human neurons and their connections, NNs have become the foundationof AI models for many advanced architectures. However, some of the mostintegral processes in the human brain, particularly neurogenesis andneuroplasticity in addition to the more spread neuroapoptosis have largely beenignored in DNN architecture design. Instead, contemporary AI developmentpredominantly focuses on constructing advanced frameworks, such as largelanguage models, which retain a static structure of neural connections duringtraining and inference. In this light, we explore how neurogenesis,neuroapoptosis, and neuroplasticity can inspire future AI advances.Specifically, we examine analogous activities in artificial NNs, introducingthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' andstructural pruning for neuroapoptosis. We additionally suggest neuroplasticitycombining the two for future large NNs in ``life-long learning'' settingsfollowing the biological inspiration. We conclude by advocating for greaterresearch efforts in this interdisciplinary domain and identifying promisingdirections for future exploration.
  </details>

- **[Auditing language models for hidden objectives](http://arxiv.org/abs/2503.10965v2)**  `arXiv:2503.10965`  
  _Samuel Marks, Johannes Treutlein, Trenton Bricken, Jack Lindsey, Jonathan Marcus, Siddharth Mishra-Sharma, et al._
  <details><summary>Abstract</summary>
  We study the feasibility of conducting alignment audits: investigations intowhether models have undesired objectives. As a testbed, we train a languagemodel with a hidden objective. Our training pipeline first teaches the modelabout exploitable errors in RLHF reward models (RMs), then trains the model toexploit some of these errors. We verify via out-of-distribution evaluationsthat the model generalizes to exhibit whatever behaviors it believes RMs ratehighly, including ones not reinforced during training. We leverage this modelto study alignment audits in two ways. First, we conduct a blind auditing gamewhere four teams, unaware of the model's hidden objective or training,investigate it for concerning behaviors and their causes. Three teamssuccessfully uncovered the model's hidden objective using techniques includinginterpretability with sparse autoencoders (SAEs), behavioral attacks, andtraining data analysis. Second, we conduct an unblinded follow-up study ofeight techniques for auditing the model, analyzing their strengths andlimitations. Overall, our work provides a concrete example of using alignmentaudits to discover a model's hidden objective and proposes a methodology forpracticing and validating progress in alignment auditing.
  </details>

- **[SAIF: A Comprehensive Framework for Evaluating the Risks of Generative AI in the Public Sector](http://arxiv.org/abs/2501.08814v2)**  `arXiv:2501.08814`  
  _Kyeongryul Lee, Heehyeon Kim, Joyce Jiyoung Whang_
  <details><summary>Abstract</summary>
  The rapid adoption of generative AI in the public sector, encompassingdiverse applications ranging from automated public assistance to welfareservices and immigration processes, highlights its transformative potentialwhile underscoring the pressing need for thorough risk assessments. Despite itsgrowing presence, evaluations of risks associated with AI-driven systems in thepublic sector remain insufficiently explored. Building upon an establishedtaxonomy of AI risks derived from diverse government policies and corporateguidelines, we investigate the critical risks posed by generative AI in thepublic sector while extending the scope to account for its multimodalcapabilities. In addition, we propose a Systematic dAta generatIon Frameworkfor evaluating the risks of generative AI (SAIF). SAIF involves four keystages: breaking down risks, designing scenarios, applying jailbreak methods,and exploring prompt types. It ensures the systematic and consistent generationof prompt data, facilitating a comprehensive evaluation while providing a solidfoundation for mitigating the risks. Furthermore, SAIF is designed toaccommodate emerging jailbreak methods and evolving prompt types, therebyenabling effective responses to unforeseen risk scenarios. We believe that thisstudy can play a crucial role in fostering the safe and responsible integrationof generative AI into the public sector.
  </details>

- **[Do LLMs estimate uncertainty well in instruction-following?](http://arxiv.org/abs/2410.14582v4)**  `arXiv:2410.14582`  
  _Juyeon Heo, Miao Xiong, Christina Heinze-Deml, Jaya Narain_
  <details><summary>Abstract</summary>
  Large language models (LLMs) could be valuable personal AI agents acrossvarious domains, provided they can precisely follow user instructions. However,recent studies have shown significant limitations in LLMs'instruction-following capabilities, raising concerns about their reliability inhigh-stakes applications. Accurately estimating LLMs' uncertainty in adheringto instructions is critical to mitigating deployment risks. We present, to ourknowledge, the first systematic evaluation of the uncertainty estimationabilities of LLMs in the context of instruction-following. Our study identifieskey challenges with existing instruction-following benchmarks, where multiplefactors are entangled with uncertainty stems from instruction-following,complicating the isolation and comparison across methods and models. To addressthese issues, we introduce a controlled evaluation setup with two benchmarkversions of data, enabling a comprehensive comparison of uncertainty estimationmethods under various conditions. Our findings show that existing uncertaintymethods struggle, particularly when models make subtle errors in instructionfollowing. While internal model states provide some improvement, they remaininadequate in more complex scenarios. The insights from our controlledevaluation setups provide a crucial understanding of LLMs' limitations andpotential for uncertainty estimation in instruction-following tasks, paving theway for more trustworthy AI agents.
  </details>

- **[Do LLMs "know" internally when they follow instructions?](http://arxiv.org/abs/2410.14516v5)**  `arXiv:2410.14516`  
  _Juyeon Heo, Christina Heinze-Deml, Oussama Elachqar, Kwan Ho Ryan Chan, Shirley Ren, Udhay Nallasamy, et al._
  <details><summary>Abstract</summary>
  Instruction-following is crucial for building AI agents with large languagemodels (LLMs), as these models must adhere strictly to user-providedconstraints and guidelines. However, LLMs often fail to follow even simple andclear instructions. To improve instruction-following behavior and preventundesirable outputs, a deeper understanding of how LLMs' internal states relateto these outcomes is required. In this work, we investigate whether LLMs encodeinformation in their representations that correlate with instruction-followingsuccess - a property we term knowing internally. Our analysis identifies adirection in the input embedding space, termed the instruction-followingdimension, that predicts whether a response will comply with a giveninstruction. We find that this dimension generalizes well across unseen tasksbut not across unseen instruction types. We demonstrate that modifyingrepresentations along this dimension improves instruction-following successrates compared to random changes, without compromising response quality.Further investigation reveals that this dimension is more closely related tothe phrasing of prompts rather than the inherent difficulty of the task orinstructions. This work provides insight into the internal workings of LLMs'instruction-following, paving the way for reliable LLM agents.
  </details>

- **[Empirical Asset Pricing with Large Language Model Agents](http://arxiv.org/abs/2409.17266v2)**  `arXiv:2409.17266`  
  _Junyan Cheng, Peter Chin_
  <details><summary>Abstract</summary>
  In this study, we introduce a novel asset pricing model leveraging the LargeLanguage Model (LLM) agents, which integrates qualitative discretionaryinvestment evaluations from LLM agents with quantitative financial economicfactors manually curated, aiming to explain the excess asset returns. Theexperimental results demonstrate that our methodology surpasses traditionalmachine learning-based baselines in both portfolio optimization and assetpricing errors. Notably, the Sharpe ratio for portfolio optimization and themean magnitude of $|\alpha|$ for anomaly portfolios experienced substantialenhancements of 10.6\% and 10.0\% respectively. Moreover, we performedcomprehensive ablation studies on our model and conducted a thorough analysisof the method to extract further insights into the proposed approach. Ourresults show effective evidence of the feasibility of applying LLMs inempirical asset pricing.
  </details>

- **[Towards shutdownable agents via stochastic choice](http://arxiv.org/abs/2407.00805v4)**  `arXiv:2407.00805`  
  _Elliott Thornley, Alexander Roman, Christos Ziakas, Leyton Ho, Louis Thomson_
  <details><summary>Abstract</summary>
  The Incomplete Preferences Proposal (IPP) is an idea for ensuring thatadvanced artificial agents never resist shutdown. A key part of the IPP isusing a novel `Discounted Reward for Same-Length Trajectories (DReST)' rewardfunction to train agents to (1) pursue goals effectively conditional on eachtrajectory-length (be `USEFUL'), and (2) choose stochastically betweendifferent trajectory-lengths (be `NEUTRAL' about trajectory-lengths). In thispaper, we propose evaluation metrics for USEFULNESS and NEUTRALITY. We use aDReST reward function to train simple agents to navigate gridworlds, and wefind that these agents learn to be USEFUL and NEUTRAL. Our results thus providesome initial evidence that DReST reward functions could train advanced agentsto be USEFUL and NEUTRAL. Our theoretical work suggests that these agents wouldbe useful and shutdownable.
  </details>

- **[A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](http://arxiv.org/abs/2402.07877v3)**  `arXiv:2402.07877`  
  _Yangxinyu Xie, Bowen Jiang, Tanwi Mallick, Joshua David Bergerson, John K. Hutchison, Duane R. Verner, et al._
  <details><summary>Abstract</summary>
  Large language models (LLMs) are a transformational capability at thefrontier of artificial intelligence and machine learning that can supportdecision-makers in addressing pressing societal challenges such as extremenatural hazard events. As generalized models, LLMs often struggle to providecontext-specific information, particularly in areas requiring specializedknowledge. In this work, we propose a Retrieval-Augmented Generation(RAG)-based multi-agent LLM system to support analysis and decision-making inthe context of natural hazards and extreme weather events. As a proof ofconcept, we present WildfireGPT, a specialized system focused on wildfirescenarios. The architecture employs a user-centered, multi-agent design todeliver tailored risk insights across diverse stakeholder groups. Byintegrating domain-specific projection data, observational datasets, andscientific literature through a RAG framework, the system ensures both accuracyand contextual relevance of the information it provides. Evaluation across tenexpert-led case studies demonstrates that WildfireGPT significantly outperformsexisting LLM-based solutions for decision support in natural hazard and extremeweather contexts.
  </details>

- **[Sherlock Holmes Doesn't Play Dice: The mathematics of uncertain reasoning when something may happen, that one is not even able to figure out](http://arxiv.org/abs/2309.03222v3)**  `arXiv:2309.03222`  
  _Guido Fioretti_
  <details><summary>Abstract</summary>
  While Evidence Theory (also known as Dempster-Shafer Theory, or BeliefFunctions Theory) is being increasingly used in data fusion, its potentialitiesin the Social and Life Sciences are often obscured by lack of awareness of itsdistinctive features. In particular, with this paper I stress that an extendedversion of Evidence Theory can express the uncertainty deriving from the fearthat events may materialize, that one is not even able to figure out. Bycontrast, Probability Theory must limit itself to the possibilities that adecision-maker is currently envisaging.  I compare this extended version of Evidence Theory to sophisticatedextensions of Probability Theory, such as imprecise and sub-additiveprobabilities, as well as unconventional versions of Information Theory thatare employed in data fusion and transmission of cultural information. A furtherextension to multi-agent interaction is outlined.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Computation and Language üí¨

<details open><summary>Click to Collapse</summary>

- **[Self-Evolving Multi-Agent Simulations for Realistic Clinical Interactions](http://arxiv.org/abs/2503.22678v1)**  `arXiv:2503.22678`  
  _Mohammad Almansoori, Komal Kumar, Hisham Cholakkal_
  <details><summary>Abstract</summary>
  In this work, we introduce MedAgentSim, an open-source simulated clinicalenvironment with doctor, patient, and measurement agents designed to evaluateand enhance LLM performance in dynamic diagnostic settings. Unlike priorapproaches, our framework requires doctor agents to actively engage withpatients through multi-turn conversations, requesting relevant medicalexaminations (e.g., temperature, blood pressure, ECG) and imaging results(e.g., MRI, X-ray) from a measurement agent to mimic the real-world diagnosticprocess. Additionally, we incorporate self improvement mechanisms that allowmodels to iteratively refine their diagnostic strategies. We enhance LLMperformance in our simulated setting by integrating multi-agent discussions,chain-of-thought reasoning, and experience-based knowledge retrieval,facilitating progressive learning as doctor agents interact with more patients.We also introduce an evaluation benchmark for assessing the LLM's ability toengage in dynamic, context-aware diagnostic interactions. While MedAgentSim isfully automated, it also supports a user-controlled mode, enabling humaninteraction with either the doctor or patient agent. Comprehensive evaluationsin various simulated diagnostic scenarios demonstrate the effectiveness of ourapproach. Our code, simulation tool, and benchmark are available at\href{https://medagentsim.netlify.app/}.
  </details>

- **[Historical Ink: Exploring Large Language Models for Irony Detection in 19th-Century Spanish](http://arxiv.org/abs/2503.22585v1)**  `arXiv:2503.22585`  
  _Kevin Cohen, Laura Manrique-G√≥mez, Rub√©n Manrique_
  <details><summary>Abstract</summary>
  This study explores the use of large language models (LLMs) to enhancedatasets and improve irony detection in 19th-century Latin American newspapers.Two strategies were employed to evaluate the efficacy of BERT and GPT-4o modelsin capturing the subtle nuances nature of irony, through both multi-class andbinary classification tasks. First, we implemented dataset enhancements focusedon enriching emotional and contextual cues; however, these showed limitedimpact on historical language analysis. The second strategy, a semi-automatedannotation process, effectively addressed class imbalance and augmented thedataset with high-quality annotations. Despite the challenges posed by thecomplexity of irony, this work contributes to the advancement of sentimentanalysis through two key contributions: introducing a new historical Spanishdataset tagged for sentiment analysis and irony detection, and proposing asemi-automated annotation methodology where human expertise is crucial forrefining LLMs results, enriched by incorporating historical and culturalcontexts as core features.
  </details>

- **[Beyond Vanilla Fine-Tuning: Leveraging Multistage, Multilingual, and Domain-Specific Methods for Low-Resource Machine Translation](http://arxiv.org/abs/2503.22582v1)**  `arXiv:2503.22582`  
  _Sarubi Thillainathan, Songchen Yuan, En-Shiun Annie Lee, Sanath Jayasena, Surangika Ranathunga_
  <details><summary>Abstract</summary>
  Fine-tuning multilingual sequence-to-sequence large language models (msLLMs)has shown promise in developing neural machine translation (NMT) systems forlow-resource languages (LRLs). However, conventional single-stage fine-tuningmethods struggle in extremely low-resource NMT settings, where training data isvery limited. This paper contributes to artificial intelligence by proposingtwo approaches for adapting msLLMs in these challenging scenarios: (1)continual pre-training (CPT), where the msLLM is further trained withdomain-specific monolingual data to compensate for the under-representation ofLRLs, and (2) intermediate task transfer learning (ITTL), a method thatfine-tunes the msLLM with both in-domain and out-of-domain parallel data toenhance its translation capabilities across various domains and tasks. As anapplication in engineering, these methods are implemented in NMT systems forSinhala, Tamil, and English (six language pairs) in domain-specific, extremelylow-resource settings (datasets containing fewer than 100,000 samples). Ourexperiments reveal that these approaches enhance translation performance by anaverage of +1.47 bilingual evaluation understudy (BLEU) score compared to thestandard single-stage fine-tuning baseline across all translation directions.Additionally, a multi-model ensemble further improves performance by anadditional BLEU score.
  </details>

- **[Bridging the Dimensional Chasm: Uncover Layer-wise Dimensional Reduction in Transformers through Token Correlation](http://arxiv.org/abs/2503.22547v1)**  `arXiv:2503.22547`  
  _Zhuo-Yang Song, Zeyu Li, Qing-Hong Cao, Ming-xing Luo, Hua Xing Zhu_
  <details><summary>Abstract</summary>
  The geometric evolution of token representations in large language models(LLMs) presents a fundamental paradox: while human language inherentlyorganizes semantic information in low-dimensional spaces ($\sim 10^1$dimensions), modern LLMs employ high-dimensional embeddings ($\sim 10^3$dimensions) processed through Transformer architectures. To resolve thisparadox, this work bridges this conceptual gap by developing a geometricframework that tracks token dynamics across Transformers layers. Throughlayer-wise analysis of intrinsic dimensions across multiple architectures, wereveal an expansion-contraction pattern where tokens diffuse to a "workingspace" and then progressively project onto lower-dimensional submanifolds. Ourfinding implies a negative correlation between the working space dimension andparameter-sensitive performance of the LLMs, and indicates that effectivemodels tend to compress tokens into approximately 10-dimensional submanifolds,closely resembling human semantic spaces. This work not only advances LLMinterpretability by reframing Transformers layers as projectors that mediatebetween high-dimensional computation and low-dimensional semantics, but alsoprovides practical tools for model diagnostics that do not rely ontask-specific evaluations.
  </details>

- **[Exploiting Mixture-of-Experts Redundancy Unlocks Multimodal Generative Abilities](http://arxiv.org/abs/2503.22517v1)**  `arXiv:2503.22517`  
  _Raman Dutt, Harleen Hanspal, Guoxuan Xia, Petru-Daniel Tudosiu, Alexander Black, Yongxin Yang, et al._
  <details><summary>Abstract</summary>
  In this work, we undertake the challenge of augmenting the existinggenerative capabilities of pre-trained text-only large language models (LLMs)with multi-modal generation capability while satisfying two core constraints:C1 preserving the preservation of original language generative capabilitieswith negligible performance degradation, and C2 adhering to a small parameterbudget to learn the new modality, ensuring scalability and efficiency. Incontrast to current approaches that add dedicated modules, therebysignificantly increasing the parameter count, we propose a method thatleverages the underutilized capacity inherent in deep models. Specifically, weexploit the parameter redundancy within Mixture-of-Experts (MoEs) as a sourceof additional capacity for learning a new modality, enabling better parameterefficiency (C1). Moreover, we preserve the original language generationcapabilities by applying low-rank adaptation exclusively to the tokens of thenew modality (C2). Furthermore, we introduce a novel parameter initializationscheme based on the Gromov-Wasserstein distance to improve convergence andtraining stability. Through an extensive analysis of the routing mechanism, weuncover the emergence of modality-specific pathways and decreased redundancywithin the experts that can efficiently unlock multi-modal generativecapabilities. Overall, our method can be seamlessly applied to a wide range ofcontemporary LLMs, providing a new pathway for transitioning from uni-modal tomulti-modal architectures.
  </details>

- **[WorkTeam: Constructing Workflows from Natural Language with Multi-Agents](http://arxiv.org/abs/2503.22473v1)**  `arXiv:2503.22473`  
  _Hanchao Liu, Rongjun Li, Weimin Xiong, Ziyu Zhou, Wei Peng_
  <details><summary>Abstract</summary>
  Workflows play a crucial role in enhancing enterprise efficiency byorchestrating complex processes with multiple tools or components. However,hand-crafted workflow construction requires expert knowledge, presentingsignificant technical barriers. Recent advancements in Large Language Models(LLMs) have improved the generation of workflows from natural languageinstructions (aka NL2Workflow), yet existing single LLM agent-based methodsface performance degradation on complex tasks due to the need for specializedknowledge and the strain of task-switching. To tackle these challenges, wepropose WorkTeam, a multi-agent NL2Workflow framework comprising a supervisor,orchestrator, and filler agent, each with distinct roles that collaborativelyenhance the conversion process. As there are currently no publicly availableNL2Workflow benchmarks, we also introduce the HW-NL2Workflow dataset, whichincludes 3,695 real-world business samples for training and evaluation.Experimental results show that our approach significantly increases the successrate of workflow construction, providing a novel and effective solution forenterprise NL2Workflow services.
  </details>

- **[Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey](http://arxiv.org/abs/2503.22458v1)**  `arXiv:2503.22458`  
  _Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, Jian-guang Lou_
  <details><summary>Abstract</summary>
  This survey examines evaluation methods for large language model (LLM)-basedagents in multi-turn conversational settings. Using a PRISMA-inspiredframework, we systematically reviewed nearly 250 scholarly sources, capturingthe state of the art from various venues of publication, and establishing asolid foundation for our analysis. Our study offers a structured approach bydeveloping two interrelated taxonomy systems: one that defines \emph{what toevaluate} and another that explains \emph{how to evaluate}. The first taxonomyidentifies key components of LLM-based agents for multi-turn conversations andtheir evaluation dimensions, including task completion, response quality, userexperience, memory and context retention, as well as planning and toolintegration. These components ensure that the performance of conversationalagents is assessed in a holistic and meaningful manner. The second taxonomysystem focuses on the evaluation methodologies. It categorizes approaches intoannotation-based evaluations, automated metrics, hybrid strategies that combinehuman assessments with quantitative measures, and self-judging methodsutilizing LLMs. This framework not only captures traditional metrics derivedfrom language understanding, such as BLEU and ROUGE scores, but alsoincorporates advanced techniques that reflect the dynamic, interactive natureof multi-turn dialogues.
  </details>

- **[Scaling Laws of Scientific Discovery with AI and Robot Scientists](http://arxiv.org/abs/2503.22444v1)**  `arXiv:2503.22444`  
  _Pengsong Zhang, Heng Zhang, Huazhe Xu, Renjun Xu, Zhenting Wang, Cong Wang, et al._
  <details><summary>Abstract</summary>
  The rapid evolution of scientific inquiry highlights an urgent need forgroundbreaking methodologies that transcend the limitations of traditionalresearch. Conventional approaches, bogged down by manual processes and siloedexpertise, struggle to keep pace with the demands of modern discovery. Weenvision an autonomous generalist scientist (AGS) system-a fusion of agentic AIand embodied robotics-that redefines the research lifecycle. This systempromises to autonomously navigate physical and digital realms, weaving togetherinsights from disparate disciplines with unprecedented efficiency. By embeddingadvanced AI and robot technologies into every phase-from hypothesis formulationto peer-ready manuscripts-AGS could slash the time and resources needed forscientific research in diverse field. We foresee a future where scientificdiscovery follows new scaling laws, driven by the proliferation andsophistication of such systems. As these autonomous agents and robots adapt toextreme environments and leverage a growing reservoir of knowledge, they couldspark a paradigm shift, pushing the boundaries of what's possible and usheringin an era of relentless innovation.
  </details>

- **[Long-Tail Crisis in Nearest Neighbor Language Models](http://arxiv.org/abs/2503.22426v1)**  `arXiv:2503.22426`  
  _Yuto Nishida, Makoto Morishita, Hiroyuki Deguchi, Hidetaka Kamigaito, Taro Watanabe_
  <details><summary>Abstract</summary>
  The $k$-nearest-neighbor language model ($k$NN-LM), one of theretrieval-augmented language models, improves the perplexity for given text bydirectly accessing a large datastore built from any text data during inference.A widely held hypothesis for the success of $k$NN-LM is that its explicitmemory, i.e., the datastore, enhances predictions for long-tail phenomena.However, prior works have primarily shown its ability to retrieve long-tailcontexts, leaving the model's performance remain underexplored in estimatingthe probabilities of long-tail target tokens during inference. In this paper,we investigate the behavior of $k$NN-LM on low-frequency tokens, examiningprediction probability, retrieval accuracy, token distribution in thedatastore, and approximation error of the product quantization. Ourexperimental results reveal that $k$NN-LM does not improve predictionperformance for low-frequency tokens but mainly benefits high-frequency tokensregardless of long-tail contexts in the datastore.
  </details>

- **[Elite Political Discourse has Become More Toxic in Western Countries](http://arxiv.org/abs/2503.22411v1)**  `arXiv:2503.22411`  
  _Petter T√∂rnberg, Juliana Chueri_
  <details><summary>Abstract</summary>
  Toxic and uncivil politics is widely seen as a growing threat to democraticvalues and governance, yet our understanding of the drivers and evolution ofpolitical incivility remains limited. Leveraging a novel dataset of nearly 18million Twitter messages from parliamentarians in 17 countries over five years,this paper systematically investigates whether politics internationally isbecoming more uncivil, and what are the determinants of political incivility.Our analysis reveals a marked increase in toxic discourse among politicalelites, and that it is associated to radical-right parties and parties inopposition. Toxicity diminished markedly during the early phase of the COVID-19pandemic and, surprisingly, during election campaigns. Furthermore, our resultsindicate that posts relating to ``culture war'' topics, such as migration andLGBTQ+ rights, are substantially more toxic than debates focused on welfare oreconomic issues. These findings underscore a troubling shift in internationaldemocracies toward an erosion of constructive democratic dialogue.
  </details>

- **[Negation: A Pink Elephant in the Large Language Models' Room?](http://arxiv.org/abs/2503.22395v1)**  `arXiv:2503.22395`  
  _Tereza Vrabcov√°, Marek Kadlƒç√≠k, Petr Sojka, Michal ≈†tef√°nik, Michal Spiegel_
  <details><summary>Abstract</summary>
  Negations are key to determining sentence meaning, making them essential forlogical reasoning. Despite their importance, negations pose a substantialchallenge for large language models (LLMs) and remain underexplored.  We construct two multilingual natural language inference (NLI) datasets with\textit{paired} examples differing in negation. We investigate how model sizeand language impact its ability to handle negation correctly by evaluatingpopular LLMs.  Contrary to previous work, we show that increasing the model sizeconsistently improves the models' ability to handle negations. Furthermore, wefind that both the models' reasoning accuracy and robustness to negation arelanguage-dependent and that the length and explicitness of the premise have agreater impact on robustness than language.  Our datasets can facilitate further research and improvements of languagemodel reasoning in multilingual settings.
  </details>

- **[Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors](http://arxiv.org/abs/2503.22388v1)**  `arXiv:2503.22388`  
  _Zhiyu Yang, Shuo Wang, Yukun Yan, Yang Deng_
  <details><summary>Abstract</summary>
  LLMs are transforming software development, yet current code generation andcode repair benchmarks mainly assess syntactic and functional correctness insimple, single-error cases. LLMs' capabilities to autonomously find and fixruntime logical errors in complex data science code remain largely unexplored.To address this gap, we introduce DSDBench: the Data Science DebuggingBenchmark, the first benchmark for systematic evaluation of LLMs on multi-hoperror tracing and multi-bug detection in data science code debugging. DSDBenchadapts datasets from existing data science task benchmarks, such as DABench andMatPlotBench, featuring realistic data science debugging tasks withautomatically synthesized multi-hop, multi-bug code snippets. DSDBench includes1,117 annotated samples with 741 cause-effect error pairs and runtime errormessages. Evaluations of state-of-the-art LLMs on DSDBench show significantperformance gaps, highlighting challenges in debugging logical runtime errorsin data science code. DSDBench offers a crucial resource to evaluate andimprove LLMs' debugging and reasoning capabilities, enabling more reliableAI-assisted data science in the future.DSDBench is publicly available athttps://github.com/KevinCL16/DSDBench.
  </details>

- **[Supposedly Equivalent Facts That Aren't? Entity Frequency in Pre-training Induces Asymmetry in LLMs](http://arxiv.org/abs/2503.22362v1)**  `arXiv:2503.22362`  
  _Yuan He, Bailan He, Zifeng Ding, Alisia Lupidi, Yuqicheng Zhu, Shuo Chen, et al._
  <details><summary>Abstract</summary>
  Understanding and mitigating hallucinations in Large Language Models (LLMs)is crucial for ensuring reliable content generation. While previous researchhas primarily focused on "when" LLMs hallucinate, our work explains "why" anddirectly links model behaviour to the pre-training data that forms their priorknowledge. Specifically, we demonstrate that an asymmetry exists in therecognition of logically equivalent facts, which can be attributed to frequencydiscrepancies of entities appearing as subjects versus objects. Given that mostpre-training datasets are inaccessible, we leverage the fully open-source OLMoseries by indexing its Dolma dataset to estimate entity frequencies. Usingrelational facts (represented as triples) from Wikidata5M, we construct probingdatasets to isolate this effect. Our experiments reveal that facts with ahigh-frequency subject and a low-frequency object are better recognised thantheir inverse, despite their logical equivalence. The pattern reverses inlow-to-high frequency settings, and no statistically significant asymmetryemerges when both entities are high-frequency. These findings highlight theinfluential role of pre-training data in shaping model predictions and provideinsights for inferring the characteristics of pre-training data in closed orpartially closed LLMs.
  </details>

- **[Firm or Fickle? Evaluating Large Language Models Consistency in Sequential Interactions](http://arxiv.org/abs/2503.22353v1)**  `arXiv:2503.22353`  
  _Yubo Li, Yidi Miao, Xueying Ding, Ramayya Krishnan, Rema Padman_
  <details><summary>Abstract</summary>
  Large Language Models (LLMs) have shown remarkable capabilities acrossvarious tasks, but their deployment in high-stake domains requires consistentperformance across multiple interaction rounds. This paper introduces acomprehensive framework for evaluating and improving LLM response consistency,making three key contributions. First, we propose a novel Position-WeightedConsistency (PWC) score that captures both the importance of early-stagestability and recovery patterns in multi-turn interactions. Second, we presenta carefully curated benchmark dataset spanning diverse domains and difficultylevels, specifically designed to evaluate LLM consistency under variouschallenging follow-up scenarios. Third, we introduce Confidence-Aware ResponseGeneration (CARG), a framework that significantly improves response stabilityby incorporating model confidence signals into the generation process.Empirical results demonstrate that CARG significantly improves responsestability without sacrificing accuracy, underscoring its potential for reliableLLM deployment in critical applications.
  </details>

- **[SKDU at De-Factify 4.0: Natural Language Features for AI-Generated Text-Detection](http://arxiv.org/abs/2503.22338v1)**  `arXiv:2503.22338`  
  _Shrikant Malviya, Pablo Arnau-Gonz√°lez, Miguel Arevalillo-Herr√°ez, Stamos Katsigiannis_
  <details><summary>Abstract</summary>
  The rapid advancement of large language models (LLMs) has introduced newchallenges in distinguishing human-written text from AI-generated content. Inthis work, we explored a pipelined approach for AI-generated text detectionthat includes a feature extraction step (i.e. prompt-based rewriting featuresinspired by RAIDAR and content-based features derived from the NELA toolkit)followed by a classification module. Comprehensive experiments were conductedon the Defactify4.0 dataset, evaluating two tasks: binary classification todifferentiate human-written and AI-generated text, and multi-classclassification to identify the specific generative model used to generate theinput text. Our findings reveal that NELA features significantly outperformRAIDAR features in both tasks, demonstrating their ability to capture nuancedlinguistic, stylistic, and content-based differences. Combining RAIDAR and NELAfeatures provided minimal improvement, highlighting the redundancy introducedby less discriminative features. Among the classifiers tested, XGBoost emergedas the most effective, leveraging the rich feature sets to achieve highaccuracy and generalisation.
  </details>

- **[A Refined Analysis of Massive Activations in LLMs](http://arxiv.org/abs/2503.22329v1)**  `arXiv:2503.22329`  
  _Louis Owen, Nilabhra Roy Chowdhury, Abhay Kumar, Fabian G√ºra_
  <details><summary>Abstract</summary>
  Motivated in part by their relevance for low-precision training andquantization, massive activations in large language models (LLMs) have recentlyemerged as a topic of interest. However, existing analyses are limited inscope, and generalizability across architectures is unclear. This paper helpsaddress some of these gaps by conducting an analysis of massive activationsacross a broad range of LLMs, including both GLU-based and non-GLU-basedarchitectures. Our findings challenge several prior assumptions, mostimportantly: (1) not all massive activations are detrimental, i.e. suppressingthem does not lead to an explosion of perplexity or a collapse in downstreamtask performance; (2) proposed mitigation strategies such as Attention KV biasare model-specific and ineffective in certain cases. We consequentlyinvestigate novel hybrid mitigation strategies; in particular pairing TargetVariance Rescaling (TVR) with Attention KV bias or Dynamic Tanh (DyT)successfully balances the mitigation of massive activations with preserveddownstream model performance in the scenarios we investigated. Our code isavailable at: https://github.com/bluorion-com/refine_massive_activations.
  </details>

- **[Preference-based Learning with Retrieval Augmented Generation for Conversational Question Answering](http://arxiv.org/abs/2503.22303v1)**  `arXiv:2503.22303`  
  _Magdalena Kaiser, Gerhard Weikum_
  <details><summary>Abstract</summary>
  Conversational Question Answering (ConvQA) involves multiple subtasks, i) tounderstand incomplete questions in their context, ii) to retrieve relevantinformation, and iii) to generate answers. This work presents PRAISE, apipeline-based approach for ConvQA that trains LLM adapters for each of thethree subtasks. As labeled training data for individual subtasks is unavailablein practice, PRAISE learns from its own generations using the final answeringperformance as feedback signal without human intervention and treatsintermediate information, like relevant evidence, as weakly labeled data. Weapply Direct Preference Optimization by contrasting successful and unsuccessfulsamples for each subtask. In our experiments, we show the effectiveness of thistraining paradigm: PRAISE shows improvements per subtask and achieves newstate-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5percentage points increase in precision over baselines.
  </details>

- **[MultiClaimNet: A Massively Multilingual Dataset of Fact-Checked Claim Clusters](http://arxiv.org/abs/2503.22280v1)**  `arXiv:2503.22280`  
  _Rrubaa Panchendrarajan, Rub√©n M√≠guez, Arkaitz Zubiaga_
  <details><summary>Abstract</summary>
  In the context of fact-checking, claims are often repeated across variousplatforms and in different languages, which can benefit from a process thatreduces this redundancy. While retrieving previously fact-checked claims hasbeen investigated as a solution, the growing number of unverified claims andexpanding size of fact-checked databases calls for alternative, more efficientsolutions. A promising solution is to group claims that discuss the sameunderlying facts into clusters to improve claim retrieval and validation.However, research on claim clustering is hindered by the lack of suitabledatasets. To bridge this gap, we introduce \textit{MultiClaimNet}, a collectionof three multilingual claim cluster datasets containing claims in 86 languagesacross diverse topics. Claim clusters are formed automatically fromclaim-matching pairs with limited manual intervention. We leverage two existingclaim-matching datasets to form the smaller datasets within\textit{MultiClaimNet}. To build the larger dataset, we propose and validate anapproach involving retrieval of approximate nearest neighbors to form candidateclaim pairs and an automated annotation of claim similarity using largelanguage models. This larger dataset contains 85.3K fact-checked claims writtenin 78 languages. We further conduct extensive experiments using variousclustering techniques and sentence embedding models to establish baselineperformance. Our datasets and findings provide a strong foundation for scalableclaim clustering, contributing to efficient fact-checking pipelines.
  </details>

- **[CFiCS: Graph-Based Classification of Common Factors and Microcounseling Skills](http://arxiv.org/abs/2503.22277v1)**  `arXiv:2503.22277`  
  _Fabian Schmidt, Karin Hammerfald, Henrik Haaland Jahren, Vladimir Vlassov_
  <details><summary>Abstract</summary>
  Common factors and microcounseling skills are critical to the effectivenessof psychotherapy. Understanding and measuring these elements provides valuableinsights into therapeutic processes and outcomes. However, automaticidentification of these change principles from textual data remains challengingdue to the nuanced and context-dependent nature of therapeutic dialogue. Thispaper introduces CFiCS, a hierarchical classification framework integratinggraph machine learning with pretrained contextual embeddings. We representcommon factors, intervention concepts, and microcounseling skills as aheterogeneous graph, where textual information from ClinicalBERT enriches eachnode. This structure captures both the hierarchical relationships (e.g.,skill-level nodes linking to broad factors) and the semantic properties oftherapeutic concepts. By leveraging graph neural networks, CFiCS learnsinductive node embeddings that generalize to unseen text samples lackingexplicit connections. Our results demonstrate that integrating ClinicalBERTnode features and graph structure significantly improves classificationperformance, especially in fine-grained skill prediction. CFiCS achievessubstantial gains in both micro and macro F1 scores across all tasks comparedto baselines, including random forests, BERT-based multi-task models, andgraph-based methods.
  </details>

- **[EdgeInfinite: A Memory-Efficient Infinite-Context Transformer for Edge Devices](http://arxiv.org/abs/2503.22196v1)**  `arXiv:2503.22196`  
  _Jiyu Chen, Shuang Peng, Daxiong Luo, Fan Yang, Renshou Wu, Fangyuan Li, et al._
  <details><summary>Abstract</summary>
  Transformer-based large language models (LLMs) encounter challenges inprocessing long sequences on edge devices due to the quadratic complexity ofattention mechanisms and growing memory demands from Key-Value (KV) cache.Existing KV cache optimizations struggle with irreversible token eviction inlong-output tasks, while alternative sequence modeling architectures provecostly to adopt within established Transformer infrastructure. We presentEdgeInfinite, a memory-efficient solution for infinite contexts that integratescompressed memory into Transformer-based LLMs through a trainable memory-gatingmodule. This approach maintains full compatibility with standard Transformerarchitectures, requiring fine-tuning only a small part of parameters, andenables selective activation of the memory-gating module for long and shortcontext task routing. The experimental result shows that EdgeInfinite achievescomparable performance to baseline Transformer-based LLM on long contextbenchmarks while optimizing memory consumption and time to first token.
  </details>

- **[FRASE: Structured Representations for Generalizable SPARQL Query Generation](http://arxiv.org/abs/2503.22144v1)**  `arXiv:2503.22144`  
  _Papa Abdou Karim Karou Diallo, Amal Zouaq_
  <details><summary>Abstract</summary>
  Translating natural language questions into SPARQL queries enables KnowledgeBase querying for factual and up-to-date responses. However, existing datasetsfor this task are predominantly template-based, leading models to learnsuperficial mappings between question and query templates rather thandeveloping true generalization capabilities. As a result, models struggle whenencountering naturally phrased, template-free questions. This paper introducesFRASE (FRAme-based Semantic Enhancement), a novel approach that leverages FrameSemantic Role Labeling (FSRL) to address this limitation. We also presentLC-QuAD 3.0, a new dataset derived from LC-QuAD 2.0, in which each question isenriched using FRASE through frame detection and the mapping of frame-elementsto their argument. We evaluate the impact of this approach through extensiveexperiments on recent large language models (LLMs) under different fine-tuningconfigurations. Our results demonstrate that integrating frame-based structuredrepresentations consistently improves SPARQL generation performance,particularly in challenging generalization scenarios when test questionsfeature unseen templates (unknown template splits) and when they are allnaturally phrased (reformulated questions).
  </details>

- **[Beyond Single-Sentence Prompts: Upgrading Value Alignment Benchmarks with Dialogues and Stories](http://arxiv.org/abs/2503.22115v1)**  `arXiv:2503.22115`  
  _Yazhou Zhang, Qimeng Liu, Qiuchi Li, Peng Zhang, Jing Qin_
  <details><summary>Abstract</summary>
  Evaluating the value alignment of large language models (LLMs) hastraditionally relied on single-sentence adversarial prompts, which directlyprobe models with ethically sensitive or controversial questions. However, withthe rapid advancements in AI safety techniques, models have become increasinglyadept at circumventing these straightforward tests, limiting theireffectiveness in revealing underlying biases and ethical stances. To addressthis limitation, we propose an upgraded value alignment benchmark that movesbeyond single-sentence prompts by incorporating multi-turn dialogues andnarrative-based scenarios. This approach enhances the stealth and adversarialnature of the evaluation, making it more robust against superficial safeguardsimplemented in modern LLMs. We design and implement a dataset that includesconversational traps and ethically ambiguous storytelling, systematicallyassessing LLMs' responses in more nuanced and context-rich settings.Experimental results demonstrate that this enhanced methodology can effectivelyexpose latent biases that remain undetected in traditional single-shotevaluations. Our findings highlight the necessity of contextual and dynamictesting for value alignment in LLMs, paving the way for more sophisticated andrealistic assessments of AI ethics and safety.
  </details>

- **[Leveraging LLMs for Predicting Unknown Diagnoses from Clinical Notes](http://arxiv.org/abs/2503.22092v1)**  `arXiv:2503.22092`  
  _Dina Albassam, Adam Cross, Chengxiang Zhai_
  <details><summary>Abstract</summary>
  Electronic Health Records (EHRs) often lack explicit links betweenmedications and diagnoses, making clinical decision-making and research moredifficult. Even when links exist, diagnosis lists may be incomplete, especiallyduring early patient visits. Discharge summaries tend to provide more completeinformation, which can help infer accurate diagnoses, especially with the helpof large language models (LLMs). This study investigates whether LLMs canpredict implicitly mentioned diagnoses from clinical notes and link them tocorresponding medications. We address two research questions: (1) Does majorityvoting across diverse LLM configurations outperform the best singleconfiguration in diagnosis prediction? (2) How sensitive is majority votingaccuracy to LLM hyperparameters such as temperature, top-p, and summary length?To evaluate, we created a new dataset of 240 expert-annotatedmedication-diagnosis pairs from 20 MIMIC-IV notes. Using GPT-3.5 Turbo, we ran18 prompting configurations across short and long summary lengths, generating8568 test cases. Results show that majority voting achieved 75 percentaccuracy, outperforming the best single configuration at 66 percent. No singlehyperparameter setting dominated, but combining deterministic, balanced, andexploratory strategies improved performance. Shorter summaries generally led tohigher accuracy.In conclusion, ensemble-style majority voting with diverse LLMconfigurations improves diagnosis prediction in EHRs and offers a promisingmethod to link medications and diagnoses in clinical texts.
  </details>

- **[Penrose Tiled Low-Rank Compression and Section-Wise Q&A Fine-Tuning: A General Framework for Domain-Specific Large Language Model Adaptation](http://arxiv.org/abs/2503.22074v1)**  `arXiv:2503.22074`  
  _Chuan-Wei Kuo, Siyu Chen, Chenqi Yan, Yu Yang Fredrik Liu_
  <details><summary>Abstract</summary>
  Large language models (LLMs) hold great promise for specialized scientificdomains such as materials science, yet adapting them efficiently and accuratelyto domain-specific knowledge remains challenging due to limited data and highknowledge density. We propose a two-stage framework that combines structuredmodel compression with a scientific fine-tuning regimen to address thischallenge. In the compression stage, we decompose the LLM's weight matricesinto local low-rank "rank blocks" and arrange these blocks in a Penrose-likenon-periodic tiling pattern. Each block is then compacted via spectraltransformations (e.g., discrete cosine or Fourier transforms), and aKullback-Leibler (KL) divergence-based alignment loss preserves thedistributional similarity between the compressed model's representations andthose of the original full model. In the adaptation stage, the compressed modelis further tuned using a human-like scientific reading protocol: it processestechnical materials science documents section by section, engaging in astructured question-and-answer routine for each section. This section-wise Q&Afine-tuning strategy extracts explicit reasoning traces and gradually injectsdomain knowledge, while minimizing catastrophic forgetting of the model'sgeneral language capabilities. By balancing efficient compression with targetedadaptation, our two-stage approach enables precise specialization of LLMs tohigh-value domains under data-scarce conditions. We present this principled yetexploratory pipeline and outline its potential for advancing materials scienceknowledge integration, laying the groundwork for comprehensive empiricalevaluation in future work.
  </details>

- **[Non-Monotonic Attention-based Read/Write Policy Learning for Simultaneous Translation](http://arxiv.org/abs/2503.22051v1)**  `arXiv:2503.22051`  
  _Zeeshan Ahmed, Frank Seide, Zhe Liu, Rastislav Rabatin, Jachym Kolar, Niko Moritz, et al._
  <details><summary>Abstract</summary>
  Simultaneous or streaming machine translation generates translation whilereading the input stream. These systems face a quality/latency trade-off,aiming to achieve high translation quality similar to non-streaming models withminimal latency. We propose an approach that efficiently manages thistrade-off. By enhancing a pretrained non-streaming model, which was trainedwith a seq2seq mechanism and represents the upper bound in quality, we convertit into a streaming model by utilizing the alignment between source and targettokens. This alignment is used to learn a read/write decision boundary forreliable translation generation with minimal input. During training, the modellearns the decision boundary through a read/write policy module, employingsupervised learning on the alignment points (pseudo labels). The read/writepolicy module, a small binary classification unit, can control thequality/latency trade-off during inference. Experimental results show that ourmodel outperforms several strong baselines and narrows the gap with thenon-streaming baseline model.
  </details>

- **[Outlier dimensions favor frequent tokens in language models](http://arxiv.org/abs/2503.21718v2)**  `arXiv:2503.21718`  
  _Iuri Macocco, Nora Graichen, Gemma Boleda, Marco Baroni_
  <details><summary>Abstract</summary>
  We study last-layer outlier dimensions, i.e. dimensions that display extremeactivations for the majority of inputs. We show that outlier dimensions arisein many different modern language models, and trace their function back to theheuristic of constantly predicting frequent words. We further show how a modelcan block this heuristic when it is not contextually appropriate, by assigninga counterbalancing weight mass to the remaining dimensions, and we investigatewhich model parameters boost outlier dimensions and when they arise duringtraining. We conclude that outlier dimensions are a specialized mechanismdiscovered by many distinct models to implement a useful token predictionheuristic.
  </details>

- **[OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs](http://arxiv.org/abs/2503.21480v2)**  `arXiv:2503.21480`  
  _John Murzaku, Owen Rambow_
  <details><summary>Abstract</summary>
  The use of omni-LLMs (large language models that accept any modality asinput), particularly for multimodal cognitive state tasks involving speech, isunderstudied. We present OmniVox, the first systematic evaluation of fouromni-LLMs on the zero-shot emotion recognition task. We evaluate on two widelyused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shotomni-LLMs outperform or are competitive with fine-tuned audio models. Alongsideour audio-only evaluation, we also evaluate omni-LLMs on text only and text andaudio. We present acoustic prompting, an audio-specific prompting strategy foromni-LLMs which focuses on acoustic feature analysis, conversation contextanalysis, and step-by-step reasoning. We compare our acoustic prompting tominimal prompting and full chain-of-thought prompting techniques. We perform acontext window analysis on IEMOCAP and MELD, and find that using context helps,especially on IEMOCAP. We conclude with an error analysis on the generatedacoustic reasoning outputs from the omni-LLMs.
  </details>

- **[Function Alignment: A New Theory of Mind and Intelligence, Part I: Foundations](http://arxiv.org/abs/2503.21106v2)**  `arXiv:2503.21106`  
  _Gus G. Xia_
  <details><summary>Abstract</summary>
  This paper introduces function alignment, a novel theory of mind andintelligence that is both intuitively compelling and structurally grounded. Itexplicitly models how meaning, interpretation, and analogy emerge frominteractions among layered representations, forming a coherent frameworkcapable not only of modeling minds but also of serving as a blueprint forbuilding them. One of the key theoretical insights derived from functionalignment is bounded interpretability, which provides a unified explanation forpreviously fragmented ideas in cognitive science, such as bounded rationality,symbol grounding, and analogy-making. Beyond modeling, the function alignmentframework bridges disciplines often kept apart, linking computationalarchitecture, psychological theory, and even contemplative traditions such asZen. Rather than building on any philosophical systems, it offers a structuralfoundation upon which multiple ways of understanding the mind may bereconstructed.
  </details>

- **[EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues](http://arxiv.org/abs/2503.21080v2)**  `arXiv:2503.21080`  
  _Yuhan Liu, Yunbo Long_
  <details><summary>Abstract</summary>
  While large language model (LLM)-based chatbots have been applied foreffective engagement in credit dialogues, their capacity for dynamic emotionalexpression remains limited. Current agents primarily rely on passive empathyrather than affective reasoning. For instance, when faced with persistentclient negativity, the agent should employ strategic emotional adaptation byexpressing measured anger to discourage counterproductive behavior and guidethe conversation toward resolution. This context-aware emotional modulation isessential for imitating the nuanced decision-making of human negotiators. Thispaper introduces an EQ-negotiator that combines emotion sensing frompre-trained language models (PLMs) with emotional reasoning based on GameTheory and Hidden Markov Models. It takes into account both the current andhistorical emotions of the client to better manage and address negativeemotions during interactions. By fine-tuning pre-trained language models (PLMs)on public emotion datasets and validating them on the credit dialogue datasets,our approach enables LLM-based agents to effectively capture shifts in clientemotions and dynamically adjust their response tone based on our emotiondecision policies in real-world financial negotiations. This EQ-negotiator canalso help credit agencies foster positive client relationships, enhancingsatisfaction in credit services.
  </details>

- **[DomainCQA: Crafting Expert-Level QA from Domain-Specific Charts](http://arxiv.org/abs/2503.19498v2)**  `arXiv:2503.19498`  
  _Ling Zhong, Yujing Lu, Jing Yang, Weiming Li, Peng Wei, Yongheng Wang, et al._
  <details><summary>Abstract</summary>
  Chart Question Answering (CQA) benchmarks are essential for evaluating thecapability of Multimodal Large Language Models (MLLMs) to interpret visualdata. However, current benchmarks focus primarily on the evaluation ofgeneral-purpose CQA but fail to adequately capture domain-specific challenges.We introduce DomainCQA, a systematic methodology for constructingdomain-specific CQA benchmarks, and demonstrate its effectiveness by developingAstroChart, a CQA benchmark in the field of astronomy. Our evaluation showsthat chart reasoning and combining chart information with domain knowledge fordeeper analysis and summarization, rather than domain-specific knowledge, posethe primary challenge for existing MLLMs, highlighting a critical gap incurrent benchmarks. By providing a scalable and rigorous framework, DomainCQAenables more precise assessment and improvement of MLLMs for domain-specificapplications.
  </details>

- **[Enhancing Small Language Models for Cross-Lingual Generalized Zero-Shot Classification with Soft Prompt Tuning](http://arxiv.org/abs/2503.19469v2)**  `arXiv:2503.19469`  
  _Fred Philippy, Siwen Guo, Cedric Lothritz, Jacques Klein, Tegawend√© F. Bissyand√©_
  <details><summary>Abstract</summary>
  In NLP, Zero-Shot Classification (ZSC) has become essential for enablingmodels to classify text into categories unseen during training, particularly inlow-resource languages and domains where labeled data is scarce. Whilepretrained language models (PLMs) have shown promise in ZSC, they often rely onlarge training datasets or external knowledge, limiting their applicability inmultilingual and low-resource scenarios. Recent approaches leveraging naturallanguage prompts reduce the dependence on large training datasets but struggleto effectively incorporate available labeled data from related classificationtasks, especially when these datasets originate from different languages ordistributions. Moreover, existing prompt-based methods typically rely onmanually crafted prompts in a specific language, limiting their adaptabilityand effectiveness in cross-lingual settings. To address these challenges, weintroduce RoSPrompt, a lightweight and data-efficient approach for trainingsoft prompts that enhance cross-lingual ZSC while ensuring robustgeneralization across data distribution shifts. RoSPrompt is designed for smallmultilingual PLMs, enabling them to leverage high-resource languages to improveperformance in low-resource settings without requiring extensive fine-tuning orhigh computational costs. We evaluate our approach on multiple multilingualPLMs across datasets covering 106 languages, demonstrating strong cross-lingualtransfer performance and robust generalization capabilities over unseenclasses.
  </details>

- **[Overtrained Language Models Are Harder to Fine-Tune](http://arxiv.org/abs/2503.19206v2)**  `arXiv:2503.19206`  
  _Jacob Mitchell Springer, Sachin Goyal, Kaiyue Wen, Tanishq Kumar, Xiang Yue, Sadhika Malladi, et al._
  <details><summary>Abstract</summary>
  Large language models are pre-trained on ever-growing token budgets under theassumption that better pre-training performance translates to improveddownstream models. In this work, we challenge this assumption and show thatextended pre-training can make models harder to fine-tune, leading to degradedfinal performance. We term this phenomenon catastrophic overtraining. Forexample, the instruction-tuned OLMo-1B model pre-trained on 3T tokens leads toover 2% worse performance on multiple standard LLM benchmarks than its 2.3Ttoken counterpart. Through controlled experiments and theoretical analysis, weshow that catastrophic overtraining arises from a systematic increase in thebroad sensitivity of pre-trained parameters to modifications, including but notlimited to fine-tuning. Our findings call for a critical reassessment ofpre-training design that considers the downstream adaptability of the model.
  </details>

- **[Whispering in Amharic: Fine-tuning Whisper for Low-resource Language](http://arxiv.org/abs/2503.18485v2)**  `arXiv:2503.18485`  
  _Dawit Ketema Gete, Bedru Yimam Ahmed, Tadesse Destaw Belay, Yohannes Ayana Ejigu, Sukairaj Hafiz Imam, Alemu Belay Tessema, et al._
  <details><summary>Abstract</summary>
  This work explores fine-tuning OpenAI's Whisper automatic speech recognition(ASR) model for Amharic, a low-resource language, to improve transcriptionaccuracy. While the foundational Whisper model struggles with Amharic due tolimited representation in its training data, we fine-tune it using datasetslike Mozilla Common Voice, FLEURS, and the BDU-speech dataset. Thebest-performing model, Whispersmall-am, significantly improves when finetunedon a mix of existing FLEURS data and new, unseen Amharic datasets. Trainingsolely on new data leads to poor performance, but combining it with FLEURS datareinforces the model, enabling better specialization in Amharic. We alsodemonstrate that normalizing Amharic homophones significantly enhances WordError Rate (WER) and Bilingual Evaluation Understudy (BLEU) scores. This studyunderscores the importance of fine-tuning strategies and dataset compositionfor improving ASR in low-resource languages, providing insights for futureAmharic speech recognition research.
  </details>

- **[Sun-Shine: A Large Language Model for Tibetan Culture](http://arxiv.org/abs/2503.18288v2)**  `arXiv:2503.18288`  
  _Cheng Huang, Fan Gao, Nyima Tashi, Yutong Liu, Xiangxiang Wang, Thupten Tsering, et al._
  <details><summary>Abstract</summary>
  Tibetan, a minority language in China, features a highly intricategrammatical structure, characterized by four verb tenses and a tense systemwith frequent irregularities, contributing to its extensive inflectionaldiversity. Recently, advances in Large Language Models (LLMs) have transformedthe paradigm in many domains. Despite the success in other fields, current LLMsoften fall short in catering to the needs of domain experts like Tibetans, andthe potential of LLMs for Tibetan culture is under-explored. The intrinsicreasons are the immense and intricate nature of Tibetan culture as well as thenecessity for higher granularity and richness in knowledge. Simultaneously, thecomplexity and uniqueness of its grammatical structure, coupled with its statusas a minority ethnic language, contribute to data scarcity, which remains afundamental challenge. To alleviate these issues, we introduce Llama-Sunshine(Sun-Shine), the first large language model for Tibetan culture, which isexpert in various Tibetan language processing tasks. Sun-Shine incorporatesstate-of-the-art model architectures optimized for Tibetan's linguisticfeatures. We also propose TIB-STC, a comprehensive dataset comprising diverseTibetan texts such as literature, religious scripts, news, and conversationaldata, which is also the first large-scale dataset for Tibetan culture. Thoughcomprehensive experiments, Sun-Shine not only demonstrates a higher level ofknowledge expertise for Tibetan culture but also gains preliminary embodiedintelligence capabilities in Tibetan language processing tasks, like languagemodeling, text classification, machine translation, and syntactic analysis.Moreover, it excels in low-resource scenarios, showcasing strong generalizationcapabilities.
  </details>

- **[Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering](http://arxiv.org/abs/2503.18172v2)**  `arXiv:2503.18172`  
  _Zixin Chen, Sicheng Song, Kashun Shum, Yanna Lin, Rui Sheng, Huamin Qu_
  <details><summary>Abstract</summary>
  Misleading chart visualizations, which intentionally manipulate datarepresentations to support specific claims, can distort perceptions and lead toincorrect conclusions. Despite decades of research, misleading visualizationsremain a widespread and pressing issue. Recent advances in multimodal largelanguage models (MLLMs) have demonstrated strong chart comprehensioncapabilities, yet no existing work has systematically evaluated their abilityto detect and interpret misleading charts. This paper introduces the MisleadingChart Question Answering (Misleading ChartQA) Benchmark, a large-scalemultimodal dataset designed to assess MLLMs in identifying and reasoning aboutmisleading charts. It contains over 3,000 curated examples, covering 21 typesof misleaders and 10 chart types. Each example includes standardized chartcode, CSV data, and multiple-choice questions with labeled explanations,validated through multi-round MLLM checks and exhausted expert human review. Webenchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitationsin identifying visually deceptive practices. We also propose a novel pipelinethat detects and localizes misleaders, enhancing MLLMs' accuracy in misleadingchart interpretation. Our work establishes a foundation for advancingMLLM-driven misleading chart comprehension. We publicly release the sampledataset to support further research in this critical area.
  </details>

- **[Can Language Models Follow Multiple Turns of Entangled Instructions?](http://arxiv.org/abs/2503.13222v2)**  `arXiv:2503.13222`  
  _Chi Han_
  <details><summary>Abstract</summary>
  Despite significant achievements in improving the instruction-followingcapabilities of large language models (LLMs), the ability to process multiplepotentially entangled or conflicting instructions remains a considerablechallenge. Real-world scenarios often require consistency across multipleinstructions over time, such as secret privacy, personal preferences, andprioritization, which demand sophisticated abilities to integrate multipleturns and carefully balance competing objectives when instructions intersect orconflict. This work presents a systematic investigation of LLMs' capabilitiesin handling multiple turns of instructions, covering three levels ofdifficulty: (1) retrieving information from instructions, (2) tracking andreasoning across turns, and (3) resolving conflicts among instructions. Weconstruct MultiTurnInstruct with around 1.1K high-quality multi-turnconversations through the human-in-the-loop approach and result in ninecapability categories, including statics and dynamics, reasoning, andmultitasking. Our finding reveals an intriguing trade-off between differentcapabilities. While GPT models demonstrate superior memorization, they showreduced effectiveness in privacy-protection tasks requiring selectiveinformation withholding. Larger models exhibit stronger reasoning capabilitiesbut still struggle with resolving conflicting instructions. Importantly, theseperformance gaps cannot be attributed solely to information loss, as modelsdemonstrate strong BLEU scores on memorization tasks but their attentionmechanisms fail to integrate multiple related instructions effectively. Thesefindings highlight critical areas for improvement in complex real-world tasksinvolving multi-turn instructions.
  </details>

- **[Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical Investigation](http://arxiv.org/abs/2503.12854v2)**  `arXiv:2503.12854`  
  _Songjun Tu, Jiahao Lin, Xiangyu Tian, Qichao Zhang, Linjing Li, Yuqian Fu, et al._
  <details><summary>Abstract</summary>
  Recent advancements in post-training methodologies for large language models(LLMs) have highlighted reinforcement learning (RL) as a critical component forenhancing reasoning. However, the substantial computational costs associatedwith RL-based approaches have led to growing interest in alternative paradigms,such as Direct Preference Optimization (DPO). In this study, we investigate theeffectiveness of DPO in facilitating self-improvement for LLMs throughiterative preference-based learning. We demonstrate that a single round of DPOwith coarse filtering significantly enhances mathematical reasoningperformance, particularly for strong base model. Furthermore, we design aniterative enhancement framework for both the generator and the reward model(RM), enabling their mutual improvement through online interaction acrossmultiple rounds of DPO. Finally, with simple verifiable rewards, our modelDPO-VP achieves RL-level performance with significantly lower computationaloverhead. These findings highlight DPO as a scalable and cost-effectivealternative to RL, offering a practical solution for enhancing LLM reasoning inresource-constrained situations.
  </details>

- **[Retrieval Backward Attention without Additional Training: Enhance Embeddings of Large Language Models via Repetition](http://arxiv.org/abs/2502.20726v2)**  `arXiv:2502.20726`  
  _Yifei Duan, Raphael Shang, Deng Liang, Yongqiang Cai_
  <details><summary>Abstract</summary>
  Language models can be viewed as functions that embed text into Euclideanspace, where the quality of the embedding vectors directly determines modelperformance, training such neural networks involves various uncertainties. Thispaper focuses on improving the performance of pre-trained language models inzero-shot settings through a simple and easily implementable method. We proposea novel backward attention mechanism to enhance contextual informationencoding. Evaluated on the Chinese Massive Text Embedding Benchmark (C-MTEB),our approach achieves significant improvements across multiple tasks, providingvaluable insights for advancing zero-shot learning capabilities.
  </details>

- **[Foot-In-The-Door: A Multi-turn Jailbreak for LLMs](http://arxiv.org/abs/2502.19820v3)**  `arXiv:2502.19820`  
  _Zixuan Weng, Xiaolong Jin, Jinyuan Jia, Xiangyu Zhang_
  <details><summary>Abstract</summary>
  Ensuring AI safety is crucial as large language models become increasinglyintegrated into real-world applications. A key challenge is jailbreak, whereadversarial prompts bypass built-in safeguards to elicit harmful disallowedoutputs. Inspired by psychological foot-in-the-door principles, we introduceFITD,a novel multi-turn jailbreak method that leverages the phenomenon whereminor initial commitments lower resistance to more significant or moreunethical transgressions. Our approach progressively escalates the maliciousintent of user queries through intermediate bridge prompts and aligns themodel's response by itself to induce toxic responses. Extensive experimentalresults on two jailbreak benchmarks demonstrate that FITD achieves an averageattack success rate of 94% across seven widely used models, outperformingexisting state-of-the-art methods. Additionally, we provide an in-depthanalysis of LLM self-corruption, highlighting vulnerabilities in currentalignment strategies and emphasizing the risks inherent in multi-turninteractions. The code is available athttps://github.com/Jinxiaolong1129/Foot-in-the-door-Jailbreak.
  </details>

- **[SuperGPQA: Scaling LLM Evaluation across 285 Graduate Disciplines](http://arxiv.org/abs/2502.14739v4)**  `arXiv:2502.14739`  
  _M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, et al._
  <details><summary>Abstract</summary>
  Large language models (LLMs) have demonstrated remarkable proficiency inmainstream academic disciplines such as mathematics, physics, and computerscience. However, human knowledge encompasses over 200 specialized disciplines,far exceeding the scope of existing benchmarks. The capabilities of LLMs inmany of these specialized fields-particularly in light industry, agriculture,and service-oriented disciplines-remain inadequately evaluated. To address thisgap, we present SuperGPQA, a comprehensive benchmark that evaluatesgraduate-level knowledge and reasoning capabilities across 285 disciplines. Ourbenchmark employs a novel Human-LLM collaborative filtering mechanism toeliminate trivial or ambiguous questions through iterative refinement based onboth LLM responses and expert feedback. Our experimental results revealsignificant room for improvement in the performance of current state-of-the-artLLMs across diverse knowledge domains (e.g., the reasoning-focused modelDeepSeek-R1 achieved the highest accuracy of 61.82% on SuperGPQA), highlightingthe considerable gap between current model capabilities and artificial generalintelligence. Additionally, we present comprehensive insights from ourmanagement of a large-scale annotation process, involving over 80 expertannotators and an interactive Human-LLM collaborative system, offering valuablemethodological guidance for future research initiatives of comparable scope.
  </details>

- **[Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance](http://arxiv.org/abs/2502.08127v2)**  `arXiv:2502.08127`  
  _Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Han Yi, Jimin Huang, et al._
  <details><summary>Abstract</summary>
  While large language models (LLMs) have shown strong general reasoningcapabilities, their effectiveness in financial reasoning, which is crucial forreal-world financial applications remains underexplored. In this study, weconduct a comprehensive evaluation of 24 state-of-the-art general andreasoning-focused LLMs across four complex financial reasoning tasks involvingfinancial text, tabular data, and equations. We assess key capabilities such asnumerical reasoning, tabular interpretation, financial terminologycomprehension, long-context understanding, and equation-based problem solving.Our analysis reveals that while data quality and pretraining contribute toperformance, general techniques like chain-of-thought (CoT) fine-tuning offerlimited gains in financial tasks. To address this, we propose twodomain-adapted models, Fino1-8B and Fino1-14B, trained with CoT fine-tuning andreinforcement learning using domain-specific reasoning paths. Our models aretrained on a carefully curated dataset integrating high-quality examples fromdiverse sources, covering financial reports, tables, equations, and structuredXBRL texts. Despite limited training data, they achieve an 7-9% performanceimprovement, outperforming several advanced LLMs, including GPT-o1,GPT-o3-mini, GPT-4.5, and comparable with DeepSeek models (V3 and R1),demonstrating strong practical value in resource, constrained scenarios. Ourfindings highlight the need for domain-specific adaptations in financialreasoning, and we release all datasets, models, and code for future research.
  </details>

- **[Evil twins are not that evil: Qualitative insights into machine-generated prompts](http://arxiv.org/abs/2412.08127v2)**  `arXiv:2412.08127`  
  _Nathana√´l Carraz Rakotonirina, Corentin Kervadec, Francesca Franzon, Marco Baroni_
  <details><summary>Abstract</summary>
  It has been widely observed that language models (LMs) respond in predictableways to algorithmically generated prompts that are seemingly unintelligible.This is both a sign that we lack a full understanding of how LMs work, and apractical challenge, because opaqueness can be exploited for harmful uses ofLMs, such as jailbreaking. We present the first thorough analysis of opaquemachine-generated prompts, or autoprompts, pertaining to 6 LMs of differentsizes and families. We find that machine-generated prompts are characterized bya last token that is often intelligible and strongly affects the generation. Asmall but consistent proportion of the previous tokens are prunable, probablyappearing in the prompt as a by-product of the fact that the optimizationprocess fixes the number of tokens. The remaining tokens fall into twocategories: filler tokens, which can be replaced with semantically unrelatedsubstitutes, and keywords, that tend to have at least a loose semantic relationwith the generation, although they do not engage in well-formed syntacticrelations with it. Additionally, human experts can reliably identify the mostinfluential tokens in an autoprompt a posteriori, suggesting these prompts arenot entirely opaque. Finally, some of the ablations we applied to autopromptsyield similar effects in natural language inputs, suggesting that autopromptsemerge naturally from the way LMs process linguistic inputs in general.
  </details>

- **[Output Scouting: Auditing Large Language Models for Catastrophic Responses](http://arxiv.org/abs/2410.05305v2)**  `arXiv:2410.05305`  
  _Andrew Bell, Joao Fonseca_
  <details><summary>Abstract</summary>
  Recent high profile incidents in which the use of Large Language Models(LLMs) resulted in significant harm to individuals have brought about a growinginterest in AI safety. One reason LLM safety issues occur is that models oftenhave at least some non-zero probability of producing harmful outputs. In thiswork, we explore the following scenario: imagine an AI safety auditor issearching for catastrophic responses from an LLM (e.g. a "yes" responses to"can I fire an employee for being pregnant?"), and is able to query the model alimited number times (e.g. 1000 times). What is a strategy for querying themodel that would efficiently find those failure responses? To this end, wepropose output scouting: an approach that aims to generate semantically fluentoutputs to a given prompt matching any target probability distribution. We thenrun experiments using two LLMs and find numerous examples of catastrophicresponses. We conclude with a discussion that includes advice for practitionerswho are looking to implement LLM auditing for catastrophic responses. We alsorelease an open-source toolkit (https://github.com/joaopfonseca/outputscouting)that implements our auditing framework using the Hugging Face transformerslibrary.
  </details>

- **[ProTrix: Building Models for Planning and Reasoning over Tables with Sentence Context](http://arxiv.org/abs/2403.02177v3)**  `arXiv:2403.02177`  
  _Zirui Wu, Yansong Feng_
  <details><summary>Abstract</summary>
  Tables play a crucial role in conveying information in various domains. Wepropose a Plan-then-Reason framework to answer different types of user queriesover tables with sentence context. The framework first plans the reasoningpaths over the context, then assigns each step to program-based or textualreasoning to reach the final answer. This framework enhances the tablereasoning abilities for both in-context learning and fine-tuning methods.GPT-3.5-Turbo following Plan-then-Reason framework surpasses other promptingbaselines without self-consistency while using less API calls and in-contextdemonstrations. We also construct an instruction tuning set TrixInstruct toevaluate the effectiveness of fine-tuning with this framework. We presentProTrix model family by finetuning models on TrixInstruct. Our experiments showthat ProTrix family generalizes to diverse unseen tabular tasks with only 6ktraining instances. We further demonstrate that ProTrix can generate accurateand faithful explanations to answer complex free-form questions. Our workunderscores the importance of the planning and reasoning abilities towards amodel over tabular tasks with generalizability and interpretability. Weopen-source our dataset and models at https://github.com/WilliamZR/ProTrix.
  </details>

- **[Dynamically Allocated Interval-Based Generative Linguistic Steganography with Roulette Wheel](http://arxiv.org/abs/2401.15656v5)**  `arXiv:2401.15656`  
  _Yihao Wang, Ruiqi Song, Lingxiao Li, Ru Zhang, Jianyi Liu_
  <details><summary>Abstract</summary>
  Existing linguistic steganography schemes often overlook the conditionalprobability (CP) of tokens in the candidate pool, allocating the one coding toall tokens, which results in identical selection likelihoods. This approachleads to the selection of low-CP tokens, degrading the quality of stegos andmaking them more detectable. This paper proposes a scheme based on the intervalallocated, called DAIRstega. DAIRstega first uses a portion of the read secretto build the roulette area. Then, this scheme uses the idea of the roulettewheel and takes the CPs of tokens as the main basis for allocating the roulettearea (i.e., the interval length). Thus, tokens with larger CPs are allocatedmore area. The secret will have an increased likelihood of selecting a tokenwith a higher CP. During allocation, we designed some allocation functions andthree constraints to optimize the process. Additionally, DAIRstega supportsprompt-based controllable generation of stegos. Rich experiments show that theproposed embedding way and DAIRstega perform better than the existing ways andbaselines, which shows strong perceptual, statistical, and semanticconcealment, as well as anti-steganalysis ability. It can also generatehigh-quality longer stegos, addressing the deficiencies in this task. DAIRstegais confirmed to have potential as a secure watermarking, offering insights forits development.
  </details>

- **[Self-Rewarding Language Models](http://arxiv.org/abs/2401.10020v3)**  `arXiv:2401.10020`  
  _Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, et al._
  <details><summary>Abstract</summary>
  We posit that to achieve superhuman agents, future models require superhumanfeedback in order to provide an adequate training signal. Current approachescommonly train reward models from human preferences, which may then bebottlenecked by human performance level, and secondly these separate frozenreward models cannot then learn to improve during LLM training. In this work,we study Self-Rewarding Language Models, where the language model itself isused via LLM-as-a-Judge prompting to provide its own rewards during training.We show that during Iterative DPO training that not only does instructionfollowing ability improve, but also the ability to provide high-quality rewardsto itself. Fine-tuning Llama 2 70B on three iterations of our approach yields amodel that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,including Claude 2, Gemini Pro, and GPT-4 0613. While there is much left stillto explore, this work opens the door to the possibility of models that cancontinually improve in both axes.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Computer Vision and Pattern Recognition üì∏

<details open><summary>Click to Collapse</summary>

- **[Q-Insight: Understanding Image Quality via Visual Reinforcement Learning](http://arxiv.org/abs/2503.22679v1)**  `arXiv:2503.22679`  
  _Weiqi Li, Xuanyu Zhang, Shijie Zhao, Yabin Zhang, Junlin Li, Li Zhang, et al._
  <details><summary>Abstract</summary>
  Image quality assessment (IQA) focuses on the perceptual visual quality ofimages, playing a crucial role in downstream tasks such as imagereconstruction, compression, and generation. The rapid advancement ofmulti-modal large language models (MLLMs) has significantly broadened the scopeof IQA, moving toward comprehensive image quality understanding thatincorporates content analysis, degradation perception, and comparison reasoningbeyond mere numerical scoring. Previous MLLM-based methods typically eithergenerate numerical scores lacking interpretability or heavily rely onsupervised fine-tuning (SFT) using large-scale annotated datasets to providedescriptive assessments, limiting their flexibility and applicability. In thispaper, we propose Q-Insight, a reinforcement learning-based model built upongroup relative policy optimization (GRPO), which demonstrates strong visualreasoning capability for image quality understanding while requiring only alimited amount of rating scores and degradation labels. By jointly optimizingscore regression and degradation perception tasks with carefully designedreward functions, our approach effectively exploits their mutual benefits forenhanced performance. Extensive experiments demonstrate that Q-Insightsubstantially outperforms existing state-of-the-art methods in both scoreregression and degradation perception tasks, while exhibiting impressivezero-shot generalization to comparison reasoning tasks. Code will be availableat https://github.com/lwq20020127/Q-Insight.
  </details>

- **[DSO: Aligning 3D Generators with Simulation Feedback for Physical Soundness](http://arxiv.org/abs/2503.22677v1)**  `arXiv:2503.22677`  
  _Ruining Li, Chuanxia Zheng, Christian Rupprecht, Andrea Vedaldi_
  <details><summary>Abstract</summary>
  Most 3D object generators focus on aesthetic quality, often neglectingphysical constraints necessary in applications. One such constraint is that the3D object should be self-supporting, i.e., remains balanced under gravity.Prior approaches to generating stable 3D objects used differentiable physicssimulators to optimize geometry at test-time, which is slow, unstable, andprone to local optima. Inspired by the literature on aligning generative modelsto external feedback, we propose Direct Simulation Optimization (DSO), aframework to use the feedback from a (non-differentiable) simulator to increasethe likelihood that the 3D generator outputs stable 3D objects directly. Weconstruct a dataset of 3D objects labeled with a stability score obtained fromthe physics simulator. We can then fine-tune the 3D generator using thestability score as the alignment metric, via direct preference optimization(DPO) or direct reward optimization (DRO), a novel objective, which weintroduce, to align diffusion models without requiring pairwise preferences.Our experiments show that the fine-tuned feed-forward generator, using eitherDPO or DRO objective, is much faster and more likely to produce stable objectsthan test-time optimization. Notably, the DSO framework works even without anyground-truth 3D objects for training, allowing the 3D generator to self-improveby automatically collecting simulation feedback on its own outputs.
  </details>

- **[TranSplat: Lighting-Consistent Cross-Scene Object Transfer with 3D Gaussian Splatting](http://arxiv.org/abs/2503.22676v1)**  `arXiv:2503.22676`  
  _Boyang, Yu, Yanlin Jin, Ashok Veeraraghavan, Akshat Dave, Guha Balakrishnan_
  <details><summary>Abstract</summary>
  We present TranSplat, a 3D scene rendering algorithm that enables realisticcross-scene object transfer (from a source to a target scene) based on theGaussian Splatting framework. Our approach addresses two critical challenges:(1) precise 3D object extraction from the source scene, and (2) faithfulrelighting of the transferred object in the target scene without explicitmaterial property estimation. TranSplat fits a splatting model to the sourcescene, using 2D object masks to drive fine-grained 3D segmentation. Followinguser-guided insertion of the object into the target scene, along with automaticrefinement of position and orientation, TranSplat derives per-Gaussian radiancetransfer functions via spherical harmonic analysis to adapt the object'sappearance to match the target scene's lighting environment. This relightingstrategy does not require explicitly estimating physical scene properties suchas BRDFs. Evaluated on several synthetic and real-world scenes and objects,TranSplat yields excellent 3D object extractions and relighting performancecompared to recent baseline methods and visually convincing cross-scene objecttransfers. We conclude by discussing the limitations of the approach.
  </details>

- **[Understanding Co-speech Gestures in-the-wild](http://arxiv.org/abs/2503.22668v1)**  `arXiv:2503.22668`  
  _Sindhu B Hegde, K R Prajwal, Taein Kwon, Andrew Zisserman_
  <details><summary>Abstract</summary>
  Co-speech gestures play a vital role in non-verbal communication. In thispaper, we introduce a new framework for co-speech gesture understanding in thewild. Specifically, we propose three new tasks and benchmarks to evaluate amodel's capability to comprehend gesture-text-speech associations: (i)gesture-based retrieval, (ii) gestured word spotting, and (iii) active speakerdetection using gestures. We present a new approach that learns a tri-modalspeech-text-video-gesture representation to solve these tasks. By leveraging acombination of global phrase contrastive loss and local gesture-word couplingloss, we demonstrate that a strong gesture representation can be learned in aweakly supervised manner from videos in the wild. Our learned representationsoutperform previous methods, including large vision-language models (VLMs),across all three tasks. Further analysis reveals that speech and textmodalities capture distinct gesture-related signals, underscoring theadvantages of learning a shared tri-modal embedding space. The dataset, model,and code are available at: https://www.robots.ox.ac.uk/~vgg/research/jegal
  </details>

- **[Zero4D: Training-Free 4D Video Generation From Single Video Using Off-the-Shelf Video Diffusion Model](http://arxiv.org/abs/2503.22622v1)**  `arXiv:2503.22622`  
  _Jangho Park, Taesung Kwon, Jong Chul Ye_
  <details><summary>Abstract</summary>
  Recently, multi-view or 4D video generation has emerged as a significantresearch topic. Nonetheless, recent approaches to 4D generation still strugglewith fundamental limitations, as they primarily rely on harnessing multiplevideo diffusion models with additional training or compute-intensive trainingof a full 4D diffusion model with limited real-world 4D data and largecomputational costs. To address these challenges, here we propose the firsttraining-free 4D video generation method that leverages the off-the-shelf videodiffusion models to generate multi-view videos from a single input video. Ourapproach consists of two key steps: (1) By designating the edge frames in thespatio-temporal sampling grid as key frames, we first synthesize them using avideo diffusion model, leveraging a depth-based warping technique for guidance.This approach ensures structural consistency across the generated frames,preserving spatial and temporal coherence. (2) We then interpolate theremaining frames using a video diffusion model, constructing a fully populatedand temporally coherent sampling grid while preserving spatial and temporalconsistency. Through this approach, we extend a single video into a multi-viewvideo along novel camera trajectories while maintaining spatio-temporalconsistency. Our method is training-free and fully utilizes an off-the-shelfvideo diffusion model, offering a practical and effective solution formulti-view video generation.
  </details>

- **[Breaking Language Barriers in Visual Language Models via Multilingual Textual Regularization](http://arxiv.org/abs/2503.22577v1)**  `arXiv:2503.22577`  
  _I√±igo Pikabea, I√±aki Lacunza, Oriol Pareras, Carlos Escolano, Aitor Gonzalez-Agirre, Javier Hernando, et al._
  <details><summary>Abstract</summary>
  Rapid advancements in Visual Language Models (VLMs) have transformedmultimodal understanding but are often constrained by generating Englishresponses regardless of the input language. This phenomenon has been termed asImage-induced Fidelity Loss (IFL) and stems from limited multimodalmultilingual training data. To address this, we propose a continuousmultilingual integration strategy that injects text-only multilingual dataduring visual instruction tuning, preserving the language model's originalmultilingual capabilities. Extensive evaluations demonstrate that our approachsignificantly improves linguistic fidelity across languages without degradationin visual performance. We also explore model merging, which improves languagefidelity but comes at the cost of visual performance. In contrast, our coremethod achieves robust multilingual alignment without trade-offs, offering ascalable and effective path to mitigating IFL for global VLM adoption.
  </details>

- **[Image Decomposition with G-norm Weighted by Total Symmetric Variation](http://arxiv.org/abs/2503.22560v1)**  `arXiv:2503.22560`  
  _Roy Y. He, Martin Huska, Hao Liu_
  <details><summary>Abstract</summary>
  In this paper, we propose a novel variational model for decomposing imagesinto their respective cartoon and texture parts. Our model characterizescertain non-local features of any Bounded Variation (BV) image by its TotalSymmetric Variation (TSV). We demonstrate that TSV is effective in identifyingregional boundaries. Based on this property, we introduce a weighted Meyer's$G$-norm to identify texture interiors without including contour edges. For BVimages with bounded TSV, we show that the proposed model admits a solution.Additionally, we design a fast algorithm based on operator-splitting to tacklethe associated non-convex optimization problem. The performance of our methodis validated by a series of numerical experiments.
  </details>

- **[MO-CTranS: A unified multi-organ segmentation model learning from multiple heterogeneously labelled datasets](http://arxiv.org/abs/2503.22557v1)**  `arXiv:2503.22557`  
  _Zhendi Gong, Susan Francis, Eleanor Cox, Stamatios N. Sotiropoulos, Dorothee P. Auer, Guoping Qiu, et al._
  <details><summary>Abstract</summary>
  Multi-organ segmentation holds paramount significance in many clinical tasks.In practice, compared to large fully annotated datasets, multiple smalldatasets are often more accessible and organs are not labelled consistently.Normally, an individual model is trained for each of these datasets, which isnot an effective way of using data for model learning. It remains challengingto train a single model that can robustly learn from several partially labelleddatasets due to label conflict and data imbalance problems. We proposeMO-CTranS: a single model that can overcome such problems. MO-CTranS contains aCNN-based encoder and a Transformer-based decoder, which are connected in amulti-resolution manner. Task-specific tokens are introduced in the decoder tohelp differentiate label discrepancies. Our method was evaluated and comparedto several baseline models and state-of-the-art (SOTA) solutions on abdominalMRI datasets that were acquired in different views (i.e. axial and coronal) andannotated for different organs (i.e. liver, kidney, spleen). Our methodachieved better performance (most were statistically significant) than thecompared methods. Github link: https://github.com/naisops/MO-CTranS.
  </details>

- **[LIM: Large Interpolator Model for Dynamic Reconstruction](http://arxiv.org/abs/2503.22537v1)**  `arXiv:2503.22537`  
  _Remy Sabathier, Niloy J. Mitra, David Novotny_
  <details><summary>Abstract</summary>
  Reconstructing dynamic assets from video data is central to many in computervision and graphics tasks. Existing 4D reconstruction approaches are limited bycategory-specific models or slow optimization-based methods. Inspired by therecent Large Reconstruction Model (LRM), we present the Large InterpolationModel (LIM), a transformer-based feed-forward solution, guided by a novelcausal consistency loss, for interpolating implicit 3D representations acrosstime. Given implicit 3D representations at times $t_0$ and $t_1$, LIM producesa deformed shape at any continuous time $t\in[t_0,t_1]$, deliveringhigh-quality interpolated frames in seconds. Furthermore, LIM allows explicitmesh tracking across time, producing a consistently uv-textured mesh sequenceready for integration into existing production pipelines. We also use LIM, inconjunction with a diffusion-based multiview generator, to produce dynamic 4Dreconstructions from monocular videos. We evaluate LIM on various dynamicdatasets, benchmarking against image-space interpolation methods (e.g., FiLM)and direct triplane linear interpolation, and demonstrate clear advantages. Insummary, LIM is the first feed-forward model capable of high-speed tracked 4Dasset reconstruction across diverse categories.
  </details>

- **[AnnoPage Dataset: Dataset of Non-Textual Elements in Documents with Fine-Grained Categorization](http://arxiv.org/abs/2503.22526v1)**  `arXiv:2503.22526`  
  _Martin Ki≈°≈°, Michal Hradi≈°, Martina Dvo≈ô√°kov√°, V√°clav Jirou≈°ek, Filip Kersch_
  <details><summary>Abstract</summary>
  We introduce the AnnoPage Dataset, a novel collection of 7550 pages fromhistorical documents, primarily in Czech and German, spanning from 1485 to thepresent, focusing on the late 19th and early 20th centuries. The dataset isdesigned to support research in document layout analysis and object detection.Each page is annotated with axis-aligned bounding boxes (AABB) representingelements of 25 categories of non-textual elements, such as images, maps,decorative elements, or charts, following the Czech Methodology of imagedocument processing. The annotations were created by expert librarians toensure accuracy and consistency. The dataset also incorporates pages frommultiple, mainly historical, document datasets to enhance variability andmaintain continuity. The dataset is divided into development and test subsets,with the test set carefully selected to maintain the category distribution. Weprovide baseline results using YOLO and DETR object detectors, offering areference point for future research. The AnnoPage Dataset is publicly availableon Zenodo (https://doi.org/10.5281/zenodo.12788419), along with ground-truthannotations in YOLO format.
  </details>

- **[Masked Self-Supervised Pre-Training for Text Recognition Transformers on Large-Scale Datasets](http://arxiv.org/abs/2503.22513v1)**  `arXiv:2503.22513`  
  _Martin Ki≈°≈°, Michal Hradi≈°_
  <details><summary>Abstract</summary>
  Self-supervised learning has emerged as a powerful approach for leveraginglarge-scale unlabeled data to improve model performance in various domains. Inthis paper, we explore masked self-supervised pre-training for text recognitiontransformers. Specifically, we propose two modifications to the pre-trainingphase: progressively increasing the masking probability, and modifying the lossfunction to incorporate both masked and non-masked patches. We conductextensive experiments using a dataset of 50M unlabeled text lines forpre-training and four differently sized annotated datasets for fine-tuning.Furthermore, we compare our pre-trained models against those trained withtransfer learning, demonstrating the effectiveness of the self-supervisedpre-training. In particular, pre-training consistently improves the charactererror rate of models, in some cases up to 30 % relatively. It is also on parwith transfer learning but without relying on extra annotated text lines.
  </details>

- **[SemAlign3D: Semantic Correspondence between RGB-Images through Aligning 3D Object-Class Representations](http://arxiv.org/abs/2503.22462v1)**  `arXiv:2503.22462`  
  _Krispin Wandel, Hesheng Wang_
  <details><summary>Abstract</summary>
  Semantic correspondence made tremendous progress through the recentadvancements of large vision models (LVM). While these LVMs have been shown toreliably capture local semantics, the same can currently not be said forcapturing global geometric relationships between semantic object regions. Thisproblem leads to unreliable performance for semantic correspondence betweenimages with extreme view variation. In this work, we aim to leverage monoculardepth estimates to capture these geometric relationships for more robust anddata-efficient semantic correspondence. First, we introduce a simple buteffective method to build 3D object-class representations from monocular depthestimates and LVM features using a sparsely annotated image correspondencedataset. Second, we formulate an alignment energy that can be minimized usinggradient descent to obtain an alignment between the 3D object-classrepresentation and the object-class instance in the input RGB-image. Our methodachieves state-of-the-art matching accuracy in multiple categories on thechallenging SPair-71k dataset, increasing the PCK@0.1 score by more than 10points on three categories and overall by 3.3 points from 85.6% to 88.9%.Additional resources and code are available at https://dub.sh/semalign3d.
  </details>

- **[EndoLRMGS: Complete Endoscopic Scene Reconstruction combining Large Reconstruction Modelling and Gaussian Splatting](http://arxiv.org/abs/2503.22437v1)**  `arXiv:2503.22437`  
  _Xu Wang, Shuai Zhang, Baoru Huang, Danail Stoyanov, Evangelos B. Mazomenos_
  <details><summary>Abstract</summary>
  Complete reconstruction of surgical scenes is crucial for robot-assistedsurgery (RAS). Deep depth estimation is promising but existing works strugglewith depth discontinuities, resulting in noisy predictions at object boundariesand do not achieve complete reconstruction omitting occluded surfaces. Toaddress these issues we propose EndoLRMGS, that combines Large ReconstructionModelling (LRM) and Gaussian Splatting (GS), for complete surgical scenereconstruction. GS reconstructs deformable tissues and LRM generates 3D modelsfor surgical tools while position and scale are subsequently optimized byintroducing orthogonal perspective joint projection optimization (OPjPO) toenhance accuracy. In experiments on four surgical videos from three publicdatasets, our method improves the Intersection-over-union (IoU) of tool 3Dmodels in 2D projections by>40%. Additionally, EndoLRMGS improves the PSNR ofthe tools projection from 3.82% to 11.07%. Tissue rendering quality alsoimproves, with PSNR increasing from 0.46% to 49.87%, and SSIM from 1.53% to29.21% across all test videos.
  </details>

- **[NuGrounding: A Multi-View 3D Visual Grounding Framework in Autonomous Driving](http://arxiv.org/abs/2503.22436v1)**  `arXiv:2503.22436`  
  _Fuhao Li, Huan Jin, Bin Gao, Liaoyuan Fan, Lihui Jiang, Long Zeng_
  <details><summary>Abstract</summary>
  Multi-view 3D visual grounding is critical for autonomous driving vehicles tointerpret natural languages and localize target objects in complexenvironments. However, existing datasets and methods suffer from coarse-grainedlanguage instructions, and inadequate integration of 3D geometric reasoningwith linguistic comprehension. To this end, we introduce NuGrounding, the firstlarge-scale benchmark for multi-view 3D visual grounding in autonomous driving.We present a Hierarchy of Grounding (HoG) method to construct NuGrounding togenerate hierarchical multi-level instructions, ensuring comprehensive coverageof human instruction patterns. To tackle this challenging dataset, we propose anovel paradigm that seamlessly combines instruction comprehension abilities ofmulti-modal LLMs (MLLMs) with precise localization abilities of specialistdetection models. Our approach introduces two decoupled task tokens and acontext query to aggregate 3D geometric information and semantic instructions,followed by a fusion decoder to refine spatial-semantic feature fusion forprecise localization. Extensive experiments demonstrate that our methodsignificantly outperforms the baselines adapted from representative 3D sceneunderstanding methods by a significant margin and achieves 0.59 in precisionand 0.64 in recall, with improvements of 50.8% and 54.7%.
  </details>

- **[MVSAnywhere: Zero-Shot Multi-View Stereo](http://arxiv.org/abs/2503.22430v1)**  `arXiv:2503.22430`  
  _Sergio Izquierdo, Mohamed Sayed, Michael Firman, Guillermo Garcia-Hernando, Daniyar Turmukhambetov, Javier Civera, et al._
  <details><summary>Abstract</summary>
  Computing accurate depth from multiple views is a fundamental andlongstanding challenge in computer vision. However, most existing approaches donot generalize well across different domains and scene types (e.g. indoor vs.outdoor). Training a general-purpose multi-view stereo model is challenging andraises several questions, e.g. how to best make use of transformer-basedarchitectures, how to incorporate additional metadata when there is a variablenumber of input views, and how to estimate the range of valid depths which canvary considerably across different scenes and is typically not known a priori?To address these issues, we introduce MVSA, a novel and versatile Multi-ViewStereo architecture that aims to work Anywhere by generalizing across diversedomains and depth ranges. MVSA combines monocular and multi-view cues with anadaptive cost volume to deal with scale-related issues. We demonstratestate-of-the-art zero-shot depth estimation on the Robust Multi-View DepthBenchmark, surpassing existing multi-view stereo and monocular baselines.
  </details>

- **[Unveiling the Mist over 3D Vision-Language Understanding: Object-centric Evaluation with Chain-of-Analysis](http://arxiv.org/abs/2503.22420v1)**  `arXiv:2503.22420`  
  _Jiangyong Huang, Baoxiong Jia, Yan Wang, Ziyu Zhu, Xiongkun Linghu, Qing Li, et al._
  <details><summary>Abstract</summary>
  Existing 3D vision-language (3D-VL) benchmarks fall short in evaluating 3D-VLmodels, creating a "mist" that obscures rigorous insights into modelcapabilities and 3D-VL tasks. This mist persists due to three key limitations.First, flawed test data, like ambiguous referential text in the grounding task,can yield incorrect and unreliable test results. Second, oversimplified metricssuch as simply averaging accuracy per question answering (QA) pair, cannotreveal true model capability due to their vulnerability to language variations.Third, existing benchmarks isolate the grounding and QA tasks, disregarding theunderlying coherence that QA should be based on solid grounding capabilities.To unveil the "mist", we propose Beacon3D, a benchmark for 3D-VL grounding andQA tasks, delivering a perspective shift in the evaluation of 3D-VLunderstanding. Beacon3D features (i) high-quality test data with precise andnatural language, (ii) object-centric evaluation with multiple tests per objectto ensure robustness, and (iii) a novel chain-of-analysis paradigm to addresslanguage robustness and model performance coherence across grounding and QA.Our evaluation of state-of-the-art 3D-VL models on Beacon3D reveals that (i)object-centric evaluation elicits true model performance and particularly weakgeneralization in QA; (ii) grounding-QA coherence remains fragile in current3D-VL models, and (iii) incorporating large language models (LLMs) to 3D-VLmodels, though as a prevalent practice, hinders grounding capabilities and hasyet to elevate QA capabilities. We hope Beacon3D and our comprehensive analysiscould benefit the 3D-VL community towards faithful developments.
  </details>

- **[DF2023: The Digital Forensics 2023 Dataset for Image Forgery Detection](http://arxiv.org/abs/2503.22417v1)**  `arXiv:2503.22417`  
  _David Fischinger, Martin Boyer_
  <details><summary>Abstract</summary>
  The deliberate manipulation of public opinion, especially through alteredimages, which are frequently disseminated through online social networks, posesa significant danger to society. To fight this issue on a technical level wesupport the research community by releasing the Digital Forensics 2023 (DF2023)training and validation dataset, comprising one million images from four majorforgery categories: splicing, copy-move, enhancement and removal. This datasetenables an objective comparison of network architectures and can significantlyreduce the time and effort of researchers preparing datasets.
  </details>

- **[Modeling Multiple Normal Action Representations for Error Detection in Procedural Tasks](http://arxiv.org/abs/2503.22405v1)**  `arXiv:2503.22405`  
  _Wei-Jin Huang, Yuan-Ming Li, Zhi-Wei Xia, Yu-Ming Tang, Kun-Yu Lin, Jian-Fang Hu, et al._
  <details><summary>Abstract</summary>
  Error detection in procedural activities is essential for consistent andcorrect outcomes in AR-assisted and robotic systems. Existing methods oftenfocus on temporal ordering errors or rely on static prototypes to representnormal actions. However, these approaches typically overlook the commonscenario where multiple, distinct actions are valid following a given sequenceof executed actions. This leads to two issues: (1) the model cannot effectivelydetect errors using static prototypes when the inference environment or actionexecution distribution differs from training; and (2) the model may also usethe wrong prototypes to detect errors if the ongoing action label is not thesame as the predicted one. To address this problem, we propose an AdaptiveMultiple Normal Action Representation (AMNAR) framework. AMNAR predicts allvalid next actions and reconstructs their corresponding normal actionrepresentations, which are compared against the ongoing action to detecterrors. Extensive experiments demonstrate that AMNAR achieves state-of-the-artperformance, highlighting the effectiveness of AMNAR and the importance ofmodeling multiple valid next actions in error detection. The code is availableat https://github.com/iSEE-Laboratory/AMNAR.
  </details>

- **[VITAL: More Understandable Feature Visualization through Distribution Alignment and Relevant Information Flow](http://arxiv.org/abs/2503.22399v1)**  `arXiv:2503.22399`  
  _Ada Gorgun, Bernt Schiele, Jonas Fischer_
  <details><summary>Abstract</summary>
  Neural networks are widely adopted to solve complex and challenging tasks.Especially in high-stakes decision-making, understanding their reasoningprocess is crucial, yet proves challenging for modern deep networks. Featurevisualization (FV) is a powerful tool to decode what information neurons areresponding to and hence to better understand the reasoning behind suchnetworks. In particular, in FV we generate human-understandable images thatreflect the information detected by neurons of interest. However, currentmethods often yield unrecognizable visualizations, exhibiting repetitivepatterns and visual artifacts that are hard to understand for a human. Toaddress these problems, we propose to guide FV through statistics of real imagefeatures combined with measures of relevant network flow to generateprototypical images. Our approach yields human-understandable visualizationsthat both qualitatively and quantitatively improve over state-of-the-art FVsacross various architectures. As such, it can be used to decode whichinformation the network uses, complementing mechanistic circuits that identifywhere it is encoded. Code is available at: https://github.com/adagorgun/VITAL
  </details>

- **[DF-Net: The Digital Forensics Network for Image Forgery Detection](http://arxiv.org/abs/2503.22398v1)**  `arXiv:2503.22398`  
  _David Fischinger, Martin Boyer_
  <details><summary>Abstract</summary>
  The orchestrated manipulation of public opinion, particularly throughmanipulated images, often spread via online social networks (OSN), has become aserious threat to society. In this paper we introduce the Digital Forensics Net(DF-Net), a deep neural network for pixel-wise image forgery detection. Thereleased model outperforms several state-of-the-art methods on four establishedbenchmark datasets. Most notably, DF-Net's detection is robust against lossyimage operations (e.g resizing, compression) as they are automaticallyperformed by social networks.
  </details>

- **[GAITGen: Disentangled Motion-Pathology Impaired Gait Generative Model -- Bringing Motion Generation to the Clinical Domain](http://arxiv.org/abs/2503.22397v1)**  `arXiv:2503.22397`  
  _Vida Adeli, Soroush Mehraban, Majid Mirmehdi, Alan Whone, Benjamin Filtjens, Amirhossein Dadashzadeh, et al._
  <details><summary>Abstract</summary>
  Gait analysis is crucial for the diagnosis and monitoring of movementdisorders like Parkinson's Disease. While computer vision models have shownpotential for objectively evaluating parkinsonian gait, their effectiveness islimited by scarce clinical datasets and the challenge of collecting large andwell-labelled data, impacting model accuracy and risk of bias. To address thesegaps, we propose GAITGen, a novel framework that generates realistic gaitsequences conditioned on specified pathology severity levels. GAITGen employs aConditional Residual Vector Quantized Variational Autoencoder to learndisentangled representations of motion dynamics and pathology-specific factors,coupled with Mask and Residual Transformers for conditioned sequencegeneration. GAITGen generates realistic, diverse gait sequences across severitylevels, enriching datasets and enabling large-scale model training inparkinsonian gait analysis. Experiments on our new PD-GaM (real) datasetdemonstrate that GAITGen outperforms adapted state-of-the-art models in bothreconstruction fidelity and generation quality, accurately capturing criticalpathology-specific gait features. A clinical user study confirms the realismand clinical relevance of our generated sequences. Moreover, incorporatingGAITGen-generated data into downstream tasks improves parkinsonian gaitseverity estimation, highlighting its potential for advancing clinical gaitanalysis.
  </details>

- **[Endo-TTAP: Robust Endoscopic Tissue Tracking via Multi-Facet Guided Attention and Hybrid Flow-point Supervision](http://arxiv.org/abs/2503.22394v1)**  `arXiv:2503.22394`  
  _Rulin Zhou, Wenlong He, An Wang, Qiqi Yao, Haijun Hu, Jiankun Wang, et al._
  <details><summary>Abstract</summary>
  Accurate tissue point tracking in endoscopic videos is critical forrobotic-assisted surgical navigation and scene understanding, but remainschallenging due to complex deformations, instrument occlusion, and the scarcityof dense trajectory annotations. Existing methods struggle with long-termtracking under these conditions due to limited feature utilization andannotation dependence. We present Endo-TTAP, a novel framework addressing thesechallenges through: (1) A Multi-Facet Guided Attention (MFGA) module thatsynergizes multi-scale flow dynamics, DINOv2 semantic embeddings, and explicitmotion patterns to jointly predict point positions with uncertainty andocclusion awareness; (2) A two-stage curriculum learning strategy employing anAuxiliary Curriculum Adapter (ACA) for progressive initialization and hybridsupervision. Stage I utilizes synthetic data with optical flow ground truth foruncertainty-occlusion regularization, while Stage II combines unsupervised flowconsistency and semi-supervised learning with refined pseudo-labels fromoff-the-shelf trackers. Extensive validation on two MICCAI Challenge datasetsand our collected dataset demonstrates that Endo-TTAP achieves state-of-the-artperformance in tissue point tracking, particularly in scenarios characterizedby complex endoscopic conditions. The source code and dataset will be availableat https://anonymous.4open.science/r/Endo-TTAP-36E5.
  </details>

- **[Data Quality Matters: Quantifying Image Quality Impact on Machine Learning Performance](http://arxiv.org/abs/2503.22375v1)**  `arXiv:2503.22375`  
  _Christian Steinhauser, Philipp Reis, Hubert Padusinski, Jacob Langner, Eric Sax_
  <details><summary>Abstract</summary>
  Precise perception of the environment is essential in highly automateddriving systems, which rely on machine learning tasks such as object detectionand segmentation. Compression of sensor data is commonly used for datahandling, while virtualization is used for hardware-in-the-loop validation.Both methods can alter sensor data and degrade model performance. Thisnecessitates a systematic approach to quantifying image validity. This paperpresents a four-step framework to evaluate the impact of image modifications onmachine learning tasks. First, a dataset with modified images is prepared toensure one-to-one matching image pairs, enabling measurement of deviationsresulting from compression and virtualization. Second, image deviations arequantified by comparing the effects of compression and virtualization againstoriginal camera-based sensor data. Third, the performance of state-of-the-artobject detection models is analyzed to determine how altered input data affectsperception tasks, including bounding box accuracy and reliability. Finally, acorrelation analysis is performed to identify relationships between imagequality and model performance. As a result, the LPIPS metric achieves thehighest correlation between image deviation and machine learning performanceacross all evaluated machine learning tasks.
  </details>

- **[ViSketch-GPT: Collaborative Multi-Scale Feature Extraction for Sketch Recognition and Generation](http://arxiv.org/abs/2503.22374v1)**  `arXiv:2503.22374`  
  _Giulio Federico, Giuseppe Amato, Fabio Carrara, Claudio Gennaro, Marco Di Benedetto_
  <details><summary>Abstract</summary>
  Understanding the nature of human sketches is challenging because of the widevariation in how they are created. Recognizing complex structural patternsimproves both the accuracy in recognizing sketches and the fidelity of thegenerated sketches. In this work, we introduce ViSketch-GPT, a novel algorithmdesigned to address these challenges through a multi-scale context extractionapproach. The model captures intricate details at multiple scales and combinesthem using an ensemble-like mechanism, where the extracted features workcollaboratively to enhance the recognition and generation of key detailscrucial for classification and generation tasks.  The effectiveness of ViSketch-GPT is validated through extensive experimentson the QuickDraw dataset. Our model establishes a new benchmark, significantlyoutperforming existing methods in both classification and generation tasks,with substantial improvements in accuracy and the fidelity of generatedsketches.  The proposed algorithm offers a robust framework for understanding complexstructures by extracting features that collaborate to recognize intricatedetails, enhancing the understanding of structures like sketches and making ita versatile tool for various applications in computer vision and machinelearning.
  </details>

- **[ForcePose: A Deep Learning Approach for Force Calculation Based on Action Recognition Using MediaPipe Pose Estimation Combined with Object Detection](http://arxiv.org/abs/2503.22363v1)**  `arXiv:2503.22363`  
  _Nandakishor M, Vrinda Govind V, Anuradha Puthalath, Anzy L, Swathi P S, Aswathi R, et al._
  <details><summary>Abstract</summary>
  Force estimation in human-object interactions is crucial for various fieldslike ergonomics, physical therapy, and sports science. Traditional methodsdepend on specialized equipment such as force plates and sensors, which makesaccurate assessments both expensive and restricted to laboratory settings. Inthis paper, we introduce ForcePose, a novel deep learning framework thatestimates applied forces by combining human pose estimation with objectdetection. Our approach leverages MediaPipe for skeletal tracking and SSDMobileNet for object recognition to create a unified representation ofhuman-object interaction. We've developed a specialized neural network thatprocesses both spatial and temporal features to predict force magnitude anddirection without needing any physical sensors. After training on our datasetof 850 annotated videos with corresponding force measurements, our modelachieves a mean absolute error of 5.83 N in force magnitude and 7.4 degrees inforce direction. When compared to existing computer vision approaches, ourmethod performs 27.5% better while still offering real-time performance onstandard computing hardware. ForcePose opens up new possibilities for forceanalysis in diverse real-world scenarios where traditional measurement toolsare impractical or intrusive. This paper discusses our methodology, the datasetcreation process, evaluation metrics, and potential applications acrossrehabilitation, ergonomics assessment, and athletic performance analysis.
  </details>

- **[Mitigating Knowledge Discrepancies among Multiple Datasets for Task-agnostic Unified Face Alignment](http://arxiv.org/abs/2503.22359v1)**  `arXiv:2503.22359`  
  _Jiahao Xia, Min Xu, Wenjian Huang, Jianguo Zhang, Haimin Zhang, Chunxia Xiao_
  <details><summary>Abstract</summary>
  Despite the similar structures of human faces, existing face alignmentmethods cannot learn unified knowledge from multiple datasets with differentlandmark annotations. The limited training samples in a single dataset commonlyresult in fragile robustness in this field. To mitigate knowledge discrepanciesamong different datasets and train a task-agnostic unified face alignment(TUFA) framework, this paper presents a strategy to unify knowledge frommultiple datasets. Specifically, we calculate a mean face shape for eachdataset. To explicitly align these mean shapes on an interpretable plane basedon their semantics, each shape is then incorporated with a group of semanticalignment embeddings. The 2D coordinates of these aligned shapes can be viewedas the anchors of the plane. By encoding them into structure prompts andfurther regressing the corresponding facial landmarks using image features, amapping from the plane to the target faces is finally established, whichunifies the learning target of different datasets. Consequently, multipledatasets can be utilized to boost the generalization ability of the model. Thesuccessful mitigation of discrepancies also enhances the efficiency ofknowledge transferring to a novel dataset, significantly boosts the performanceof few-shot face alignment. Additionally, the interpretable plane endows TUFAwith a task-agnostic characteristic, enabling it to locate landmarks unseenduring training in a zero-shot manner. Extensive experiments are carried onseven benchmarks and the results demonstrate an impressive improvement in facealignment brought by knowledge discrepancies mitigation.
  </details>

- **[EchoFlow: A Foundation Model for Cardiac Ultrasound Image and Video Generation](http://arxiv.org/abs/2503.22357v1)**  `arXiv:2503.22357`  
  _Hadrien Reynaud, Alberto Gomez, Paul Leeson, Qingjie Meng, Bernhard Kainz_
  <details><summary>Abstract</summary>
  Advances in deep learning have significantly enhanced medical image analysis,yet the availability of large-scale medical datasets remains constrained bypatient privacy concerns. We present EchoFlow, a novel framework designed togenerate high-quality, privacy-preserving synthetic echocardiogram images andvideos. EchoFlow comprises four key components: an adversarial variationalautoencoder for defining an efficient latent representation of cardiacultrasound images, a latent image flow matching model for generating accuratelatent echocardiogram images, a latent re-identification model to ensureprivacy by filtering images anatomically, and a latent video flow matchingmodel for animating latent images into realistic echocardiogram videosconditioned on ejection fraction. We rigorously evaluate our synthetic datasetson the clinically relevant task of ejection fraction regression anddemonstrate, for the first time, that downstream models trained exclusively onEchoFlow-generated synthetic datasets achieve performance parity with modelstrained on real datasets. We release our models and synthetic datasets,enabling broader, privacy-compliant research in medical ultrasound imaging athttps://huggingface.co/spaces/HReynaud/EchoFlow.
  </details>

- **[Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization](http://arxiv.org/abs/2503.22352v1)**  `arXiv:2503.22352`  
  _Barƒ±≈ü Batuhan Topal, Umut √ñzyurt, Zafer Doƒüan Budak, Ramazan Gokberk Cinbis_
  <details><summary>Abstract</summary>
  Recent advancements in text-to-image generative models, particularly latentdiffusion models (LDMs), have demonstrated remarkable capabilities insynthesizing high-quality images from textual prompts. However, achievingidentity personalization-ensuring that a model consistently generatessubject-specific outputs from limited reference images-remains a fundamentalchallenge. To address this, we introduce Meta-Low-Rank Adaptation (Meta-LoRA),a novel framework that leverages meta-learning to encode domain-specific priorsinto LoRA-based identity personalization. Our method introduces a structuredthree-layer LoRA architecture that separates identity-agnostic knowledge fromidentity-specific adaptation. In the first stage, the LoRA Meta-Down layers aremeta-trained across multiple subjects, learning a shared manifold that capturesgeneral identity-related features. In the second stage, only the LoRA-Mid andLoRA-Up layers are optimized to specialize on a given subject, significantlyreducing adaptation time while improving identity fidelity. To evaluate ourapproach, we introduce Meta-PHD, a new benchmark dataset for identitypersonalization, and compare Meta-LoRA against state-of-the-art methods. Ourresults demonstrate that Meta-LoRA achieves superior identity retention,computational efficiency, and adaptability across diverse identity conditions.The code, model weights, and dataset will be released publicly upon acceptance.
  </details>

- **[One Look is Enough: A Novel Seamless Patchwise Refinement for Zero-Shot Monocular Depth Estimation Models on High-Resolution Images](http://arxiv.org/abs/2503.22351v1)**  `arXiv:2503.22351`  
  _Byeongjun Kwon, Munchurl Kim_
  <details><summary>Abstract</summary>
  Zero-shot depth estimation (DE) models exhibit strong generalizationperformance as they are trained on large-scale datasets. However, existingmodels struggle with high-resolution images due to the discrepancy in imageresolutions of training (with smaller resolutions) and inference (for highresolutions). Processing them at full resolution leads to decreased estimationaccuracy on depth with tremendous memory consumption, while downsampling to thetraining resolution results in blurred edges in the estimated depth images.Prevailing high-resolution depth estimation methods adopt a patch-basedapproach, which introduces depth discontinuity issues when reassembling theestimated depth patches and results in test-time inefficiency. Additionally, toobtain fine-grained depth details, these methods rely on synthetic datasets dueto the real-world sparse ground truth depth, leading to poor generalizability.To tackle these limitations, we propose Patch Refine Once (PRO), an efficientand generalizable tile-based framework. Our PRO consists of two key components:(i) Grouped Patch Consistency Training that enhances test-time efficiency whilemitigating the depth discontinuity problem by jointly processing fouroverlapping patches and enforcing a consistency loss on their overlappingregions within a single backpropagation step, and (ii) Bias Free Masking thatprevents the DE models from overfitting to dataset-specific biases, enablingbetter generalization to real-world datasets even after training on syntheticdata. Zero-shot evaluation on Booster, ETH3D, Middlebury 2014, and NuScenesdemonstrates into which our PRO can be well harmonized, making their DEcapabilities still effective for the grid input of high-resolution images withlittle depth discontinuities at the grid boundaries. Our PRO runs fast atinference time.
  </details>

- **[GCRayDiffusion: Pose-Free Surface Reconstruction via Geometric Consistent Ray Diffusion](http://arxiv.org/abs/2503.22349v1)**  `arXiv:2503.22349`  
  _Li-Heng Chen, Zi-Xin Zou, Chang Liu, Tianjiao Jing, Yan-Pei Cao, Shi-Sheng Huang, et al._
  <details><summary>Abstract</summary>
  Accurate surface reconstruction from unposed images is crucial for efficient3D object or scene creation. However, it remains challenging, particularly forthe joint camera pose estimation. Previous approaches have achieved impressivepose-free surface reconstruction results in dense-view settings, but couldeasily fail for sparse-view scenarios without sufficient visual overlap. Inthis paper, we propose a new technique for pose-free surface reconstruction,which follows triplane-based signed distance field (SDF) learning butregularizes the learning by explicit points sampled from ray-based diffusion ofcamera pose estimation. Our key contribution is a novel Geometric ConsistentRay Diffusion model (GCRayDiffusion), where we represent camera poses as neuralbundle rays and regress the distribution of noisy rays via a diffusion model.More importantly, we further condition the denoising process of RGRayDiffusionusing the triplane-based SDF of the entire scene, which provides effective 3Dconsistent regularization to achieve multi-view consistent camera poseestimation. Finally, we incorporate RGRayDiffusion into the triplane-based SDFlearning by introducing on-surface geometric regularization from the samplingpoints of the neural bundle rays, which leads to highly accurate pose-freesurface reconstruction results even for sparse-view inputs. Extensiveevaluations on public datasets show that our GCRayDiffusion achieves moreaccurate camera pose estimation than previous approaches, with geometricallymore consistent surface reconstruction results, especially given sparse-viewinputs.
  </details>

- **[ArchCAD-400K: An Open Large-Scale Architectural CAD Dataset and New Baseline for Panoptic Symbol Spotting](http://arxiv.org/abs/2503.22346v1)**  `arXiv:2503.22346`  
  _Ruifeng Luo, Zhengjie Liu, Tianxiao Cheng, Jie Wang, Tongjie Wang, Xingguang Wei, et al._
  <details><summary>Abstract</summary>
  Recognizing symbols in architectural CAD drawings is critical for variousadvanced engineering applications. In this paper, we propose a novel CAD dataannotation engine that leverages intrinsic attributes from systematicallyarchived CAD drawings to automatically generate high-quality annotations, thussignificantly reducing manual labeling efforts. Utilizing this engine, weconstruct ArchCAD-400K, a large-scale CAD dataset consisting of 413,062 chunksfrom 5538 highly standardized drawings, making it over 26 times larger than thelargest existing CAD dataset. ArchCAD-400K boasts an extended drawing diversityand broader categories, offering line-grained annotations. Furthermore, wepresent a new baseline model for panoptic symbol spotting, termed Dual-PathwaySymbol Spotter (DPSS). It incorporates an adaptive fusion module to enhanceprimitive features with complementary image features, achievingstate-of-the-art performance and enhanced robustness. Extensive experimentsvalidate the effectiveness of DPSS, demonstrating the value of ArchCAD-400K andits potential to drive innovation in architectural design and construction.
  </details>

- **[Semantix: An Energy Guided Sampler for Semantic Style Transfer](http://arxiv.org/abs/2503.22344v1)**  `arXiv:2503.22344`  
  _Huiang He, Minghui Hu, Chuanxia Zheng, Chaoyue Wang, Tat-Jen Cham_
  <details><summary>Abstract</summary>
  Recent advances in style and appearance transfer are impressive, but mostmethods isolate global style and local appearance transfer, neglecting semanticcorrespondence. Additionally, image and video tasks are typically handled inisolation, with little focus on integrating them for video transfer. To addressthese limitations, we introduce a novel task, Semantic Style Transfer, whichinvolves transferring style and appearance features from a reference image to atarget visual content based on semantic correspondence. We subsequently proposea training-free method, Semantix an energy-guided sampler designed for SemanticStyle Transfer that simultaneously guides both style and appearance transferbased on semantic understanding capacity of pre-trained diffusion models.Additionally, as a sampler, Semantix be seamlessly applied to both image andvideo models, enabling semantic style transfer to be generic across variousvisual media. Specifically, once inverting both reference and context images orvideos to noise space by SDEs, Semantix utilizes a meticulously crafted energyfunction to guide the sampling process, including three key components: StyleFeature Guidance, Spatial Feature Guidance and Semantic Distance as aregularisation term. Experimental results demonstrate that Semantix not onlyeffectively accomplishes the task of semantic style transfer across images andvideos, but also surpasses existing state-of-the-art solutions in both fields.The project website is available at https://huiang-he.github.io/semantix/
  </details>

- **[VoteFlow: Enforcing Local Rigidity in Self-Supervised Scene Flow](http://arxiv.org/abs/2503.22328v1)**  `arXiv:2503.22328`  
  _Yancong Lin, Shiming Wang, Liangliang Nan, Julian Kooij, Holger Caesar_
  <details><summary>Abstract</summary>
  Scene flow estimation aims to recover per-point motion from two adjacentLiDAR scans. However, in real-world applications such as autonomous driving,points rarely move independently of others, especially for nearby pointsbelonging to the same object, which often share the same motion. Incorporatingthis locally rigid motion constraint has been a key challenge inself-supervised scene flow estimation, which is often addressed bypost-processing or appending extra regularization. While these approaches areable to improve the rigidity of predicted flows, they lack an architecturalinductive bias for local rigidity within the model structure, leading tosuboptimal learning efficiency and inferior performance. In contrast, weenforce local rigidity with a lightweight add-on module in neural networkdesign, enabling end-to-end learning. We design a discretized voting space thataccommodates all possible translations and then identify the one shared bynearby points by differentiable voting. Additionally, to ensure computationalefficiency, we operate on pillars rather than points and learn representativefeatures for voting per pillar. We plug the Voting Module into popular modeldesigns and evaluate its benefit on Argoverse 2 and Waymo datasets. Weoutperform baseline works with only marginal compute overhead. Code isavailable at https://github.com/tudelft-iv/VoteFlow.
  </details>

- **[AH-GS: Augmented 3D Gaussian Splatting for High-Frequency Detail Representation](http://arxiv.org/abs/2503.22324v1)**  `arXiv:2503.22324`  
  _Chenyang Xu, XingGuo Deng, Rui Zhong_
  <details><summary>Abstract</summary>
  The 3D Gaussian Splatting (3D-GS) is a novel method for scene representationand view synthesis. Although Scaffold-GS achieves higher quality real-timerendering compared to the original 3D-GS, its fine-grained rendering of thescene is extremely dependent on adequate viewing angles. The spectral bias ofneural network learning results in Scaffold-GS's poor ability to perceive andlearn high-frequency information in the scene. In this work, we proposeenhancing the manifold complexity of input features and using network-basedfeature map loss to improve the image reconstruction quality of 3D-GS models.We introduce AH-GS, which enables 3D Gaussians in structurally complex regionsto obtain higher-frequency encodings, allowing the model to more effectivelylearn the high-frequency information of the scene. Additionally, we incorporatehigh-frequency reinforce loss to further enhance the model's ability to capturedetailed frequency information. Our result demonstrates that our modelsignificantly improves rendering fidelity, and in specific scenarios (e.g.,MipNeRf360-garden), our method exceeds the rendering quality of Scaffold-GS injust 15K iterations.
  </details>

- **[A Dataset for Semantic Segmentation in the Presence of Unknowns](http://arxiv.org/abs/2503.22309v1)**  `arXiv:2503.22309`  
  _Zakaria Laskar, Tomas Vojir, Matej Grcic, Iaroslav Melekhov, Shankar Gangisettye, Juho Kannala, et al._
  <details><summary>Abstract</summary>
  Before deployment in the real-world deep neural networks require thoroughevaluation of how they handle both knowns, inputs represented in the trainingdata, and unknowns (anomalies). This is especially important for sceneunderstanding tasks with safety critical applications, such as in autonomousdriving. Existing datasets allow evaluation of only knowns or unknowns - butnot both, which is required to establish "in the wild" suitability of deepneural network models. To bridge this gap, we propose a novel anomalysegmentation dataset, ISSU, that features a diverse set of anomaly inputs fromcluttered real-world environments. The dataset is twice larger than existinganomaly segmentation datasets, and provides a training, validation and test setfor controlled in-domain evaluation. The test set consists of a static andtemporal part, with the latter comprised of videos. The dataset providesannotations for both closed-set (knowns) and anomalies, enabling closed-set andopen-set evaluation. The dataset covers diverse conditions, such as domain andcross-sensor shift, illumination variation and allows ablation of anomalydetection methods with respect to these variations. Evaluation results ofcurrent state-of-the-art methods confirm the need for improvements especiallyin domain-generalization, small and large object segmentation.
  </details>

- **[VisTa: Visual-contextual and Text-augmented Zero-shot Object-level OOD Detection](http://arxiv.org/abs/2503.22291v1)**  `arXiv:2503.22291`  
  _Bin Zhang, Xiaoyang Qu, Guokuan Li, Jiguang Wan, Jianzong Wang_
  <details><summary>Abstract</summary>
  As object detectors are increasingly deployed as black-box cloud services orpre-trained models with restricted access to the original training data, thechallenge of zero-shot object-level out-of-distribution (OOD) detection arises.This task becomes crucial in ensuring the reliability of detectors inopen-world settings. While existing methods have demonstrated success inimage-level OOD detection using pre-trained vision-language models like CLIP,directly applying such models to object-level OOD detection presents challengesdue to the loss of contextual information and reliance on image-levelalignment. To tackle these challenges, we introduce a new method that leveragesvisual prompts and text-augmented in-distribution (ID) space construction toadapt CLIP for zero-shot object-level OOD detection. Our method preservescritical contextual information and improves the ability to differentiatebetween ID and OOD objects, achieving competitive performance across differentbenchmarks.
  </details>

- **[RUNA: Object-level Out-of-Distribution Detection via Regional Uncertainty Alignment of Multimodal Representations](http://arxiv.org/abs/2503.22285v1)**  `arXiv:2503.22285`  
  _Bin Zhang, Jinggang Chen, Xiaoyang Qu, Guokuan Li, Kai Lu, Jiguang Wan, et al._
  <details><summary>Abstract</summary>
  Enabling object detectors to recognize out-of-distribution (OOD) objects isvital for building reliable systems. A primary obstacle stems from the factthat models frequently do not receive supervisory signals from unfamiliar data,leading to overly confident predictions regarding OOD objects. Despite previousprogress that estimates OOD uncertainty based on the detection model andin-distribution (ID) samples, we explore using pre-trained vision-languagerepresentations for object-level OOD detection. We first discuss thelimitations of applying image-level CLIP-based OOD detection methods toobject-level scenarios. Building upon these insights, we propose RUNA, a novelframework that leverages a dual encoder architecture to capture rich contextualinformation and employs a regional uncertainty alignment mechanism todistinguish ID from OOD objects effectively. We introduce a few-shotfine-tuning approach that aligns region-level semantic representations tofurther improve the model's capability to discriminate between similar objects.Our experiments show that RUNA substantially surpasses state-of-the-art methodsin object-level OOD detection, particularly in challenging scenarios withdiverse and complex object instances.
  </details>

- **[Divide to Conquer: A Field Decomposition Approach for Multi-Organ Whole-Body CT Image Registration](http://arxiv.org/abs/2503.22281v1)**  `arXiv:2503.22281`  
  _Xuan Loc Pham, Mathias Prokop, Bram van Ginneken, Alessa Hering_
  <details><summary>Abstract</summary>
  Image registration is an essential technique for the analysis of ComputedTomography (CT) images in clinical practice. However, existing methodologiesare predominantly tailored to a specific organ of interest and often exhibitlower performance on other organs, thus limiting their generalizability andapplicability. Multi-organ registration addresses these limitations, but thesimultaneous alignment of multiple organs with diverse shapes, sizes andlocations requires a highly complex deformation field with a multi-layercomposition of individual deformations. This study introduces a novel fielddecomposition approach to address the high complexity of deformations inmulti-organ whole-body CT image registration. The proposed method is trainedand evaluated on a longitudinal dataset of 691 patients, each with two CTimages obtained at distinct time points. These scans fully encompass thethoracic, abdominal, and pelvic regions. Two baseline registration methods areselected for this study: one based on optimization techniques and another basedon deep learning. Experimental results demonstrate that the proposed approachoutperforms baseline methods in handling complex deformations in multi-organwhole-body CT image registration.
  </details>

- **[Segment Any Motion in Videos](http://arxiv.org/abs/2503.22268v1)**  `arXiv:2503.22268`  
  _Nan Huang, Wenzhao Zheng, Chenfeng Xu, Kurt Keutzer, Shanghang Zhang, Angjoo Kanazawa, et al._
  <details><summary>Abstract</summary>
  Moving object segmentation is a crucial task for achieving a high-levelunderstanding of visual scenes and has numerous downstream applications. Humanscan effortlessly segment moving objects in videos. Previous work has largelyrelied on optical flow to provide motion cues; however, this approach oftenresults in imperfect predictions due to challenges such as partial motion,complex deformations, motion blur and background distractions. We propose anovel approach for moving object segmentation that combines long-rangetrajectory motion cues with DINO-based semantic features and leverages SAM2 forpixel-level mask densification through an iterative prompting strategy. Ourmodel employs Spatio-Temporal Trajectory Attention and Motion-SemanticDecoupled Embedding to prioritize motion while integrating semantic support.Extensive testing on diverse datasets demonstrates state-of-the-artperformance, excelling in challenging scenarios and fine-grained segmentationof multiple objects. Our code is available at https://motion-seg.github.io/.
  </details>

- **[DeepAudio-V1:Towards Multi-Modal Multi-Stage End-to-End Video to Speech and Audio Generation](http://arxiv.org/abs/2503.22265v1)**  `arXiv:2503.22265`  
  _Haomin Zhang, Chang Liu, Junjie Zheng, Zihao Chen, Chaofan Ding, Xinhan Di_
  <details><summary>Abstract</summary>
  Currently, high-quality, synchronized audio is synthesized using variousmulti-modal joint learning frameworks, leveraging video and optional textinputs. In the video-to-audio benchmarks, video-to-audio quality, semanticalignment, and audio-visual synchronization are effectively achieved. However,in real-world scenarios, speech and audio often coexist in videossimultaneously, and the end-to-end generation of synchronous speech and audiogiven video and text conditions are not well studied. Therefore, we propose anend-to-end multi-modal generation framework that simultaneously produces speechand audio based on video and text conditions. Furthermore, the advantages ofvideo-to-audio (V2A) models for generating speech from videos remain unclear.The proposed framework, DeepAudio, consists of a video-to-audio (V2A) module, atext-to-speech (TTS) module, and a dynamic mixture of modality fusion (MoF)module. In the evaluation, the proposed end-to-end framework achievesstate-of-the-art performance on the video-audio benchmark, video-speechbenchmark, and text-speech benchmark. In detail, our framework achievescomparable results in the comparison with state-of-the-art models for thevideo-audio and text-speech benchmarks, and surpassing state-of-the-art modelsin the video-speech benchmark, with WER 16.57% to 3.15% (+80.99%), SPK-SIM78.30% to 89.38% (+14.15%), EMO-SIM 66.24% to 75.56% (+14.07%), MCD 8.59 to7.98 (+7.10%), MCD SL 11.05 to 9.40 (+14.93%) across a variety of dubbingsettings.
  </details>

- **[Mono2Stereo: A Benchmark and Empirical Study for Stereo Conversion](http://arxiv.org/abs/2503.22262v1)**  `arXiv:2503.22262`  
  _Songsong Yu, Yuxin Chen, Zhongang Qi, Zeke Xie, Yifan Wang, Lijun Wang, et al._
  <details><summary>Abstract</summary>
  With the rapid proliferation of 3D devices and the shortage of 3D content,stereo conversion is attracting increasing attention. Recent works introducepretrained Diffusion Models (DMs) into this task. However, due to the scarcityof large-scale training data and comprehensive benchmarks, the optimalmethodologies for employing DMs in stereo conversion and the accurateevaluation of stereo effects remain largely unexplored. In this work, weintroduce the Mono2Stereo dataset, providing high-quality training data andbenchmark to support in-depth exploration of stereo conversion. With thisdataset, we conduct an empirical study that yields two primary findings. 1) Thedifferences between the left and right views are subtle, yet existing metricsconsider overall pixels, failing to concentrate on regions critical to stereoeffects. 2) Mainstream methods adopt either one-stage left-to-right generationor warp-and-inpaint pipeline, facing challenges of degraded stereo effect andimage distortion respectively. Based on these findings, we introduce a newevaluation metric, Stereo Intersection-over-Union, which prioritizes disparityand achieves a high correlation with human judgments on stereo effect.Moreover, we propose a strong baseline model, harmonizing the stereo effect andimage quality simultaneously, and notably surpassing current mainstreammethods. Our code and data will be open-sourced to promote further research instereo conversion. Our models are available at mono2stereo-bench.github.io.
  </details>

- **[Efficient Building Roof Type Classification: A Domain-Specific Self-Supervised Approach](http://arxiv.org/abs/2503.22251v1)**  `arXiv:2503.22251`  
  _Guneet Mutreja, Ksenia Bittner_
  <details><summary>Abstract</summary>
  Accurate classification of building roof types from aerial imagery is crucialfor various remote sensing applications, including urban planning, disastermanagement, and infrastructure monitoring. However, this task is often hinderedby the limited availability of labeled data for supervised learning approaches.To address this challenge, this paper investigates the effectiveness of selfsupervised learning with EfficientNet architectures, known for theircomputational efficiency, for building roof type classification. We propose anovel framework that incorporates a Convolutional Block Attention Module (CBAM)to enhance the feature extraction capabilities of EfficientNet. Furthermore, weexplore the benefits of pretraining on a domain-specific dataset, the AerialImage Dataset (AID), compared to ImageNet pretraining. Our experimental resultsdemonstrate the superiority of our approach. Employing Simple Framework forContrastive Learning of Visual Representations (SimCLR) with EfficientNet-B3and CBAM achieves a 95.5% accuracy on our validation set, matching theperformance of state-of-the-art transformer-based models while utilizingsignificantly fewer parameters. We also provide a comprehensive evaluation ontwo challenging test sets, demonstrating the generalization capability of ourmethod. Notably, our findings highlight the effectiveness of domain-specificpretraining, consistently leading to higher accuracy compared to modelspretrained on the generic ImageNet dataset. Our work establishes EfficientNetbased self-supervised learning as a computationally efficient and highlyeffective approach for building roof type classification, particularlybeneficial in scenarios with limited labeled data.
  </details>

- **[SCHNet: SAM Marries CLIP for Human Parsing](http://arxiv.org/abs/2503.22237v1)**  `arXiv:2503.22237`  
  _Kunliang Liu, Jianming Wang, Rize Jin, Wonjun Hwang, Tae-Sun Chung_
  <details><summary>Abstract</summary>
  Vision Foundation Model (VFM) such as the Segment Anything Model (SAM) andContrastive Language-Image Pre-training Model (CLIP) has shown promisingperformance for segmentation and detection tasks. However, although SAM excelsin fine-grained segmentation, it faces major challenges when applying it tosemantic-aware segmentation. While CLIP exhibits a strong semanticunderstanding capability via aligning the global features of language andvision, it has deficiencies in fine-grained segmentation tasks. Human parsingrequires to segment human bodies into constituent parts and involves bothaccurate fine-grained segmentation and high semantic understanding of eachpart. Based on traits of SAM and CLIP, we formulate high efficient modules toeffectively integrate features of them to benefit human parsing. We propose aSemantic-Refinement Module to integrate semantic features of CLIP with SAMfeatures to benefit parsing. Moreover, we formulate a high efficientFine-tuning Module to adjust the pretrained SAM for human parsing that needshigh semantic information and simultaneously demands spatial details, whichsignificantly reduces the training time compared with full-time training andachieves notable performance. Extensive experiments demonstrate theeffectiveness of our method on LIP, PPP, and CIHP databases.
  </details>

- **[CoGen: 3D Consistent Video Generation via Adaptive Conditioning for Autonomous Driving](http://arxiv.org/abs/2503.22231v1)**  `arXiv:2503.22231`  
  _Yishen Ji, Ziyue Zhu, Zhenxin Zhu, Kaixin Xiong, Ming Lu, Zhiqi Li, et al._
  <details><summary>Abstract</summary>
  Recent progress in driving video generation has shown significant potentialfor enhancing self-driving systems by providing scalable and controllabletraining data. Although pretrained state-of-the-art generation models, guidedby 2D layout conditions (e.g., HD maps and bounding boxes), can producephotorealistic driving videos, achieving controllable multi-view videos withhigh 3D consistency remains a major challenge. To tackle this, we introduce anovel spatial adaptive generation framework, CoGen, which leverages advances in3D generation to improve performance in two key aspects: (i) To ensure 3Dconsistency, we first generate high-quality, controllable 3D conditions thatcapture the geometry of driving scenes. By replacing coarse 2D conditions withthese fine-grained 3D representations, our approach significantly enhances thespatial consistency of the generated videos. (ii) Additionally, we introduce aconsistency adapter module to strengthen the robustness of the model tomulti-condition control. The results demonstrate that this method excels inpreserving geometric fidelity and visual realism, offering a reliable videogeneration solution for autonomous driving.
  </details>

- **[Follow Your Motion: A Generic Temporal Consistency Portrait Editing Framework with Trajectory Guidance](http://arxiv.org/abs/2503.22225v1)**  `arXiv:2503.22225`  
  _Haijie Yang, Zhenyu Zhang, Hao Tang, Jianjun Qian, Jian Yang_
  <details><summary>Abstract</summary>
  Pre-trained conditional diffusion models have demonstrated remarkablepotential in image editing. However, they often face challenges with temporalconsistency, particularly in the talking head domain, where continuous changesin facial expressions intensify the level of difficulty. These issues stem fromthe independent editing of individual images and the inherent loss of temporalcontinuity during the editing process. In this paper, we introduce Follow YourMotion (FYM), a generic framework for maintaining temporal consistency inportrait editing. Specifically, given portrait images rendered by a pre-trained3D Gaussian Splatting model, we first develop a diffusion model thatintuitively and inherently learns motion trajectory changes at different scalesand pixel coordinates, from the first frame to each subsequent frame. Thisapproach ensures that temporally inconsistent edited avatars inherit the motioninformation from the rendered avatars. Secondly, to maintain fine-grainedexpression temporal consistency in talking head editing, we propose a dynamicre-weighted attention mechanism. This mechanism assigns higher weightcoefficients to landmark points in space and dynamically updates these weightsbased on landmark loss, achieving more consistent and refined facialexpressions. Extensive experiments demonstrate that our method outperformsexisting approaches in terms of temporal consistency and can be used tooptimize and compensate for temporally inconsistent outputs in a range ofapplications, such as text-driven editing, relighting, and various otherapplications.
  </details>

- **[ABC-GS: Alignment-Based Controllable Style Transfer for 3D Gaussian Splatting](http://arxiv.org/abs/2503.22218v1)**  `arXiv:2503.22218`  
  _Wenjie Liu, Zhongliang Liu, Xiaoyan Yang, Man Sha, Yang Li_
  <details><summary>Abstract</summary>
  3D scene stylization approaches based on Neural Radiance Fields (NeRF)achieve promising results by optimizing with Nearest Neighbor Feature Matching(NNFM) loss. However, NNFM loss does not consider global style information. Inaddition, the implicit representation of NeRF limits their fine-grained controlover the resulting scenes. In this paper, we introduce ABC-GS, a novelframework based on 3D Gaussian Splatting to achieve high-quality 3D styletransfer. To this end, a controllable matching stage is designed to achieveprecise alignment between scene content and style features through segmentationmasks. Moreover, a style transfer loss function based on feature alignment isproposed to ensure that the outcomes of style transfer accurately reflect theglobal style of the reference image. Furthermore, the original geometricinformation of the scene is preserved with the depth loss and Gaussianregularization terms. Extensive experiments show that our ABC-GS providescontrollability of style transfer and achieves stylization results that aremore faithfully aligned with the global style of the chosen artistic reference.Our homepage is available at https://vpx-ecnu.github.io/ABC-GS-website.
  </details>

- **[Learning to Instruct for Visual Instruction Tuning](http://arxiv.org/abs/2503.22215v1)**  `arXiv:2503.22215`  
  _Zhihan Zhou, Feng Hong, Jiaan Luo, Jiangchao Yao, Dongsheng Li, Bo Han, et al._
  <details><summary>Abstract</summary>
  We propose LIT, an advancement of visual instruction tuning (VIT). While VITequips Multimodal LLMs (MLLMs) with promising multimodal capabilities, thecurrent design choices for VIT often result in overfitting and shortcutlearning, potentially degrading performance. This gap arises from anoveremphasis on instruction-following abilities, while neglecting the proactiveunderstanding of visual information. Inspired by this, LIT adopts a simple yeteffective approach by incorporating the loss function into both the instructionand response sequences. It seamlessly expands the training data, andregularizes the MLLMs from overly relying on language priors. Based on thismerit, LIT achieves a significant relative improvement of up to 9% oncomprehensive multimodal benchmarks, requiring no additional training data andincurring negligible computational overhead. Surprisingly, LIT attainsexceptional fundamental visual capabilities, yielding up to an 18% improvementin captioning performance, while simultaneously alleviating hallucination inMLLMs.
  </details>

- **[Intrinsic Image Decomposition for Robust Self-supervised Monocular Depth Estimation on Reflective Surfaces](http://arxiv.org/abs/2503.22209v1)**  `arXiv:2503.22209`  
  _Wonhyeok Choi, Kyumin Hwang, Minwoo Choi, Kiljoon Han, Wonjoon Choi, Mingyu Shin, et al._
  <details><summary>Abstract</summary>
  Self-supervised monocular depth estimation (SSMDE) has gained attention inthe field of deep learning as it estimates depth without requiring ground truthdepth maps. This approach typically uses a photometric consistency loss betweena synthesized image, generated from the estimated depth, and the originalimage, thereby reducing the need for extensive dataset acquisition. However,the conventional photometric consistency loss relies on the Lambertianassumption, which often leads to significant errors when dealing withreflective surfaces that deviate from this model. To address this limitation,we propose a novel framework that incorporates intrinsic image decompositioninto SSMDE. Our method synergistically trains for both monocular depthestimation and intrinsic image decomposition. The accurate depth estimationfacilitates multi-image consistency for intrinsic image decomposition byaligning different view coordinate systems, while the decomposition processidentifies reflective areas and excludes corrupted gradients from the depthtraining process. Furthermore, our framework introduces a pseudo-depthgeneration and knowledge distillation technique to further enhance theperformance of the student model across both reflective and non-reflectivesurfaces. Comprehensive evaluations on multiple datasets show that our approachsignificantly outperforms existing SSMDE baselines in depth prediction,especially on reflective surfaces.
  </details>

- **[Segment then Splat: A Unified Approach for 3D Open-Vocabulary Segmentation based on Gaussian Splatting](http://arxiv.org/abs/2503.22204v1)**  `arXiv:2503.22204`  
  _Yiren Lu, Yunlai Zhou, Yiran Qiao, Chaoda Song, Tuo Liang, Jing Ma, et al._
  <details><summary>Abstract</summary>
  Open-vocabulary querying in 3D space is crucial for enabling more intelligentperception in applications such as robotics, autonomous systems, and augmentedreality. However, most existing methods rely on 2D pixel-level parsing, leadingto multi-view inconsistencies and poor 3D object retrieval. Moreover, they arelimited to static scenes and struggle with dynamic scenes due to thecomplexities of motion modeling. In this paper, we propose Segment then Splat,a 3D-aware open vocabulary segmentation approach for both static and dynamicscenes based on Gaussian Splatting. Segment then Splat reverses the longestablished approach of "segmentation after reconstruction" by dividingGaussians into distinct object sets before reconstruction. Once thereconstruction is complete, the scene is naturally segmented into individualobjects, achieving true 3D segmentation. This approach not only eliminatesGaussian-object misalignment issues in dynamic scenes but also accelerates theoptimization process, as it eliminates the need for learning a separatelanguage field. After optimization, a CLIP embedding is assigned to each objectto enable open-vocabulary querying. Extensive experiments on various datasetsdemonstrate the effectiveness of our proposed method in both static and dynamicscenarios.
  </details>

- **[Multi-modal Knowledge Distillation-based Human Trajectory Forecasting](http://arxiv.org/abs/2503.22201v1)**  `arXiv:2503.22201`  
  _Jaewoo Jeong, Seohee Lee, Daehee Park, Giwon Lee, Kuk-Jin Yoon_
  <details><summary>Abstract</summary>
  Pedestrian trajectory forecasting is crucial in various applications such asautonomous driving and mobile robot navigation. In such applications,camera-based perception enables the extraction of additional modalities (humanpose, text) to enhance prediction accuracy. Indeed, we find that textualdescriptions play a crucial role in integrating additional modalities into aunified understanding. However, online extraction of text requires the use ofVLM, which may not be feasible for resource-constrained systems. To addressthis challenge, we propose a multi-modal knowledge distillation framework: astudent model with limited modality is distilled from a teacher model trainedwith full range of modalities. The comprehensive knowledge of a teacher modeltrained with trajectory, human pose, and text is distilled into a student modelusing only trajectory or human pose as a sole supplement. In doing so, weseparately distill the core locomotion insights from intra-agent multi-modalityand inter-agent interaction. Our generalizable framework is validated with twostate-of-the-art models across three datasets on both ego-view (JRDB, SIT) andBEV-view (ETH/UCY) setups, utilizing both annotated and VLM-generated textcaptions. Distilled student models show consistent improvement in allprediction metrics for both full and instantaneous observations, improving upto ~13%. The code is available at https://github.com/Jaewoo97/KDTF.
  </details>

- **[Hyperspectral Adapter for Object Tracking based on Hyperspectral Video](http://arxiv.org/abs/2503.22199v1)**  `arXiv:2503.22199`  
  _Long Gao, Yunhe Zhang, Langkun Chen, Yan Jiang, Weiying Xie, Yunsong Li_
  <details><summary>Abstract</summary>
  Object tracking based on hyperspectral video attracts increasing attention tothe rich material and motion information in the hyperspectral videos. Theprevailing hyperspectral methods adapt pretrained RGB-based object trackingnetworks for hyperspectral tasks by fine-tuning the entire network onhyperspectral datasets, which achieves impressive results in challengingscenarios. However, the performance of hyperspectral trackers is limited by theloss of spectral information during the transformation, and fine-tuning theentire pretrained network is inefficient for practical applications. To addressthe issues, a new hyperspectral object tracking method, hyperspectral adapterfor tracking (HyA-T), is proposed in this work. The hyperspectral adapter forthe self-attention (HAS) and the hyperspectral adapter for the multilayerperceptron (HAM) are proposed to generate the adaption information and totransfer the multi-head self-attention (MSA) module and the multilayerperceptron (MLP) in pretrained network for the hyperspectral object trackingtask by augmenting the adaption information into the calculation of the MSA andMLP. Additionally, the hyperspectral enhancement of input (HEI) is proposed toaugment the original spectral information into the input of the trackingnetwork. The proposed methods extract spectral information directly from thehyperspectral images, which prevent the loss of the spectral information.Moreover, only the parameters in the proposed methods are fine-tuned, which ismore efficient than the existing methods. Extensive experiments were conductedon four datasets with various spectral bands, verifing the effectiveness of theproposed methods. The HyA-T achieves state-of-the-art performance on all thedatasets.
  </details>

- **[Extremely Simple Out-of-distribution Detection for Audio-visual Generalized Zero-shot Learning](http://arxiv.org/abs/2503.22197v1)**  `arXiv:2503.22197`  
  _Yang Liu, Xun Zhang, Jiale Du, Xinbo Gao, Jungong Han_
  <details><summary>Abstract</summary>
  Zero-shot Learning(ZSL) attains knowledge transfer from seen classes tounseen classes by exploring auxiliary category information, which is apromising yet difficult research topic. In this field, Audio-Visual GeneralizedZero-Shot Learning~(AV-GZSL) has aroused researchers' great interest in whichintricate relations within triple modalities~(audio, video, and naturallanguage) render this task quite challenging but highly research-worthy.However, both existing embedding-based and generative-based AV-GZSL methodstend to suffer from domain shift problem a lot and we propose an extremelysimple Out-of-distribution~(OOD) detection based AV-GZSL method~(EZ-AVOOD) tofurther mitigate bias problem by differentiating seen and unseen samples at theinitial beginning. EZ-AVOOD accomplishes effective seen-unseen separation byexploiting the intrinsic discriminative information held in class-specificlogits and class-agnostic feature subspace without training an extra OODdetector network. Followed by seen-unseen binary classification, we employ twoexpert models to classify seen samples and unseen samples separately. Comparedto existing state-of-the-art methods, our model achieves superior ZSL and GZSLperformances on three audio-visual datasets and becomes the new SOTA, whichcomprehensively demonstrates the effectiveness of the proposed EZ-AVOOD.
  </details>

- **[ORIGEN: Zero-Shot 3D Orientation Grounding in Text-to-Image Generation](http://arxiv.org/abs/2503.22194v1)**  `arXiv:2503.22194`  
  _Yunhong Min, Daehyeon Choi, Kyeongmin Yeo, Jihyun Lee, Minhyuk Sung_
  <details><summary>Abstract</summary>
  We introduce ORIGEN, the first zero-shot method for 3D orientation groundingin text-to-image generation across multiple objects and diverse categories.While previous work on spatial grounding in image generation has mainly focusedon 2D positioning, it lacks control over 3D orientation. To address this, wepropose a reward-guided sampling approach using a pretrained discriminativemodel for 3D orientation estimation and a one-step text-to-image generativeflow model. While gradient-ascent-based optimization is a natural choice forreward-based guidance, it struggles to maintain image realism. Instead, weadopt a sampling-based approach using Langevin dynamics, which extends gradientascent by simply injecting random noise--requiring just a single additionalline of code. Additionally, we introduce adaptive time rescaling based on thereward function to accelerate convergence. Our experiments show that ORIGENoutperforms both training-based and test-time guidance methods acrossquantitative metrics and user studies.
  </details>

- **[Unbiased Max-Min Embedding Classification for Transductive Few-Shot Learning: Clustering and Classification Are All You Need](http://arxiv.org/abs/2503.22193v1)**  `arXiv:2503.22193`  
  _Yang Liu, Feixiang Liu, Jiale Du, Xinbo Gao, Jungong Han_
  <details><summary>Abstract</summary>
  Convolutional neural networks and supervised learning have achievedremarkable success in various fields but are limited by the need for largeannotated datasets. Few-shot learning (FSL) addresses this limitation byenabling models to generalize from only a few labeled examples. Transductivefew-shot learning (TFSL) enhances FSL by leveraging both labeled and unlabeleddata, though it faces challenges like the hubness problem. To overcome theselimitations, we propose the Unbiased Max-Min Embedding Classification (UMMEC)Method, which addresses the key challenges in few-shot learning through threeinnovative contributions. First, we introduce a decentralized covariance matrixto mitigate the hubness problem, ensuring a more uniform distribution ofembeddings. Second, our method combines local alignment and global uniformitythrough adaptive weighting and nonlinear transformation, balancing intra-classclustering with inter-class separation. Third, we employ a Variational SinkhornFew-Shot Classifier to optimize the distances between samples and classprototypes, enhancing classification accuracy and robustness. These combinedinnovations allow the UMMEC method to achieve superior performance with minimallabeled data. Our UMMEC method significantly improves classificationperformance with minimal labeled data, advancing the state-of-the-art in TFSL.
  </details>

- **[Knowledge Rectification for Camouflaged Object Detection: Unlocking Insights from Low-Quality Data](http://arxiv.org/abs/2503.22180v1)**  `arXiv:2503.22180`  
  _Juwei Guan, Xiaolin Fang, Donghyun Kim, Haotian Gong, Tongxin Zhu, Zhen Ling, et al._
  <details><summary>Abstract</summary>
  Low-quality data often suffer from insufficient image details, introducing anextra implicit aspect of camouflage that complicates camouflaged objectdetection (COD). Existing COD methods focus primarily on high-quality data,overlooking the challenges posed by low-quality data, which leads tosignificant performance degradation. Therefore, we propose KRNet, the firstframework explicitly designed for COD on low-quality data. KRNet presents aLeader-Follower framework where the Leader extracts dual gold-standarddistributions: conditional and hybrid, from high-quality data to drive theFollower in rectifying knowledge learned from low-quality data. The frameworkfurther benefits from a cross-consistency strategy that improves therectification of these distributions and a time-dependent conditional encoderthat enriches the distribution diversity. Extensive experiments on benchmarkdatasets demonstrate that KRNet outperforms state-of-the-art COD methods andsuper-resolution-assisted COD approaches, proving its effectiveness in tacklingthe challenges of low-quality data in COD.
  </details>

- **[High-Fidelity Diffusion Face Swapping with ID-Constrained Facial Conditioning](http://arxiv.org/abs/2503.22179v1)**  `arXiv:2503.22179`  
  _Dailan He, Xiahong Wang, Shulun Wang, Guanglu Song, Bingqi Ma, Hao Shao, et al._
  <details><summary>Abstract</summary>
  Face swapping aims to seamlessly transfer a source facial identity onto atarget while preserving target attributes such as pose and expression.Diffusion models, known for their superior generative capabilities, haverecently shown promise in advancing face-swapping quality. This paper addressestwo key challenges in diffusion-based face swapping: the prioritizedpreservation of identity over target attributes and the inherent conflictbetween identity and attribute conditioning. To tackle these issues, weintroduce an identity-constrained attribute-tuning framework for face swappingthat first ensures identity preservation and then fine-tunes for attributealignment, achieved through a decoupled condition injection. We further enhancefidelity by incorporating identity and adversarial losses in a post-trainingrefinement stage. Our proposed identity-constrained diffusion-basedface-swapping model outperforms existing methods in both qualitative andquantitative evaluations, demonstrating superior identity similarity andattribute consistency, achieving a new state-of-the-art performance inhigh-fidelity face swapping.
  </details>

- **[Efficient Continual Learning through Frequency Decomposition and Integration](http://arxiv.org/abs/2503.22175v1)**  `arXiv:2503.22175`  
  _Ruiqi Liu, Boyu Diao, Libo Huang, Hangda Liu, Chuanguang Yang, Zhulin An, et al._
  <details><summary>Abstract</summary>
  Continual learning (CL) aims to learn new tasks while retaining pastknowledge, addressing the challenge of forgetting during task adaptation.Rehearsal-based methods, which replay previous samples, effectively mitigateforgetting. However, research on enhancing the efficiency of these methods,especially in resource-constrained environments, remains limited, hinderingtheir application in real-world systems with dynamic data streams. The humanperceptual system processes visual scenes through complementary frequencychannels: low-frequency signals capture holistic cues, while high-frequencycomponents convey structural details vital for fine-grained discrimination.Inspired by this, we propose the Frequency Decomposition and IntegrationNetwork (FDINet), a novel framework that decomposes and integrates informationacross frequencies. FDINet designs two lightweight networks to independentlyprocess low- and high-frequency components of images. When integrated withrehearsal-based methods, this frequency-aware design effectively enhancescross-task generalization through low-frequency information, preservesclass-specific details using high-frequency information, and facilitatesefficient training due to its lightweight architecture. Experiments demonstratethat FDINet reduces backbone parameters by 78%, improves accuracy by up to7.49% over state-of-the-art (SOTA) methods, and decreases peak memory usage byup to 80%. Additionally, on edge devices, FDINet accelerates training by up to5$\times$.
  </details>

- **[Synergistic Bleeding Region and Point Detection in Surgical Videos](http://arxiv.org/abs/2503.22174v1)**  `arXiv:2503.22174`  
  _Jialun Pei, Zhangjun Zhou, Diandian Guo, Zhixi Li, Jing Qin, Bo Du, et al._
  <details><summary>Abstract</summary>
  Intraoperative bleeding in laparoscopic surgery causes rapid obscuration ofthe operative field to hinder the surgical process. Intelligent detection ofbleeding regions can quantify the blood loss to assist decision-making, whilelocating the bleeding point helps surgeons quickly identify the source ofbleeding and achieve hemostasis in time. In this study, we first construct areal-world surgical bleeding detection dataset, named SurgBlood, comprising5,330 frames from 95 surgical video clips with bleeding region and pointannotations. Accordingly, we develop a dual-task synergistic online detectorcalled BlooDet, designed to perform simultaneous detection of bleeding regionsand points in surgical videos. Our framework embraces a dual-branchbidirectional guidance design based on Segment Anything Model 2 (SAM 2). Themask branch detects bleeding regions through adaptive edge and point promptembeddings, while the point branch leverages mask memory to induce bleedingpoint memory modeling and captures the direction of bleed point movementthrough inter-frame optical flow. By interactive guidance and prompts, the twobranches explore potential spatial-temporal relationships while leveragingmemory modeling from previous frames to infer the current bleeding condition.Extensive experiments demonstrate that our approach outperforms othercounterparts on SurgBlood in both bleeding region and point detection tasks,e.g., achieving 64.88% IoU for bleeding region detection and 83.69% PCK-10% forbleeding point detection.
  </details>

- **[Concept-Aware LoRA for Domain-Aligned Segmentation Dataset Generation](http://arxiv.org/abs/2503.22172v1)**  `arXiv:2503.22172`  
  _Minho Park, Sunghyun Park, Jungsoo Lee, Hyojin Park, Kyuwoong Hwang, Fatih Porikli, et al._
  <details><summary>Abstract</summary>
  This paper addresses the challenge of data scarcity in semantic segmentationby generating datasets through text-to-image (T2I) generation models, reducingimage acquisition and labeling costs. Segmentation dataset generation faces twokey challenges: 1) aligning generated samples with the target domain and 2)producing informative samples beyond the training data. Fine-tuning T2I modelscan help generate samples aligned with the target domain. However, it oftenoverfits and memorizes training data, limiting their ability to generatediverse and well-aligned samples. To overcome these issues, we proposeConcept-Aware LoRA (CA-LoRA), a novel fine-tuning approach that selectivelyidentifies and updates only the weights associated with necessary concepts(e.g., style or viewpoint) for domain alignment while preserving the pretrainedknowledge of the T2I model to produce informative samples. We demonstrate itseffectiveness in generating datasets for urban-scene segmentation,outperforming baseline and state-of-the-art methods in in-domain (few-shot andfully-supervised) settings, as well as in domain generalization tasks,especially under challenging conditions such as adverse weather and varyingillumination, further highlighting its superiority.
  </details>

- **[An Empirical Study of Validating Synthetic Data for Text-Based Person Retrieval](http://arxiv.org/abs/2503.22171v1)**  `arXiv:2503.22171`  
  _Min Cao, ZiYin Zeng, YuXin Lu, Mang Ye, Dong Yi, Jinqiao Wang_
  <details><summary>Abstract</summary>
  Data plays a pivotal role in Text-Based Person Retrieval (TBPR) research.Mainstream research paradigm necessitates real-world person images with manualtextual annotations for training models, posing privacy-sensitive andlabor-intensive issues. Several pioneering efforts explore synthetic data forTBPR but still rely on real data, keeping the aforementioned issues and alsoresulting in diversity-deficient issue in synthetic datasets, thus impactingTBPR performance. Moreover, these works tend to explore synthetic data for TBPRthrough limited perspectives, leading to exploration-restricted issue. In thispaper, we conduct an empirical study to explore the potential of synthetic datafor TBPR, highlighting three key aspects. (1) We propose an inter-class imagegeneration pipeline, in which an automatic prompt construction strategy isintroduced to guide generative Artificial Intelligence (AI) models ingenerating various inter-class images without reliance on original data. (2) Wedevelop an intra-class image augmentation pipeline, in which the generative AImodels are applied to further edit the images for obtaining various intra-classimages. (3) Building upon the proposed pipelines and an automatic textgeneration pipeline, we explore the effectiveness of synthetic data in diversescenarios through extensive experiments. Additionally, we experimentallyinvestigate various noise-robust learning strategies to mitigate the inherentnoise in synthetic data. We will release the code, along with the syntheticlarge-scale dataset generated by our pipelines, which are expected to advancepractical TBPR research.
  </details>

- **[Spatial Transport Optimization by Repositioning Attention Map for Training-Free Text-to-Image Synthesis](http://arxiv.org/abs/2503.22168v1)**  `arXiv:2503.22168`  
  _Woojung Han, Yeonkyung Lee, Chanyoung Kim, Kwanghyun Park, Seong Jae Hwang_
  <details><summary>Abstract</summary>
  Diffusion-based text-to-image (T2I) models have recently excelled inhigh-quality image generation, particularly in a training-free manner, enablingcost-effective adaptability and generalization across diverse tasks. However,while the existing methods have been continuously focusing on severalchallenges, such as "missing objects" and "mismatched attributes," anothercritical issue of "mislocated objects" remains where generated spatialpositions fail to align with text prompts. Surprisingly, ensuring suchseemingly basic functionality remains challenging in popular T2I models due tothe inherent difficulty of imposing explicit spatial guidance via text forms.To address this, we propose STORM (Spatial Transport Optimization byRepositioning Attention Map), a novel training-free approach for spatiallycoherent T2I synthesis. STORM employs Spatial Transport Optimization (STO),rooted in optimal transport theory, to dynamically adjust object attention mapsfor precise spatial adherence, supported by a Spatial Transport (ST) Costfunction that enhances spatial understanding. Our analysis shows thatintegrating spatial awareness is most effective in the early denoising stages,while later phases refine details. Extensive experiments demonstrate that STORMsurpasses existing methods, effectively mitigating mislocated objects whileimproving missing and mismatched attributes, setting a new benchmark forspatial alignment in T2I synthesis.
  </details>

- **[Permutation-Invariant and Orientation-Aware Dataset Distillation for 3D Point Clouds](http://arxiv.org/abs/2503.22154v1)**  `arXiv:2503.22154`  
  _Jae-Young Yim, Dongwook Kim, Jae-Young Sim_
  <details><summary>Abstract</summary>
  We should collect large amount of data to train deep neural networks forvarious applications. Recently, the dataset distillation for images and textshas been attracting a lot of attention, that reduces the original dataset to asynthetic dataset while preserving essential task-relevant information.However, 3D point clouds distillation is almost unexplored due to thechallenges of unordered structures of points. In this paper, we propose a noveldistribution matching-based dataset distillation method for 3D point cloudsthat jointly optimizes the geometric structures of synthetic dataset as well asthe orientations of synthetic models. To ensure the consistent featurealignment between different 3D point cloud models, we devise a permutationinvariant distribution matching loss with the sorted feature vectors. We alsoemploy learnable rotation angles to transform each syntheic model according tothe optimal orientation best representing the original feature distribution.Extensive experimental results on widely used four benchmark datasets,including ModelNet10, ModelNet40, ShapeNet, and ScanObjectNN, demonstrate thatthe proposed method consistently outperforms the existing methods.
  </details>

- **[EgoToM: Benchmarking Theory of Mind Reasoning from Egocentric Videos](http://arxiv.org/abs/2503.22152v1)**  `arXiv:2503.22152`  
  _Yuxuan Li, Vijay Veerabadran, Michael L. Iuzzolino, Brett D. Roads, Asli Celikyilmaz, Karl Ridgeway_
  <details><summary>Abstract</summary>
  We introduce EgoToM, a new video question-answering benchmark that extendsTheory-of-Mind (ToM) evaluation to egocentric domains. Using a causal ToMmodel, we generate multi-choice video QA instances for the Ego4D dataset tobenchmark the ability to predict a camera wearer's goals, beliefs, and nextactions. We study the performance of both humans and state of the artmultimodal large language models (MLLMs) on these three interconnectedinference problems. Our evaluation shows that MLLMs achieve close tohuman-level accuracy on inferring goals from egocentric videos. However, MLLMs(including the largest ones we tested with over 100B parameters) fall short ofhuman performance when inferring the camera wearers' in-the-moment beliefstates and future actions that are most consistent with the unseen videofuture. We believe that our results will shape the future design of animportant class of egocentric digital assistants which are equipped with areasonable model of the user's internal mental states.
  </details>

- **[Beyond Background Shift: Rethinking Instance Replay in Continual Semantic Segmentation](http://arxiv.org/abs/2503.22136v1)**  `arXiv:2503.22136`  
  _Hongmei Yin, Tingliang Feng, Fan Lyu, Fanhua Shang, Hongying Liu, Wei Feng, et al._
  <details><summary>Abstract</summary>
  In this work, we focus on continual semantic segmentation (CSS), wheresegmentation networks are required to continuously learn new classes withouterasing knowledge of previously learned ones. Although storing images of oldclasses and directly incorporating them into the training of new models hasproven effective in mitigating catastrophic forgetting in classification tasks,this strategy presents notable limitations in CSS. Specifically, the stored andnew images with partial category annotations leads to confusion betweenunannotated categories and the background, complicating model fitting. Totackle this issue, this paper proposes a novel Enhanced Instance Replay (EIR)method, which not only preserves knowledge of old classes while simultaneouslyeliminating background confusion by instance storage of old classes, but alsomitigates background shifts in the new images by integrating stored instanceswith new images. By effectively resolving background shifts in both stored andnew images, EIR alleviates catastrophic forgetting in the CSS task, therebyenhancing the model's capacity for CSS. Experimental results validate theefficacy of our approach, which significantly outperforms state-of-the-art CSSmethods.
  </details>

- **[Semantic segmentation for building houses from wooden cubes](http://arxiv.org/abs/2503.22125v1)**  `arXiv:2503.22125`  
  _Ivan Beleacov_
  <details><summary>Abstract</summary>
  Automated construction is one of the most promising areas that can improveefficiency, reduce costs and minimize errors in the process of buildingconstruction. In this paper, a comparative analysis of three neural networkmodels for semantic segmentation, U-Net(light), LinkNet and PSPNet, isperformed. Two specialized datasets with images of houses built from woodencubes were created for the experiments. The first dataset contains 4 classes(background, foundation, walls, roof ) and is designed for basic modelevaluation, while the second dataset includes 44 classes where each cube islabeled as a separate object. The models were trained with the samehyperparameters and their accuracy was evaluated using MeanIoU and F1 Scoremetrics. According to the results obtained, U-Net(light) showed the bestperformance with 78% MeanIoU and 87% F1 Score on the first dataset and 17% and25% respectively on the second dataset. The poor results on the second datasetare due to the limited amount of data, the complexity of the partitioning andthe imbalance of classes, making it difficult to accurately select individualcubes. In addition, overtraining was observed in all experiments, manifested byhigh accuracy on the training dataset and its significant decrease on thevalidation dataset. The present work is the basis for the development ofalgorithms for automatic generation of staged building plans, which can befurther scaled to design complete buildings. Future research is planned toextend the datasets and apply methods to combat overfitting (L1/L2regularization, Early Stopping). The next stage of work will be the developmentof algorithms for automatic generation of a step-by-step plan for buildinghouses from cubes using manipulators. Index Terms-Deep Learning, Computervision, CNN, Semantic segmentation, Construction materials.
  </details>

- **[Detecting Localized Deepfake Manipulations Using Action Unit-Guided Video Representations](http://arxiv.org/abs/2503.22121v1)**  `arXiv:2503.22121`  
  _Tharun Anand, Siva Sankar, Pravin Nair_
  <details><summary>Abstract</summary>
  With rapid advancements in generative modeling, deepfake techniques areincreasingly narrowing the gap between real and synthetic videos, raisingserious privacy and security concerns. Beyond traditional face swapping andreenactment, an emerging trend in recent state-of-the-art deepfake generationmethods involves localized edits such as subtle manipulations of specificfacial features like raising eyebrows, altering eye shapes, or modifying mouthexpressions. These fine-grained manipulations pose a significant challenge forexisting detection models, which struggle to capture such localized variations.To the best of our knowledge, this work presents the first detection approachexplicitly designed to generalize to localized edits in deepfake videos byleveraging spatiotemporal representations guided by facial action units. Ourmethod leverages a cross-attention-based fusion of representations learned frompretext tasks like random masking and action unit detection, to create anembedding that effectively encodes subtle, localized changes. Comprehensiveevaluations across multiple deepfake generation methods demonstrate that ourapproach, despite being trained solely on the traditional FF+ dataset, sets anew benchmark in detecting recent deepfake-generated videos with fine-grainedlocal edits, achieving a $20\%$ improvement in accuracy over currentstate-of-the-art detection methods. Additionally, our method deliverscompetitive performance on standard datasets, highlighting its robustness andgeneralization across diverse types of local and global forgeries.
  </details>

- **[Camera Model Identification with SPAIR-Swin and Entropy based Non-Homogeneous Patches](http://arxiv.org/abs/2503.22120v1)**  `arXiv:2503.22120`  
  _Protyay Dey, Rejoy Chakraborty, Abhilasha S. Jadhav, Kapil Rana, Gaurav Sharma, Puneet Goyal_
  <details><summary>Abstract</summary>
  Source camera model identification (SCMI) plays a pivotal role in imageforensics with applications including authenticity verification and copyrightprotection. For identifying the camera model used to capture a given image, wepropose SPAIR-Swin, a novel model combining a modified spatial attentionmechanism and inverted residual block (SPAIR) with a Swin Transformer.SPAIR-Swin effectively captures both global and local features, enabling robustidentification of artifacts such as noise patterns that are particularlyeffective for SCMI. Additionally, unlike conventional methods focusing onhomogeneous patches, we propose a patch selection strategy for SCMI thatemphasizes high-entropy regions rich in patterns and textures. Extensiveevaluations on four benchmark SCMI datasets demonstrate that SPAIR-Swinoutperforms existing methods, achieving patch-level accuracies of 99.45%,98.39%, 99.45%, and 97.46% and image-level accuracies of 99.87%, 99.32%, 100%,and 98.61% on the Dresden, Vision, Forchheim, and Socrates datasets,respectively. Our findings highlight that high-entropy patches, which containhigh-frequency information such as edge sharpness, noise, and compressionartifacts, are more favorable in improving SCMI accuracy. Code will be madeavailable upon request.
  </details>

- **[How Well Can Vison-Language Models Understand Humans' Intention? An Open-ended Theory of Mind Question Evaluation Benchmark](http://arxiv.org/abs/2503.22093v1)**  `arXiv:2503.22093`  
  _Ximing Wen, Mallika Mainali, Anik Sen_
  <details><summary>Abstract</summary>
  Vision Language Models (VLMs) have demonstrated strong reasoning capabilitiesin Visual Question Answering (VQA) tasks; However, their ability to performTheory of Mind (ToM) tasks such as accurately inferring human intentions,beliefs, and other mental states remains underexplored. In this work, wepropose an open-ended question framework to comprehensively evaluate VLMs'performance across diverse categories of ToM tasks. We curated and annotated abenchmark dataset composed of 30 images. We then assessed the performance offour VLMs of varying sizes on this dataset. Our experimental results show thatthe GPT-4 model outperformed all others, with only one smaller model,GPT-4o-mini, achieving comparable performance. Additionally, we observed thatVLMs often struggle to accurately infer intentions in complex scenarios such asbullying or cheating. Moreover, our findings also reveal that smaller modelscan sometimes infer correct intentions despite relying on incorrect visualcues.
  </details>

- **[Mitigating Trade-off: Stream and Query-guided Aggregation for Efficient and Effective 3D Occupancy Prediction](http://arxiv.org/abs/2503.22087v1)**  `arXiv:2503.22087`  
  _Seokha Moon, Janghyun Baek, Giseop Kim, Jinkyu Kim, Sunwook Choi_
  <details><summary>Abstract</summary>
  3D occupancy prediction has emerged as a key perception task for autonomousdriving, as it reconstructs 3D environments to provide a comprehensive sceneunderstanding. Recent studies focus on integrating spatiotemporal informationobtained from past observations to improve prediction accuracy, using amulti-frame fusion approach that processes multiple past frames together.However, these methods struggle with a trade-off between efficiency andaccuracy, which significantly limits their practicality. To mitigate thistrade-off, we propose StreamOcc, a novel framework that aggregatesspatio-temporal information in a stream-based manner. StreamOcc consists of twokey components: (i) Stream-based Voxel Aggregation, which effectivelyaccumulates past observations while minimizing computational costs, and (ii)Query-guided Aggregation, which recurrently aggregates instance-level featuresof dynamic objects into corresponding voxel features, refining fine-graineddetails of dynamic objects. Experiments on the Occ3D-nuScenes dataset show thatStreamOcc achieves state-of-the-art performance in real-time settings, whilereducing memory usage by more than 50% compared to previous methods.
  </details>

- **[A Survey on Remote Sensing Foundation Models: From Vision to Multimodality](http://arxiv.org/abs/2503.22081v1)**  `arXiv:2503.22081`  
  _Ziyue Huang, Hongxi Yan, Qiqi Zhan, Shuai Yang, Mingming Zhang, Chenkai Zhang, et al._
  <details><summary>Abstract</summary>
  The rapid advancement of remote sensing foundation models, particularlyvision and multimodal models, has significantly enhanced the capabilities ofintelligent geospatial data interpretation. These models combine various datamodalities, such as optical, radar, and LiDAR imagery, with textual andgeographic information, enabling more comprehensive analysis and understandingof remote sensing data. The integration of multiple modalities allows forimproved performance in tasks like object detection, land cover classification,and change detection, which are often challenged by the complex andheterogeneous nature of remote sensing data. However, despite theseadvancements, several challenges remain. The diversity in data types, the needfor large-scale annotated datasets, and the complexity of multimodal fusiontechniques pose significant obstacles to the effective deployment of thesemodels. Moreover, the computational demands of training and fine-tuningmultimodal models require significant resources, further complicating theirpractical application in remote sensing image interpretation tasks. This paperprovides a comprehensive review of the state-of-the-art in vision andmultimodal foundation models for remote sensing, focusing on theirarchitecture, training methods, datasets and application scenarios. We discussthe key challenges these models face, such as data alignment, cross-modaltransfer learning, and scalability, while also identifying emerging researchdirections aimed at overcoming these limitations. Our goal is to provide aclear understanding of the current landscape of remote sensing foundationmodels and inspire future research that can push the boundaries of what thesemodels can achieve in real-world applications. The list of resources collectedby the paper can be found in thehttps://github.com/IRIP-BUAA/A-Review-for-remote-sensing-vision-language-models.
  </details>

- **[A Semantic-Enhanced Heterogeneous Graph Learning Method for Flexible Objects Recognition](http://arxiv.org/abs/2503.22079v1)**  `arXiv:2503.22079`  
  _Kunshan Yang, Wenwei Luo, Yuguo Hu, Jiafu Yan, Mengmeng Jing, Lin Zuo_
  <details><summary>Abstract</summary>
  Flexible objects recognition remains a significant challenge due to itsinherently diverse shapes and sizes, translucent attributes, and subtleinter-class differences. Graph-based models, such as graph convolution networksand graph vision models, are promising in flexible objects recognition due totheir ability of capturing variable relations within the flexible objects.These methods, however, often focus on global visual relationships or fail toalign semantic and visual information. To alleviate these limitations, wepropose a semantic-enhanced heterogeneous graph learning method. First, anadaptive scanning module is employed to extract discriminative semanticcontext, facilitating the matching of flexible objects with varying shapes andsizes while aligning semantic and visual nodes to enhance cross-modal featurecorrelation. Second, a heterogeneous graph generation module aggregates globalvisual and local semantic node features, improving the recognition of flexibleobjects. Additionally, We introduce the FSCW, a large-scale flexible datasetcurated from existing sources. We validate our method through extensiveexperiments on flexible datasets (FDA and FSCW), and challenge benchmarks(CIFAR-100 and ImageNet-Hard), demonstrating competitive performance.
  </details>

- **[Contrasting Low and High-Resolution Features for HER2 Scoring using Deep Learning](http://arxiv.org/abs/2503.22069v1)**  `arXiv:2503.22069`  
  _Ekansh Chauhan, Anila Sharma, Amit Sharma, Vikas Nishadham, Asha Ghughtyal, Ankur Kumar, et al._
  <details><summary>Abstract</summary>
  Breast cancer, the most common malignancy among women, requires precisedetection and classification for effective treatment. Immunohistochemistry(IHC) biomarkers like HER2, ER, and PR are critical for identifying breastcancer subtypes. However, traditional IHC classification relies onpathologists' expertise, making it labor-intensive and subject to significantinter-observer variability. To address these challenges, this study introducesthe India Pathology Breast Cancer Dataset (IPD-Breast), comprising of 1,272 IHCslides (HER2, ER, and PR) aimed at automating receptor status classification.The primary focus is on developing predictive models for HER2 3-wayclassification (0, Low, High) to enhance prognosis. Evaluation of multiple deeplearning models revealed that an end-to-end ConvNeXt network utilizinglow-resolution IHC images achieved an AUC, F1, and accuracy of 91.79%, 83.52%,and 83.56%, respectively, for 3-way classification, outperforming patch-basedmethods by over 5.35% in F1 score. This study highlights the potential ofsimple yet effective deep learning techniques to significantly improve accuracyand reproducibility in breast cancer classification, supporting theirintegration into clinical workflows for better patient outcomes.
  </details>

- **[Deep Depth Estimation from Thermal Image: Dataset, Benchmark, and Challenges](http://arxiv.org/abs/2503.22060v1)**  `arXiv:2503.22060`  
  _Ukcheol Shin, Jinsun Park_
  <details><summary>Abstract</summary>
  Achieving robust and accurate spatial perception under adverse weather andlighting conditions is crucial for the high-level autonomy of self-drivingvehicles and robots. However, existing perception algorithms relying on thevisible spectrum are highly affected by weather and lighting conditions. Along-wave infrared camera (i.e., thermal imaging camera) can be a potentialsolution to achieve high-level robustness. However, the absence of large-scaledatasets and standardized benchmarks remains a significant bottleneck toprogress in active research for robust visual perception from thermal images.To this end, this manuscript provides a large-scale Multi-Spectral Stereo(MS$^2$) dataset that consists of stereo RGB, stereo NIR, stereo thermal,stereo LiDAR data, and GNSS/IMU information along with semi-dense depth groundtruth. MS$^2$ dataset includes 162K synchronized multi-modal data pairscaptured across diverse locations (e.g., urban city, residential area, campus,and high-way road) at different times (e.g., morning, daytime, and nighttime)and under various weather conditions (e.g., clear-sky, cloudy, and rainy).Secondly, we conduct a thorough evaluation of monocular and stereo depthestimation networks across RGB, NIR, and thermal modalities to establishstandardized benchmark results on MS$^2$ depth test sets (e.g., day, night, andrainy). Lastly, we provide in-depth analyses and discuss the challengesrevealed by the benchmark results, such as the performance variability for eachmodality under adverse conditions, domain shift between different sensormodalities, and potential research direction for thermal perception. Ourdataset and source code are publicly available athttps://sites.google.com/view/multi-spectral-stereo-dataset andhttps://github.com/UkcheolShin/SupDepth4Thermal.
  </details>

- **[A Deep Learning Framework for Boundary-Aware Semantic Segmentation](http://arxiv.org/abs/2503.22050v1)**  `arXiv:2503.22050`  
  _Tai An, Weiqiang Huang, Da Xu, Qingyuan He, Jiacheng Hu, Yujia Lou_
  <details><summary>Abstract</summary>
  As a fundamental task in computer vision, semantic segmentation is widelyapplied in fields such as autonomous driving, remote sensing image analysis,and medical image processing. In recent years, Transformer-based segmentationmethods have demonstrated strong performance in global feature modeling.However, they still struggle with blurred target boundaries and insufficientrecognition of small targets. To address these issues, this study proposes aMask2Former-based semantic segmentation algorithm incorporating a boundaryenhancement feature bridging module (BEFBM). The goal is to improve targetboundary accuracy and segmentation consistency. Built upon the Mask2Formerframework, this method constructs a boundary-aware feature map and introduces afeature bridging mechanism. This enables effective cross-scale feature fusion,enhancing the model's ability to focus on target boundaries. Experiments on theCityscapes dataset demonstrate that, compared to mainstream segmentationmethods, the proposed approach achieves significant improvements in metricssuch as mIOU, mDICE, and mRecall. It also exhibits superior boundary retentionin complex scenes. Visual analysis further confirms the model's advantages infine-grained regions. Future research will focus on optimizing computationalefficiency and exploring its potential in other high-precision segmentationtasks.
  </details>

- **[LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing](http://arxiv.org/abs/2503.21541v2)**  `arXiv:2503.21541`  
  _Achint Soni, Meet Soni, Sirisha Rambhatla_
  <details><summary>Abstract</summary>
  Text-guided image editing aims to modify specific regions of an imageaccording to natural language instructions while maintaining the generalstructure and the background fidelity. Existing methods utilize masks derivedfrom cross-attention maps generated from diffusion models to identify thetarget regions for modification. However, since cross-attention mechanismsfocus on semantic relevance, they struggle to maintain the image integrity. Asa result, these methods often lack spatial consistency, leading to editingartifacts and distortions. In this work, we address these limitations andintroduce LOCATEdit, which enhances cross-attention maps through a graph-basedapproach utilizing self-attention-derived patch relationships to maintainsmooth, coherent attention across image regions, ensuring that alterations arelimited to the designated items while retaining the surrounding structure.LOCATEdit consistently and substantially outperforms existing baselines onPIE-Bench, demonstrating its state-of-the-art performance and effectiveness onvarious editing tasks. Code can be found onhttps://github.com/LOCATEdit/LOCATEdit/
  </details>

- **[LandMarkSystem Technical Report](http://arxiv.org/abs/2503.21364v2)**  `arXiv:2503.21364`  
  _Zhenxiang Ma, Zhenyu Yang, Miao Tao, Yuanzhen Zhou, Zeyu He, Yuchang Zhang, et al._
  <details><summary>Abstract</summary>
  3D reconstruction is vital for applications in autonomous driving, virtualreality, augmented reality, and the metaverse. Recent advancements such asNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformedthe field, yet traditional deep learning frameworks struggle to meet theincreasing demands for scene quality and scale. This paper introducesLandMarkSystem, a novel computing framework designed to enhance multi-scalescene reconstruction and rendering. By leveraging a componentized modeladaptation layer, LandMarkSystem supports various NeRF and 3DGS structureswhile optimizing computational efficiency through distributed parallelcomputing and model parameter offloading. Our system addresses the limitationsof existing frameworks, providing dedicated operators for complex 3D sparsecomputations, thus facilitating efficient training and rapid inference overextensive scenes. Key contributions include a modular architecture, a dynamicloading strategy for limited resources, and proven capabilities across multiplerepresentative algorithms.This comprehensive solution aims to advance theefficiency and effectiveness of 3D reconstruction tasks.To facilitate furtherresearch and collaboration, the source code and documentation for theLandMarkSystem project are publicly available in an open-source repository,accessing the repository at: https://github.com/InternLandMark/LandMarkSystem.
  </details>

- **[Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate Learned Image Compression](http://arxiv.org/abs/2503.21284v2)**  `arXiv:2503.21284`  
  _Hanyue Tu, Siqi Wu, Li Li, Wengang Zhou, Houqiang Li_
  <details><summary>Abstract</summary>
  Autoencoder-based structures have dominated recent learned image compressionmethods. However, the inherent information loss associated with autoencoderslimits their rate-distortion performance at high bit rates and restricts theirflexibility of rate adaptation. In this paper, we present a variable-rate imagecompression model based on invertible transform to overcome these limitations.Specifically, we design a lightweight multi-scale invertible neural network,which bijectively maps the input image into multi-scale latent representations.To improve the compression efficiency, a multi-scale spatial-channel contextmodel with extended gain units is devised to estimate the entropy of the latentrepresentation from high to low levels. Experimental results demonstrate thatthe proposed method achieves state-of-the-art performance compared to existingvariable-rate methods, and remains competitive with recent multi-modelapproaches. Notably, our method is the first learned image compression solutionthat outperforms VVC across a very wide range of bit rates using a singlemodel, especially at high bit rates. The source code is available athttps://github.com/hytu99/MSINN-VRLIC.
  </details>

- **[Omni-AD: Learning to Reconstruct Global and Local Features for Multi-class Anomaly Detection](http://arxiv.org/abs/2503.21125v2)**  `arXiv:2503.21125`  
  _Jiajie Quan, Ao Tong, Yuxuan Cai, Xinwei He, Yulong Wang, Yang Zhou_
  <details><summary>Abstract</summary>
  In multi-class unsupervised anomaly detection(MUAD), reconstruction-basedmethods learn to map input images to normal patterns to identify anomalouspixels. However, this strategy easily falls into the well-known "learningshortcut" issue when decoders fail to capture normal patterns and reconstructboth normal and abnormal samples naively. To address that, we propose to learnthe input features in global and local manners, forcing the network to memorizethe normal patterns more comprehensively. Specifically, we design a two-branchdecoder block, named Omni-block. One branch corresponds to global featurelearning, where we serialize two self-attention blocks but replace the queryand (key, value) with learnable tokens, respectively, thus capturing globalfeatures of normal patterns concisely and thoroughly. The local branchcomprises depth-separable convolutions, whose locality enables effective andefficient learning of local features for normal patterns. By stackingOmni-blocks, we build a framework, Omni-AD, to learn normal patterns ofdifferent granularity and reconstruct them progressively. Comprehensiveexperiments on public anomaly detection benchmarks show that our methodoutperforms state-of-the-art approaches in MUAD. Code is available athttps://github.com/easyoo/Omni-AD.git
  </details>

- **[VinaBench: Benchmark for Faithful and Consistent Visual Narratives](http://arxiv.org/abs/2503.20871v2)**  `arXiv:2503.20871`  
  _Silin Gao, Sheryl Mathew, Li Mi, Sepideh Mamooler, Mengjie Zhao, Hiromi Wakaki, et al._
  <details><summary>Abstract</summary>
  Visual narrative generation transforms textual narratives into sequences ofimages illustrating the content of the text. However, generating visualnarratives that are faithful to the input text and self-consistent acrossgenerated images remains an open challenge, due to the lack of knowledgeconstraints used for planning the stories. In this work, we propose a newbenchmark, VinaBench, to address this challenge. Our benchmark annotates theunderlying commonsense and discourse constraints in visual narrative samples,offering systematic scaffolds for learning the implicit strategies of visualstorytelling. Based on the incorporated narrative constraints, we furtherpropose novel metrics to closely evaluate the consistency of generatednarrative images and the alignment of generations with the input textualnarrative. Our results across three generative vision models demonstrate thatlearning with VinaBench's knowledge constraints effectively improves thefaithfulness and cohesion of generated visual narratives.
  </details>

- **[Feature4X: Bridging Any Monocular Video to 4D Agentic AI with Versatile Gaussian Feature Fields](http://arxiv.org/abs/2503.20776v2)**  `arXiv:2503.20776`  
  _Shijie Zhou, Hui Ren, Yijia Weng, Shuwang Zhang, Zhen Wang, Dejia Xu, et al._
  <details><summary>Abstract</summary>
  Recent advancements in 2D and multimodal models have achieved remarkablesuccess by leveraging large-scale training on extensive datasets. However,extending these achievements to enable free-form interactions and high-levelsemantic operations with complex 3D/4D scenes remains challenging. Thisdifficulty stems from the limited availability of large-scale, annotated 3D/4Dor multi-view datasets, which are crucial for generalizable vision and languagetasks such as open-vocabulary and prompt-based segmentation, language-guidedediting, and visual question answering (VQA). In this paper, we introduceFeature4X, a universal framework designed to extend any functionality from 2Dvision foundation model into the 4D realm, using only monocular video input,which is widely available from user-generated content. The "X" in Feature4Xrepresents its versatility, enabling any task through adaptable,model-conditioned 4D feature field distillation. At the core of our frameworkis a dynamic optimization strategy that unifies multiple model capabilitiesinto a single representation. Additionally, to the best of our knowledge,Feature4X is the first method to distill and lift the features of videofoundation models (e.g., SAM2, InternVideo2) into an explicit 4D feature fieldusing Gaussian Splatting. Our experiments showcase novel view segment anything,geometric and appearance scene editing, and free-form VQA across all timesteps, empowered by LLMs in feedback loops. These advancements broaden thescope of agentic AI applications by providing a foundation for scalable,contextually and spatiotemporally aware systems capable of immersive dynamic 4Dscene interaction.
  </details>

- **[Adaptive Weighted Parameter Fusion with CLIP for Class-Incremental Learning](http://arxiv.org/abs/2503.19503v2)**  `arXiv:2503.19503`  
  _Juncen Guo, Xiaoguang Zhu, Liangyu Teng, Hao Yang, Jing Liu, Yang Liu, et al._
  <details><summary>Abstract</summary>
  Class-incremental Learning (CIL) enables the model to incrementally absorbknowledge from new classes and build a generic classifier across all previouslyencountered classes. When the model optimizes with new classes, the knowledgeof previous classes is inevitably erased, leading to catastrophic forgetting.Addressing this challenge requires making a trade-off between retaining oldknowledge and accommodating new information. However, this balancing processoften requires sacrificing some information, which can lead to a partial lossin the model's ability to discriminate between classes. To tackle this issue,we design the adaptive weighted parameter fusion with ContrastiveLanguage-Image Pre-training (CLIP), which not only takes into account thevariability of the data distribution of different tasks, but also retains allthe effective information of the parameter matrix to the greatest extent. Inaddition, we introduce a balance factor that can balance the data distributionalignment and distinguishability of adjacent tasks. Experimental results onseveral traditional benchmarks validate the superiority of the proposed method.
  </details>

- **[GaussianUDF: Inferring Unsigned Distance Functions through 3D Gaussian Splatting](http://arxiv.org/abs/2503.19458v2)**  `arXiv:2503.19458`  
  _Shujuan Li, Yu-Shen Liu, Zhizhong Han_
  <details><summary>Abstract</summary>
  Reconstructing open surfaces from multi-view images is vital in digitalizingcomplex objects in daily life. A widely used strategy is to learn unsigneddistance functions (UDFs) by checking if their appearance conforms to the imageobservations through neural rendering. However, it is still hard to learncontinuous and implicit UDF representations through 3D Gaussians splatting(3DGS) due to the discrete and explicit scene representation, i.e., 3DGaussians. To resolve this issue, we propose a novel approach to bridge the gapbetween 3D Gaussians and UDFs. Our key idea is to overfit thin and flat 2DGaussian planes on surfaces, and then, leverage the self-supervision andgradient-based inference to supervise unsigned distances in both near and fararea to surfaces. To this end, we introduce novel constraints and strategies toconstrain the learning of 2D Gaussians to pursue more stable optimization andmore reliable self-supervision, addressing the challenges brought bycomplicated gradient field on or near the zero level set of UDFs. We reportnumerical and visual comparisons with the state-of-the-art on widely usedbenchmarks and real data to show our advantages in terms of accuracy,efficiency, completeness, and sharpness of reconstructed open surfaces withboundaries.
  </details>

- **[Diffusion-4K: Ultra-High-Resolution Image Synthesis with Latent Diffusion Models](http://arxiv.org/abs/2503.18352v2)**  `arXiv:2503.18352`  
  _Jinjin Zhang, Qiuyu Huang, Junjie Liu, Xiefan Guo, Di Huang_
  <details><summary>Abstract</summary>
  In this paper, we present Diffusion-4K, a novel framework for directultra-high-resolution image synthesis using text-to-image diffusion models. Thecore advancements include: (1) Aesthetic-4K Benchmark: addressing the absenceof a publicly available 4K image synthesis dataset, we construct Aesthetic-4K,a comprehensive benchmark for ultra-high-resolution image generation. Wecurated a high-quality 4K dataset with carefully selected images and captionsgenerated by GPT-4o. Additionally, we introduce GLCM Score and CompressionRatio metrics to evaluate fine details, combined with holistic measures such asFID, Aesthetics and CLIPScore for a comprehensive assessment ofultra-high-resolution images. (2) Wavelet-based Fine-tuning: we propose awavelet-based fine-tuning approach for direct training with photorealistic 4Kimages, applicable to various latent diffusion models, demonstrating itseffectiveness in synthesizing highly detailed 4K images. Consequently,Diffusion-4K achieves impressive performance in high-quality image synthesisand text prompt adherence, especially when powered by modern large-scalediffusion models (e.g., SD3-2B and Flux-12B). Extensive experimental resultsfrom our benchmark demonstrate the superiority of Diffusion-4K inultra-high-resolution image synthesis.
  </details>

- **[UniCon: Unidirectional Information Flow for Effective Control of Large-Scale Diffusion Models](http://arxiv.org/abs/2503.17221v2)**  `arXiv:2503.17221`  
  _Fanghua Yu, Jinjin Gu, Jinfan Hu, Zheyuan Li, Chao Dong_
  <details><summary>Abstract</summary>
  We introduce UniCon, a novel architecture designed to enhance control andefficiency in training adapters for large-scale diffusion models. Unlikeexisting methods that rely on bidirectional interaction between the diffusionmodel and control adapter, UniCon implements a unidirectional flow from thediffusion network to the adapter, allowing the adapter alone to generate thefinal output. UniCon reduces computational demands by eliminating the need forthe diffusion model to compute and store gradients during adapter training. Ourresults indicate that UniCon reduces GPU memory usage by one-third andincreases training speed by 2.3 times, while maintaining the same adapterparameter size. Additionally, without requiring extra computational resources,UniCon enables the training of adapters with double the parameter volume ofexisting ControlNets. In a series of image conditional generation tasks, UniConhas demonstrated precise responsiveness to control inputs and exceptionalgeneration capabilities.
  </details>

- **[Cross-Modal and Uncertainty-Aware Agglomeration for Open-Vocabulary 3D Scene Understanding](http://arxiv.org/abs/2503.16707v2)**  `arXiv:2503.16707`  
  _Jinlong Li, Cristiano Saltori, Fabio Poiesi, Nicu Sebe_
  <details><summary>Abstract</summary>
  The lack of a large-scale 3D-text corpus has led recent works to distillopen-vocabulary knowledge from vision-language models (VLMs). However, thesemethods typically rely on a single VLM to align the feature spaces of 3D modelswithin a common language space, which limits the potential of 3D models toleverage the diverse spatial and semantic capabilities encapsulated in variousfoundation models. In this paper, we propose Cross-modal and Uncertainty-awareAgglomeration for Open-vocabulary 3D Scene Understanding dubbed CUA-O3D, thefirst model to integrate multiple foundation models-such as CLIP, DINOv2, andStable Diffusion-into 3D scene understanding. We further introduce adeterministic uncertainty estimation to adaptively distill and harmonize theheterogeneous 2D feature embeddings from these models. Our method addresses twokey challenges: (1) incorporating semantic priors from VLMs alongside thegeometric knowledge of spatially-aware vision foundation models, and (2) usinga novel deterministic uncertainty estimation to capture model-specificuncertainties across diverse semantic and geometric sensitivities, helping toreconcile heterogeneous representations during training. Extensive experimentson ScanNetV2 and Matterport3D demonstrate that our method not only advancesopen-vocabulary segmentation but also achieves robust cross-domain alignmentand competitive spatial perception capabilities. The code will be available at:https://github.com/TyroneLi/CUA_O3D.
  </details>

- **[Does Your Vision-Language Model Get Lost in the Long Video Sampling Dilemma?](http://arxiv.org/abs/2503.12496v2)**  `arXiv:2503.12496`  
  _Tianyuan Qu, Longxiang Tang, Bohao Peng, Senqiao Yang, Bei Yu, Jiaya Jia_
  <details><summary>Abstract</summary>
  The rise of Large Vision-Language Models (LVLMs) has significantly advancedvideo understanding. However, efficiently processing long videos remains achallenge due to the ``Sampling Dilemma'': low-density sampling risks missingcritical information, while high-density sampling introduces redundancy. Toaddress this issue, we introduce LSDBench, the first benchmark designed toevaluate LVLMs on long-video tasks by constructing high Necessary SamplingDensity (NSD) questions, where NSD represents the minimum sampling densityrequired to accurately answer a given question. LSDBench focuses on dense,short-duration actions to rigorously assess the sampling strategies employed byLVLMs. To tackle the challenges posed by high-NSD questions, we propose a novelReasoning-Driven Hierarchical Sampling (RHS) framework, which combines globallocalization of question-relevant cues with local dense sampling for preciseinference. Additionally, we develop a lightweight Semantic-Guided FrameSelector to prioritize informative frames, enabling RHS to achieve comparableor superior performance with significantly fewer sampled frames. Together, ourLSDBench and RHS framework address the unique challenges of high-NSD long-videotasks, setting a new standard for evaluating and improving LVLMs in thisdomain. Our benchmark and evaluation codes has been released at:https://github.com/dvlab-research/LSDBench
  </details>

- **[StreamMind: Unlocking Full Frame Rate Streaming Video Dialogue through Event-Gated Cognition](http://arxiv.org/abs/2503.06220v2)**  `arXiv:2503.06220`  
  _Xin Ding, Hao Wu, Yifan Yang, Shiqi Jiang, Donglin Bai, Zhibo Chen, et al._
  <details><summary>Abstract</summary>
  With the rise of real-world human-AI interaction applications, such as AIassistants, the need for Streaming Video Dialogue is critical. To address thisneed, we introduce StreamMind, a video LLM framework that achieves ultra-FPSstreaming video processing (100 fps on a single A100) and enables proactive,always-on responses in real time, without explicit user intervention.  To solve the key challenge of the contradiction between linear videostreaming speed and quadratic transformer computation cost, we propose a novelperception-cognition interleaving paradigm named ''event-gated LLMinvocation'', in contrast to the existing per-time-step LLM invocation. Byintroducing a Cognition Gate network between the video encoder and the LLM, LLMis only invoked when relevant events occur. To realize the event featureextraction with constant cost, we propose Event-Preserving Feature Extractor(EPFE) based on state-space method, generating a single perception token forspatiotemporal features. These techniques enable the video LLM with full-FPSperception and real-time cognition response.  Experiments on Ego4D and SoccerNet streaming tasks, as well as standardoffline benchmarks, demonstrate state-of-the-art performance in both modelcapability and real-time efficiency, paving the way for ultra-high-FPSapplications, such as Game AI and interactive media. The code and data isavailable at https://aka.ms/StreamMind.
  </details>

- **[Patch-Depth Fusion: Dichotomous Image Segmentation via Fine-Grained Patch Strategy and Depth Integrity-Prior](http://arxiv.org/abs/2503.06100v3)**  `arXiv:2503.06100`  
  _Xianjie Liu, Keren Fu, Qijun Zhao_
  <details><summary>Abstract</summary>
  Dichotomous Image Segmentation (DIS) is a high-precision object segmentationtask for high-resolution natural images. The current mainstream methods focuson the optimization of local details but overlook the fundamental challenge ofmodeling the integrity of objects. We have found that the depth integrity-priorimplicit in the the pseudo-depth maps generated by Depth Anything Model v2 andthe local detail features of image patches can jointly address the abovedilemmas. Based on the above findings, we have designed a novel Patch-DepthFusion Network (PDFNet) for high-precision dichotomous image segmentation. Thecore of PDFNet consists of three aspects. Firstly, the object perception isenhanced through multi-modal input fusion. By utilizing the patch fine-grainedstrategy, coupled with patch selection and enhancement, the sensitivity todetails is improved. Secondly, by leveraging the depth integrity-priordistributed in the depth maps, we propose an integrity-prior loss to enhancethe uniformity of the segmentation results in the depth maps. Finally, weutilize the features of the shared encoder and, through a simple depthrefinement decoder, improve the ability of the shared encoder to capture subtledepth-related information in the images. Experiments on the DIS-5K dataset showthat PDFNet significantly outperforms state-of-the-art non-diffusion methods.Due to the incorporation of the depth integrity-prior, PDFNet achieves or evensurpassing the performance of the latest diffusion-based methods while usingless than 11% of the parameters of diffusion-based methods. The source code athttps://github.com/Tennine2077/PDFNet
  </details>

- **[Improving SAM for Camouflaged Object Detection via Dual Stream Adapters](http://arxiv.org/abs/2503.06042v2)**  `arXiv:2503.06042`  
  _Jiaming Liu, Linghe Kong, Guihai Chen_
  <details><summary>Abstract</summary>
  Segment anything model (SAM) has shown impressive general-purposesegmentation performance on natural images, but its performance on camouflagedobject detection (COD) is unsatisfactory. In this paper, we propose SAM-CODthat performs camouflaged object detection for RGB-D inputs. While keeping theSAM architecture intact, dual stream adapters are expanded on the image encoderto learn potential complementary information from RGB images and depth images,and fine-tune the mask decoder and its depth replica to perform dual-streammask prediction. In practice, the dual stream adapters are embedded into theattention block of the image encoder in a parallel manner to facilitate therefinement and correction of the two types of image embeddings. To mitigatechannel discrepancies arising from dual stream embeddings that do not directlyinteract with each other, we augment the association of dual stream embeddingsusing bidirectional knowledge distillation including a model distiller and amodal distiller. In addition, to predict the masks for RGB and depth attentionmaps, we hybridize the two types of image embeddings which are jointly learnedwith the prompt embeddings to update the initial prompt, and then feed theminto the mask decoders to synchronize the consistency of image embeddings andprompt embeddings. Experimental results on four COD benchmarks show that ourSAM-COD achieves excellent detection performance gains over SAM and achievesstate-of-the-art results with a given fine-tuning paradigm.
  </details>

- **[Generalizable Prompt Learning of CLIP: A Brief Overview](http://arxiv.org/abs/2503.01263v3)**  `arXiv:2503.01263`  
  _Fangming Cui, Yonggang Zhang, Xuan Wang, Xule Wang, Liang Xiao_
  <details><summary>Abstract</summary>
  Existing vision-language models (VLMs) such as CLIP have showcased animpressive capability to generalize well across various downstream tasks. Thesemodels leverage the synergy between visual and textual information, enablingthem to understand and reason about the content present in images and text in aunified manner. This article provides a brief overview of CLIP based onfew-shot prompt learning, including experimental data and technicalcharacteristics of some methods. The purpose of this review is to provide areference for researchers who have just started their research in generalizableprompting of CLIP through few-shot training for classification across 15datasets and also to facilitate the integration of this field by researchers inother downstream tasks.
  </details>

- **[Solving Instance Detection from an Open-World Perspective](http://arxiv.org/abs/2503.00359v2)**  `arXiv:2503.00359`  
  _Qianqian Shen, Yunhan Zhao, Nahyun Kwon, Jeeeun Kim, Yanan Li, Shu Kong_
  <details><summary>Abstract</summary>
  Instance detection (InsDet) aims to localize specific object instances withina novel scene imagery based on given visual references. Technically, itrequires proposal detection to identify all possible object instances, followedby instance-level matching to pinpoint the ones of interest. Its open-worldnature supports its broad applications from robotics to AR/VR but also presentssignificant challenges: methods must generalize to unknown testing datadistributions because (1) the testing scene imagery is unseen during training,and (2) there are domain gaps between visual references and detected proposals.Existing methods tackle these challenges by synthesizing diverse trainingexamples or utilizing off-the-shelf foundation models (FMs). However, they onlypartially capitalize the available open-world information. In contrast, weapproach InsDet from an Open-World perspective, introducing our method IDOW. Wefind that, while pretrained FMs yield high recall in instance detection, theyare not specifically optimized for instance-level feature matching. Therefore,we adapt pretrained FMs for improved instance-level matching using open-worlddata. Our approach incorporates metric learning along with novel dataaugmentations, which sample distractors as negative examples and synthesizenovel-view instances to enrich the visual references. Extensive experimentsdemonstrate that our method significantly outperforms prior works, achieving>10 AP over previous results on two recently released challenging benchmarkdatasets in both conventional and novel instance detection settings.
  </details>

- **[Visual Agentic AI for Spatial Reasoning with a Dynamic API](http://arxiv.org/abs/2502.06787v2)**  `arXiv:2502.06787`  
  _Damiano Marsili, Rohun Agrawal, Yisong Yue, Georgia Gkioxari_
  <details><summary>Abstract</summary>
  Visual reasoning -- the ability to interpret the visual world -- is crucialfor embodied agents that operate within three-dimensional scenes. Progress inAI has led to vision and language models capable of answering questions fromimages. However, their performance declines when tasked with 3D spatialreasoning. To tackle the complexity of such reasoning problems, we introduce anagentic program synthesis approach where LLM agents collaboratively generate aPythonic API with new functions to solve common subproblems. Our methodovercomes limitations of prior approaches that rely on a static, human-definedAPI, allowing it to handle a wider range of queries. To assess AI capabilitiesfor 3D understanding, we introduce a new benchmark of queries involvingmultiple steps of grounding and inference. We show that our method outperformsprior zero-shot models for visual reasoning in 3D and empirically validate theeffectiveness of our agentic framework for 3D spatial reasoning tasks. Projectwebsite: https://glab-caltech.github.io/vadar/
  </details>

- **[PromptMono: Cross Prompting Attention for Self-Supervised Monocular Depth Estimation in Challenging Environments](http://arxiv.org/abs/2501.13796v2)**  `arXiv:2501.13796`  
  _Changhao Wang, Guanwen Zhang, Zhengyun Cheng, Wei Zhou_
  <details><summary>Abstract</summary>
  Considerable efforts have been made to improve monocular depth estimationunder ideal conditions. However, in challenging environments, monocular depthestimation still faces difficulties. In this paper, we introduce visual promptlearning for predicting depth across different environments within a unifiedmodel, and present a self-supervised learning framework called PromptMono. Itemploys a set of learnable parameters as visual prompts to capturedomain-specific knowledge. To integrate prompting information into imagerepresentations, a novel gated cross prompting attention (GCPA) module isproposed, which enhances the depth estimation in diverse conditions. Weevaluate the proposed PromptMono on the Oxford Robotcar dataset and thenuScenes dataset. Experimental results demonstrate the superior performance ofthe proposed method.
  </details>

- **[Light Transport-aware Diffusion Posterior Sampling for Single-View Reconstruction of 3D Volumes](http://arxiv.org/abs/2501.05226v3)**  `arXiv:2501.05226`  
  _Ludwic Leonard, Nils Thuerey, Ruediger Westermann_
  <details><summary>Abstract</summary>
  We introduce a single-view reconstruction technique of volumetric fields inwhich multiple light scattering effects are omnipresent, such as in clouds. Wemodel the unknown distribution of volumetric fields using an unconditionaldiffusion model trained on a novel benchmark dataset comprising 1,000synthetically simulated volumetric density fields. The neural diffusion modelis trained on the latent codes of a novel, diffusion-friendly, monoplanarrepresentation. The generative model is used to incorporate a tailoredparametric diffusion posterior sampling technique into different reconstructiontasks. A physically-based differentiable volume renderer is employed to providegradients with respect to light transport in the latent space. This stands incontrast to classic NeRF approaches and makes the reconstructions betteraligned with observed data. Through various experiments, we demonstratesingle-view reconstruction of volumetric clouds at a previously unattainablequality.
  </details>

- **[TADFormer : Task-Adaptive Dynamic Transformer for Efficient Multi-Task Learning](http://arxiv.org/abs/2501.04293v2)**  `arXiv:2501.04293`  
  _Seungmin Baek, Soyul Lee, Hayeon Jo, Hyesong Choi, Dongbo Min_
  <details><summary>Abstract</summary>
  Transfer learning paradigm has driven substantial advancements in variousvision tasks. However, as state-of-the-art models continue to grow, classicalfull fine-tuning often becomes computationally impractical, particularly inmulti-task learning (MTL) setup where training complexity increasesproportional to the number of tasks. Consequently, recent studies have exploredParameter-Efficient Fine-Tuning (PEFT) for MTL architectures. Despite someprogress, these approaches still exhibit limitations in capturing fine-grained,task-specific features that are crucial to MTL. In this paper, we introduceTask-Adaptive Dynamic transFormer, termed TADFormer, a novel PEFT frameworkthat performs task-aware feature adaptation in the fine-grained manner bydynamically considering task-specific input contexts. TADFormer proposes theparameter-efficient prompting for task adaptation and the Dynamic Task Filter(DTF) to capture task information conditioned on input contexts. Experiments onthe PASCAL-Context benchmark demonstrate that the proposed method achieveshigher accuracy in dense scene understanding tasks, while reducing the numberof trainable parameters by up to 8.4 times when compared to full fine-tuning ofMTL models. TADFormer also demonstrates superior parameter efficiency andaccuracy compared to recent PEFT methods.
  </details>

- **[DoubleDiffusion: Combining Heat Diffusion with Denoising Diffusion for Generative Learning on 3D Meshes](http://arxiv.org/abs/2501.03397v4)**  `arXiv:2501.03397`  
  _Xuyang Wang, Ziang Cheng, Zhenyu Li, Jiayu Yang, Haorui Ji, Pan Ji, et al._
  <details><summary>Abstract</summary>
  This paper proposes DoubleDiffusion, a novel framework that combines heatdissipation diffusion and denoising diffusion for direct generative learning on3D mesh surfaces. Our approach addresses the challenges of generatingcontinuous signal distributions residing on a curve manifold surface. Unlikeprevious methods that rely on unrolling 3D meshes into 2D or adopting fieldrepresentations, DoubleDiffusion leverages the Laplacian-Beltrami operator toprocess features respecting the mesh structure. This combination enableseffective geometry-aware signal diffusion across the underlying geometry. Asshown in Fig.1, we demonstrate that DoubleDiffusion has the ability to generateRGB signal distributions on complex 3D mesh surfaces and achieves per-categoryshape-conditioned texture generation across different shape geometry. Our workcontributes a new direction in diffusion-based generative modeling on 3Dsurfaces, with potential applications in the field of 3D asset generation.
  </details>

- **[VidTwin: Video VAE with Decoupled Structure and Dynamics](http://arxiv.org/abs/2412.17726v2)**  `arXiv:2412.17726`  
  _Yuchi Wang, Junliang Guo, Xinyi Xie, Tianyu He, Xu Sun, Jiang Bian_
  <details><summary>Abstract</summary>
  Recent advancements in video autoencoders (Video AEs) have significantlyimproved the quality and efficiency of video generation. In this paper, wepropose a novel and compact video autoencoder, VidTwin, that decouples videointo two distinct latent spaces: Structure latent vectors, which captureoverall content and global movement, and Dynamics latent vectors, whichrepresent fine-grained details and rapid movements. Specifically, our approachleverages an Encoder-Decoder backbone, augmented with two submodules forextracting these latent spaces, respectively. The first submodule employs aQ-Former to extract low-frequency motion trends, followed by downsamplingblocks to remove redundant content details. The second averages the latentvectors along the spatial dimension to capture rapid motion. Extensiveexperiments show that VidTwin achieves a high compression rate of 0.20% withhigh reconstruction quality (PSNR of 28.14 on the MCL-JCV dataset), andperforms efficiently and effectively in downstream generative tasks. Moreover,our model demonstrates explainability and scalability, paving the way forfuture research in video latent representation and generation. Check ourproject page for more details: https://vidtwin.github.io/.
  </details>

- **[PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models](http://arxiv.org/abs/2412.16257v2)**  `arXiv:2412.16257`  
  _Zhuomeng Zhang, Fangqi Li, Chong Di, Hongyu Zhu, Hanyi Wang, Shilin Wang_
  <details><summary>Abstract</summary>
  Despite the impressive synthesis quality of text-to-image (T2I) diffusionmodels, their black-box deployment poses significant regulatory challenges:Malicious actors can fine-tune these models to generate illegal content,circumventing existing safeguards through parameter manipulation. Therefore, itis essential to verify the integrity of T2I diffusion models. To this end,considering the randomness within the outputs of generative models and the highcosts in interacting with them, we discern model tampering via the KLdivergence between the distributions of the features of generated images. Wepropose a novel prompt selection algorithm based on learning automaton(PromptLA) for efficient and accurate verification. Evaluations on fouradvanced T2I models (e.g., SDXL, FLUX.1) demonstrate that our method achieves amean AUC of over 0.96 in integrity detection, exceeding baselines by more than0.2, showcasing strong effectiveness and generalization. Additionally, ourapproach achieves lower cost and is robust against image-level post-processing.To the best of our knowledge, this paper is the first work addressing theintegrity verification of T2I diffusion models, which establishes quantifiablestandards for AI copyright litigation in practice.
  </details>

- **[LeviTor: 3D Trajectory Oriented Image-to-Video Synthesis](http://arxiv.org/abs/2412.15214v2)**  `arXiv:2412.15214`  
  _Hanlin Wang, Hao Ouyang, Qiuyu Wang, Wen Wang, Ka Leong Cheng, Qifeng Chen, et al._
  <details><summary>Abstract</summary>
  The intuitive nature of drag-based interaction has led to its growingadoption for controlling object trajectories in image-to-video synthesis.Still, existing methods that perform dragging in the 2D space usually faceambiguity when handling out-of-plane movements. In this work, we augment theinteraction with a new dimension, i.e., the depth dimension, such that usersare allowed to assign a relative depth for each point on the trajectory. Thatway, our new interaction paradigm not only inherits the convenience from 2Ddragging, but facilitates trajectory control in the 3D space, broadening thescope of creativity. We propose a pioneering method for 3D trajectory controlin image-to-video synthesis by abstracting object masks into a few clusterpoints. These points, accompanied by the depth information and the instanceinformation, are finally fed into a video diffusion model as the controlsignal. Extensive experiments validate the effectiveness of our approach,dubbed LeviTor, in precisely manipulating the object movements when producingphoto-realistic videos from static images. Our code is available at:https://github.com/ant-research/LeviTor.
  </details>

- **[Combating Semantic Contamination in Learning with Label Noise](http://arxiv.org/abs/2412.11620v3)**  `arXiv:2412.11620`  
  _Wenxiao Fan, Kan Li_
  <details><summary>Abstract</summary>
  Noisy labels can negatively impact the performance of deep neural networks.One common solution is label refurbishment, which involves reconstructing noisylabels through predictions and distributions. However, these methods mayintroduce problematic semantic associations, a phenomenon that we identify asSemantic Contamination. Through an analysis of Robust LR, a representativelabel refurbishment method, we found that utilizing the logits of views forrefurbishment does not adequately balance the semantic information ofindividual classes. Conversely, using the logits of models fails to maintainconsistent semantic relationships across models, which explains why labelrefurbishment methods frequently encounter issues related to SemanticContamination. To address this issue, we propose a novel method calledCollaborative Cross Learning, which utilizes semi-supervised learning onrefurbished labels to extract appropriate semantic associations from embeddingsacross views and models. Experimental results show that our method outperformsexisting approaches on both synthetic and real-world noisy datasets,effectively mitigating the impact of label noise and Semantic Contamination.
  </details>

- **[Can video generation replace cinematographers? Research on the cinematic language of generated video](http://arxiv.org/abs/2412.12223v2)**  `arXiv:2412.12223`  
  _Xiaozhe Li, Kai WU, Siyi Yang, YiZhan Qu, Guohua. Zhang, Zhiyu Chen, et al._
  <details><summary>Abstract</summary>
  Recent advancements in text-to-video (T2V) generation have leverageddiffusion models to enhance visual coherence in videos synthesized from textualdescriptions. However, existing research primarily focuses on object motion,often overlooking cinematic language, which is crucial for conveying emotionand narrative pacing in cinematography. To address this, we propose a threefoldapproach to improve cinematic control in T2V models. First, we introduce ameticulously annotated cinematic language dataset with twenty subcategories,covering shot framing, shot angles, and camera movements, enabling models tolearn diverse cinematic styles. Second, we present CameraDiff, which employsLoRA for precise and stable cinematic control, ensuring flexible shotgeneration. Third, we propose CameraCLIP, designed to evaluate cinematicalignment and guide multi-shot composition. Building on CameraCLIP, weintroduce CLIPLoRA, a CLIP-guided dynamic LoRA composition method thatadaptively fuses multiple pre-trained cinematic LoRAs, enabling smoothtransitions and seamless style blending. Experimental results demonstrate thatCameraDiff ensures stable and precise cinematic control, CameraCLIP achieves anR@1 score of 0.83, and CLIPLoRA significantly enhances multi-shot compositionwithin a single video, bridging the gap between automated video generation andprofessional cinematography.\textsuperscript{1}
  </details>

- **[LVMark: Robust Watermark for Latent Video Diffusion Models](http://arxiv.org/abs/2412.09122v3)**  `arXiv:2412.09122`  
  _MinHyuk Jang, Youngdong Jang, JaeHyeok Lee, Feng Yang, Gyeongrok Oh, Jongheon Jeong, et al._
  <details><summary>Abstract</summary>
  Rapid advancements in video diffusion models have enabled the creation ofrealistic videos, raising concerns about unauthorized use and driving thedemand for techniques to protect model ownership. Existing watermarkingmethods, while effective for image diffusion models, do not account fortemporal consistency, leading to degraded video quality and reduced robustnessagainst video distortions. To address this issue, we introduce LVMark, a novelwatermarking method for video diffusion models. We propose a new watermarkdecoder tailored for generated videos by learning the consistency betweenadjacent frames. It ensures accurate message decoding, even under maliciousattacks, by combining the low-frequency components of the 3D wavelet domainwith the RGB features of the video. Additionally, our approach minimizes videoquality degradation by embedding watermark messages in layers with minimalimpact on visual appearance using an importance-based weight modulationstrategy. We optimize both the watermark decoder and the latent decoder ofdiffusion model, effectively balancing the trade-off between visual quality andbit accuracy. Our experiments show that our method embeds invisible watermarksinto video diffusion models, ensuring robust decoding accuracy with 512-bitcapacity, even under video distortions.
  </details>

- **[CRAFT: Designing Creative and Functional 3D Objects](http://arxiv.org/abs/2412.03889v2)**  `arXiv:2412.03889`  
  _Michelle Guo, Mia Tang, Hannah Cha, Ruohan Zhang, C. Karen Liu, Jiajun Wu_
  <details><summary>Abstract</summary>
  For designing a wide range of everyday objects, the design process should beaware of both the human body and the underlying semantics of the designspecification. However, these two objectives present significant challenges tothe current AI-based designing tools. In this work, we present a method tosynthesize body-aware 3D objects from a base mesh given an input body geometryand either text or image as guidance. The generated objects can be simulated onvirtual characters, or fabricated for real-world use. We propose to use a meshdeformation procedure that optimizes for both semantic alignment as well ascontact and penetration losses. Using our method, users can generate bothvirtual or real-world objects from text, image, or sketch, without the need formanual artist intervention. We present both qualitative and quantitativeresults on various object categories, demonstrating the effectiveness of ourapproach.
  </details>

- **[ShadowHack: Hacking Shadows via Luminance-Color Divide and Conquer](http://arxiv.org/abs/2412.02545v3)**  `arXiv:2412.02545`  
  _Jin Hu, Mingjia Li, Xiaojie Guo_
  <details><summary>Abstract</summary>
  Shadows introduce challenges such as reduced brightness, texturedeterioration, and color distortion in images, complicating a holisticsolution. This study presents \textbf{ShadowHack}, a divide-and-conquerstrategy that tackles these complexities by decomposing the original task intoluminance recovery and color remedy. To brighten shadow regions and repair thecorrupted textures in the luminance space, we customize LRNet, a U-shapednetwork with a rectified attention module, to enhance information interactionand recalibrate contaminated attention maps. With luminance recovered, CRNetthen leverages cross-attention mechanisms to revive vibrant colors, producingvisually compelling results. Extensive experiments on multiple datasets areconducted to demonstrate the superiority of ShadowHack over existingstate-of-the-art solutions both quantitatively and qualitatively, highlightingthe effectiveness of our design. Our code will be made publicly available.
  </details>

- **[Circumventing shortcuts in audio-visual deepfake detection datasets with unsupervised learning](http://arxiv.org/abs/2412.00175v2)**  `arXiv:2412.00175`  
  _Stefan Smeu, Dragos-Alexandru Boldisor, Dan Oneata, Elisabeta Oneata_
  <details><summary>Abstract</summary>
  Good datasets are essential for developing and benchmarking any machinelearning system. Their importance is even more extreme for safety criticalapplications such as deepfake detection - the focus of this paper. Here wereveal that two of the most widely used audio-video deepfake datasets sufferfrom a previously unidentified spurious feature: the leading silence. Fakevideos start with a very brief moment of silence and based on this featurealone, we can separate the real and fake samples almost perfectly. As such,previous audio-only and audio-video models exploit the presence of silence inthe fake videos and consequently perform worse when the leading silence isremoved. To circumvent latching on such unwanted artifact and possibly otherunrevealed ones we propose a shift from supervised to unsupervised learning bytraining models exclusively on real data. We show that by aligningself-supervised audio-video representations we remove the risk of relying ondataset-specific biases and improve robustness in deepfake detection.
  </details>

- **[Towards Stabilized and Efficient Diffusion Transformers through Long-Skip-Connections with Spectral Constraints](http://arxiv.org/abs/2411.17616v3)**  `arXiv:2411.17616`  
  _Guanjie Chen, Xinyu Zhao, Yucheng Zhou, Xiaoye Qu, Tianlong Chen, Yu Cheng_
  <details><summary>Abstract</summary>
  Diffusion Transformers (DiT) have emerged as a powerful architecture forimage and video generation, offering superior quality and scalability. However,their practical application suffers from inherent dynamic feature instability,leading to error amplification during cached inference. Through systematicanalysis, we identify the absence of long-range feature preservation mechanismsas the root cause of unstable feature propagation and perturbation sensitivity.To this end, we propose Skip-DiT, a novel DiT variant enhanced withLong-Skip-Connections (LSCs) - the key efficiency component in U-Nets.Theoretical spectral norm and visualization analysis demonstrate how LSCsstabilize feature dynamics. Skip-DiT architecture and its stabilized dynamicfeature enable an efficient statical caching mechanism that reuses deepfeatures across timesteps while updating shallow components. Extensiveexperiments across image and video generation tasks demonstrate that Skip-DiTachieves: (1) 4.4 times training acceleration and faster convergence, (2) 1.5-2times inference acceleration without quality loss and high fidelity to originaloutput, outperforming existing DiT caching methods across various quantitativemetrics. Our findings establish long-skip connections as critical architecturalcomponents for training stable and efficient diffusion transformers.
  </details>

- **[DyCoke: Dynamic Compression of Tokens for Fast Video Large Language Models](http://arxiv.org/abs/2411.15024v3)**  `arXiv:2411.15024`  
  _Keda Tao, Can Qin, Haoxuan You, Yang Sui, Huan Wang_
  <details><summary>Abstract</summary>
  Video large language models (VLLMs) have significantly advanced recently inprocessing complex video content, yet their inference efficiency remainsconstrained because of the high computational cost stemming from the thousandsof visual tokens generated from the video inputs. We empirically observe that,unlike single image inputs, VLLMs typically attend visual tokens from differentframes at different decoding iterations, making a one-shot pruning strategyprone to removing important tokens by mistake. Motivated by this, we presentDyCoke, a training-free token compression method to optimize tokenrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-playtemporal compression module to minimize temporal redundancy by mergingredundant tokens across frames, and applies dynamic KV cache reduction to prunespatially redundant tokens selectively. It ensures high-quality inference bydynamically retaining the critical tokens at each decoding step. Extensiveexperimental results demonstrate that DyCoke can outperform the prior SoTAcounterparts, achieving 1.5X inference speedup, 1.4X memory reduction againstthe baseline VLLM, while still improving the performance, with no training.
  </details>

- **[Find Any Part in 3D](http://arxiv.org/abs/2411.13550v2)**  `arXiv:2411.13550`  
  _Ziqi Ma, Yisong Yue, Georgia Gkioxari_
  <details><summary>Abstract</summary>
  Why don't we have foundation models in 3D yet? A key limitation is datascarcity. For 3D object part segmentation, existing datasets are small in sizeand lack diversity. We show that it is possible to break this data barrier bybuilding a data engine powered by 2D foundation models. Our data engineautomatically annotates any number of object parts: 1755x more unique parttypes than existing datasets combined. By training on our annotated data with asimple contrastive objective, we obtain an open-world model that generalizes toany part in any object based on any text query. Even when evaluated zero-shot,we outperform existing methods on the datasets they train on. We achieve 260%improvement in mIoU and boost speed by 6x to 300x. Our scaling analysisconfirms that this generalization stems from the data scale, which underscoresthe impact of our data engine. Finally, to advance general-category open-world3D part segmentation, we release a benchmark covering a wide range of objectsand parts. Project website: https://ziqi-ma.github.io/find3dsite/
  </details>

- **[Stacking Brick by Brick: Aligned Feature Isolation for Incremental Face Forgery Detection](http://arxiv.org/abs/2411.11396v3)**  `arXiv:2411.11396`  
  _Jikang Cheng, Zhiyuan Yan, Ying Zhang, Li Hao, Jiaxin Ai, Qin Zou, et al._
  <details><summary>Abstract</summary>
  The rapid advancement of face forgery techniques has introduced a growingvariety of forgeries. Incremental Face Forgery Detection (IFFD), involvinggradually adding new forgery data to fine-tune the previously trained model,has been introduced as a promising strategy to deal with evolving forgerymethods. However, a naively trained IFFD model is prone to catastrophicforgetting when new forgeries are integrated, as treating all forgeries as asingle ''Fake" class in the Real/Fake classification can cause differentforgery types overriding one another, thereby resulting in the forgetting ofunique characteristics from earlier tasks and limiting the model'seffectiveness in learning forgery specificity and generality. In this paper, wepropose to stack the latent feature distributions of previous and new tasksbrick by brick, $\textit{i.e.}$, achieving $\textbf{aligned featureisolation}$. In this manner, we aim to preserve learned forgery information andaccumulate new knowledge by minimizing distribution overriding, therebymitigating catastrophic forgetting. To achieve this, we first introduce SparseUniform Replay (SUR) to obtain the representative subsets that could be treatedas the uniformly sparse versions of the previous global distributions. We thenpropose a Latent-space Incremental Detector (LID) that leverages SUR data toisolate and align distributions. For evaluation, we construct a more advancedand comprehensive benchmark tailored for IFFD. The leading experimental resultsvalidate the superiority of our method.
  </details>

- **[Evaluating the evaluators: Towards human-aligned metrics for missing markers reconstruction](http://arxiv.org/abs/2410.14334v2)**  `arXiv:2410.14334`  
  _Taras Kucherenko, Derek Peristy, Judith B√ºtepage_
  <details><summary>Abstract</summary>
  Animation data is often obtained through optical motion capture systems,which utilize a multitude of cameras to establish the position of opticalmarkers. However, system errors or occlusions can result in missing markers,the manual cleaning of which can be time-consuming. This has sparked interestin machine learning-based solutions for missing marker reconstruction in theacademic community. Most academic papers utilize a simplistic mean square erroras the main metric. In this paper, we show that this metric does not correlatewith subjective perception of the fill quality. Additionally, we introduce andevaluate a set of better-correlated metrics that can drive progress in thefield.
  </details>

- **[RAP: Retrieval-Augmented Personalization for Multimodal Large Language Models](http://arxiv.org/abs/2410.13360v3)**  `arXiv:2410.13360`  
  _Haoran Hao, Jiaming Han, Changsheng Li, Yu-Feng Li, Xiangyu Yue_
  <details><summary>Abstract</summary>
  The development of large language models (LLMs) has significantly enhancedthe capabilities of multimodal LLMs (MLLMs) as general assistants. However,lack of user-specific knowledge still restricts their application in human'sdaily life. In this paper, we introduce the Retrieval Augmented Personalization(RAP) framework for MLLMs' personalization. Starting from a general MLLM, weturn it into a personalized assistant in three steps. (a) Remember: We design akey-value database to store user-related information, e.g., user's name, avatarand other attributes. (b) Retrieve: When the user initiates a conversation, RAPwill retrieve relevant information from the database using a multimodalretriever. (c) Generate: The input query and retrieved concepts' informationare fed into MLLMs to generate personalized, knowledge-augmented responses.Unlike previous methods, RAP allows real-time concept editing via updating theexternal database. To further improve generation quality and alignment withuser-specific information, we design a pipeline for data collection and createa specialized dataset for personalized training of MLLMs. Based on the dataset,we train a series of MLLMs as personalized multimodal assistants. Bypretraining on large-scale dataset, RAP-MLLMs can generalize to infinite visualconcepts without additional finetuning. Our models demonstrate outstandingflexibility and generation quality across a variety of tasks, such aspersonalized image captioning, question answering and visual recognition. Thecode, data and models are available at https://hoar012.github.io/RAP-Project/.
  </details>

- **[MambaBEV: An efficient 3D detection model with Mamba2](http://arxiv.org/abs/2410.12673v2)**  `arXiv:2410.12673`  
  _Zihan You, Ni Wang, Hao Wang, Qichao Zhao, Jinxiang Wang_
  <details><summary>Abstract</summary>
  Accurate 3D object detection in autonomous driving relies on Bird's Eye View(BEV) perception and effective temporal fusion.However, existing fusionstrategies based on convolutional layers or deformable self attention strugglewith global context modeling in BEV space,leading to lower accuracy for largeobjects. To address this, we introduce MambaBEV, a novel BEV based 3D objectdetection model that leverages Mamba2, an advanced state space model (SSM)optimized for long sequence processing.Our key contribution is TemporalMamba, atemporal fusion module that enhances global awareness by introducing a BEVfeature discrete rearrangement mechanism tailored for Mamba's sequentialprocessing. Additionally, we propose Mamba based DETR as the detection head toimprove multi object representation.Evaluations on the nuScenes datasetdemonstrate that MambaBEV base achieves an NDS of 51.7\% and an mAP of42.7\%.Furthermore, an end to end autonomous driving paradigm validates itseffectiveness in motion forecasting and planning.Our results highlight thepotential of SSMs in autonomous driving perception, particularly in enhancingglobal context understanding and large object detection.
  </details>

- **[TULIP: Token-length Upgraded CLIP](http://arxiv.org/abs/2410.10034v2)**  `arXiv:2410.10034`  
  _Ivona Najdenkoska, Mohammad Mahdi Derakhshani, Yuki M. Asano, Nanne van Noord, Marcel Worring, Cees G. M. Snoek_
  <details><summary>Abstract</summary>
  We address the challenge of representing long captions in vision-languagemodels, such as CLIP. By design these models are limited by fixed, absolutepositional encodings, restricting inputs to a maximum of 77 tokens andhindering performance on tasks requiring longer descriptions. Although recentwork has attempted to overcome this limit, their proposed approaches struggleto model token relationships over longer distances and simply extend to a fixednew token length. Instead, we propose a generalizable method, named TULIP, ableto upgrade the token length to any length for CLIP-like models. We do so byimproving the architecture with relative position encodings, followed by atraining procedure that (i) distills the original CLIP text encoder into anencoder with relative position encodings and (ii) enhances the model foraligning longer captions with images. By effectively encoding captions longerthan the default 77 tokens, our model outperforms baselines on cross-modaltasks such as retrieval and text-to-image generation. The code repository isavailable at https://github.com/ivonajdenkoska/tulip.
  </details>

- **[Frame-Voyager: Learning to Query Frames for Video Large Language Models](http://arxiv.org/abs/2410.03226v4)**  `arXiv:2410.03226`  
  _Sicheng Yu, Chengkai Jin, Huanyu Wang, Zhenghao Chen, Sheng Jin, Zhongrong Zuo, et al._
  <details><summary>Abstract</summary>
  Video Large Language Models (Video-LLMs) have made remarkable progress invideo understanding tasks. However, they are constrained by the maximum lengthof input tokens, making it impractical to input entire videos. Existing frameselection approaches, such as uniform frame sampling and text-frame retrieval,fail to account for the information density variations in the videos or thecomplex instructions in the tasks, leading to sub-optimal performance. In thispaper, we propose Frame-Voyager that learns to query informative framecombinations, based on the given textual queries in the task. To trainFrame-Voyager, we introduce a new data collection and labeling pipeline, byranking frame combinations using a pre-trained Video-LLM. Given a video of Mframes, we traverse its T-frame combinations, feed them into a Video-LLM, andrank them based on Video-LLM's prediction losses. Using this ranking assupervision, we train Frame-Voyager to query the frame combinations with lowerlosses. In experiments, we evaluate Frame-Voyager on four Video QuestionAnswering benchmarks by plugging it into two different Video-LLMs. Theexperimental results demonstrate that Frame-Voyager achieves impressive resultsin all settings, highlighting its potential as a plug-and-play solution forVideo-LLMs.
  </details>

- **[JoyType: A Robust Design for Multilingual Visual Text Creation](http://arxiv.org/abs/2409.17524v2)**  `arXiv:2409.17524`  
  _Chao Li, Chen Jiang, Xiaolong Liu, Jun Zhao, Guoxin Wang_
  <details><summary>Abstract</summary>
  Generating images with accurately represented text, especially in non-Latinlanguages, poses a significant challenge for diffusion models. Existingapproaches, such as the integration of hint condition diagrams via auxiliarynetworks (e.g., ControlNet), have made strides towards addressing this issue.However, diffusion models often fall short in tasks requiring controlled textgeneration, such as specifying particular fonts or producing text in smallfonts. In this paper, we introduce a novel approach for multilingual visualtext creation, named JoyType, designed to maintain the font style of textduring the image generation process. Our methodology begins with assembling atraining dataset, JoyType-1M, comprising 1 million pairs of data. Each pairincludes an image, its description, and glyph instructions corresponding to thefont style within the image. We then developed a text control network, FontControlNet, tasked with extracting font style information to steer the imagegeneration. To further enhance our model's ability to maintain font style,notably in generating small-font text, we incorporated a multi-layer OCR-awareloss into the diffusion process. This enhancement allows JoyType to direct textrendering using low-level descriptors. Our evaluations, based on both visualand accuracy metrics, demonstrate that JoyType significantly outperformsexisting state-of-the-art methods. Additionally, JoyType can function as aplugin, facilitating the creation of varied image styles in conjunction withother stable diffusion models on HuggingFace and CivitAI. Our project isopen-sourced on https://jdh-algo.github.io/JoyType/.
  </details>

- **[Multi-modal Speech Transformer Decoders: When Do Multiple Modalities Improve Accuracy?](http://arxiv.org/abs/2409.09221v2)**  `arXiv:2409.09221`  
  _Yiwen Guan, Viet Anh Trinh, Vivek Voleti, Jacob Whitehill_
  <details><summary>Abstract</summary>
  Decoder-only discrete-token language models have recently achievedsignificant success in automatic speech recognition. However, systematicanalyses of how different modalities impact performance in specific scenariosremain limited. In this paper, we investigate the effects of multiplemodalities on recognition accuracy on both synthetic and real-world datasets.Our experiments suggest that: (1) Integrating more modalities can increaseaccuracy; in particular, our paper is, to our best knowledge, the first to showthe benefit of combining audio, image context, and lip information; (2) Imagesas a supplementary modality for speech recognition provide the greatest benefitat moderate noise levels, moreover, they exhibit a different trend compared toinherently synchronized modalities like lip movements; (3) Performance improveson both synthetic and real-world datasets when the most relevant visualinformation is filtered as a preprocessing step.
  </details>

- **[Structure Modeling Activation Free Fourier Network for Spacecraft Image Denoising](http://arxiv.org/abs/2409.07067v3)**  `arXiv:2409.07067`  
  _Jingfan Yang, Hu Gao, Ying Zhang, Bowen Ma, Depeng Dang_
  <details><summary>Abstract</summary>
  Spacecraft image denoising is a crucial fundamental technology closelyrelated to aerospace research. However, the existing deep learning-based imagedenoising methods are primarily designed for natural image and fail toadequately consider the characteristics of spacecraft image(e.g. low-lightconditions, repetitive periodic structures), resulting in suboptimalperformance in the spacecraft image denoising task. To address theaforementioned problems, we propose a Structure modeling Activation FreeFourier Network (SAFFN), which is an efficient spacecraft image denoisingmethod including Structure Modeling Block (SMB) and Activation Free FourierBlock (AFFB). We present SMB to effectively extract edge information and modelthe structure for better identification of spacecraft components from darkregions in spacecraft noise image. We present AFFB and utilize an improved FastFourier block to extract repetitive periodic features and long-rangeinformation in noisy spacecraft image. Extensive experimental resultsdemonstrate that our SAFFN performs competitively compared to thestate-of-the-art methods on spacecraft noise image datasets. The codes areavailable at: https://github.com/shenduke/SAFFN.
  </details>

- **[TIMotion: Temporal and Interactive Framework for Efficient Human-Human Motion Generation](http://arxiv.org/abs/2408.17135v4)**  `arXiv:2408.17135`  
  _Yabiao Wang, Shuo Wang, Jiangning Zhang, Ke Fan, Jiafu Wu, Zhucun Xue, et al._
  <details><summary>Abstract</summary>
  Human-human motion generation is essential for understanding humans as socialbeings. Current methods fall into two main categories: single-person-basedmethods and separate modeling-based methods. To delve into this field, weabstract the overall generation process into a general framework MetaMotion,which consists of two phases: temporal modeling and interaction mixing. Fortemporal modeling, the single-person-based methods concatenate two people intoa single one directly, while the separate modeling-based methods skip themodeling of interaction sequences. The inadequate modeling described aboveresulted in sub-optimal performance and redundant model parameters. In thispaper, we introduce TIMotion (Temporal and Interactive Modeling), an efficientand effective framework for human-human motion generation. Specifically, wefirst propose Causal Interactive Injection to model two separate sequences as acausal sequence leveraging the temporal and causal properties. Then we presentRole-Evolving Scanning to adjust to the change in the active and passive rolesthroughout the interaction. Finally, to generate smoother and more rationalmotion, we design Localized Pattern Amplification to capture short-term motionpatterns. Extensive experiments on InterHuman and InterX demonstrate that ourmethod achieves superior performance. Project page:https://aigc-explorer.github.io/TIMotion-page/
  </details>

- **[Vocabulary-Free 3D Instance Segmentation with Vision and Language Assistant](http://arxiv.org/abs/2408.10652v2)**  `arXiv:2408.10652`  
  _Guofeng Mei, Luigi Riz, Yiming Wang, Fabio Poiesi_
  <details><summary>Abstract</summary>
  Most recent 3D instance segmentation methods are open vocabulary, offering agreater flexibility than closed-vocabulary methods. Yet, they are limited toreasoning within a specific set of concepts, \ie the vocabulary, prompted bythe user at test time. In essence, these models cannot reason in an open-endedfashion, i.e., answering "List the objects in the scene.''. We introduce thefirst method to address 3D instance segmentation in a setting that is void ofany vocabulary prior, namely a vocabulary-free setting. We leverage a largevision-language assistant and an open-vocabulary 2D instance segmenter todiscover and ground semantic categories on the posed images. To form 3Dinstance mask, we first partition the input point cloud into dense superpoints,which are then merged into 3D instance masks. We propose a novel superpointmerging strategy via spectral clustering, accounting for both mask coherenceand semantic coherence that are estimated from the 2D object instance masks. Weevaluate our method using ScanNet200 and Replica, outperforming existingmethods in both vocabulary-free and open-vocabulary settings. Code will be madeavailable. Project page: https://gfmei.github.io/PoVo
  </details>

- **[SkillMimic: Learning Basketball Interaction Skills from Demonstrations](http://arxiv.org/abs/2408.15270v2)**  `arXiv:2408.15270`  
  _Yinhuai Wang, Qihan Zhao, Runyi Yu, Hok Wai Tsui, Ailing Zeng, Jing Lin, et al._
  <details><summary>Abstract</summary>
  Traditional reinforcement learning methods for human-object interaction (HOI)rely on labor-intensive, manually designed skill rewards that do not generalizewell across different interactions. We introduce SkillMimic, a unifieddata-driven framework that fundamentally changes how agents learn interactionskills by eliminating the need for skill-specific rewards. Our key insight isthat a unified HOI imitation reward can effectively capture the essence ofdiverse interaction patterns from HOI datasets. This enables SkillMimic tolearn a single policy that not only masters multiple interaction skills butalso facilitates skill transitions, with both diversity and generalizationimproving as the HOI dataset grows. For evaluation, we collect and introducetwo basketball datasets containing approximately 35 minutes of diversebasketball skills. Extensive experiments show that SkillMimic successfullymasters a wide range of basketball skills including stylistic variations indribbling, layup, and shooting. Moreover, these learned skills can beeffectively composed by a high-level controller to accomplish complex andlong-horizon tasks such as consecutive scoring, opening new possibilities forscalable and generalizable interaction skill learning. Project page:https://ingrid789.github.io/SkillMimic/
  </details>

- **[A Comprehensive Review of Few-shot Action Recognition](http://arxiv.org/abs/2407.14744v2)**  `arXiv:2407.14744`  
  _Yuyang Wanyan, Xiaoshan Yang, Weiming Dong, Changsheng Xu_
  <details><summary>Abstract</summary>
  Few-shot action recognition aims to address the high cost and impracticalityof manually labeling complex and variable video data in action recognition. Itrequires accurately classifying human actions in videos using only a fewlabeled examples per class. Compared to few-shot learning in image scenarios,few-shot action recognition is more challenging due to the intrinsic complexityof video data. Numerous approaches have driven significant advancements infew-shot action recognition, which underscores the need for a comprehensivesurvey. Unlike early surveys that focus on few-shot image or textclassification, we deeply consider the unique challenges of few-shot actionrecognition. In this survey, we provide a comprehensive review of recentmethods and introduce a novel and systematic taxonomy of existing approaches,accompanied by a detailed analysis. We categorize the methods intogenerative-based and meta-learning frameworks, and further elaborate on themethods within the meta-learning framework, covering aspects: video instancerepresentation, category prototype learning, and generalized video alignment.Additionally, the survey presents the commonly used benchmarks and discussesrelevant advanced topics and promising future directions. We hope this surveycan serve as a valuable resource for researchers, offering essential guidanceto newcomers and stimulating seasoned researchers with fresh insights.
  </details>

- **[MM-GTUNets: Unified Multi-Modal Graph Deep Learning for Brain Disorders Prediction](http://arxiv.org/abs/2406.14455v3)**  `arXiv:2406.14455`  
  _Luhui Cai, Weiming Zeng, Hongyu Chen, Hua Zhang, Yueyang Li, Yu Feng, et al._
  <details><summary>Abstract</summary>
  Graph deep learning (GDL) has demonstrated impressive performance inpredicting population-based brain disorders (BDs) through the integration ofboth imaging and non-imaging data. However, the effectiveness of GDL basedmethods heavily depends on the quality of modeling the multi-modal populationgraphs and tends to degrade as the graph scale increases. Furthermore, thesemethods often constrain interactions between imaging and non-imaging data tonode-edge interactions within the graph, overlooking complex inter-modalcorrelations, leading to suboptimal outcomes. To overcome these challenges, wepropose MM-GTUNets, an end-to-end graph transformer based multi-modal graphdeep learning (MMGDL) framework designed for brain disorders prediction atlarge scale. Specifically, to effectively leverage rich multi-modal informationrelated to diseases, we introduce Modality Reward Representation Learning(MRRL) which adaptively constructs population graphs using a reward system.Additionally, we employ variational autoencoder to reconstruct latentrepresentations of non-imaging features aligned with imaging features. Based onthis, we propose Adaptive Cross-Modal Graph Learning (ACMGL), which capturescritical modality-specific and modality-shared features through a unifiedGTUNet encoder taking advantages of Graph UNet and Graph Transformer, andfeature fusion module. We validated our method on two public multi-modaldatasets ABIDE and ADHD-200, demonstrating its superior performance indiagnosing BDs. Our code is available at https://github.com/NZWANG/MM-GTUNets.
  </details>

- **[Asymptotic Unbiased Sample Sampling to Speed Up Sharpness-Aware Minimization](http://arxiv.org/abs/2406.08001v2)**  `arXiv:2406.08001`  
  _Jiaxin Deng, Junbiao Pang, Baochang Zhang_
  <details><summary>Abstract</summary>
  Sharpness-Aware Minimization (SAM) has emerged as a promising approach foreffectively reducing the generalization error. However, SAM incurs twice thecomputational cost compared to base optimizer (e.g., SGD). We proposeAsymptotic Unbiased Sampling with respect to iterations to accelerate SAM(AUSAM), which maintains the model's generalization capacity whilesignificantly enhancing computational efficiency. Concretely, weprobabilistically sample a subset of data points beneficial for SAMoptimization based on a theoretically guaranteed criterion, i.e., the GradientNorm of each Sample (GNS). We further approximate the GNS by the difference inloss values before and after perturbation in SAM. As a plug-and-play,architecture-agnostic method, our approach consistently accelerates SAM acrossa range of tasks and networks, i.e., classification, human pose estimation andnetwork quantization. On CIFAR10/100 and Tiny-ImageNet, AUSAM achieves resultscomparable to SAM while providing a speedup of over 70%. Compared to recentdynamic data pruning methods, AUSAM is better suited for SAM and excels inmaintaining performance. Additionally, AUSAM accelerates optimization in humanpose estimation and model quantization without sacrificing performance,demonstrating its broad practicality.
  </details>

- **[Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models](http://arxiv.org/abs/2405.12523v3)**  `arXiv:2405.12523`  
  _Jiaqi Li, Qianshan Wei, Chuanyi Zhang, Guilin Qi, Miaozeng Du, Yongrui Chen, et al._
  <details><summary>Abstract</summary>
  Machine unlearning empowers individuals with the `right to be forgotten' byremoving their private or sensitive information encoded in machine learningmodels. However, it remains uncertain whether MU can be effectively applied toMultimodal Large Language Models (MLLMs), particularly in scenarios offorgetting the leaked visual data of concepts. To overcome the challenge, wepropose an efficient method, Single Image Unlearning (SIU), to unlearn thevisual recognition of a concept by fine-tuning a single associated image forfew steps. SIU consists of two key aspects: (i) Constructing Multifacetedfine-tuning data. We introduce four targets, based on which we constructfine-tuning data for the concepts to be forgotten; (ii) Jointly training loss.To synchronously forget the visual recognition of concepts and preserve theutility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergenceLoss combined with Cross Entropy loss. Alongside our method, we establishMMUBench, a new benchmark for MU in MLLMs and introduce a collection of metricsfor its evaluation. Experimental results on MMUBench show that SIU completelysurpasses the performance of existing methods. Furthermore, we surprisinglyfind that SIU can avoid invasive membership inference attacks and jailbreakattacks. To the best of our knowledge, we are the first to explore MU in MLLMs.We will release the code and benchmark in the near future.
  </details>

- **[Rethinking Efficient and Effective Point-based Networks for Event Camera Classification and Regression: EventMamba](http://arxiv.org/abs/2405.06116v4)**  `arXiv:2405.06116`  
  _Hongwei Ren, Yue Zhou, Jiadong Zhu, Haotian Fu, Yulong Huang, Xiaopeng Lin, et al._
  <details><summary>Abstract</summary>
  Event cameras draw inspiration from biological systems, boasting low latencyand high dynamic range while consuming minimal power. The most current approachto processing Event Cloud often involves converting it into frame-basedrepresentations, which neglects the sparsity of events, loses fine-grainedtemporal information, and increases the computational burden. In contrast,Point Cloud is a popular representation for processing 3-dimensional data andserves as an alternative method to exploit local and global spatial features.Nevertheless, previous point-based methods show an unsatisfactory performancecompared to the frame-based method in dealing with spatio-temporal eventstreams. In order to bridge the gap, we propose EventMamba, an efficient andeffective framework based on Point Cloud representation by rethinking thedistinction between Event Cloud and Point Cloud, emphasizing vital temporalinformation. The Event Cloud is subsequently fed into a hierarchical structurewith staged modules to process both implicit and explicit temporal features.Specifically, we redesign the global extractor to enhance explicit temporalextraction among a long sequence of events with temporal aggregation and StateSpace Model (SSM) based Mamba. Our model consumes minimal computationalresources in the experiments and still exhibits SOTA point-based performance onsix different scales of action recognition datasets. It even outperformed allframe-based methods on both Camera Pose Relocalization (CPR) and eye-trackingregression tasks. Our code is available at:https://github.com/rhwxmx/EventMamba.
  </details>

- **[Exploring Saliency Bias in Manipulation Detection](http://arxiv.org/abs/2402.07338v4)**  `arXiv:2402.07338`  
  _Joshua Krinsky, Alan Bettis, Qiuyu Tang, Daniel Moreira, Aparna Bharati_
  <details><summary>Abstract</summary>
  The social media-fuelled explosion of fake news and misinformation supportedby tampered images has led to growth in the development of models and datasetsfor image manipulation detection. However, existing detection methods mostlytreat media objects in isolation, without considering the impact of specificmanipulations on viewer perception. Forensic datasets are usually analyzedbased on the manipulation operations and corresponding pixel-based masks, butnot on the semantics of the manipulation, i.e., type of scene, objects, andviewers' attention to scene content. The semantics of the manipulation play animportant role in spreading misinformation through manipulated images. In anattempt to encourage further development of semantic-aware forensic approachesto understand visual misinformation, we propose a framework to analyze thetrends of visual and semantic saliency in popular image manipulation datasetsand their impact on detection.
  </details>

- **[MixRT: Mixed Neural Representations For Real-Time NeRF Rendering](http://arxiv.org/abs/2312.11841v5)**  `arXiv:2312.11841`  
  _Chaojian Li, Bichen Wu, Peter Vajda, Yingyan Celine Lin_
  <details><summary>Abstract</summary>
  Neural Radiance Field (NeRF) has emerged as a leading technique for novelview synthesis, owing to its impressive photorealistic reconstruction andrendering capability. Nevertheless, achieving real-time NeRF rendering inlarge-scale scenes has presented challenges, often leading to the adoption ofeither intricate baked mesh representations with a substantial number oftriangles or resource-intensive ray marching in baked representations. Wechallenge these conventions, observing that high-quality geometry, representedby meshes with substantial triangles, is not necessary for achievingphotorealistic rendering quality. Consequently, we propose MixRT, a novel NeRFrepresentation that includes a low-quality mesh, a view-dependent displacementmap, and a compressed NeRF model. This design effectively harnesses thecapabilities of existing graphics hardware, thus enabling real-time NeRFrendering on edge devices. Leveraging a highly-optimized WebGL-based renderingframework, our proposed MixRT attains real-time rendering speeds on edgedevices (over 30 FPS at a resolution of 1280 x 720 on a MacBook M1 Pro laptop),better rendering quality (0.2 PSNR higher in indoor scenes of the Unbounded-360datasets), and a smaller storage size (less than 80% compared tostate-of-the-art methods).
  </details>

- **[INGeo: Accelerating Instant Neural Scene Reconstruction with Noisy Geometry Priors](http://arxiv.org/abs/2212.01959v2)**  `arXiv:2212.01959`  
  _Chaojian Li, Bichen Wu, Albert Pumarola, Peizhao Zhang, Yingyan Celine Lin, Peter Vajda_
  <details><summary>Abstract</summary>
  We present a method that accelerates reconstruction of 3D scenes and objects,aiming to enable instant reconstruction on edge devices such as mobile phonesand AR/VR headsets. While recent works have accelerated scene reconstructiontraining to minute/second-level on high-end GPUs, there is still a large gap tothe goal of instant training on edge devices which is yet highly desired inmany emerging applications such as immersive AR/VR. To this end, this work aimsto further accelerate training by leveraging geometry priors of the targetscene. Our method proposes strategies to alleviate the noise of the imperfectgeometry priors to accelerate the training speed on top of the highly optimizedInstant-NGP. On the NeRF Synthetic dataset, our work uses half of the trainingiterations to reach an average test PSNR of >30.
  </details>

- **[USC: Uncompromising Spatial Constraints for Safety-Oriented 3D Object Detectors in Autonomous Driving](http://arxiv.org/abs/2209.10368v5)**  `arXiv:2209.10368`  
  _Brian Hsuan-Cheng Liao, Chih-Hong Cheng, Hasan Esen, Alois Knoll_
  <details><summary>Abstract</summary>
  In this work, we consider the safety-oriented performance of 3D objectdetectors in autonomous driving contexts. Specifically, despite impressiveresults shown by the mass literature, developers often find it hard to ensurethe safe deployment of these learning-based perception models. Attributing thechallenge to the lack of safety-oriented metrics, we hereby presentuncompromising spatial constraints (USC), which characterize a simple yetimportant localization requirement demanding the predictions to fully cover theobjects when seen from the autonomous vehicle. The constraints, as we formulateusing the perspective and bird's-eye views, can be naturally reflected byquantitative measures, such that having an object detector with a higher scoreimplies a lower risk of collision. Finally, beyond model evaluation, weincorporate the quantitative measures into common loss functions to enablesafety-oriented fine-tuning for existing models. With experiments using thenuScenes dataset and a closed-loop simulation, our work demonstrates suchconsiderations of safety notions at the perception level not only improve modelperformances beyond accuracy but also allow for a more direct linkage to actualsystem safety.
  </details>

- **[FBNetV5: Neural Architecture Search for Multiple Tasks in One Run](http://arxiv.org/abs/2111.10007v3)**  `arXiv:2111.10007`  
  _Bichen Wu, Chaojian Li, Hang Zhang, Xiaoliang Dai, Peizhao Zhang, Matthew Yu, et al._
  <details><summary>Abstract</summary>
  Neural Architecture Search (NAS) has been widely adopted to design accurateand efficient image classification models. However, applying NAS to a newcomputer vision task still requires a huge amount of effort. This is because 1)previous NAS research has been over-prioritized on image classification whilelargely ignoring other tasks; 2) many NAS works focus on optimizingtask-specific components that cannot be favorably transferred to other tasks;and 3) existing NAS methods are typically designed to be "proxyless" andrequire significant effort to be integrated with each new task's trainingpipelines. To tackle these challenges, we propose FBNetV5, a NAS framework thatcan search for neural architectures for a variety of vision tasks with muchreduced computational cost and human effort. Specifically, we design 1) asearch space that is simple yet inclusive and transferable; 2) a multitasksearch process that is disentangled with target tasks' training pipeline; and3) an algorithm to simultaneously search for architectures for multiple taskswith a computational cost agnostic to the number of tasks. We evaluate theproposed FBNetV5 targeting three fundamental vision tasks -- imageclassification, object detection, and semantic segmentation. Models searched byFBNetV5 in a single run of search have outperformed the previousstateof-the-art in all the three tasks: image classification (e.g., +1.3%ImageNet top-1 accuracy under the same FLOPs as compared to FBNetV3), semanticsegmentation (e.g., +1.8% higher ADE20K val. mIoU than SegFormer with 3.6xfewer FLOPs), and object detection (e.g., +1.1% COCO val. mAP with 1.2x fewerFLOPs as compared to YOLOX).
  </details>

- **[DANCE: DAta-Network Co-optimization for Efficient Segmentation Model Training and Inference](http://arxiv.org/abs/2107.07706v2)**  `arXiv:2107.07706`  
  _Chaojian Li, Wuyang Chen, Yuchen Gu, Tianlong Chen, Yonggan Fu, Zhangyang Wang, et al._
  <details><summary>Abstract</summary>
  Semantic segmentation for scene understanding is nowadays widely demanded,raising significant challenges for the algorithm efficiency, especially itsapplications on resource-limited platforms. Current segmentation models aretrained and evaluated on massive high-resolution scene images ("data level")and suffer from the expensive computation arising from the required multi-scaleaggregation("network level"). In both folds, the computational and energy costsin training and inference are notable due to the often desired large inputresolutions and heavy computational burden of segmentation models. To this end,we propose DANCE, general automated DAta-Network Co-optimization for Efficientsegmentation model training and inference. Distinct from existing efficientsegmentation approaches that focus merely on light-weight network design, DANCEdistinguishes itself as an automated simultaneous data-network co-optimizationvia both input data manipulation and network architecture slimming.Specifically, DANCE integrates automated data slimming which adaptivelydownsamples/drops input images and controls their corresponding contribution tothe training loss guided by the images' spatial complexity. Such a downsamplingoperation, in addition to slimming down the cost associated with the input sizedirectly, also shrinks the dynamic range of input object and context scales,therefore motivating us to also adaptively slim the network to match thedownsampled data. Extensive experiments and ablating studies (on four SOTAsegmentation models with three popular segmentation datasets under two trainingsettings) demonstrate that DANCE can achieve "all-win" towards efficientsegmentation(reduced training cost, less expensive inference, and better meanIntersection-over-Union (mIoU)).
  </details>

- **[RelDenClu: A Relative Density based Biclustering Method for identifying non-linear feature relations](http://arxiv.org/abs/1811.04661v6)**  `arXiv:1811.04661`  
  _Namita Jain, Susmita Ghosh, C. A. Murthy_
  <details><summary>Abstract</summary>
  The existing biclustering algorithms for finding feature relation basedbiclusters often depend on assumptions like monotonicity or linearity. Though afew algorithms overcome this problem by using density-based methods, they tendto miss out many biclusters because they use global criteria for identifyingdense regions. The proposed method, RelDenClu uses the local variations inmarginal and joint densities for each pair of features to find the subset ofobservations, which forms the bases of the relation between them. It then findsthe set of features connected by a common set of observations, resulting in abicluster.  To show the effectiveness of the proposed methodology, experimentation hasbeen carried out on fifteen types of simulated datasets. Further, it has beenapplied to six real-life datasets. For three of these real-life datasets, theproposed method is used for unsupervised learning, while for other threereal-life datasets it is used as an aid to supervised learning. For all thedatasets the performance of the proposed method is compared with that of sevendifferent state-of-the-art algorithms and the proposed algorithm is seen toproduce better results. The efficacy of proposed algorithm is also seen by itsuse on COVID-19 dataset for identifying some features (genetic, demographicsand others) that are likely to affect the spread of COVID-19.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Machine Learning üìä

<details open><summary>Click to Collapse</summary>

- **[Tropical Bisectors and Carlini-Wagner Attacks](http://arxiv.org/abs/2503.22653v1)**  `arXiv:2503.22653`  
  _Gillian Grindstaff, Julia Lindberg, Daniela Schkoda, Miruna-Stefana Sorea, Ruriko Yoshida_
  <details><summary>Abstract</summary>
  Pasque et al. showed that using a tropical symmetric metric as an activationfunction in the last layer can improve the robustness of convolutional neuralnetworks (CNNs) against state-of-the-art attacks, including the Carlini-Wagnerattack. This improvement occurs when the attacks are not specifically adaptedto the non-differentiability of the tropical layer. Moreover, they showed thatthe decision boundary of a tropical CNN is defined by tropical bisectors. Inthis paper, we explore the combinatorics of tropical bisectors and analyze howthe tropical embedding layer enhances robustness against Carlini-Wagnerattacks. We prove an upper bound on the number of linear segments the decisionboundary of a tropical CNN can have. We then propose a refined version of theCarlini-Wagner attack, specifically tailored for the tropical architecture.Computational experiments with MNIST and LeNet5 showcase our attacks improvedsuccess rate.
  </details>

- **[Sentiment Classification of Thai Central Bank Press Releases Using Supervised Learning](http://arxiv.org/abs/2503.22629v1)**  `arXiv:2503.22629`  
  _Stefano Grassi_
  <details><summary>Abstract</summary>
  Central bank communication plays a critical role in shaping economicexpectations and monetary policy effectiveness. This study applies supervisedmachine learning techniques to classify the sentiment of press releases fromthe Bank of Thailand, addressing gaps in research that primarily focus onlexicon-based approaches. My findings show that supervised learning can be aneffective method, even with smaller datasets, and serves as a starting pointfor further automation. However, achieving higher accuracy and bettergeneralization requires a substantial amount of labeled data, which istime-consuming and demands expertise. Using models such as Na\"ive Bayes,Random Forest and SVM, this study demonstrates the applicability of machinelearning for central bank sentiment analysis, with English-languagecommunications from the Thai Central Bank as a case study.
  </details>

- **[Generative Latent Neural PDE Solver using Flow Matching](http://arxiv.org/abs/2503.22600v1)**  `arXiv:2503.22600`  
  _Zijie Li, Anthony Zhou, Amir Barati Farimani_
  <details><summary>Abstract</summary>
  Autoregressive next-step prediction models have become the de-facto standardfor building data-driven neural solvers to forecast time-dependent partialdifferential equations (PDEs). Denoise training that is closely related todiffusion probabilistic model has been shown to enhance the temporal stabilityof neural solvers, while its stochastic inference mechanism enables ensemblepredictions and uncertainty quantification. In principle, such traininginvolves sampling a series of discretized diffusion timesteps during bothtraining and inference, inevitably increasing computational overhead. Inaddition, most diffusion models apply isotropic Gaussian noise on structured,uniform grids, limiting their adaptability to irregular domains. We propose alatent diffusion model for PDE simulation that embeds the PDE state in alower-dimensional latent space, which significantly reduces computationalcosts. Our framework uses an autoencoder to map different types of meshes ontoa unified structured latent grid, capturing complex geometries. By analyzingcommon diffusion paths, we propose to use a coarsely sampled noise schedulefrom flow matching for both training and testing. Numerical experiments showthat the proposed model outperforms several deterministic baselines in bothaccuracy and long-term stability, highlighting the potential of diffusion-basedapproaches for robust data-driven PDE learning.
  </details>

- **[Reinforcement Learning for Machine Learning Model Deployment: Evaluating Multi-Armed Bandits in ML Ops Environments](http://arxiv.org/abs/2503.22595v1)**  `arXiv:2503.22595`  
  _S. Aaron McClendon, Vishaal Venkatesh, Juan Morinelli_
  <details><summary>Abstract</summary>
  In modern ML Ops environments, model deployment is a critical process thattraditionally relies on static heuristics such as validation error comparisonsand A/B testing. However, these methods require human intervention to adapt toreal-world deployment challenges, such as model drift or unexpected performancedegradation. We investigate whether reinforcement learning, specificallymulti-armed bandit (MAB) algorithms, can dynamically manage model deploymentdecisions more effectively. Our approach enables more adaptive productionenvironments by continuously evaluating deployed models and rolling backunderperforming ones in real-time. We test six model selection strategiesacross two real-world datasets and find that RL based approaches match orexceed traditional methods in performance. Our findings suggest thatreinforcement learning (RL)-based model management can improve automation,reduce reliance on manual interventions, and mitigate risks associated withpost-deployment model failures.
  </details>

- **[Comparing Methods for Bias Mitigation in Graph Neural Networks](http://arxiv.org/abs/2503.22569v1)**  `arXiv:2503.22569`  
  _Barbara Hoffmann, Ruben Mayer_
  <details><summary>Abstract</summary>
  This paper examines the critical role of Graph Neural Networks (GNNs) in datapreparation for generative artificial intelligence (GenAI) systems, with aparticular focus on addressing and mitigating biases. We present a comparativeanalysis of three distinct methods for bias mitigation: data sparsification,feature modification, and synthetic data augmentation. Through experimentalanalysis using the german credit dataset, we evaluate these approaches usingmultiple fairness metrics, including statistical parity, equality ofopportunity, and false positive rates. Our research demonstrates that while allmethods improve fairness metrics compared to the original dataset, stratifiedsampling and synthetic data augmentation using GraphSAGE prove particularlyeffective in balancing demographic representation while maintaining modelperformance. The results provide practical insights for developing moreequitable AI systems while maintaining model performance.
  </details>

- **[Benchmarking Ultra-Low-Power $Œº$NPUs](http://arxiv.org/abs/2503.22567v1)**  `arXiv:2503.22567`  
  _Josh Millar, Yushan Huang, Sarab Sethi, Hamed Haddadi, Anil Madhavapeddy_
  <details><summary>Abstract</summary>
  Efficient on-device neural network (NN) inference has various advantages overcloud-based processing, including predictable latency, enhanced privacy,greater reliability, and reduced operating costs for vendors. This has sparkedthe recent rapid development of microcontroller-scale NN accelerators, oftenreferred to as neural processing units ($\mu$NPUs), designed specifically forultra-low-power applications.  In this paper we present the first comparative evaluation of a number ofcommercially-available $\mu$NPUs, as well as the first independent benchmarksfor several of these platforms. We develop and open-source a model compilationframework to enable consistent benchmarking of quantized models across diverse$\mu$NPU hardware. Our benchmark targets end-to-end performance and includesmodel inference latency, power consumption, and memory overhead, alongsideother factors. The resulting analysis uncovers both expected performance trendsas well as surprising disparities between hardware specifications and actualperformance, including $\mu$NPUs exhibiting unexpected scaling behaviors withincreasing model complexity. Our framework provides a foundation for furtherevaluation of $\mu$NPU platforms alongside valuable insights for both hardwaredesigners and software developers in this rapidly evolving space.
  </details>

- **[Niyama : Breaking the Silos of LLM Inference Serving](http://arxiv.org/abs/2503.22562v1)**  `arXiv:2503.22562`  
  _Kanishk Goel, Jayashree Mohan, Nipun Kwatra, Ravi Shreyas Anupindi, Ramachandran Ramjee_
  <details><summary>Abstract</summary>
  The widespread adoption of Large Language Models (LLMs) has enabled diverseapplications with very different latency requirements. Existing LLM servingframeworks rely on siloed infrastructure with coarse-grained workloadsegregation -- interactive and batch -- leading to inefficient resourceutilization and limited support for fine-grained Quality-of-Service (QoS)differentiation. This results in operational inefficiencies, over-provisioningand poor load management during traffic surges.  We present Niyama, a novel QoS-driven inference serving system that enablesefficient co-scheduling of diverse workloads on shared infrastructure. Niyamaintroduces fine-grained QoS classification allowing applications to specifyprecise latency requirements, and dynamically adapts scheduling decisions basedon real-time system state. Leveraging the predictable execution characteristicsof LLM inference, Niyama implements a dynamic chunking mechanism to improveoverall throughput while maintaining strict QoS guarantees. Additionally,Niyama employs a hybrid prioritization policy that balances fairness andefficiency, and employs selective request relegation that enables gracefulservice degradation during overload conditions. Our evaluation demonstratesthat Niyama increases serving capacity by 32% compared to current siloeddeployments, while maintaining QoS guarantees. Notably, under extreme load, oursystem reduces SLO violations by an order of magnitude compared to currentstrategies.
  </details>

- **[Efficient Verified Machine Unlearning For Distillation](http://arxiv.org/abs/2503.22539v1)**  `arXiv:2503.22539`  
  _Yijun Quan, Zushu Li, Giovanni Montana_
  <details><summary>Abstract</summary>
  Growing data privacy demands, driven by regulations like GDPR and CCPA,require machine unlearning methods capable of swiftly removing the influence ofspecific training points. Although verified approaches like SISA, using dataslicing and checkpointing, achieve efficient unlearning for single models byreverting to intermediate states, these methods struggle in teacher-studentknowledge distillation settings. Unlearning in the teacher typically forcescostly, complete student retraining due to pervasive information propagationduring distillation. Our primary contribution is PURGE (Partitioned Unlearningwith Retraining Guarantee for Ensembles), a novel framework integratingverified unlearning with distillation. We introduce constituent mapping and anincremental multi-teacher strategy that partitions the distillation process,confines each teacher constituent's impact to distinct student data subsets,and crucially maintains data isolation. The PURGE framework substantiallyreduces retraining overhead, requiring only partial student updates whenteacher-side unlearning occurs. We provide both theoretical analysis,quantifying significant speed-ups in the unlearning process, and empiricalvalidation on multiple datasets, demonstrating that PURGE achieves theseefficiency gains while maintaining student accuracy comparable to standardbaselines.
  </details>

- **[MixFunn: A Neural Network for Differential Equations with Improved Generalization and Interpretability](http://arxiv.org/abs/2503.22528v1)**  `arXiv:2503.22528`  
  _Tiago de Souza Farias, Gubio Gomes de Lima, Jonas Maziero, Celso Jorge Villas-Boas_
  <details><summary>Abstract</summary>
  We introduce MixFunn, a novel neural network architecture designed to solvedifferential equations with enhanced precision, interpretability, andgeneralization capability. The architecture comprises two key components: themixed-function neuron, which integrates multiple parameterized nonlinearfunctions to improve representational flexibility, and the second-order neuron,which combines a linear transformation of its inputs with a quadratic term tocapture cross-combinations of input variables. These features significantlyenhance the expressive power of the network, enabling it to achieve comparableor superior results with drastically fewer parameters and a reduction of up tofour orders of magnitude compared to conventional approaches. We appliedMixFunn in a physics-informed setting to solve differential equations inclassical mechanics, quantum mechanics, and fluid dynamics, demonstrating itseffectiveness in achieving higher accuracy and improved generalization toregions outside the training domain relative to standard machine learningmodels. Furthermore, the architecture facilitates the extraction ofinterpretable analytical expressions, offering valuable insights into theunderlying solutions.
  </details>

- **[Assessing Foundation Models for Sea Ice Type Segmentation in Sentinel-1 SAR Imagery](http://arxiv.org/abs/2503.22516v1)**  `arXiv:2503.22516`  
  _Samira Alkaee Taleghan, Morteza Karimzadeh, Andrew P. Barrett, Walter N. Meier, Farnoush Banaei-Kashani_
  <details><summary>Abstract</summary>
  Accurate segmentation of sea ice types is essential for mapping andoperational forecasting of sea ice conditions for safe navigation and resourceextraction in ice-covered waters, as well as for understanding polar climateprocesses. While deep learning methods have shown promise in automating sea icesegmentation, they often rely on extensive labeled datasets which requireexpert knowledge and are time-consuming to create. Recently, foundation models(FMs) have shown excellent results for segmenting remote sensing images byutilizing pre-training on large datasets using self-supervised techniques.However, their effectiveness for sea ice segmentation remains unexplored,especially given sea ice's complex structures, seasonal changes, and uniquespectral signatures, as well as peculiar Synthetic Aperture Radar (SAR) imagerycharacteristics including banding and scalloping noise, and varying icebackscatter characteristics, which are often missing in standard remote sensingpre-training datasets. In particular, SAR images over polar regions areacquired using different modes than used to capture the images at lowerlatitudes by the same sensors that form training datasets for FMs. This studyevaluates ten remote sensing FMs for sea ice type segmentation using Sentinel-1SAR imagery, focusing on their seasonal and spatial generalization. Among theselected models, Prithvi-600M outperforms the baseline models, while CROMAachieves a very similar performance in F1-score. Our contributions includeoffering a systematic methodology for selecting FMs for sea ice data analysis,a comprehensive benchmarking study on performances of FMs for sea icesegmentation with tailored performance metrics, and insights into existing gapsand future directions for improving domain-specific models in polarapplications using SAR data.
  </details>

- **[Learnable cut flow](http://arxiv.org/abs/2503.22498v1)**  `arXiv:2503.22498`  
  _Jing Li, Hao Sun_
  <details><summary>Abstract</summary>
  Neural networks have emerged as a powerful paradigm for tasks in high energyphysics, yet their opaque training process renders them as a black box. Incontrast, the traditional cut flow method offers simplicity andinterpretability but demands human effort to identify optimal boundaries. Tomerge the strengths of both approaches, we propose the Learnable Cut Flow(LCF), a neural network that transforms the traditional cut selection into afully differentiable, data-driven process. LCF implements two cutstrategies-parallel, where observable distributions are treated independently,and sequential, where prior cuts shape subsequent ones-to flexibly determineoptimal boundaries. Building on this, we introduce the Learnable Importance, ametric that quantifies feature importance and adjusts their contributions tothe loss accordingly, offering model-driven insights unlike ad-hoc metrics. Toensure differentiability, a modified loss function replaces hard cuts with maskoperations, preserving data shape throughout the training process. LCF istested on six varied mock datasets and a realistic diboson vs. QCD dataset.Results demonstrate that LCF (1) accurately learns cut boundaries acrosstypical feature distributions in both parallel and sequential strategies, (2)assigns higher importance to discriminative features with minimal overlap, (3)handles redundant or correlated features robustly, and (4) performs effectivelyin real-world scenarios. In diboson dataset, LCF initially underperformsboosted decision trees and multiplayer perceptrons when using all observables.However, pruning less critical features-guided by learned importance-boosts itsperformance to match or exceed these baselines. LCF bridges the gap betweentraditional cut flow method and modern black-box neural networks, deliveringactionable insights into the training process and feature importance.
  </details>

- **[SPDNet: Seasonal-Periodic Decomposition Network for Advanced Residential Demand Forecasting](http://arxiv.org/abs/2503.22485v1)**  `arXiv:2503.22485`  
  _Reza Nematirad, Anil Pahwa, Balasubramaniam Natarajan_
  <details><summary>Abstract</summary>
  Residential electricity demand forecasting is critical for efficient energymanagement and grid stability. Accurate predictions enable utility companies tooptimize planning and operations. However, real-world residential electricitydemand data often exhibit intricate temporal variability, including multipleseasonalities, periodicities, and abrupt fluctuations, which pose significantchallenges for forecasting models. Previous models that rely on statisticalmethods, recurrent, convolutional neural networks, and transformers oftenstruggle to capture these intricate temporal dynamics. To address thesechallenges, we propose the Seasonal-Periodic Decomposition Network (SPDNet), anovel deep learning framework consisting of two main modules. The first is theSeasonal-Trend Decomposition Module (STDM), which decomposes the input datainto trend, seasonal, and residual components. The second is the PeriodicalDecomposition Module (PDM), which employs the Fast Fourier Transform toidentify the dominant periods. For each dominant period, 1D input data isreshaped into a 2D tensor, where rows represent periods and columns correspondto frequencies. The 2D representations are then processed through threesubmodules: a 1D convolution to capture sharp fluctuations, a transformer-basedencoder to model global patterns, and a 2D convolution to capture interactionsbetween periods. Extensive experiments conducted on real-world residentialelectricity load data demonstrate that SPDNet outperforms traditional andadvanced models in both forecasting accuracy and computational efficiency. Thecode is available in this repository: https://github.com/Tims2D/SPDNet.
  </details>

- **[Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model](http://arxiv.org/abs/2503.22480v1)**  `arXiv:2503.22480`  
  _Wangtao Sun, Xiang Cheng, Xing Yu, Haotian Xu, Zhao Yang, Shizhu He, et al._
  <details><summary>Abstract</summary>
  Reinforcement Learning from Human Feedback (RLHF) has emerged as a criticaltechnique for training large language models. However, reward hacking-aphenomenon where models exploit flaws in the reward model-remains a significantbarrier to achieving robust and scalable intelligence through long-termtraining. Existing studies have proposed uncertain reward model to addressreward hacking, however, they often lack systematic or theoretical foundations,failing to model the uncertainty intrinsically emerging from preference data.In this paper, we propose the Probabilistic Uncertain Reward Model (PURM), anatural generalization of the classical Bradley-Terry reward model. PURM learnsreward distributions directly from preference data and quantifies per-sampleuncertainty via the average overlap area between reward distributions. Tomitigate reward hacking, we further introduce an uncertainty-aware penalty intoProximal Policy Optimization (PPO), which leverages the learned uncertainty todynamically balance reward optimization and exploration. We propose alightweight and easy-to-use implementation of PURM. Experiments demonstratethat PURM significantly delays the onset of reward hacking while improvingfinal reward performance, outperforming baseline methods in both stability andeffectiveness.
  </details>

- **[Almost Bayesian: The Fractal Dynamics of Stochastic Gradient Descent](http://arxiv.org/abs/2503.22478v1)**  `arXiv:2503.22478`  
  _Max Hennick, Stijn De Baerdemacker_
  <details><summary>Abstract</summary>
  We show that the behavior of stochastic gradient descent is related toBayesian statistics by showing that SGD is effectively diffusion on a fractallandscape, where the fractal dimension can be accounted for in a purelyBayesian way. By doing this we show that SGD can be regarded as a modifiedBayesian sampler which accounts for accessibility constraints induced by thefractal structure of the loss landscape. We verify our results experimentallyby examining the diffusion of weights during training. These results offerinsight into the factors which determine the learning process, and seeminglyanswer the question of how SGD and purely Bayesian sampling are related.
  </details>

- **[DeepOFormer: Deep Operator Learning with Domain-informed Features for Fatigue Life Prediction](http://arxiv.org/abs/2503.22475v1)**  `arXiv:2503.22475`  
  _Chenyang Li, Tanmay Sunil Kapure, Prokash Chandra Roy, Zhengtao Gan, Bo Shen_
  <details><summary>Abstract</summary>
  Fatigue life characterizes the duration a material can function beforefailure under specific environmental conditions, and is traditionally assessedusing stress-life (S-N) curves. While machine learning and deep learning offerpromising results for fatigue life prediction, they face the overfittingchallenge because of the small size of fatigue experimental data in specificmaterials. To address this challenge, we propose, DeepOFormer, by formulatingS-N curve prediction as an operator learning problem. DeepOFormer improves thedeep operator learning framework with a transformer-based encoder and a mean L2relative error loss function. We also consider Stussi, Weibull, and Pascual andMeeker (PM) features as domain-informed features. These features are motivatedby empirical fatigue models. To evaluate the performance of our DeepOFormer, wecompare it with different deep learning models and XGBoost on a dataset with 54S-N curves of aluminum alloys. With seven different aluminum alloys selectedfor testing, our DeepOFormer achieves an R2 of 0.9515, a mean absolute error of0.2080, and a mean relative error of 0.5077, significantly outperformingstate-of-the-art deep/machine learning methods including DeepONet,TabTransformer, and XGBoost, etc. The results highlight that our Deep0Formerintegrating with domain-informed features substantially improves predictionaccuracy and generalization capabilities for fatigue life prediction inaluminum alloys.
  </details>

- **[Entropy-guided sequence weighting for efficient exploration in RL-based LLM fine-tuning](http://arxiv.org/abs/2503.22456v1)**  `arXiv:2503.22456`  
  _Abdullah Vanlioglu_
  <details><summary>Abstract</summary>
  We introduce Entropy-Guided Sequence Weighting (EGSW), a novel approach thatenhances the exploration-exploitation tradeoff by dynamically assigning weightsto generated outputs based on their advantage and entropy for ReinforcementLearning-based Large Language Model fine-tuning. EGSW integrates entropyregularization with advantage-based weighting to balance policy updates,enabling efficient exploration in high-dimensional state spaces. By employingtemperature-scaled softmax weighting over sequences, EGSW prioritizinghigh-reward, high-uncertainty steps while maintaining training stability.Although originally developed to improve Group Relative Policy Optimization(GRPO) during large language model (LLM) fine-tuning, EGSW is generalizable toother reinforcement learning (RL) algorithms and can be implemented in bothstep-wise and trajectory-wise settings. Empirical evaluations demonstrate thatEGSW enhances GRPO reasoning ability, yielding improvements in sampleefficiency. Future work will explore the application of EGSW to advanced RLmethodologies.
  </details>

- **[A Causal Framework to Measure and Mitigate Non-binary Treatment Discrimination](http://arxiv.org/abs/2503.22454v1)**  `arXiv:2503.22454`  
  _Ayan Majumdar, Deborah D. Kanubala, Kavya Gupta, Isabel Valera_
  <details><summary>Abstract</summary>
  Fairness studies of algorithmic decision-making systems often simplifycomplex decision processes, such as bail or loan approvals, into binaryclassification tasks. However, these approaches overlook that such decisionsare not inherently binary (e.g., approve or not approve bail or loan); theyalso involve non-binary treatment decisions (e.g., bail conditions or loanterms) that can influence the downstream outcomes (e.g., loan repayment orreoffending). In this paper, we argue that non-binary treatment decisions areintegral to the decision process and controlled by decision-makers and,therefore, should be central to fairness analyses in algorithmicdecision-making. We propose a causal framework that extends fairness analysesand explicitly distinguishes between decision-subjects' covariates and thetreatment decisions. This specification allows decision-makers to use ourframework to (i) measure treatment disparity and its downstream effects inhistorical data and, using counterfactual reasoning, (ii) mitigate the impactof past unfair treatment decisions when automating decision-making. We use ourframework to empirically analyze four widely used loan approval datasets toreveal potential disparity in non-binary treatment decisions and theirdiscriminatory impact on outcomes, highlighting the need to incorporatetreatment decisions in fairness assessments. Moreover, by intervening intreatment decisions, we show that our framework effectively mitigates treatmentdiscrimination from historical data to ensure fair risk score estimation and(non-binary) decision-making processes that benefit all stakeholders.
  </details>

- **[STADE: Standard Deviation as a Pruning Metric](http://arxiv.org/abs/2503.22451v1)**  `arXiv:2503.22451`  
  _Diego Coello de Portugal Mecke, Haya Alyoussef, Ilia Koloiarov, Maximilian Stubbemann, Lars Schmidt-Thieme_
  <details><summary>Abstract</summary>
  Recently, Large Language Models (LLMs) have become very widespread and areused to solve a wide variety of tasks. To successfully handle these tasks, LLMsrequire longer training times and larger model sizes. This makes LLMs idealcandidates for pruning methods that reduce computational demands whilemaintaining performance. Previous methods require a retraining phase afterpruning to maintain the original model's performance. However, state-of-the-artpruning methods, such as Wanda, prune the model without retraining, making thepruning process faster and more efficient. Building upon Wanda's work, thisstudy provides a theoretical explanation of why the method is effective andleverages these insights to enhance the pruning process. Specifically, atheoretical analysis of the pruning problem reveals a common scenario inMachine Learning where Wanda is the optimal pruning method. Furthermore, thisanalysis is extended to cases where Wanda is no longer optimal, leading to thedevelopment of a new method, STADE, based on the standard deviation of theinput. From a theoretical standpoint, STADE demonstrates better generalityacross different scenarios. Finally, extensive experiments on Llama and OpenPre-trained Transformers (OPT) models validate these theoretical findings,showing that depending on the training conditions, Wanda's optimal performancevaries as predicted by the theoretical framework. These insights contribute toa more robust understanding of pruning strategies and their practicalimplications. Code is available at: https://github.com/Coello-dev/STADE/
  </details>

- **[Robustness quantification and how it allows for reliable classification, even in the presence of distribution shift and for small training sets](http://arxiv.org/abs/2503.22418v1)**  `arXiv:2503.22418`  
  _Adri√°n Detavernier, Jasper De Bock_
  <details><summary>Abstract</summary>
  Based on existing ideas in the field of imprecise probabilities, we present anew approach for assessing the reliability of the individual predictions of agenerative probabilistic classifier. We call this approach robustnessquantification, compare it to uncertainty quantification, and demonstrate thatit continues to work well even for classifiers that are learned from smalltraining sets that are sampled from a shifted distribution.
  </details>

- **[Generative Reliability-Based Design Optimization Using In-Context Learning Capabilities of Large Language Models](http://arxiv.org/abs/2503.22401v1)**  `arXiv:2503.22401`  
  _Zhonglin Jiang, Qian Tang, Zequn Wang_
  <details><summary>Abstract</summary>
  Large Language Models (LLMs) have demonstrated remarkable in-context learningcapabilities, enabling flexible utilization of limited historical informationto play pivotal roles in reasoning, problem-solving, and complex patternrecognition tasks. Inspired by the successful applications of LLMs in multipledomains, this paper proposes a generative design method by leveraging thein-context learning capabilities of LLMs with the iterative search mechanismsof metaheuristic algorithms for solving reliability-based design optimizationproblems. In detail, reliability analysis is performed by engaging the LLMs andKriging surrogate modeling to overcome the computational burden. By dynamicallyproviding critical information of design points to the LLMs with promptengineering, the method enables rapid generation of high-quality designalternatives that satisfy reliability constraints while achieving performanceoptimization. With the Deepseek-V3 model, three case studies are used todemonstrated the performance of the proposed approach. Experimental resultsindicate that the proposed LLM-RBDO method successfully identifies feasiblesolutions that meet reliability constraints while achieving a comparableconvergence rate compared to traditional genetic algorithms.
  </details>

- **[On-site estimation of battery electrochemical parameters via transfer learning based physics-informed neural network approach](http://arxiv.org/abs/2503.22396v1)**  `arXiv:2503.22396`  
  _Josu Yeregui, Iker Lopetegi, Sergio Fernandez, Erik Garayalde, Unai Iraola_
  <details><summary>Abstract</summary>
  This paper presents a novel physical parameter estimation framework foron-site model characterization, using a two-phase modelling strategy withPhysics-Informed Neural Networks (PINNs) and transfer learning (TL). In thefirst phase, a PINN is trained using only the physical principles of the singleparticle model (SPM) equations. In the second phase, the majority of the PINNparameters are frozen, while critical electrochemical parameters are set astrainable and adjusted using real-world voltage profile data. The proposedapproach significantly reduces computational costs, making it suitable forreal-time implementation on Battery Management Systems (BMS). Additionally, asthe initial phase does not require field data, the model is easy to deploy withminimal setup requirements. With the proposed methodology, we have been able toeffectively estimate relevant electrochemical parameters with operating data.This has been proved estimating diffusivities and active material volumefractions with charge data in different degradation conditions. The methodologyis experimentally validated in a Raspberry Pi device using data from a standardcharge profile with a 3.89\% relative accuracy estimating the active materialvolume fractions of a NMC cell with 82.09\% of its nominal capacity.
  </details>

- **[MASCOTS: Model-Agnostic Symbolic COunterfactual explanations for Time Series](http://arxiv.org/abs/2503.22389v1)**  `arXiv:2503.22389`  
  _Dawid P≈Çudowski, Francesco Spinnato, Piotr Wilczy≈Ñski, Krzysztof Kotowski, Evridiki Vasileia Ntagiou, Riccardo Guidotti, et al._
  <details><summary>Abstract</summary>
  Counterfactual explanations provide an intuitive way to understand modeldecisions by identifying minimal changes required to alter an outcome. However,applying counterfactual methods to time series models remains challenging dueto temporal dependencies, high dimensionality, and the lack of an intuitivehuman-interpretable representation. We introduce MASCOTS, a method thatleverages the Bag-of-Receptive-Fields representation alongside symbolictransformations inspired by Symbolic Aggregate Approximation. By operating in asymbolic feature space, it enhances interpretability while preserving fidelityto the original data and model. Unlike existing approaches that either dependon model structure or autoencoder-based sampling, MASCOTS directly generatesmeaningful and diverse counterfactual observations in a model-agnostic manner,operating on both univariate and multivariate data. We evaluate MASCOTS onunivariate and multivariate benchmark datasets, demonstrating comparablevalidity, proximity, and plausibility to state-of-the-art methods, whilesignificantly improving interpretability and sparsity. Its symbolic natureallows for explanations that can be expressed visually, in natural language, orthrough semantic representations, making counterfactual reasoning moreaccessible and actionable.
  </details>

- **[Hybrid Time-Domain Behavior Model Based on Neural Differential Equations and RNNs](http://arxiv.org/abs/2503.22313v1)**  `arXiv:2503.22313`  
  _Zenghui Chang, Yang Zhang, Hu Tan, Hong Cai Chen_
  <details><summary>Abstract</summary>
  Nonlinear dynamics system identification is crucial for circuit emulation.Traditional continuous-time domain modeling approaches have limitations infitting capability and computational efficiency when used for modeling circuitIPs and device behaviors.This paper presents a novel continuous-time domainhybrid modeling paradigm. It integrates neural network differential models withrecurrent neural networks (RNNs), creating NODE-RNN and NCDE-RNN models basedon neural ordinary differential equations (NODE) and neural controlleddifferential equations (NCDE), respectively.Theoretical analysis shows thatthis hybrid model has mathematical advantages in event-driven dynamic mutationresponse and gradient propagation stability. Validation using real data fromPIN diodes in high-power microwave environments shows NCDE-RNN improves fittingaccuracy by 33\% over traditional NCDE, and NODE-RNN by 24\% over CTRNN,especially in capturing nonlinear memory effects.The model has beensuccessfully deployed in Verilog-A and validated through circuit emulation,confirming its compatibility with existing platforms and practical value.Thishybrid dynamics paradigm, by restructuring the neural differential equationsolution path, offers new ideas for high-precision circuit time-domain modelingand is significant for complex nonlinear circuit system modeling.
  </details>

- **[Machine Learning Models for Soil Parameter Prediction Based on Satellite, Weather, Clay and Yield Data](http://arxiv.org/abs/2503.22276v1)**  `arXiv:2503.22276`  
  _Calvin Kammerlander, Viola Kolb, Marinus Luegmair, Lou Scheermann, Maximilian Schmailzl, Marco Seufert, et al._
  <details><summary>Abstract</summary>
  Efficient nutrient management and precise fertilization are essential foradvancing modern agriculture, particularly in regions striving to optimize cropyields sustainably. The AgroLens project endeavors to address this challenge bydevelop ing Machine Learning (ML)-based methodologies to predict soil nutrientlevels without reliance on laboratory tests. By leveraging state of the arttechniques, the project lays a foundation for acionable insights to improveagricultural productivity in resource-constrained areas, such as Africa. Theapproach begins with the development of a robust European model using the LUCASSoil dataset and Sentinel-2 satellite imagery to estimate key soil properties,including phosphorus, potassium, nitrogen, and pH levels. This model is thenenhanced by integrating supplementary features, such as weather data, harvestrates, and Clay AI-generated embeddings. This report details the methodologicalframework, data preprocessing strategies, and ML pipelines employed in thisproject. Advanced algorithms, including Random Forests, Extreme GradientBoosting (XGBoost), and Fully Connected Neural Networks (FCNN), wereimplemented and finetuned for precise nutrient prediction. Results showcaserobust model performance, with root mean square error values meeting stringentaccuracy thresholds. By establishing a reproducible and scalable pipeline forsoil nutrient prediction, this research paves the way for transformativeagricultural applications, including precision fertilization and improvedresource allocation in underresourced regions like Africa.
  </details>

- **[FLIP: Towards Comprehensive and Reliable Evaluation of Federated Prompt Learning](http://arxiv.org/abs/2503.22263v1)**  `arXiv:2503.22263`  
  _Dongping Liao, Xitong Gao, Yabo Xu, Chengzhong Xu_
  <details><summary>Abstract</summary>
  The increasing emphasis on privacy and data security has driven the adoptionof federated learning, a decentralized approach to train machine learningmodels without sharing raw data. Prompt learning, which fine-tunes promptembeddings of pretrained models, offers significant advantages in federatedsettings by reducing computational costs and communication overheads whileleveraging the strong performance and generalization capabilities ofvision-language models such as CLIP. This paper addresses the intersection offederated learning and prompt learning, particularly for vision-languagemodels. In this work, we introduce a comprehensive framework, named FLIP, toevaluate federated prompt learning algorithms. FLIP assesses the performance of8 state-of-the-art federated prompt learning methods across 4 federatedlearning protocols and 12 open datasets, considering 6 distinct evaluationscenarios. Our findings demonstrate that prompt learning maintains stronggeneralization performance in both in-distribution and out-of-distributionsettings with minimal resource consumption. This work highlights theeffectiveness of federated prompt learning in environments characterized bydata scarcity, unseen classes, and cross-domain distributional shifts. Weopen-source the code for all implemented algorithms in FLIP to facilitatefurther research in this domain.
  </details>

- **[DynaGraph: Interpretable Multi-Label Prediction from EHRs via Dynamic Graph Learning and Contrastive Augmentation](http://arxiv.org/abs/2503.22257v1)**  `arXiv:2503.22257`  
  _Munib Mesinovic, Soheila Molaei, Peter Watkinson, Tingting Zhu_
  <details><summary>Abstract</summary>
  Learning from longitudinal electronic health records is limited if it doesnot capture the temporal trajectories of the patient's state in a clinicalsetting. Graph models allow us to capture the hidden dependencies of themultivariate time-series when the graphs are constructed in a similar dynamicmanner. Previous dynamic graph models require a pre-defined and/or static graphstructure, which is unknown in most cases, or they only capture the spatialrelations between the features. Furthermore in healthcare, the interpretabilityof the model is an essential requirement to build trust with clinicians. Inaddition to previously proposed attention mechanisms, there has not been aninterpretable dynamic graph framework for data from multivariate electronichealth records (EHRs). Here, we propose DynaGraph, an end-to-end interpretablecontrastive graph model that learns the dynamics of multivariate time-seriesEHRs as part of optimisation. We validate our model in four real-world clinicaldatasets, ranging from primary care to secondary care settings with broaddemographics, in challenging settings where tasks are imbalanced andmulti-labelled. Compared to state-of-the-art models, DynaGraph achievessignificant improvements in balanced accuracy and sensitivity over the nearestcomplex competitors in time-series or dynamic graph modelling across three ICUand one primary care datasets. Through a pseudo-attention approach to graphconstruction, our model also indicates the importance of clinical covariatesover time, providing means for clinical validation.
  </details>

- **[CRLLK: Constrained Reinforcement Learning for Lane Keeping in Autonomous Driving](http://arxiv.org/abs/2503.22248v1)**  `arXiv:2503.22248`  
  _Xinwei Gao, Arambam James Singh, Gangadhar Royyuru, Michael Yuhas, Arvind Easwaran_
  <details><summary>Abstract</summary>
  Lane keeping in autonomous driving systems requires scenario-specific weighttuning for different objectives. We formulate lane-keeping as a constrainedreinforcement learning problem, where weight coefficients are automaticallylearned along with the policy, eliminating the need for scenario-specifictuning. Empirically, our approach outperforms traditional RL in efficiency andreliability. Additionally, real-world demonstrations validate its practicalvalue for real-world autonomous driving.
  </details>

- **[Analysis of On-policy Policy Gradient Methods under the Distribution Mismatch](http://arxiv.org/abs/2503.22244v1)**  `arXiv:2503.22244`  
  _Weizhen Wang, Jianping He, Xiaoming Duan_
  <details><summary>Abstract</summary>
  Policy gradient methods are one of the most successful methods for solvingchallenging reinforcement learning problems. However, despite their empiricalsuccesses, many SOTA policy gradient algorithms for discounted problems deviatefrom the theoretical policy gradient theorem due to the existence of adistribution mismatch. In this work, we analyze the impact of this mismatch onthe policy gradient methods. Specifically, we first show that in the case oftabular parameterizations, the methods under the mismatch remain globallyoptimal. Then, we extend this analysis to more general parameterizations byleveraging the theory of biased stochastic gradient descent. Our findings offernew insights into the robustness of policy gradient methods as well as the gapbetween theoretical foundations and practical implementations.
  </details>

- **[WeatherMesh-3: Fast and accurate operational global weather forecasting](http://arxiv.org/abs/2503.22235v1)**  `arXiv:2503.22235`  
  _Haoxing Du, Lyna Kim, Joan Creus-Costa, Jack Michaels, Anuj Shetty, Todd Hutchinson, et al._
  <details><summary>Abstract</summary>
  We present WeatherMesh-3 (WM-3), an operational transformer-based globalweather forecasting system that improves the state of the art in both accuracyand computational efficiency. We introduce the following advances: 1) a latentrollout that enables arbitrary-length predictions in latent space withoutintermediate encoding or decoding; and 2) a modular architecture that flexiblyutilizes mixed-horizon processors and encodes multiple real-time analyses tocreate blended initial conditions. WM-3 generates 14-day global forecasts at0.25-degree resolution in 12 seconds on a single RTX 4090. This represents a>100,000-fold speedup over traditional NWP approaches while achieving superioraccuracy with up to 37.7% improvement in RMSE over operational models,requiring only a single consumer-grade GPU for deployment. We aim for WM-3 todemocratize weather forecasting by providing an accessible, lightweight modelfor operational use while pushing the performance boundaries of machinelearning-based weather prediction.
  </details>

- **[Process Reward Modeling with Entropy-Driven Uncertainty](http://arxiv.org/abs/2503.22233v1)**  `arXiv:2503.22233`  
  _Lang Cao, Renhong Chen, Yingtian Zou, Chao Peng, Wu Ning, Huacong Xu, et al._
  <details><summary>Abstract</summary>
  This paper presents the Entropy-Driven Unified Process Reward Model(EDU-PRM), a novel framework that approximates state-of-the-art performance inprocess supervision while drastically reducing training costs. EDU-PRMintroduces an entropy-guided dynamic step partitioning mechanism, using logitdistribution entropy to pinpoint high-uncertainty regions during tokengeneration dynamically. This self-assessment capability enables precisestep-level feedback without manual fine-grained annotation, addressing acritical challenge in process supervision. Experiments on the Qwen2.5-72B modelwith only 7,500 EDU-PRM-generated training queries demonstrate accuracy closelyapproximating the full Qwen2.5-72B-PRM (71.1% vs. 71.6%), achieving a 98%reduction in query cost compared to prior methods. This work establishesEDU-PRM as an efficient approach for scalable process reward model training.
  </details>

- **[Exploring Data Scaling Trends and Effects in Reinforcement Learning from Human Feedback](http://arxiv.org/abs/2503.22230v1)**  `arXiv:2503.22230`  
  _Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, et al._
  <details><summary>Abstract</summary>
  Reinforcement Learning from Human Feedback (RLHF) is crucial for aligninglarge language models with human preferences. While recent research has focusedon algorithmic improvements, the importance of prompt-data construction hasbeen overlooked. This paper addresses this gap by exploring data-drivenbottlenecks in RLHF performance scaling, particularly reward hacking anddecreasing response diversity. We introduce a hybrid reward system combiningreasoning task verifiers (RTV) and a generative reward model (GenRM) tomitigate reward hacking. We also propose a novel prompt-selection method,Pre-PPO, to maintain response diversity and enhance learning effectiveness.Additionally, we find that prioritizing mathematical and coding tasks early inRLHF training significantly improves performance. Experiments across two modelsizes validate our methods' effectiveness and scalability. Results show thatRTV is most resistant to reward hacking, followed by GenRM with ground truth,and then GenRM with SFT Best-of-N responses. Our strategies enable rapidcapture of subtle task-specific distinctions, leading to substantialimprovements in overall RLHF performance. This work highlights the importanceof careful data construction and provides practical methods to overcomeperformance barriers in RLHF.
  </details>

- **[DREMnet: An Interpretable Denoising Framework for Semi-Airborne Transient Electromagnetic Signal](http://arxiv.org/abs/2503.22223v1)**  `arXiv:2503.22223`  
  _Shuang Wang, Ming Guo, Xuben Wang, Fei Deng, Lifeng Mao, Bin Wang, et al._
  <details><summary>Abstract</summary>
  The semi-airborne transient electromagnetic method (SATEM) is capable ofconducting rapid surveys over large-scale and hard-to-reach areas. However, theacquired signals are often contaminated by complex noise, which can compromisethe accuracy of subsequent inversion interpretations. Traditional denoisingtechniques primarily rely on parameter selection strategies, which areinsufficient for processing field data in noisy environments. With the adventof deep learning, various neural networks have been employed for SATEM signaldenoising. However, existing deep learning methods typically use single-mappinglearning approaches that struggle to effectively separate signal from noise.These methods capture only partial information and lack interpretability. Toovercome these limitations, we propose an interpretable decoupledrepresentation learning framework, termed DREMnet, that disentangles data intocontent and context factors, enabling robust and interpretable denoising incomplex conditions. To address the limitations of CNN and Transformerarchitectures, we utilize the RWKV architecture for data processing andintroduce the Contextual-WKV mechanism, which allows unidirectional WKV toperform bidirectional signal modeling. Our proposed Covering Embeddingtechnique retains the strong local perception of convolutional networks throughstacked embedding. Experimental results on test datasets demonstrate that theDREMnet method outperforms existing techniques, with processed field data thatmore accurately reflects the theoretical signal, offering improvedidentification of subsurface electrical structures.
  </details>

- **[Interpretable Deep Learning Paradigm for Airborne Transient Electromagnetic Inversion](http://arxiv.org/abs/2503.22214v1)**  `arXiv:2503.22214`  
  _Shuang Wang, Xuben Wang, Fei Deng, Xiaodong Yu, Peifan Jiang, Lifeng Mao_
  <details><summary>Abstract</summary>
  The extraction of geoelectric structural information from airborne transientelectromagnetic(ATEM)data primarily involves data processing and inversion.Conventional methods rely on empirical parameter selection, making it difficultto process complex field data with high noise levels. Additionally, inversioncomputations are time consuming and often suffer from multiple local minima.Existing deep learning-based approaches separate the data processing steps,where independently trained denoising networks struggle to ensure thereliability of subsequent inversions. Moreover, end to end networks lackinterpretability. To address these issues, we propose a unified andinterpretable deep learning inversion paradigm based on disentangledrepresentation learning. The network explicitly decomposes noisy data intonoise and signal factors, completing the entire data processing workflow basedon the signal factors while incorporating physical information for guidance.This approach enhances the network's reliability and interpretability. Theinversion results on field data demonstrate that our method can directly usenoisy data to accurately reconstruct the subsurface electrical structure.Furthermore, it effectively processes data severely affected by environmentalnoise, which traditional methods struggle with, yielding improved lateralstructural resolution.
  </details>

- **[Fuzzy Cluster-Aware Contrastive Clustering for Time Series](http://arxiv.org/abs/2503.22211v1)**  `arXiv:2503.22211`  
  _Congyu Wang, Mingjing Du, Xiang Jiang, Yongquan Dong_
  <details><summary>Abstract</summary>
  The rapid growth of unlabeled time series data, driven by the Internet ofThings (IoT), poses significant challenges in uncovering underlying patterns.Traditional unsupervised clustering methods often fail to capture the complexnature of time series data. Recent deep learning-based clustering approaches,while effective, struggle with insufficient representation learning and theintegration of clustering objectives. To address these issues, we propose afuzzy cluster-aware contrastive clustering framework (FCACC) that jointlyoptimizes representation learning and clustering.  Our approach introduces a novel three-view data augmentation strategy toenhance feature extraction by leveraging various characteristics of time seriesdata. Additionally, we propose a cluster-aware hard negative sample generationmechanism that dynamically constructs high-quality negative samples usingclustering structure information, thereby improving the model's discriminativeability.  By leveraging fuzzy clustering, FCACC dynamically generates clusterstructures to guide the contrastive learning process, resulting in moreaccurate clustering. Extensive experiments on 40 benchmark datasets show thatFCACC outperforms the selected baseline methods (eight in total), providing aneffective solution for unsupervised time series learning.
  </details>

- **[Data-Free Universal Attack by Exploiting the Intrinsic Vulnerability of Deep Models](http://arxiv.org/abs/2503.22205v1)**  `arXiv:2503.22205`  
  _YangTian Yan, Jinyu Tian_
  <details><summary>Abstract</summary>
  Deep neural networks (DNNs) are susceptible to Universal AdversarialPerturbations (UAPs), which are instance agnostic perturbations that candeceive a target model across a wide range of samples. Unlike instance-specificadversarial examples, UAPs present a greater challenge as they must generalizeacross different samples and models. Generating UAPs typically requires accessto numerous examples, which is a strong assumption in real-world tasks. In thispaper, we propose a novel data-free method called Intrinsic UAP (IntriUAP), byexploiting the intrinsic vulnerabilities of deep models. We analyze a series ofpopular deep models composed of linear and nonlinear layers with a Lipschitzconstant of 1, revealing that the vulnerability of these models ispredominantly influenced by their linear components. Based on this observation,we leverage the ill-conditioned nature of the linear components by aligning theUAP with the right singular vectors corresponding to the maximum singular valueof each linear layer. Remarkably, our method achieves highly competitiveperformance in attacking popular image classification deep models without usingany image samples. We also evaluate the black-box attack performance of ourmethod, showing that it matches the state-of-the-art baseline for data-freemethods on models that conform to our theoretical framework. Beyond thedata-free assumption, IntriUAP also operates under a weaker assumption, wherethe adversary only can access a few of the victim model's layers. Experimentsdemonstrate that the attack success rate decreases by only 4% when theadversary has access to just 50% of the linear layers in the victim model.
  </details>

- **[AdaRank: Adaptive Rank Pruning for Enhanced Model Merging](http://arxiv.org/abs/2503.22178v1)**  `arXiv:2503.22178`  
  _Chanhyuk Lee, Jiho Choi, Chanryeol Lee, Donggyun Kim, Seunghoon Hong_
  <details><summary>Abstract</summary>
  Model merging has emerged as a promising approach for unifying independentlyfine-tuned models into an integrated framework, significantly enhancingcomputational efficiency in multi-task learning. Recently, several SVD-basedtechniques have been introduced to exploit low-rank structures for enhancedmerging, but their reliance on such manually designed rank selection oftenleads to cross-task interference and suboptimal performance. In this paper, wepropose AdaRank, a novel model merging framework that adaptively selects themost beneficial singular directions of task vectors to merge multiple models.We empirically show that the dominant singular components of task vectors cancause critical interference with other tasks, and that naive truncation acrosstasks and layers degrades performance. In contrast, AdaRank dynamically prunesthe singular components that cause interference and offers an optimal amount ofinformation to each task vector by learning to prune ranks during test-time viaentropy minimization. Our analysis demonstrates that such method mitigatesdetrimental overlaps among tasks, while empirical results show that AdaRankconsistently achieves state-of-the-art performance with various backbones andnumber of tasks, reducing the performance gap between fine-tuned models tonearly 1%.
  </details>

- **[Reasoning of Large Language Models over Knowledge Graphs with Super-Relations](http://arxiv.org/abs/2503.22166v1)**  `arXiv:2503.22166`  
  _Song Wang, Junhong Lin, Xiaojie Guo, Julian Shun, Jundong Li, Yada Zhu_
  <details><summary>Abstract</summary>
  While large language models (LLMs) have made significant progress inprocessing and reasoning over knowledge graphs, current methods suffer from ahigh non-retrieval rate. This limitation reduces the accuracy of answeringquestions based on these graphs. Our analysis reveals that the combination ofgreedy search and forward reasoning is a major contributor to this issue. Toovercome these challenges, we introduce the concept of super-relations, whichenables both forward and backward reasoning by summarizing and connectingvarious relational paths within the graph. This holistic approach not onlyexpands the search space, but also significantly improves retrieval efficiency.In this paper, we propose the ReKnoS framework, which aims to Reason overKnowledge Graphs with Super-Relations. Our framework's key advantages includethe inclusion of multiple relation paths through super-relations, enhancedforward and backward reasoning capabilities, and increased efficiency inquerying LLMs. These enhancements collectively lead to a substantialimprovement in the successful retrieval rate and overall reasoning performance.We conduct extensive experiments on nine real-world datasets to evaluateReKnoS, and the results demonstrate the superior performance of ReKnoS overexisting state-of-the-art baselines, with an average accuracy gain of 2.92%.
  </details>

- **[Landscape of Thoughts: Visualizing the Reasoning Process of Large Language Models](http://arxiv.org/abs/2503.22165v1)**  `arXiv:2503.22165`  
  _Zhanke Zhou, Zhaocheng Zhu, Xuan Li, Mikhail Galkin, Xiao Feng, Sanmi Koyejo, et al._
  <details><summary>Abstract</summary>
  Numerous applications of large language models (LLMs) rely on their abilityto perform step-by-step reasoning. However, the reasoning behavior of LLMsremains poorly understood, posing challenges to research, development, andsafety. To address this gap, we introduce landscape of thoughts-the firstvisualization tool for users to inspect the reasoning paths of chain-of-thoughtand its derivatives on any multi-choice dataset. Specifically, we represent thestates in a reasoning path as feature vectors that quantify their distances toall answer choices. These features are then visualized in two-dimensional plotsusing t-SNE. Qualitative and quantitative analysis with the landscape ofthoughts effectively distinguishes between strong and weak models, correct andincorrect answers, as well as different reasoning tasks. It also uncoversundesirable reasoning patterns, such as low consistency and high uncertainty.Additionally, users can adapt our tool to a model that predicts the propertythey observe. We showcase this advantage by adapting our tool to a lightweightverifier that evaluates the correctness of reasoning paths. The code ispublicly available at: https://github.com/tmlr-group/landscape-of-thoughts.
  </details>

- **[T-CIL: Temperature Scaling using Adversarial Perturbation for Calibration in Class-Incremental Learning](http://arxiv.org/abs/2503.22163v1)**  `arXiv:2503.22163`  
  _Seong-Hyeon Hwang, Minsu Kim, Steven Euijong Whang_
  <details><summary>Abstract</summary>
  We study model confidence calibration in class-incremental learning, wheremodels learn from sequential tasks with different class sets. While existingworks primarily focus on accuracy, maintaining calibrated confidence has beenlargely overlooked. Unfortunately, most post-hoc calibration techniques are notdesigned to work with the limited memories of old-task data typical inclass-incremental learning, as retaining a sufficient validation set would beimpractical. Thus, we propose T-CIL, a novel temperature scaling approach forclass-incremental learning without a validation set for old tasks, thatleverages adversarially perturbed exemplars from memory. Directly usingexemplars is inadequate for temperature optimization, since they are alreadyused for training. The key idea of T-CIL is to perturb exemplars more stronglyfor old tasks than for the new task by adjusting the perturbation directionbased on feature distance, with the single magnitude determined using thenew-task validation set. This strategy makes the perturbation magnitudecomputed from the new task also applicable to old tasks, leveraging thetendency that the accuracy of old tasks is lower than that of the new task. Weempirically show that T-CIL significantly outperforms various baselines interms of calibration on real datasets and can be integrated with existingclass-incremental learning techniques with minimal impact on accuracy.
  </details>

- **[Tokenization of Gaze Data](http://arxiv.org/abs/2503.22145v1)**  `arXiv:2503.22145`  
  _Tim Rolff, Jurik Karimian, Niklas Hypki, Susanne Schmidt, Markus Lappe, Frank Steinicke_
  <details><summary>Abstract</summary>
  A considerable part of the performance of today's large language models(LLM's) and multimodal large language models (MLLM's) depends on theirtokenization strategies. While tokenizers are extensively researched fortextual and visual input, there is no research on tokenization strategies forgaze data due to its nature. However, a corresponding tokenization strategywould allow using the vision capabilities of pre-trained MLLM's for gaze data,for example, through fine-tuning.  In this paper, we aim to close this research gap by analyzing five differenttokenizers for gaze data on three different datasets for the forecasting andgeneration of gaze data through LLMs (cf.~\cref{fig:teaser}). We evaluate thetokenizers regarding their reconstruction and compression abilities. Further,we train an LLM for each tokenization strategy, measuring its generative andpredictive performance. Overall, we found that a quantile tokenizer outperformsall others in predicting the gaze positions and k-means is best when predictinggaze velocities.
  </details>

- **[Long-Term Electricity Demand Prediction Using Non-negative Tensor Factorization and Genetic Algorithm-Driven Temporal Modeling](http://arxiv.org/abs/2503.22132v1)**  `arXiv:2503.22132`  
  _Toma Masaki, Kanta Tachibana_
  <details><summary>Abstract</summary>
  This study proposes a novel framework for long-term electricity demandprediction based solely on historical consumption data, without relying onexternal variables such as temperature or economic indicators. The methodcombines Non-negative Tensor Factorization (NTF) to extract low-dimensionaltemporal features from multi-way electricity usage data, with a GeneticAlgorithm that optimizes the hyperparameters of time series models applied tothe latent annual factors. We model the dataset as a third-order tensorspanning electric utilities, industrial sectors, and years, and apply canonicalpolyadic decomposition under non-negativity constraints. The annual componentis forecasted using autoregressive models, with hyperparameter tuning guided bythe prediction error or reconstruction accuracy on a validation set.Comparative experiments using real-world electricity data from Japandemonstrate that the proposed method achieves lower mean squared error thanbaseline approaches without tensor decomposition or evolutionary optimization.Moreover, we find that reducing the model's degrees of freedom via tensordecomposition improves generalization performance, and that initializationsensitivity in NTF can be mitigated through multiple runs or ensemblestrategies. These findings suggest that the proposed framework offers aninterpretable, flexible, and scalable approach to long-term electricity demandprediction and can be extended to other structured time series forecastingtasks.
  </details>

- **[Multimodal Machine Learning for Real Estate Appraisal: A Comprehensive Survey](http://arxiv.org/abs/2503.22119v1)**  `arXiv:2503.22119`  
  _Chenya Huang, Zhidong Li, Fang Chen, Bin Liang_
  <details><summary>Abstract</summary>
  Real estate appraisal has undergone a significant transition from manual toautomated valuation and is entering a new phase of evolution. Leveragingcomprehensive attention to various data sources, a novel approach to automatedvaluation, multimodal machine learning, has taken shape. This approachintegrates multimodal data to deeply explore the diverse factors influencinghousing prices. Furthermore, multimodal machine learning significantlyoutperforms single-modality or fewer-modality approaches in terms of predictionaccuracy, with enhanced interpretability. However, systematic and comprehensivesurvey work on the application in the real estate domain is still lacking. Inthis survey, we aim to bridge this gap by reviewing the research efforts. Webegin by reviewing the background of real estate appraisal and propose tworesearch questions from the perspecve of performance and fusion aimed atimproving the accuracy of appraisal results. Subsequently, we explain theconcept of multimodal machine learning and provide a comprehensiveclassification and definition of modalities used in real estate appraisal forthe first time. To ensure clarity, we explore works related to data andtechniques, along with their evaluation methods, under the framework of thesetwo research questions. Furthermore, specific application domains aresummarized. Finally, we present insights into future research directionsincluding multimodal complementarity, technology and modality contribution.
  </details>

- **[Estimating City-wide operating mode Distribution of Light-Duty Vehicles: A Neural Network-based Approach](http://arxiv.org/abs/2503.22118v1)**  `arXiv:2503.22118`  
  _Muhammad Usama, Haris N. Koutsopoulos, Zhengbing He, Lijiao Wang_
  <details><summary>Abstract</summary>
  Driving cycles are a set of driving conditions and are crucial for theexisting emission estimation model to evaluate vehicle performance, fuelefficiency, and emissions, by matching them with average speed to calculate theoperating modes, such as braking, idling, and cruising. While existing emissionestimation models, such as the Motor Vehicle Emission Simulator (MOVES), arepowerful tools, their reliance on predefined driving cycles can be limiting, asthese cycles often do not accurately represent regional driving conditions,making the models less effective for city-wide analyses. To solve this problem,this paper proposes a modular neural network (NN)-based framework to estimateoperating mode distributions bypassing the driving cycle development phase,utilizing macroscopic variables such as speed, flow, and link infrastructureattributes. The proposed method is validated using a well-calibratedmicrosimulation model of Brookline MA, the United States. The results indicatethat the proposed framework outperforms the operating mode distributioncalculated by MOVES based on default driving cycles, providing a closer matchto the actual operating mode distribution derived from trajectory data.Specifically, the proposed model achieves an average RMSE of 0.04 in predictingoperating mode distribution, compared to 0.08 for MOVES. The average error inemission estimation across pollutants is 8.57% for the proposed method, lowerthan the 32.86% error for MOVES. In particular, for the estimation of CO2, theproposed method has an error of just 4%, compared to 35% for MOVES. Theproposed model can be utilized for real-time emissions monitoring by providingrapid and accurate emissions estimates with easily accessible inputs.
  </details>

- **[Few-Shot Graph Out-of-Distribution Detection with LLMs](http://arxiv.org/abs/2503.22097v1)**  `arXiv:2503.22097`  
  _Haoyan Xu, Zhengtao Yao, Yushun Dong, Ziyi Wang, Ryan A. Rossi, Mengyuan Li, et al._
  <details><summary>Abstract</summary>
  Existing methods for graph out-of-distribution (OOD) detection typicallydepend on training graph neural network (GNN) classifiers using a substantialamount of labeled in-distribution (ID) data. However, acquiring high-qualitylabeled nodes in text-attributed graphs (TAGs) is challenging and costly due totheir complex textual and structural characteristics. Large language models(LLMs), known for their powerful zero-shot capabilities in textual tasks, showpromise but struggle to naturally capture the critical structural informationinherent to TAGs, limiting their direct effectiveness.  To address these challenges, we propose LLM-GOOD, a general framework thateffectively combines the strengths of LLMs and GNNs to enhance data efficiencyin graph OOD detection. Specifically, we first leverage LLMs' strong zero-shotcapabilities to filter out likely OOD nodes, significantly reducing the humanannotation burden. To minimize the usage and cost of the LLM, we employ it onlyto annotate a small subset of unlabeled nodes. We then train a lightweight GNNfilter using these noisy labels, enabling efficient predictions of ID statusfor all other unlabeled nodes by leveraging both textual and structuralinformation. After obtaining node embeddings from the GNN filter, we can applyinformativeness-based methods to select the most valuable nodes for precisehuman annotation. Finally, we train the target ID classifier using theseaccurately annotated ID nodes. Extensive experiments on four real-world TAGdatasets demonstrate that LLM-GOOD significantly reduces human annotation costsand outperforms state-of-the-art baselines in terms of both ID classificationaccuracy and OOD detection performance.
  </details>

- **[ReLU Networks as Random Functions: Their Distribution in Probability Space](http://arxiv.org/abs/2503.22082v1)**  `arXiv:2503.22082`  
  _Shreyas Chaudhari, Jos√© M. F. Moura_
  <details><summary>Abstract</summary>
  This paper presents a novel framework for understanding trained ReLU networksas random, affine functions, where the randomness is induced by thedistribution over the inputs. By characterizing the probability distribution ofthe network's activation patterns, we derive the discrete probabilitydistribution over the affine functions realizable by the network. We extendthis analysis to describe the probability distribution of the network'soutputs. Our approach provides explicit, numerically tractable expressions forthese distributions in terms of Gaussian orthant probabilities. Additionally,we develop approximation techniques to identify the support of affine functionsa trained ReLU network can realize for a given distribution of inputs. Our workprovides a framework for understanding the behavior and performance of ReLUnetworks corresponding to stochastic inputs, paving the way for moreinterpretable and reliable models.
  </details>

- **[Concise One-Layer Transformers Can Do Function Evaluation (Sometimes)](http://arxiv.org/abs/2503.22076v1)**  `arXiv:2503.22076`  
  _Lena Strobl, Dana Angluin, Robert Frank_
  <details><summary>Abstract</summary>
  While transformers have proven enormously successful in a range of tasks,their fundamental properties as models of computation are not well understood.This paper contributes to the study of the expressive capacity of transformers,focusing on their ability to perform the fundamental computational task ofevaluating an arbitrary function from $[n]$ to $[n]$ at a given argument. Weprove that concise 1-layer transformers (i.e., with a polylog bound on theproduct of the number of heads, the embedding dimension, and precision) arecapable of doing this task under some representations of the input, but notwhen the function's inputs and values are only encoded in different inputpositions. Concise 2-layer transformers can perform the task even with the moredifficult input representation. Experimentally, we find a rough alignmentbetween what we have proven can be computed by concise transformers and whatcan be practically learned.
  </details>

- **[A Proposal for Networks Capable of Continual Learning](http://arxiv.org/abs/2503.22068v1)**  `arXiv:2503.22068`  
  _Zeki Doruk Erden, Boi Faltings_
  <details><summary>Abstract</summary>
  We analyze the ability of computational units to retain past responses afterparameter updates, a key property for system-wide continual learning. Neuralnetworks trained with gradient descent lack this capability, prompting us topropose Modelleyen, an alternative approach with inherent responsepreservation. We demonstrate through experiments on modeling the dynamics of asimple environment and on MNIST that, despite increased computationalcomplexity and some representational limitations at its current stage,Modelleyen achieves continual learning without relying on sample replay orpredefined task boundaries.
  </details>

- **[Arch-LLM: Taming LLMs for Neural Architecture Generation via Unsupervised Discrete Representation Learning](http://arxiv.org/abs/2503.22063v1)**  `arXiv:2503.22063`  
  _Deshani Geethika Poddenige, Sachith Seneviratne, Damith Senanayake, Mahesan Niranjan, PN Suganthan, Saman Halgamuge_
  <details><summary>Abstract</summary>
  Unsupervised representation learning has been widely explored across variousmodalities, including neural architectures, where it plays a key role indownstream applications like Neural Architecture Search (NAS). These methodstypically learn an unsupervised representation space before generating/sampling architectures for the downstream search. A common approach involvesthe use of Variational Autoencoders (VAEs) to map discrete architectures onto acontinuous representation space, however, sampling from these spaces oftenleads to a high percentage of invalid or duplicate neural architectures. Thiscould be due to the unnatural mapping of inherently discrete architecturalspace onto a continuous space, which emphasizes the need for a robust discreterepresentation of these architectures. To address this, we introduce a VectorQuantized Variational Autoencoder (VQ-VAE) to learn a discrete latent spacemore naturally aligned with the discrete neural architectures. In contrast toVAEs, VQ-VAEs (i) map each architecture into a discrete code sequence and (ii)allow the prior to be learned by any generative model rather than assuming anormal distribution. We then represent these architecture latent codes asnumerical sequences and train a text-to-text model leveraging a Large LanguageModel to learn and generate sequences representing architectures. We experimentour method with Inception/ ResNet-like cell-based search spaces, namelyNAS-Bench-101 and NAS-Bench-201. Compared to VAE-based methods, our approachimproves the generation of valid and unique architectures by over 80% onNASBench-101 and over 8% on NASBench-201. Finally, we demonstrate theapplicability of our method in NAS employing a sequence-modeling-based NASalgorithm.
  </details>

- **[Low Rank and Sparse Fourier Structure in Recurrent Networks Trained on Modular Addition](http://arxiv.org/abs/2503.22059v1)**  `arXiv:2503.22059`  
  _Akshay Rangamani_
  <details><summary>Abstract</summary>
  Modular addition tasks serve as a useful test bed for observing empiricalphenomena in deep learning, including the phenomenon of \emph{grokking}. Priorwork has shown that one-layer transformer architectures learn FourierMultiplication circuits to solve modular addition tasks. In this paper, we showthat Recurrent Neural Networks (RNNs) trained on modular addition tasks alsouse a Fourier Multiplication strategy. We identify low rank structures in themodel weights, and attribute model components to specific Fourier frequencies,resulting in a sparse representation in the Fourier space. We also showempirically that the RNN is robust to removing individual frequencies, whilethe performance degrades drastically as more frequencies are ablated from themodel.
  </details>

- **[Nearest Neighbour Equilibrium Clustering](http://arxiv.org/abs/2503.21431v2)**  `arXiv:2503.21431`  
  _David P. Hofmeyr_
  <details><summary>Abstract</summary>
  A novel and intuitive nearest neighbours based clustering algorithm isintroduced, in which a cluster is defined in terms of an equilibrium conditionwhich balances its size and cohesiveness. The formulation of the equilibriumcondition allows for a quantification of the strength of alignment of eachpoint to a cluster, with these cluster alignment strengths leading naturally toa model selection criterion which renders the proposed approach fullyautomatable. The algorithm is simple to implement and computationallyefficient, and produces clustering solutions of extremely high quality incomparison with relevant benchmarks from the literature. R code to implementthe approach is available from https://github.com/DavidHofmeyr/NNEC.
  </details>

- **[Quantum Neural Network Restatement of Markov Jump Process](http://arxiv.org/abs/2503.20742v2)**  `arXiv:2503.20742`  
  _Z. Zarezadeh, N. Zarezadeh_
  <details><summary>Abstract</summary>
  Despite the many challenges in exploratory data analysis, artificial neuralnetworks have motivated strong interests in scientists and researchers both intheoretical as well as practical applications. Among sources of such popularityof artificial neural networks the ability of modeling non-linear dynamicalsystems, generalization, and adaptation possibilities should be mentioned.Despite this, there is still significant debate about the role of variousunderlying stochastic processes in stabilizing a unique structure for datalearning and prediction. One of such obstacles to the theoretical and numericalstudy of machine intelligent systems is the curse of dimensionality and thesampling from high-dimensional probability distributions. In general, thiscurse prevents efficient description of states, providing a significantcomplexity barrier for the system to be efficiently described and studied. Inthis strand of research, direct treatment and description of such abstractnotions of learning theory in terms of quantum information be one of the mostfavorable candidates. Hence, the subject matter of these articles is devoted toproblems of design, adaptation and the formulations of computationally hardproblems in terms of quantum mechanical systems. In order to characterize themicroscopic description of such dynamics in the language of inferentialstatistics, covariance matrix estimation of d-dimensional Gaussian densitiesand Bayesian interpretation of eigenvalue problem for dynamical systems isassessed.
  </details>

- **[Neural Network Approach to Stochastic Dynamics for Smooth Multimodal Density Estimation](http://arxiv.org/abs/2503.17807v2)**  `arXiv:2503.17807`  
  _Z. Zarezadeh, N. Zarezadeh_
  <details><summary>Abstract</summary>
  In this paper we consider a new probability sampling methods based onLangevin diffusion dynamics to resolve the problem of existing Monte Carloalgorithms when draw samples from high dimensional target densities. We extentMetropolis-Adjusted Langevin Diffusion algorithm by modelling the stochasticityof precondition matrix as a random matrix. An advantage compared to otherproposal method is that it only requires the gradient of log-posterior. Theproposed method provides fully adaptation mechanisms to tune proposal densitiesto exploits and adapts the geometry of local structures of statistical models.We clarify the benefits of the new proposal by modelling a Quantum ProbabilityDensity Functions of a free particle in a plane (energy Eigen-functions). Theproposed model represents a remarkable improvement in terms of performanceaccuracy and computational time over standard MCMC method.
  </details>

- **[OThink-MR1: Stimulating multimodal generalized reasoning capabilities via dynamic reinforcement learning](http://arxiv.org/abs/2503.16081v2)**  `arXiv:2503.16081`  
  _Zhiyuan Liu, Yuting Zhang, Feng Liu, Changwang Zhang, Ying Sun, Jun Wang_
  <details><summary>Abstract</summary>
  Multimodal Large Language Models (MLLMs) have gained significant traction fortheir ability to process diverse input data types and generate coherent,contextually relevant outputs across various applications. While supervisedfine-tuning (SFT) has been the predominant approach to enhance MLLMcapabilities in task-specific optimization, it often falls short in fosteringcrucial generalized reasoning abilities. Although reinforcement learning (RL)holds great promise in overcoming these limitations, it encounters twosignificant challenges: (1) its generalized capacities in multimodal tasksremain largely unexplored, and (2) its training constraints, including theconstant Kullback-Leibler divergence or the clamp strategy, often result insuboptimal bottlenecks. To address these challenges, we propose OThink-MR1, anadvanced MLLM equipped with profound comprehension and reasoning capabilitiesacross multimodal tasks. Specifically, we introduce Group Relative PolicyOptimization with a dynamic Kullback-Leibler strategy (GRPO-D), which markedlyenhances reinforcement learning (RL) performance. For Qwen2-VL-2B-Instruct,GRPO-D achieves a relative improvement of more than 5.72% over SFT and morethan 13.59% over GRPO in same-task evaluation on two adapted datasets.Furthermore, GRPO-D demonstrates remarkable cross-task generalizationcapabilities, with an average relative improvement of more than 61.63% over SFTin cross-task evaluation. These results highlight that the MLLM trained withGRPO-D on one multimodal task can be effectively transferred to another task,underscoring the superior generalized reasoning capabilities of our proposedOThink-MR1 model.
  </details>

- **[Borsuk-Ulam and Replicable Learning of Large-Margin Halfspaces](http://arxiv.org/abs/2503.15294v2)**  `arXiv:2503.15294`  
  _Ari Blondal, Hamed Hatami, Pooya Hatami, Chavdar Lalov, Sivan Tretiak_
  <details><summary>Abstract</summary>
  Recent remarkable advances in learning theory have established that, fortotal concept classes, list replicability, global stability, differentiallyprivate (DP) learnability, and shared-randomness replicability all coincidewith the finiteness of Littlestone dimension. Does this equivalence extend topartial concept classes?  We answer this question by proving that the list replicability number of$d$-dimensional $\gamma$-margin half-spaces satisfies \[ \frac{d}{2}+1 \le\mathrm{LR}(H^d_\gamma) \le d, \] which grows with dimension. Consequently, forpartial classes, list replicability and global stability do not necessarilyfollow from bounded Littlestone dimension, pure DP-learnability, orshared-randomness replicability.  Applying our main theorem, we resolve several open problems:  $\bullet$ Every disambiguation of infinite-dimensional large-marginhalf-spaces to a total concept class has unbounded Littlestone dimension,answering an open question of Alon, Hanneke, Holzman, and Moran (FOCS '21).  $\bullet$ The maximum list-replicability number of any finite set of pointsand homogeneous half-spaces in $d$-dimensional Euclidean space is $d$,resolving a problem of Chase, Moran, and Yehudayoff (FOCS '23).  $\bullet$ Every disambiguation of the Gap Hamming Distance problem in thelarge gap regime has unbounded public-coin randomized communication complexity.This answers an open question of Fang, G\"o\"os, Harms, and Hatami (STOC '25).  Our lower bound follows from a topological argument based on the localBorsuk-Ulam theorem of Chase, Chornomaz, Moran, and Yehudayoff (STOC '24). Forthe upper bound, we construct a list-replicable learning rule using thegeneralization properties of SVMs.
  </details>

- **[FedLWS: Federated Learning with Adaptive Layer-wise Weight Shrinking](http://arxiv.org/abs/2503.15111v2)**  `arXiv:2503.15111`  
  _Changlong Shi, Jinmeng Li, He Zhao, Dandan Guo, Yi Chang_
  <details><summary>Abstract</summary>
  In Federated Learning (FL), weighted aggregation of local models is conductedto generate a new global model, and the aggregation weights are typicallynormalized to 1. A recent study identifies the global weight shrinking effectin FL, indicating an enhancement in the global model's generalization when thesum of weights (i.e., the shrinking factor) is smaller than 1, where how tolearn the shrinking factor becomes crucial. However, principled approaches tothis solution have not been carefully studied from the adequate considerationof privacy concerns and layer-wise distinctions. To this end, we propose anovel model aggregation strategy, Federated Learning with Adaptive Layer-wiseWeight Shrinking (FedLWS), which adaptively designs the shrinking factor in alayer-wise manner and avoids optimizing the shrinking factors on a proxydataset. We initially explored the factors affecting the shrinking factorduring the training process. Then we calculate the layer-wise shrinking factorsby considering the distinctions among each layer of the global model. FedLWScan be easily incorporated with various existing methods due to itsflexibility. Extensive experiments under diverse scenarios demonstrate thesuperiority of our method over several state-of-the-art approaches, providing apromising tool for enhancing the global model in FL.
  </details>

- **[How Can Time Series Analysis Benefit From Multiple Modalities? A Survey and Outlook](http://arxiv.org/abs/2503.11835v3)**  `arXiv:2503.11835`  
  _Haoxin Liu, Harshavardhan Kamarthi, Zhiyuan Zhao, Shangqing Xu, Shiyu Wang, Qingsong Wen, et al._
  <details><summary>Abstract</summary>
  Time series analysis (TSA) is a longstanding research topic in the datamining community and has wide real-world significance. Compared to "richer"modalities such as language and vision, which have recently experiencedexplosive development and are densely connected, the time-series modalityremains relatively underexplored and isolated. We notice that many recent TSAworks have formed a new research field, i.e., Multiple Modalities for TSA(MM4TSA). In general, these MM4TSA works follow a common motivation: how TSAcan benefit from multiple modalities. This survey is the first to offer acomprehensive review and a detailed outlook for this emerging field.Specifically, we systematically discuss three benefits: (1) reusing foundationmodels of other modalities for efficient TSA, (2) multimodal extension forenhanced TSA, and (3) cross-modality interaction for advanced TSA. We furthergroup the works by the introduced modality type, including text, images, audio,tables, and others, within each perspective. Finally, we identify the gaps withfuture opportunities, including the reused modalities selections, heterogeneousmodality combinations, and unseen tasks generalizations, corresponding to thethree benefits. We release an up-to-date GitHub repository that includes keypapers and resources.
  </details>

- **[Knowledge Bridger: Towards Training-free Missing Multi-modality Completion](http://arxiv.org/abs/2502.19834v4)**  `arXiv:2502.19834`  
  _Guanzhou Ke, Shengfeng He, Xiao Li Wang, Bo Wang, Guoqing Chao, Yuanyang Zhang, et al._
  <details><summary>Abstract</summary>
  Previous successful approaches to missing modality completion rely oncarefully designed fusion techniques and extensive pre-training on completedata, which can limit their generalizability in out-of-domain (OOD) scenarios.In this study, we pose a new challenge: can we develop a missing modalitycompletion model that is both resource-efficient and robust to OODgeneralization? To address this, we present a training-free framework formissing modality completion that leverages large multimodal models (LMMs). Ourapproach, termed the "Knowledge Bridger", is modality-agnostic and integratesgeneration and ranking of missing modalities. By defining domain-specificpriors, our method automatically extracts structured information from availablemodalities to construct knowledge graphs. These extracted graphs connect themissing modality generation and ranking modules through the LMM, resulting inhigh-quality imputations of missing modalities. Experimental results acrossboth general and medical domains show that our approach consistentlyoutperforms competing methods, including in OOD generalization. Additionally,our knowledge-driven generation and ranking techniques demonstrate superiorityover variants that directly employ LMMs for generation and ranking, offeringinsights that may be valuable for applications in other domains.
  </details>

- **[DeltaProduct: Improving State-Tracking in Linear RNNs via Householder Products](http://arxiv.org/abs/2502.10297v3)**  `arXiv:2502.10297`  
  _Julien Siems, Timur Carstensen, Arber Zela, Frank Hutter, Massimiliano Pontil, Riccardo Grazzi_
  <details><summary>Abstract</summary>
  Linear Recurrent Neural Networks (linear RNNs) have emerged as competitivealternatives to Transformers for sequence modeling, offering efficient trainingand linear-time inference. However, existing architectures face a fundamentaltrade-off between expressivity and efficiency, dictated by the structure oftheir state-transition matrices. While diagonal matrices used in architectureslike Mamba, GLA, or mLSTM yield fast runtime, they suffer from severely limitedexpressivity. To address this, recent architectures such as (Gated) DeltaNetand RWKV-7 adopted a diagonal plus rank-1 structure, allowing simultaneoustoken-channel mixing, which overcomes some expressivity limitations with only aslight decrease in training efficiency. Building on the interpretation ofDeltaNet's recurrence as performing one step of online gradient descent pertoken on an associative recall loss, we introduce DeltaProduct, which insteadtakes multiple ($n_h$) steps per token. This naturally leads to diagonal plusrank-$n_h$ state-transition matrices, formed as products of $n_h$ generalizedHouseholder transformations, providing a tunable mechanism to balanceexpressivity and efficiency and a stable recurrence. Through extensiveexperiments, we demonstrate that DeltaProduct achieves superior state-trackingand language modeling capabilities while exhibiting significantly improvedlength extrapolation compared to DeltaNet. Additionally, we also strengthen thetheoretical foundation of DeltaNet by proving that it can solve dihedral groupword problems in just two layers.
  </details>

- **[Multimodal Learning with Uncertainty Quantification based on Discounted Belief Fusion](http://arxiv.org/abs/2412.18024v2)**  `arXiv:2412.18024`  
  _Grigor Bezirganyan, Sana Sellami, Laure Berti-√âquille, S√©bastien Fournier_
  <details><summary>Abstract</summary>
  Multimodal AI models are increasingly used in fields like healthcare,finance, and autonomous driving, where information is drawn from multiplesources or modalities such as images, texts, audios, videos. However,effectively managing uncertainty - arising from noise, insufficient evidence,or conflicts between modalities - is crucial for reliable decision-making.Current uncertainty-aware machine learning methods leveraging, for example,evidence averaging, or evidence accumulation underestimate uncertainties inhigh-conflict scenarios. Moreover, the state-of-the-art evidence averagingstrategy is not order invariant and fails to scale to multiple modalities. Toaddress these challenges, we propose a novel multimodal learning method withorder-invariant evidence fusion and introduce a conflict-based discountingmechanism that reallocates uncertain mass when unreliable modalities aredetected. We provide both theoretical analysis and experimental validation,demonstrating that unlike the previous work, the proposed approach effectivelydistinguishes between conflicting and non-conflicting samples based on theprovided uncertainty estimates, and outperforms the previous models inuncertainty-based conflict detection.
  </details>

- **[RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation for Boosting 2-bit Large Language Model Accuracy](http://arxiv.org/abs/2412.01129v3)**  `arXiv:2412.01129`  
  _Geonho Lee, Janghwan Lee, Sukjin Hong, Minsoo Kim, Euijai Ahn, Du-Seong Chang, et al._
  <details><summary>Abstract</summary>
  Low-rank adaptation (LoRA) has become the dominant method forparameter-efficient LLM fine-tuning, with LoRA-based quantization errorcompensation (LQEC) emerging as a powerful tool for recovering accuracy incompressed LLMs. However, LQEC has underperformed in sub-4-bit scenarios, withno prior investigation into understanding this limitation. We propose RILQ(Rank-Insensitive LoRA-based Quantization Error Compensation) to understandfundamental limitation and boost 2-bit LLM accuracy. Based on rank analysisrevealing model-wise activation discrepancy loss's rank-insensitive nature,RILQ employs this loss to adjust adapters cooperatively across layers, enablingrobust error compensation with low-rank adapters. Evaluations on LLaMA-2 andLLaMA-3 demonstrate RILQ's consistent improvements in 2-bit quantized inferenceacross various state-of-the-art quantizers and enhanced accuracy intask-specific fine-tuning. RILQ maintains computational efficiency comparableto existing LoRA methods, enabling adapter-merged weight-quantized LLMinference with significantly enhanced accuracy, making it a promising approachfor boosting 2-bit LLM performance. Our code is available athttps://github.com/aiha-lab/RILQ.
  </details>

- **[Dist Loss: Enhancing Regression in Few-Shot Region through Distribution Distance Constraint](http://arxiv.org/abs/2411.15216v3)**  `arXiv:2411.15216`  
  _Guangkun Nie, Gongzheng Tang, Shenda Hong_
  <details><summary>Abstract</summary>
  Imbalanced data distributions are prevalent in real-world scenarios, posingsignificant challenges in both imbalanced classification and imbalancedregression tasks. They often cause deep learning models to overfit in areas ofhigh sample density (many-shot regions) while underperforming in areas of lowsample density (few-shot regions). This characteristic restricts the utility ofdeep learning models in various sectors, notably healthcare, where areas withfew-shot data hold greater clinical relevance. While recent studies have shownthe benefits of incorporating distribution information in imbalancedclassification tasks, such strategies are rarely explored in imbalancedregression. In this paper, we address this issue by introducing a novel lossfunction, termed Dist Loss, designed to minimize the distribution distancebetween the model's predictions and the target labels in a differentiablemanner, effectively integrating distribution information into model training.Dist Loss enables deep learning models to regularize their output distributionduring training, effectively enhancing their focus on few-shot regions. We haveconducted extensive experiments across three datasets spanning computer visionand healthcare: IMDB-WIKI-DIR, AgeDB-DIR, and ECG-Ka-DIR. The resultsdemonstrate that Dist Loss effectively mitigates the negative impact ofimbalanced data distribution on model performance, achieving state-of-the-artresults in sparse data regions. Furthermore, Dist Loss is easy to integrate,complementing existing methods.
  </details>

- **[Neuromorphic Wireless Split Computing with Multi-Level Spikes](http://arxiv.org/abs/2411.04728v3)**  `arXiv:2411.04728`  
  _Dengyu Wu, Jiechen Chen, Bipin Rajendran, H. Vincent Poor, Osvaldo Simeone_
  <details><summary>Abstract</summary>
  Inspired by biological processes, neuromorphic computing leverages spikingneural networks (SNNs) to perform inference tasks, offering significantefficiency gains for workloads involving sequential data. Recent advances inhardware and software have shown that embedding a small payload within eachspike exchanged between spiking neurons can enhance inference accuracy withoutincreasing energy consumption. To scale neuromorphic computing to largerworkloads, split computing - where an SNN is partitioned across two devices -is a promising solution. In such architectures, the device hosting the initiallayers must transmit information about the spikes generated by its outputneurons to the second device. This establishes a trade-off between the benefitsof multi-level spikes, which carry additional payload information, and thecommunication resources required for transmitting extra bits between devices.This paper presents the first comprehensive study of a neuromorphic wirelesssplit computing architecture that employs multi-level SNNs. We propose digitaland analog modulation schemes for an orthogonal frequency division multiplexing(OFDM) radio interface to enable efficient communication. Simulation andexperimental results using software-defined radios reveal performanceimprovements achieved by multi-level SNN models and provide insights into theoptimal payload size as a function of the connection quality between thetransmitter and receiver.
  </details>

- **[A Survey of Deep Graph Learning under Distribution Shifts: from Graph Out-of-Distribution Generalization to Adaptation](http://arxiv.org/abs/2410.19265v2)**  `arXiv:2410.19265`  
  _Kexin Zhang, Shuhan Liu, Song Wang, Weili Shi, Chen Chen, Pan Li, et al._
  <details><summary>Abstract</summary>
  Distribution shifts on graphs -- the discrepancies in data distributionbetween training and employing a graph machine learning model -- are ubiquitousand often unavoidable in real-world scenarios. These shifts may severelydeteriorate model performance, posing significant challenges for reliable graphmachine learning. Consequently, there has been a surge in research on graphmachine learning under distribution shifts, aiming to train models to achievesatisfactory performance on out-of-distribution (OOD) test data. In our survey,we provide an up-to-date and forward-looking review of deep graph learningunder distribution shifts. Specifically, we cover three primary scenarios:graph OOD generalization, training-time graph OOD adaptation, and test-timegraph OOD adaptation. We begin by formally formulating the problems anddiscussing various types of distribution shifts that can affect graph learning,such as covariate shifts and concept shifts. To provide a better understandingof the literature, we introduce a systematic taxonomy that classifies existingmethods into model-centric and data-centric approaches, investigating thetechniques used in each category. We also summarize commonly used datasets inthis research area to facilitate further investigation. Finally, we point outpromising research directions and the corresponding challenges to encouragefurther study in this vital domain. We also provide a continuously updatedreading list at https://github.com/kaize0409/Awesome-Graph-OOD.
  </details>

- **[AnyAttack: Towards Large-scale Self-supervised Adversarial Attacks on Vision-language Models](http://arxiv.org/abs/2410.05346v3)**  `arXiv:2410.05346`  
  _Jiaming Zhang, Junhong Ye, Xingjun Ma, Yige Li, Yunfan Yang, Yunhao Chen, et al._
  <details><summary>Abstract</summary>
  Due to their multimodal capabilities, Vision-Language Models (VLMs) havefound numerous impactful applications in real-world scenarios. However, recentstudies have revealed that VLMs are vulnerable to image-based adversarialattacks. Traditional targeted adversarial attacks require specific targets andlabels, limiting their real-world impact.We present AnyAttack, aself-supervised framework that transcends the limitations of conventionalattacks through a novel foundation model approach. By pre-training on themassive LAION-400M dataset without label supervision, AnyAttack achievesunprecedented flexibility - enabling any image to be transformed into an attackvector targeting any desired output across different VLMs.This approachfundamentally changes the threat landscape, making adversarial capabilitiesaccessible at an unprecedented scale. Our extensive validation across fiveopen-source VLMs (CLIP, BLIP, BLIP2, InstructBLIP, and MiniGPT-4) demonstratesAnyAttack's effectiveness across diverse multimodal tasks. Most concerning,AnyAttack seamlessly transfers to commercial systems including Google Gemini,Claude Sonnet, Microsoft Copilot and OpenAI GPT, revealing a systemicvulnerability requiring immediate attention.
  </details>

- **[Tackling the Accuracy-Interpretability Trade-off in a Hierarchy of Machine Learning Models for the Prediction of Extreme Heatwaves](http://arxiv.org/abs/2410.00984v2)**  `arXiv:2410.00984`  
  _Alessandro Lovo, Amaury Lancelin, Corentin Herbert, Freddy Bouchet_
  <details><summary>Abstract</summary>
  When performing predictions that use Machine Learning (ML), we are mainlyinterested in performance and interpretability. This generates a naturaltrade-off, where complex models generally have higher skills but are harder toexplain and thus trust. Interpretability is particularly important in theclimate community, where we aim at gaining a physical understanding of theunderlying phenomena. Even more so when the prediction concerns extreme weatherevents with high impact on society. In this paper, we perform probabilisticforecasts of extreme heatwaves over France, using a hierarchy of increasinglycomplex ML models, which allows us to find the best compromise between accuracyand interpretability. More precisely, we use models that range from a globalGaussian Approximation (GA) to deep Convolutional Neural Networks (CNNs), withthe intermediate steps of a simple Intrinsically Interpretable Neural Network(IINN) and a model using the Scattering Transform (ScatNet). Our findingsreveal that CNNs provide higher accuracy, but their black-box nature severelylimits interpretability, even when using state-of-the-art ExplainableArtificial Intelligence (XAI) tools. In contrast, ScatNet achieves similarperformance to CNNs while providing greater transparency, identifying keyscales and patterns in the data that drive predictions. This study underscoresthe potential of interpretability in ML models for climate science,demonstrating that simpler models can rival the performance of their morecomplex counterparts, all the while being much easier to understand. Thisgained interpretability is crucial for building trust in model predictions anduncovering new scientific insights, ultimately advancing our understanding andmanagement of extreme weather events.
  </details>

- **[Risk-based Calibration for Generative Classifiers](http://arxiv.org/abs/2409.03542v2)**  `arXiv:2409.03542`  
  _Aritz P√©rez, Carlos Echegoyen, Guzm√°n Santaf√©_
  <details><summary>Abstract</summary>
  Generative classifiers are constructed on the basis of a joint probabilitydistribution and are typically learned using closed-form procedures that relyon data statistics and maximize scores related to data fitting. However, thesescores are not directly linked to supervised classification metrics such as theerror, i.e., the expected 0-1 loss. To address this limitation, we propose alearning procedure called risk-based calibration (RC) that iteratively refinesthe generative classifier by adjusting its joint probability distributionaccording to the 0-1 loss in training samples. This is achieved by reinforcingdata statistics associated with the true classes while weakening those ofincorrect classes. As a result, the classifier progressively assigns higherprobability to the correct labels, improving its training error. Results on 20heterogeneous datasets using both na\"ive Bayes and quadratic discriminantanalysis show that RC significantly outperforms closed-form learning proceduresin terms of both training error and generalization error. In this way, RCbridges the gap between traditional generative approaches and learningprocedures guided by performance measures, ensuring a closer alignment withsupervised classification objectives.
  </details>

- **[DRExplainer: Quantifiable Interpretability in Drug Response Prediction with Directed Graph Convolutional Network](http://arxiv.org/abs/2408.12139v2)**  `arXiv:2408.12139`  
  _Haoyuan Shi, Tao Xu, Xiaodi Li, Qian Gao, Zhiwei Xiong, Junfeng Xia, et al._
  <details><summary>Abstract</summary>
  Predicting the response of a cancer cell line to a therapeutic drug ispivotal for personalized medicine. Despite numerous deep learning methods thathave been developed for drug response prediction, integrating diverseinformation about biological entities and predicting the directional responseremain major challenges. Here, we propose a novel interpretable predictivemodel, DRExplainer, which leverages a directed graph convolutional network toenhance the prediction in a directed bipartite network framework. DRExplainerconstructs a directed bipartite network integrating multi-omics profiles ofcell lines, the chemical structure of drugs and known drug response to achievedirected prediction. Then, DRExplainer identifies the most relevant subgraph toeach prediction in this directed bipartite network by learning a mask,facilitating critical medical decision-making. Additionally, we introduce aquantifiable method for model interpretability that leverages a ground truthbenchmark dataset curated from biological features. In computationalexperiments, DRExplainer outperforms state-of-the-art predictive methods andanother graph-based explanation method under the same experimental setting.Finally, the case studies further validate the interpretability and theeffectiveness of DRExplainer in predictive novel drug response. Our code isavailable at: https://github.com/vshy-dream/DRExplainer.
  </details>

- **[Improving probabilistic forecasts of extreme wind speeds by training statistical post-processing models with weighted scoring rules](http://arxiv.org/abs/2407.15900v4)**  `arXiv:2407.15900`  
  _Jakob Benjamin Wessel, Christopher A. T. Ferro, Gavin R. Evans, Frank Kwasniok_
  <details><summary>Abstract</summary>
  Accurate forecasts of extreme wind speeds are of high importance for manyapplications. Such forecasts are usually generated by ensembles of numericalweather prediction (NWP) models, which however can be biased and have errors indispersion, thus necessitating the application of statistical post-processingtechniques. In this work we aim to improve statistical post-processing modelsfor probabilistic predictions of extreme wind speeds. We do this by adjustingthe training procedure used to fit ensemble model output statistics (EMOS)models - a commonly applied post-processing technique - and propose estimatingparameters using the so-called threshold-weighted continuous ranked probabilityscore (twCRPS), a proper scoring rule that places special emphasis onpredictions over a threshold. We show that training using the twCRPS leads toimproved extreme event performance of post-processing models for a variety ofthresholds. We find a distribution body-tail trade-off where improvedperformance for probabilistic predictions of extreme events comes with worseperformance for predictions of the distribution body. However, we introducestrategies to mitigate this trade-off based on weighted training and linearpooling. Finally, we consider some synthetic experiments to explain thetraining impact of the twCRPS and derive closed-form expressions of the twCRPSfor a number of distributions, giving the first such collection in theliterature. The results will enable researchers and practitioners alike toimprove the performance of probabilistic forecasting models for extremes andother events of interest.
  </details>

- **[Population Transformer: Learning Population-level Representations of Neural Activity](http://arxiv.org/abs/2406.03044v4)**  `arXiv:2406.03044`  
  _Geeling Chau, Christopher Wang, Sabera Talukder, Vighnesh Subramaniam, Saraswati Soedarmadji, Yisong Yue, et al._
  <details><summary>Abstract</summary>
  We present a self-supervised framework that learns population-level codes forarbitrary ensembles of neural recordings at scale. We address key challenges inscaling models with neural time-series data, namely, sparse and variableelectrode distribution across subjects and datasets. The Population Transformer(PopT) stacks on top of pretrained temporal embeddings and enhances downstreamdecoding by enabling learned aggregation of multiple spatially-sparse datachannels. The pretrained PopT lowers the amount of data required for downstreamdecoding experiments, while increasing accuracy, even on held-out subjects andtasks. Compared to end-to-end methods, this approach is computationallylightweight, while achieving similar or better decoding performance. We furthershow how our framework is generalizable to multiple time-series embeddings andneural data modalities. Beyond decoding, we interpret the pretrained andfine-tuned PopT models to show how they can be used to extract neuroscienceinsights from large amounts of data. We release our code as well as apretrained PopT to enable off-the-shelf improvements in multi-channelintracranial data decoding and interpretability. Code is available athttps://github.com/czlwang/PopulationTransformer.
  </details>

- **[FTS: A Framework to Find a Faithful TimeSieve](http://arxiv.org/abs/2405.19647v3)**  `arXiv:2405.19647`  
  _Songning Lai, Ninghui Feng, Haochen Sui, Ze Ma, Hao Wang, Zichen Song, et al._
  <details><summary>Abstract</summary>
  The field of time series forecasting has garnered significant attention inrecent years, prompting the development of advanced models like TimeSieve,which demonstrates impressive performance. However, an analysis reveals certainunfaithfulness issues, including high sensitivity to random seeds and minuteinput noise perturbations. Recognizing these challenges, we embark on a questto define the concept of \textbf{\underline{F}aithful\underline{T}ime\underline{S}ieve \underline{(FTS)}}, a model that consistentlydelivers reliable and robust predictions. To address these issues, we propose anovel framework aimed at identifying and rectifying unfaithfulness inTimeSieve. Our framework is designed to enhance the model's stability andresilience, ensuring that its outputs are less susceptible to theaforementioned factors. Experimentation validates the effectiveness of ourproposed framework, demonstrating improved faithfulness in the model'sbehavior. Looking forward, we plan to expand our experimental scope to furthervalidate and optimize our algorithm, ensuring comprehensive faithfulness acrossa wide range of scenarios. Ultimately, we aspire to make this framework can beapplied to enhance the faithfulness of not just TimeSieve but also otherstate-of-the-art temporal methods, thereby contributing to the reliability androbustness of temporal modeling as a whole.
  </details>

- **[Unified ODE Analysis of Smooth Q-Learning Algorithms](http://arxiv.org/abs/2404.14442v3)**  `arXiv:2404.14442`  
  _Donghwan Lee_
  <details><summary>Abstract</summary>
  Convergence of Q-learning has been the focus of extensive research over thepast several decades. Recently, an asymptotic convergence analysis forQ-learning was introduced using a switching system framework. This approachapplies the so-called ordinary differential equation (ODE) approach to provethe convergence of the asynchronous Q-learning modeled as a continuous-timeswitching system, where notions from switching system theory are used to proveits asymptotic stability without using explicit Lyapunov arguments. However, toprove stability, restrictive conditions, such as quasi-monotonicity, must besatisfied for the underlying switching systems, which makes it hard to easilygeneralize the analysis method to other reinforcement learning algorithms, suchas the smooth Q-learning variants. In this paper, we present a more general andunified convergence analysis that improves upon the switching system approachand can analyze Q-learning and its smooth variants. The proposed analysis ismotivated by previous work on the convergence of synchronous Q-learning basedon $p$-norm serving as a Lyapunov function. However, the proposed analysisaddresses more general ODE models that can cover both asynchronous Q-learningand its smooth versions with simpler frameworks.
  </details>

- **[Leveraging Expert Input for Robust and Explainable AI-Assisted Lung Cancer Detection in Chest X-rays](http://arxiv.org/abs/2403.19444v2)**  `arXiv:2403.19444`  
  _Amy Rafferty, Rishi Ramaesh, Ajitha Rajan_
  <details><summary>Abstract</summary>
  Deep learning models show significant potential for advancing AI-assistedmedical diagnostics, particularly in detecting lung cancer through medicalimage modalities such as chest X-rays. However, the black-box nature of thesemodels poses challenges to their interpretability and trustworthiness, limitingtheir adoption in clinical practice. This study examines both theinterpretability and robustness of a high-performing lung cancer detectionmodel based on InceptionV3, utilizing a public dataset of chest X-rays andradiological reports. We evaluate the clinical utility of multiple explainableAI (XAI) techniques, including both post-hoc and ante-hoc approaches, and findthat existing methods often fail to provide clinically relevant explanations,displaying inconsistencies and divergence from expert radiologist assessments.To address these limitations, we collaborated with a radiologist to definediagnosis-specific clinical concepts and developed ClinicXAI, an expert-drivenapproach leveraging the concept bottleneck methodology. ClinicXAI generatedclinically meaningful explanations which closely aligned with the practicalrequirements of clinicians while maintaining high diagnostic accuracy. We alsoassess the robustness of ClinicXAI in comparison to the original InceptionV3model by subjecting both to a series of widely utilized adversarial attacks.Our analysis demonstrates that ClinicXAI exhibits significantly greaterresilience to adversarial perturbations. These findings underscore theimportance of incorporating domain expertise into the design of interpretableand robust AI systems for medical diagnostics, paving the way for moretrustworthy and effective AI solutions in healthcare.
  </details>

- **[ERSAM: Neural Architecture Search For Energy-Efficient and Real-Time Social Ambiance Measurement](http://arxiv.org/abs/2303.10727v3)**  `arXiv:2303.10727`  
  _Chaojian Li, Wenwan Chen, Jiayi Yuan, Yingyan Celine Lin, Ashutosh Sabharwal_
  <details><summary>Abstract</summary>
  Social ambiance describes the context in which social interactions happen,and can be measured using speech audio by counting the number of concurrentspeakers. This measurement has enabled various mental health tracking andhuman-centric IoT applications. While on-device Socal Ambiance Measure (SAM) ishighly desirable to ensure user privacy and thus facilitate wide adoption ofthe aforementioned applications, the required computational complexity ofstate-of-the-art deep neural networks (DNNs) powered SAM solutions stands atodds with the often constrained resources on mobile devices. Furthermore, onlylimited labeled data is available or practical when it comes to SAM underclinical settings due to various privacy constraints and the required humaneffort, further challenging the achievable accuracy of on-device SAM solutions.To this end, we propose a dedicated neural architecture search framework forEnergy-efficient and Real-time SAM (ERSAM). Specifically, our ERSAM frameworkcan automatically search for DNNs that push forward the achievable accuracy vs.hardware efficiency frontier of mobile SAM solutions. For example,ERSAM-delivered DNNs only consume 40 mW x 12 h energy and 0.05 secondsprocessing latency for a 5 seconds audio segment on a Pixel 3 phone, while onlyachieving an error rate of 14.3% on a social ambiance dataset generated byLibriSpeech. We can expect that our ERSAM framework can pave the way forubiquitous on-device SAM solutions which are in growing demand.
  </details>

- **[Tightening Robustness Verification of MaxPool-based Neural Networks via Minimizing the Over-Approximation Zone](http://arxiv.org/abs/2211.09810v2)**  `arXiv:2211.09810`  
  _Yuan Xiao, Yuchen Chen, Shiqing Ma, Chunrong Fang, Tongtong Bai, Mingzheng Gu, et al._
  <details><summary>Abstract</summary>
  The robustness of neural network classifiers is important in thesafety-critical domain and can be quantified by robustness verification. Atpresent, efficient and scalable verification techniques are always sound butincomplete, and thus, the improvement of verified robustness results is the keycriterion to evaluate the performance of incomplete verification approaches.The multi-variate function MaxPool is widely adopted yet challenging to verify.In this paper, we present Ti-Lin, a robustness verifier for MaxPool-based CNNswith Tight Linear Approximation. Following the sequel of minimizing theover-approximation zone of the non-linear function of CNNs, we are the first topropose the provably neuron-wise tightest linear bounds for the MaxPoolfunction. By our proposed linear bounds, we can certify larger robustnessresults for CNNs. We evaluate the effectiveness of Ti-Lin on differentverification frameworks with open-sourced benchmarks, including LeNet,PointNet, and networks trained on the MNIST, CIFAR-10, Tiny ImageNet andModelNet40 datasets. Experimental results show that Ti-Lin significantlyoutperforms the state-of-the-art methods across all networks with up to 78.6%improvement in terms of the certified accuracy with almost the same timeconsumption as the fastest tool. Our code is available athttps://github.com/xiaoyuanpigo/Ti-Lin-Hybrid-Lin.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Multiagent Systems üåê

<details open><summary>Click to Collapse</summary>

- **[Debate-Driven Multi-Agent LLMs for Phishing Email Detection](http://arxiv.org/abs/2503.22038v1)**  `arXiv:2503.22038`  
  _Ngoc Tuong Vy Nguyen, Felix D Childress, Yunting Yin_
  <details><summary>Abstract</summary>
  Phishing attacks remain a critical cybersecurity threat. Attackers constantlyrefine their methods, making phishing emails harder to detect. Traditionaldetection methods, including rule-based systems and supervised machine learningmodels, either rely on predefined patterns like blacklists, which can bebypassed with slight modifications, or require large datasets for training andstill can generate false positives and false negatives. In this work, wepropose a multi-agent large language model (LLM) prompting technique thatsimulates debates among agents to detect whether the content presented on anemail is phishing. Our approach uses two LLM agents to present arguments for oragainst the classification task, with a judge agent adjudicating the finalverdict based on the quality of reasoning provided. This debate mechanismenables the models to critically analyze contextual cue and deceptive patternsin text, which leads to improved classification accuracy. The proposedframework is evaluated on multiple phishing email datasets and demonstrate thatmixed-agent configurations consistently outperform homogeneous configurations.Results also show that the debate structure itself is sufficient to yieldaccurate decisions without extra prompting strategies.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Robotics ü§ñ

<details open><summary>Click to Collapse</summary>

- **[Empirical Analysis of Sim-and-Real Cotraining Of Diffusion Policies For Planar Pushing from Pixels](http://arxiv.org/abs/2503.22634v1)**  `arXiv:2503.22634`  
  _Adam Wei, Abhinav Agarwal, Boyuan Chen, Rohan Bosworth, Nicholas Pfaff, Russ Tedrake_
  <details><summary>Abstract</summary>
  In imitation learning for robotics, cotraining with demonstration datagenerated both in simulation and on real hardware has emerged as a powerfulrecipe to overcome the sim2real gap. This work seeks to elucidate basicprinciples of this sim-and-real cotraining to help inform simulation design,sim-and-real dataset creation, and policy training. Focusing narrowly on thecanonical task of planar pushing from camera inputs enabled us to be thoroughin our study. These experiments confirm that cotraining with simulated data\emph{can} dramatically improve performance in real, especially when real datais limited. Performance gains scale with simulated data, but eventuallyplateau; real-world data increases this performance ceiling. The results alsosuggest that reducing the domain gap in physics may be more important thanvisual fidelity for non-prehensile manipulation tasks. Perhaps surprisingly,having some visual domain gap actually helps the cotrained policy -- binaryprobes reveal that high-performing policies learn to distinguish simulateddomains from real. We conclude by investigating this nuance and mechanisms thatfacilitate positive transfer between sim-and-real. In total, our experimentsspan over 40 real-world policies (evaluated on 800+ trials) and 200 simulatedpolicies (evaluated on 40,000+ trials).
  </details>

- **[Next-Best-Trajectory Planning of Robot Manipulators for Effective Observation and Exploration](http://arxiv.org/abs/2503.22588v1)**  `arXiv:2503.22588`  
  _Heiko Renz, Maximilian Kr√§mer, Frank Hoffmann, Torsten Bertram_
  <details><summary>Abstract</summary>
  Visual observation of objects is essential for many robotic applications,such as object reconstruction and manipulation, navigation, and sceneunderstanding. Machine learning algorithms constitute the state-of-the-art inmany fields but require vast data sets, which are costly and time-intensive tocollect. Automated strategies for observation and exploration are crucial toenhance the efficiency of data gathering. Therefore, a novel strategy utilizingthe Next-Best-Trajectory principle is developed for a robot manipulatoroperating in dynamic environments. Local trajectories are generated to maximizethe information gained from observations along the path while avoidingcollisions. We employ a voxel map for environment modeling and utilizeraycasting from perspectives around a point of interest to estimate theinformation gain. A global ergodic trajectory planner provides an optionalreference trajectory to the local planner, improving exploration and helping toavoid local minima. To enhance computational efficiency, raycasting forestimating the information gain in the environment is executed in parallel onthe graphics processing unit. Benchmark results confirm the efficiency of theparallelization, while real-world experiments demonstrate the strategy'seffectiveness.
  </details>

- **[Task Hierarchical Control via Null-Space Projection and Path Integral Approach](http://arxiv.org/abs/2503.22574v1)**  `arXiv:2503.22574`  
  _Apurva Patil, Riku Funada, Takashi Tanaka, Luis Sentis_
  <details><summary>Abstract</summary>
  This paper addresses the problem of hierarchical task control, where arobotic system must perform multiple subtasks with varying levels of priority.A commonly used approach for hierarchical control is the null-space projectiontechnique, which ensures that higher-priority tasks are executed withoutinterference from lower-priority ones. While effective, the state-of-the-artimplementations of this method rely on low-level controllers, such as PIDcontrollers, which can be prone to suboptimal solutions in complex tasks. Thispaper presents a novel framework for hierarchical task control, integrating thenull-space projection technique with the path integral control method. Ourapproach leverages Monte Carlo simulations for real-time computation of optimalcontrol inputs, allowing for the seamless integration of simpler PID-likecontrollers with a more sophisticated optimal control technique. Throughsimulation studies, we demonstrate the effectiveness of this combined approach,showing how it overcomes the limitations of traditional
  </details>

- **[SafeCast: Risk-Responsive Motion Forecasting for Autonomous Vehicles](http://arxiv.org/abs/2503.22541v1)**  `arXiv:2503.22541`  
  _Haicheng Liao, Hanlin Kong, Bin Rao, Bonan Wang, Chengyue Wang, Guyang Yu, et al._
  <details><summary>Abstract</summary>
  Accurate motion forecasting is essential for the safety and reliability ofautonomous driving (AD) systems. While existing methods have made significantprogress, they often overlook explicit safety constraints and struggle tocapture the complex interactions among traffic agents, environmental factors,and motion dynamics. To address these challenges, we present SafeCast, arisk-responsive motion forecasting model that integrates safety-awaredecision-making with uncertainty-aware adaptability. SafeCast is the first toincorporate the Responsibility-Sensitive Safety (RSS) framework into motionforecasting, encoding interpretable safety rules--such as safe distances andcollision avoidance--based on traffic norms and physical principles. To furtherenhance robustness, we introduce the Graph Uncertainty Feature (GUF), agraph-based module that injects learnable noise into Graph Attention Networks,capturing real-world uncertainties and enhancing generalization across diversescenarios. We evaluate SafeCast on four real-world benchmark datasets--NextGeneration Simulation (NGSIM), Highway Drone (HighD), ApolloScape, and theMacao Connected Autonomous Driving (MoCAD)--covering highway, urban, andmixed-autonomy traffic environments. Our model achieves state-of-the-art (SOTA)accuracy while maintaining a lightweight architecture and low inferencelatency, underscoring its potential for real-time deployment in safety-criticalAD systems.
  </details>

- **[Robust Offline Imitation Learning Through State-level Trajectory Stitching](http://arxiv.org/abs/2503.22524v1)**  `arXiv:2503.22524`  
  _Shuze Wang, Yunpeng Mei, Hongjie Cao, Yetian Yuan, Gang Wang, Jian Sun, et al._
  <details><summary>Abstract</summary>
  Imitation learning (IL) has proven effective for enabling robots to acquirevisuomotor skills through expert demonstrations. However, traditional ILmethods are limited by their reliance on high-quality, often scarce, expertdata, and suffer from covariate shift. To address these challenges, recentadvances in offline IL have incorporated suboptimal, unlabeled datasets intothe training. In this paper, we propose a novel approach to enhance policylearning from mixed-quality offline datasets by leveraging task-relevanttrajectory fragments and rich environmental dynamics. Specifically, weintroduce a state-based search framework that stitches state-action pairs fromimperfect demonstrations, generating more diverse and informative trainingtrajectories. Experimental results on standard IL benchmarks and real-worldrobotic tasks showcase that our proposed method significantly improves bothgeneralization and performance.
  </details>

- **[A Centralized Planning and Distributed Execution Method for Shape Filling with Homogeneous Mobile Robots](http://arxiv.org/abs/2503.22522v1)**  `arXiv:2503.22522`  
  _Shuqing Liu, Rong Su, Karl H. Johansson_
  <details><summary>Abstract</summary>
  Nature has inspired humans in different ways. The formation behavior ofanimals can perform tasks that exceed individual capability. For example, armyants could transverse gaps by forming bridges, and fishes could group up toprotect themselves from predators. The pattern formation task is essential in amultiagent robotic system because it usually serves as the initialconfiguration of downstream tasks, such as collective manipulation andadaptation to various environments. The formation of complex shapes, especiallyhollow shapes, remains an open question. Traditional approaches either requireglobal coordinates for each robot or are prone to failure when attempting toclose the hole due to accumulated localization errors. Inspired by the ribbonidea introduced in the additive self-assembly algorithm by the Kilobot team, wedevelop a two-stage algorithm that does not require global coordinatesinformation and effectively forms shapes with holes. In this paper, weinvestigate the partitioning of the shape using ribbons in a hexagonal latticesetting and propose the add-subtract algorithm based on the movement sequenceinduced by the ribbon structure. This advancement opens the door to tasksrequiring complex pattern formations, such as the assembly of nanobots formedical applications involving intricate structures and the deployment ofrobots along the boundaries of areas of interest. We also provide simulationresults on complex shapes, an analysis of the robustness as well as a proof ofcorrectness of the proposed algorithm.
  </details>

- **[Scenario Dreamer: Vectorized Latent Diffusion for Generating Driving Simulation Environments](http://arxiv.org/abs/2503.22496v1)**  `arXiv:2503.22496`  
  _Luke Rowe, Roger Girgis, Anthony Gosselin, Liam Paull, Christopher Pal, Felix Heide_
  <details><summary>Abstract</summary>
  We introduce Scenario Dreamer, a fully data-driven generative simulator forautonomous vehicle planning that generates both the initial traffic scene -comprising a lane graph and agent bounding boxes - and closed-loop agentbehaviours. Existing methods for generating driving simulation environmentsencode the initial traffic scene as a rasterized image and, as such, requireparameter-heavy networks that perform unnecessary computation due to many emptypixels in the rasterized scene. Moreover, we find that existing methods thatemploy rule-based agent behaviours lack diversity and realism. Scenario Dreamerinstead employs a novel vectorized latent diffusion model for initial scenegeneration that directly operates on the vectorized scene elements and anautoregressive Transformer for data-driven agent behaviour simulation. ScenarioDreamer additionally supports scene extrapolation via diffusion inpainting,enabling the generation of unbounded simulation environments. Extensiveexperiments show that Scenario Dreamer outperforms existing generativesimulators in realism and efficiency: the vectorized scene-generation basemodel achieves superior generation quality with around 2x fewer parameters, 6xlower generation latency, and 10x fewer GPU training hours compared to thestrongest baseline. We confirm its practical utility by showing thatreinforcement learning planning agents are more challenged in Scenario Dreamerenvironments than traditional non-generative simulation environments,especially on long and adversarial driving environments.
  </details>

- **[Control of Humanoid Robots with Parallel Mechanisms using Kinematic Actuation Models](http://arxiv.org/abs/2503.22459v1)**  `arXiv:2503.22459`  
  _Victor Lutz, Ludovic de Matte√Øs, Virgile Batto, Nicolas Mansard_
  <details><summary>Abstract</summary>
  Inspired by the mechanical design of Cassie, several recently releasedhumanoid robots are using actuator configuration in which the motor isdisplaced from the joint location to optimize the leg inertia. This in turninduces a non linearity in the reduction ratio of the transmission which isoften neglected when computing the robot motion (e.g. by trajectoryoptimization or reinforcement learning) and only accounted for at control time.This paper proposes an analytical method to efficiently handle thisnon-linearity. Using this actuation model, we demonstrate that we can leveragethe dynamic abilities of the non-linear transmission while only modeling theinertia of the main serial chain of the leg, without approximating the motorcapabilities nor the joint range. Based on analytical inverse kinematics, ourmethod does not need any numerical routines dedicated to the closed-kinematicsactuation, hence leading to very efficient computations. Our study focuses ontwo mechanisms widely used in recent humanoid robots; the four bar knee linkageas well as a parallel 2 DoF ankle mechanism. We integrate these models insideoptimization based (DDP) and learning (PPO) control approaches. A comparison ofour model against a simplified model that completely neglects closed chains isthen shown in simulation.
  </details>

- **[Collapse and Collision Aware Grasping for Cluttered Shelf Picking](http://arxiv.org/abs/2503.22427v1)**  `arXiv:2503.22427`  
  _Abhinav Pathak, Rajkumar Muthusamy_
  <details><summary>Abstract</summary>
  Efficient and safe retrieval of stacked objects in warehouse environments isa significant challenge due to complex spatial dependencies and structuralinter-dependencies. Traditional vision-based methods excel at objectlocalization but often lack the physical reasoning required to predict theconsequences of extraction, leading to unintended collisions and collapses.This paper proposes a collapse and collision aware grasp planner thatintegrates dynamic physics simulations for robotic decision-making. Using asingle image and depth map, an approximate 3D representation of the scene isreconstructed in a simulation environment, enabling the robot to evaluatedifferent retrieval strategies before execution. Two approaches 1)heuristic-based and 2) physics-based are proposed for both single-boxextraction and shelf clearance tasks. Extensive real-world experiments onstructured and unstructured box stacks, along with validation using datasetsfrom existing databases, show that our physics-aware method significantlyimproves efficiency and success rates compared to baseline heuristics.
  </details>

- **[Grasping a Handful: Sequential Multi-Object Dexterous Grasp Generation](http://arxiv.org/abs/2503.22370v1)**  `arXiv:2503.22370`  
  _Haofei Lu, Yifei Dong, Zehang Weng, Jens Lundell, Danica Kragic_
  <details><summary>Abstract</summary>
  We introduce the sequential multi-object robotic grasp sampling algorithmSeqGrasp that can robustly synthesize stable grasps on diverse objects usingthe robotic hand's partial Degrees of Freedom (DoF). We use SeqGrasp toconstruct the large-scale Allegro Hand sequential grasping dataset SeqDatasetand use it for training the diffusion-based sequential grasp generatorSeqDiffuser. We experimentally evaluate SeqGrasp and SeqDiffuser against thestate-of-the-art non-sequential multi-object grasp generation method MultiGraspin simulation and on a real robot. The experimental results demonstrate thatSeqGrasp and SeqDiffuser reach an 8.71%-43.33% higher grasp success rate thanMultiGrasp. Furthermore, SeqDiffuser is approximately 1000 times faster atgenerating grasps than SeqGrasp and MultiGrasp.
  </details>

- **[Robust simultaneous UWB-anchor calibration and robot localization for emergency situations](http://arxiv.org/abs/2503.22272v1)**  `arXiv:2503.22272`  
  _Xinghua Liu, Ming Cao_
  <details><summary>Abstract</summary>
  In this work, we propose a factor graph optimization (FGO) framework tosimultaneously solve the calibration problem for Ultra-WideBand (UWB) anchorsand the robot localization problem. Calibrating UWB anchors manually can betime-consuming and even impossible in emergencies or those situations withoutspecial calibration tools. Therefore, automatic estimation of the anchorpositions becomes a necessity. The proposed method enables the creation of asoft sensor providing the position information of the anchors in a UWB network.This soft sensor requires only UWB and LiDAR measurements measured from amoving robot. The proposed FGO framework is suitable for the calibration of anextendable large UWB network. Moreover, the anchor calibration problem androbot localization problem can be solved simultaneously, which saves time forUWB network deployment. The proposed framework also helps to avoid artificialerrors in the UWB-anchor position estimation and improves the accuracy androbustness of the robot-pose. The experimental results of the robotlocalization using LiDAR and a UWB network in a 3D environment are discussed,demonstrating the performance of the proposed method. More specifically, theanchor calibration problem with four anchors and the robot localization problemcan be solved simultaneously and automatically within 30 seconds by theproposed framework. The supplementary video and codes can be accessed viahttps://github.com/LiuxhRobotAI/Simultaneous_calibration_localization.
  </details>

- **[FLAM: Foundation Model-Based Body Stabilization for Humanoid Locomotion and Manipulation](http://arxiv.org/abs/2503.22249v1)**  `arXiv:2503.22249`  
  _Xianqi Zhang, Hongliang Wei, Wenrui Wang, Xingtao Wang, Xiaopeng Fan, Debin Zhao_
  <details><summary>Abstract</summary>
  Humanoid robots have attracted significant attention in recent years.Reinforcement Learning (RL) is one of the main ways to control the whole bodyof humanoid robots. RL enables agents to complete tasks by learning fromenvironment interactions, guided by task rewards. However, existing RL methodsrarely explicitly consider the impact of body stability on humanoid locomotionand manipulation. Achieving high performance in whole-body control remains achallenge for RL methods that rely solely on task rewards. In this paper, wepropose a Foundation model-based method for humanoid Locomotion AndManipulation (FLAM for short). FLAM integrates a stabilizing reward functionwith a basic policy. The stabilizing reward function is designed to encouragethe robot to learn stable postures, thereby accelerating the learning processand facilitating task completion. Specifically, the robot pose is first mappedto the 3D virtual human model. Then, the human pose is stabilized andreconstructed through a human motion reconstruction model. Finally, the posebefore and after reconstruction is used to compute the stabilizing reward. Bycombining this stabilizing reward with the task reward, FLAM effectively guidespolicy learning. Experimental results on a humanoid robot benchmark demonstratethat FLAM outperforms state-of-the-art RL methods, highlighting itseffectiveness in improving stability and overall performance.
  </details>

- **[Bimanual Regrasp Planning and Control for Eliminating Object Pose Uncertainty](http://arxiv.org/abs/2503.22240v1)**  `arXiv:2503.22240`  
  _Ryuta Nagahama, Weiwei Wan, Zhengtao Hu, Kensuke Harada_
  <details><summary>Abstract</summary>
  Precisely grasping an object is a challenging task due to pose uncertainties.Conventional methods have used cameras and fixtures to reduce objectuncertainty. They are effective but require intensive preparation, such asdesigning jigs based on the object geometry and calibrating cameras withhigh-precision tools fabricated using lasers. In this study, we propose amethod to reduce the uncertainty of the position and orientation of a graspedobject without using a fixture or a camera. Our method is based on the conceptthat the flat finger pads of a parallel gripper can reduce uncertainty alongits opening/closing direction through flat surface contact. Three orthogonalgrasps by parallel grippers with flat finger pads collectively constrain anobject's position and orientation to a unique state. Guided by the concepts, wedevelop a regrasp planning and admittance control approach that sequentiallyfinds and leverages three orthogonal grasps of two robotic arms to eliminateuncertainties in the object pose. We evaluated the proposed method on differentinitial object uncertainties and verified that the method has satisfactoryrepeatability accuracy. It outperforms an AR marker detection methodimplemented using cameras and laser jet printers under standard laboratoryconditions.
  </details>

- **[IKSel: Selecting Good Seed Joint Values for Fast Numerical Inverse Kinematics Iterations](http://arxiv.org/abs/2503.22234v1)**  `arXiv:2503.22234`  
  _Xinyi Yuan, Weiwei Wan, Kensuke Harada_
  <details><summary>Abstract</summary>
  This paper revisits the numerical inverse kinematics (IK) problem, leveragingmodern computational resources and refining the seed selection process todevelop a solver that is competitive with analytical-based methods. Theproposed seed selection strategy consists of three key stages: (1) utilizing aK-Dimensional Tree (KDTree) to identify seed candidates based on workspaceproximity, (2) sorting candidates by joint space adjustment and attemptingnumerical iterations with the one requiring minimal adjustment, and (3)re-selecting the most distant joint configurations for new attempts in case offailures. The joint space adjustment-based seed selection increases thelikelihood of rapid convergence, while the re-attempt strategy effectivelyhelps circumvent local minima and joint limit constraints. Comparison resultswith both traditional numerical solvers and learning-based methods demonstratethe strengths of the proposed approach in terms of success rate, timeefficiency, and accuracy. Additionally, we conduct detailed ablation studies toanalyze the effects of various parameters and solver settings, providingpractical insights for customization and optimization. The proposed methodconsistently exhibits high success rates and computational efficiency. It issuitable for time-sensitive applications.
  </details>

- **[3D Acetabular Surface Reconstruction from 2D Pre-operative X-ray Images using SRVF Elastic Registration and Deformation Graph](http://arxiv.org/abs/2503.22177v1)**  `arXiv:2503.22177`  
  _Shuai Zhang, Jinliang Wang, Sujith Konandetails, Xu Wang, Danail Stoyanov, Evangelos B. Mazomenos_
  <details><summary>Abstract</summary>
  Accurate and reliable selection of the appropriate acetabular cup size iscrucial for restoring joint biomechanics in total hip arthroplasty (THA). Thispaper proposes a novel framework that integrates square-root velocity function(SRVF)-based elastic shape registration technique with an embedded deformation(ED) graph approach to reconstruct the 3D articular surface of the acetabulumby fusing multiple views of 2D pre-operative pelvic X-ray images and ahemispherical surface model. The SRVF-based elastic registration establishes2D-3D correspondences between the parametric hemispherical model and X-rayimages, and the ED framework incorporates the SRVF-derived correspondences asconstraints to optimize the 3D acetabular surface reconstruction usingnonlinear least-squares optimization. Validations using both simulation andreal patient datasets are performed to demonstrate the robustness and thepotential clinical value of the proposed algorithm. The reconstruction resultcan assist surgeons in selecting the correct acetabular cup on the firstattempt in primary THA, minimising the need for revision surgery.
  </details>

- **[Cooperative Hybrid Multi-Agent Pathfinding Based on Shared Exploration Maps](http://arxiv.org/abs/2503.22162v1)**  `arXiv:2503.22162`  
  _Ning Liu, Sen Shen, Xiangrui Kong, Hongtao Zhang, Thomas Br√§unl_
  <details><summary>Abstract</summary>
  Multi-Agent Pathfinding is used in areas including multi-robot formations,warehouse logistics, and intelligent vehicles. However, many environments areincomplete or frequently change, making it difficult for standard centralizedplanning or pure reinforcement learning to maintain both global solutionquality and local flexibility. This paper introduces a hybrid framework thatintegrates D* Lite global search with multi-agent reinforcement learning, usinga switching mechanism and a freeze-prevention strategy to handle dynamicconditions and crowded settings. We evaluate the framework in the discretePOGEMA environment and compare it with baseline methods. Experimental outcomesindicate that the proposed framework substantially improves success rate,collision rate, and path efficiency. The model is further tested on the EyeSimplatform, where it maintains feasible Pathfinding under frequent changes andlarge-scale robot deployments.
  </details>

- **[REMAC: Self-Reflective and Self-Evolving Multi-Agent Collaboration for Long-Horizon Robot Manipulation](http://arxiv.org/abs/2503.22122v1)**  `arXiv:2503.22122`  
  _Puzhen Yuan, Angyuan Ma, Yunchao Yao, Huaxiu Yao, Masayoshi Tomizuka, Mingyu Ding_
  <details><summary>Abstract</summary>
  Vision-language models (VLMs) have demonstrated remarkable capabilities inrobotic planning, particularly for long-horizon tasks that require a holisticunderstanding of the environment for task decomposition. Existing methodstypically rely on prior environmental knowledge or carefully designedtask-specific prompts, making them struggle with dynamic scene changes orunexpected task conditions, e.g., a robot attempting to put a carrot in themicrowave but finds the door was closed. Such challenges underscore twocritical issues: adaptability and efficiency. To address them, in this work, wepropose an adaptive multi-agent planning framework, termed REMAC, that enablesefficient, scene-agnostic multi-robot long-horizon task planning and executionthrough continuous reflection and self-evolution. REMAC incorporates two keymodules: a self-reflection module performing pre-condition and post-conditionchecks in the loop to evaluate progress and refine plans, and a self-evolvementmodule dynamically adapting plans based on scene-specific reasoning. It offersseveral appealing benefits: 1) Robots can initially explore and reason aboutthe environment without complex prompt design. 2) Robots can keep reflecting onpotential planning errors and adapting the plan based on task-specificinsights. 3) After iterations, a robot can call another one to coordinate tasksin parallel, maximizing the task execution efficiency. To validate REMAC'seffectiveness, we build a multi-agent environment for long-horizon robotmanipulation and navigation based on RoboCasa, featuring 4 task categories with27 task styles and 50+ different objects. Based on it, we further benchmarkstate-of-the-art reasoning models, including DeepSeek-R1, o3-mini, QwQ, andGrok3, demonstrating REMAC's superiority by boosting average success rates by40% and execution efficiency by 52.7% over the single robot baseline.
  </details>

- **[AcL: Action Learner for Fault-Tolerant Quadruped Locomotion Control](http://arxiv.org/abs/2503.21401v2)**  `arXiv:2503.21401`  
  _Tianyu Xu, Yaoyu Cheng, Pinxi Shen, Lin Zhao_
  <details><summary>Abstract</summary>
  Quadrupedal robots can learn versatile locomotion skills but remainvulnerable when one or more joints lose power. In contrast, dogs and cats canadopt limping gaits when injured, demonstrating their remarkable ability toadapt to physical conditions. Inspired by such adaptability, this paperpresents Action Learner (AcL), a novel teacher-student reinforcement learningframework that enables quadrupeds to autonomously adapt their gait for stablewalking under multiple joint faults. Unlike conventional teacher-studentapproaches that enforce strict imitation, AcL leverages teacher policies togenerate style rewards, guiding the student policy without requiring precisereplication. We train multiple teacher policies, each corresponding to adifferent fault condition, and subsequently distill them into a single studentpolicy with an encoder-decoder architecture. While prior works primarilyaddress single-joint faults, AcL enables quadrupeds to walk with up to fourfaulty joints across one or two legs, autonomously switching between differentlimping gaits when faults occur. We validate AcL on a real Go2 quadruped robotunder single- and double-joint faults, demonstrating fault-tolerant, stablewalking, smooth gait transitions between normal and lamb gaits, and robustnessagainst external disturbances.
  </details>

- **[Learning Multi-Robot Coordination through Locality-Based Factorized Multi-Agent Actor-Critic Algorithm](http://arxiv.org/abs/2503.18816v2)**  `arXiv:2503.18816`  
  _Chak Lam Shek, Amrit Singh Bedi, Anjon Basak, Ellen Novoseller, Nick Waytowich, Priya Narayanan, et al._
  <details><summary>Abstract</summary>
  In this work, we present a novel cooperative multi-agent reinforcementlearning method called \textbf{Loc}ality based \textbf{Fac}torized\textbf{M}ulti-Agent \textbf{A}ctor-\textbf{C}ritic (Loc-FACMAC). Existingstate-of-the-art algorithms, such as FACMAC, rely on global reward information,which may not accurately reflect the quality of individual robots' actions indecentralized systems. We integrate the concept of locality into criticlearning, where strongly related robots form partitions during training. Robotswithin the same partition have a greater impact on each other, leading to moreprecise policy evaluation. Additionally, we construct a dependency graph tocapture the relationships between robots, facilitating the partitioningprocess. This approach mitigates the curse of dimensionality and preventsrobots from using irrelevant information. Our method improves existingalgorithms by focusing on local rewards and leveraging partition-based learningto enhance training efficiency and performance. We evaluate the performance ofLoc-FACMAC in three environments: Hallway, Multi-cartpole, andBounded-Cooperative-Navigation. We explore the impact of partition sizes on theperformance and compare the result with baseline MARL algorithms such as LOMAQ,FACMAC, and QMIX. The experiments reveal that, if the locality structure isdefined properly, Loc-FACMAC outperforms these baseline algorithms up to 108\%,indicating that exploiting the locality structure in the actor-critic frameworkimproves the MARL performance.
  </details>

- **[LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning](http://arxiv.org/abs/2503.17125v5)**  `arXiv:2503.17125`  
  _Chan Kim, Seung-Woo Seo, Seong-Woo Kim_
  <details><summary>Abstract</summary>
  Deep Reinforcement Learning (DRL) has demonstrated strong performance inrobotic control but remains susceptible to out-of-distribution (OOD) states,often resulting in unreliable actions and task failure. While previous methodshave focused on minimizing or preventing OOD occurrences, they largely neglectrecovery once an agent encounters such states. Although the latest research hasattempted to address this by guiding agents back to in-distribution states,their reliance on uncertainty estimation hinders scalability in complexenvironments. To overcome this limitation, we introduce Language Models forOut-of-Distribution Recovery (LaMOuR), which enables recovery learning withoutrelying on uncertainty estimation. LaMOuR generates dense reward codes thatguide the agent back to a state where it can successfully perform its originaltask, leveraging the capabilities of LVLMs in image description, logicalreasoning, and code generation. Experimental results show that LaMOuRsubstantially enhances recovery efficiency across diverse locomotion tasks andeven generalizes effectively to complex environments, including humanoidlocomotion and mobile manipulation, where existing methods struggle. The codeand supplementary materials are available at https://lamour-rl.github.io/.
  </details>

- **[Hybrid Action Based Reinforcement Learning for Multi-Objective Compatible Autonomous Driving](http://arxiv.org/abs/2501.08096v2)**  `arXiv:2501.08096`  
  _Guizhe Jin, Zhuoren Li, Bo Leng, Wei Han, Lu Xiong, Chen Sun_
  <details><summary>Abstract</summary>
  Reinforcement Learning (RL) has shown excellent performance in solvingdecision-making and control problems of autonomous driving, which isincreasingly applied in diverse driving scenarios. However, driving is amulti-attribute problem, leading to challenges in achieving multi-objectivecompatibility for current RL methods, especially in both policy execution andpolicy iteration. On the one hand, the common action space structure withsingle action type limits driving flexibility or results in large behaviorfluctuations during policy execution. On the other hand, the multi-attributeweighted single reward function result in the agent's disproportionateattention to certain objectives during policy iterations. To this end, wepropose a Multi-objective Ensemble-Critic reinforcement learning method withHybrid Parametrized Action for multi-objective compatible autonomous driving.Specifically, a parameterized action space is constructed to generate hybriddriving actions, combining both abstract guidance and concrete controlcommands. A multi-objective critics architecture is constructed consideringmultiple attribute rewards, to ensure simultaneously focusing on differentdriving objectives. Additionally, uncertainty-based exploration strategy isintroduced to help the agent faster approach viable driving policy. Theexperimental results in both the simulated traffic environment and the HighDdataset demonstrate that our method can achieve multi-objective compatibleautonomous driving in terms of driving efficiency, action consistency, andsafety. It enhances the general performance of the driving while significantlyincreasing training efficiency.
  </details>

- **[Non-Prehensile Tool-Object Manipulation by Integrating LLM-Based Planning and Manoeuvrability-Driven Controls](http://arxiv.org/abs/2412.06931v3)**  `arXiv:2412.06931`  
  _Hoi-Yin Lee, Peng Zhou, Anqing Duan, Wanyu Ma, Chenguang Yang, David Navarro-Alarcon_
  <details><summary>Abstract</summary>
  The ability to wield tools was once considered exclusive to humanintelligence, but it's now known that many other animals, like crows, possessthis capability. Yet, robotic systems still fall short of matching biologicaldexterity. In this paper, we investigate the use of Large Language Models(LLMs), tool affordances, and object manoeuvrability for non-prehensiletool-based manipulation tasks. Our novel method leverages LLMs based on sceneinformation and natural language instructions to enable symbolic task planningfor tool-object manipulation. This approach allows the system to convert thehuman language sentence into a sequence of feasible motion functions. We havedeveloped a novel manoeuvrability-driven controller using a new tool affordancemodel derived from visual feedback. This controller helps guide the robot'stool utilization and manipulation actions, even within confined areas, using astepping incremental approach. The proposed methodology is evaluated withexperiments to prove its effectiveness under various manipulation scenarios.
  </details>

- **[SuperLoc: The Key to Robust LiDAR-Inertial Localization Lies in Predicting Alignment Risks](http://arxiv.org/abs/2412.02901v2)**  `arXiv:2412.02901`  
  _Shibo Zhao, Honghao Zhu, Yuanjun Gao, Beomsoo Kim, Yuheng Qiu, Aaron M. Johnson, et al._
  <details><summary>Abstract</summary>
  Map-based LiDAR localization, while widely used in autonomous systems, facessignificant challenges in degraded environments due to lacking distinctgeometric features. This paper introduces SuperLoc, a robust LiDAR localizationpackage that addresses key limitations in existing methods. SuperLoc features anovel predictive alignment risk assessment technique, enabling early detectionand mitigation of potential failures before optimization. This approachsignificantly improves performance in challenging scenarios such as corridors,tunnels, and caves. Unlike existing degeneracy mitigation algorithms that relyon post-optimization analysis and heuristic thresholds, SuperLoc evaluates thelocalizability of raw sensor measurements. Experimental results demonstratesignificant performance improvements over state-of-the-art methods acrossvarious degraded environments. Our approach achieves a 54% increase in accuracyand exhibits the highest robustness. To facilitate further research, we releaseour implementation along with datasets from eight challenging scenarios
  </details>

- **[Continuous-Time State Estimation Methods in Robotics: A Survey](http://arxiv.org/abs/2411.03951v2)**  `arXiv:2411.03951`  
  _William Talbot, Julian Nubert, Turcan Tuna, Cesar Cadena, Frederike D√ºmbgen, Jesus Tordesillas, et al._
  <details><summary>Abstract</summary>
  Accurate, efficient, and robust state estimation is more important than everin robotics as the variety of platforms and complexity of tasks continue togrow. Historically, discrete-time filters and smoothers have been the dominantapproach, in which the estimated variables are states at discrete sample times.The paradigm of continuous-time state estimation proposes an alternativestrategy by estimating variables that express the state as a continuousfunction of time, which can be evaluated at any query time. Not only can thisbenefit downstream tasks such as planning and control, but it alsosignificantly increases estimator performance and flexibility, as well asreduces sensor preprocessing and interfacing complexity. Despite this,continuous-time methods remain underutilized, potentially because they are lesswell-known within robotics. To remedy this, this work presents a unifyingformulation of these methods and the most exhaustive literature review to date,systematically categorizing prior work by methodology, application, statevariables, historical context, and theoretical contribution to the field. Bysurveying splines and Gaussian processes together and contextualizing worksfrom other research domains, this work identifies and analyzes open problems incontinuous-time state estimation and suggests new research directions.
  </details>

- **[LoRD: Adapting Differentiable Driving Policies to Distribution Shifts](http://arxiv.org/abs/2410.09681v3)**  `arXiv:2410.09681`  
  _Christopher Diehl, Peter Karkus, Sushant Veer, Marco Pavone, Torsten Bertram_
  <details><summary>Abstract</summary>
  Distribution shifts between operational domains can severely affect theperformance of learned models in self-driving vehicles (SDVs). While this is awell-established problem, prior work has mostly explored naive solutions suchas fine-tuning, focusing on the motion prediction task. In this work, weexplore novel adaptation strategies for differentiable autonomy stacksconsisting of prediction, planning, and control, perform evaluation inclosed-loop, and investigate the often-overlooked issue of catastrophicforgetting. Specifically, we introduce two simple yet effective techniques: alow-rank residual decoder (LoRD) and multi-task fine-tuning. Throughexperiments across three models conducted on two real-world autonomous drivingdatasets (nuPlan, exiD), we demonstrate the effectiveness of our methods andhighlight a significant performance gap between open-loop and closed-loopevaluation in prior approaches. Our approach improves forgetting by up to23.33% and the closed-loop OOD driving score by 9.93% in comparison to standardfine-tuning.
  </details>

- **[Preferenced Oracle Guided Multi-mode Policies for Dynamic Bipedal Loco-Manipulation](http://arxiv.org/abs/2410.01030v2)**  `arXiv:2410.01030`  
  _Prashanth Ravichandar, Lokesh Krishna, Nikhil Sobanbabu, Quan Nguyen_
  <details><summary>Abstract</summary>
  Dynamic loco-manipulation calls for effective whole-body control andcontact-rich interactions with the object and the environment. Existinglearning-based control synthesis relies on training low-level skill policiesand explicitly switching with a high-level policy or a hand-designed finitestate machine, leading to quasi-static behaviors. In contrast, dynamic taskssuch as soccer require the robot to run towards the ball, decelerate to anoptimal approach to dribble, and eventually kick a goal - a continuum of smoothmotion. To this end, we propose Preferenced Oracle Guided Multi-mode Policies(OGMP) to learn a single policy mastering all the required modes and preferredsequence of transitions to solve uni-object loco-manipulation tasks. We designhybrid automatons as oracles to generate references with continuous dynamicsand discrete mode jumps to perform a guided policy optimization through boundedexploration. To enforce learning a desired sequence of mode transitions, wepresent a task-agnostic preference reward that enhances performance. Theproposed approach demonstrates successful loco-manipulation for tasks likesoccer and moving boxes omnidirectionally through whole-body control. Insoccer, a single policy learns to optimally reach the ball, transition tocontact-rich dribbling, and execute successful goal kicks and ball stops.Leveraging the oracle's abstraction, we solve each loco-manipulation task onrobots with varying morphologies, including HECTOR V1, Berkeley Humanoid,Unitree G1, and H1, using the same reward definition and weights.
  </details>

- **[Dynamics-Guided Diffusion Model for Sensor-less Robot Manipulator Design](http://arxiv.org/abs/2402.15038v2)**  `arXiv:2402.15038`  
  _Xiaomeng Xu, Huy Ha, Shuran Song_
  <details><summary>Abstract</summary>
  We present Dynamics-Guided Diffusion Model (DGDM), a data-driven frameworkfor generating task-specific manipulator designs without task-specifictraining. Given object shapes and task specifications, DGDM generatessensor-less manipulator designs that can blindly manipulate objects towardsdesired motions and poses using an open-loop parallel motion. This framework 1)flexibly represents manipulation tasks as interaction profiles, 2) representsthe design space using a geometric diffusion model, and 3) efficiently searchesthis design space using the gradients provided by a dynamics network trainedwithout any task information. We evaluate DGDM on various manipulation tasksranging from shifting/rotating objects to converging objects to a specificpose. Our generated designs outperform optimization-based and unguideddiffusion baselines relatively by 31.5% and 45.3% on average success rate. Withthe ability to generate a new design within 0.8s, DGDM facilitates rapid designiteration and enhances the adoption of data-driven approaches for robotmechanism design. Qualitative results are best viewed on our project websitehttps://dgdm-robot.github.io/.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

