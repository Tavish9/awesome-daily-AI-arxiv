# üöÄ Daily AI arXiv Digest

[![Total Papers](https://img.shields.io/badge/paper_today-82+-red)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)](https://github.com/Tavish9/awesome-daily-AI-arxiv/commits/main/)
[![arXiv API](https://img.shields.io/badge/powered_by-arXiv_API-009688)](https://arxiv.org/help/api)
[![License](https://img.shields.io/badge/license-CC_BY--SA_4.0-3989c9)](LICENSE)


üìå ‚Äã**Tracking Breakthroughs in**: `AI` ‚Ä¢ `NLP` ‚Ä¢ `CV` ‚Ä¢ `ML` ‚Ä¢ `Robotics`  
‚è∞ ‚Äã**Update Schedule**: [UTC 02:00](https://time.is/UTC) | [GMT+8 10:00](https://time.is/China)

## üåü Today's Highlights

- üî• Hot Topic
  - [LLM](hot_topic/LLM.md)
  - [Reasoning](hot_topic/Reasoning.md)
  - [MLLM](hot_topic/MLLM.md)
  - [3D_Generation](hot_topic/3D_Generation.md)
  - [3D_Reconstruction](hot_topic/3D_Reconstruction.md)
  - [Benchmark](hot_topic/Benchmark.md)
  - [Diffusion](hot_topic/Diffusion.md)
  - [Embodied_AI](hot_topic/Embodied_AI.md)
- üí´ Active Platform
  - [Huggingface](https://huggingface.co/papers)
  - [LlamaFactory](https://www.llamafactory.cn/daily-paper/)
  - [X (Twitter)](https://x.com/arxiv_daily)
  - [Paper Reading](https://paperreading.club/)
  - [Paper Digest](https://www.paperdigest.org/arxiv/)
  

## üìå Full Archive

| Category                                                                                | Count |
| --------------------------------------------------------------------------------------- | ----- |
| [Artificial Intelligence üß†](#artificial-intelligence-) | 19    |
| [Computation and Language üí¨](#computation-and-language-) | 19    |
| [Computer Vision and Pattern Recognition üì∏](#computer-vision-and-pattern-recognition-) | 12    |
| [Machine Learning üìä](#machine-learning-) | 5     |
| [Multiagent Systems üåê](#multiagent-systems-) | 2     |
| [Robotics ü§ñ](#robotics-) | 25    |

### Artificial Intelligence üß†

<details open><summary>Click to Collapse</summary>

- **[RIG: Synergizing Reasoning and Imagination in End-to-End Generalist Policy](http://arxiv.org/abs/2503.24388v1)**  `arXiv:2503.24388`  
  _Zhonghan Zhao, Wenwei Zhang, Haian Huang, Kuikun Liu, Jianfei Gao, Gaoang Wang, et al._
  <details><summary>Abstract</summary>
  Reasoning before action and imagining potential outcomes (i.e., world models)are essential for embodied agents operating in complex open-world environments.Yet, prior work either incorporates only one of these abilities in anend-to-end agent or integrates multiple specialized models into an agentsystem, limiting the learning efficiency and generalization of the policy.Thus, this paper makes the first attempt to synergize Reasoning and Imaginationin an end-to-end Generalist policy, termed RIG. To train RIG in an end-to-endmanner, we construct a data pipeline that progressively integrates and enrichesthe content of imagination and reasoning in the trajectories collected fromexisting agents. The joint learning of reasoning and next image generationexplicitly models the inherent correlation between reasoning, action, anddynamics of environments, and thus exhibits more than $17\times$ sampleefficiency improvements and generalization in comparison with previous works.During inference, RIG first reasons about the next action, produces potentialaction, and then predicts the action outcomes, which offers the agent a chanceto review and self-correct based on the imagination before taking real actions.Experimental results show that the synergy of reasoning and imagination notonly improves the robustness, generalization, and interoperability ofgeneralist policy but also enables test-time scaling to enhance overallperformance.
  </details>

- **[ACPBench Hard: Unrestrained Reasoning about Action, Change, and Planning](http://arxiv.org/abs/2503.24378v1)**  `arXiv:2503.24378`  
  _Harsha Kokel, Michael Katz, Kavitha Srinivas, Shirin Sohrabi_
  <details><summary>Abstract</summary>
  The ACPBench dataset provides atomic reasoning tasks required for efficientplanning. The dataset is aimed at distilling the complex plan generation taskinto separate atomic reasoning tasks in their easiest possible form, boolean ormultiple-choice questions, where the model has to choose the right answer fromthe provided options. While the aim of ACPBench is to test the simplest form ofreasoning about action and change, when tasked with planning, a model does nottypically have options to choose from and thus the reasoning required forplanning dictates an open-ended, generative form for these tasks. To that end,we introduce ACPBench Hard, a generative version of ACPBench, with open-endedquestions which the model needs to answer. Models that perform well on thesetasks could in principle be integrated into a planner or be used directly as apolicy. We discuss the complexity of these tasks as well as the complexity ofvalidating the correctness of their answers and present validation algorithmsfor each task. Equipped with these validators, we test the performance of avariety of models on our tasks and find that for most of these tasks theperformance of even the largest models is still subpar. Our experiments showthat no model outperforms another in these tasks and with a few exceptions alltested language models score below 65%, indicating that even the currentfrontier language models have a long way to go before they can reliably reasonabout planning. In fact, even the so-called reasoning models struggle withsolving these reasoning tasks. ACPBench Hard collection is available at thefollowing link: https://ibm.github.io/ACPBench
  </details>

- **[Contextual Preference Collaborative Measure Framework Based on Belief System](http://arxiv.org/abs/2503.24328v1)**  `arXiv:2503.24328`  
  _Hang Yu, Wei Wei, Zheng Tan, Jing-lei Liu_
  <details><summary>Abstract</summary>
  To reduce the human intervention in the preference measure process,thisarticle proposes a preference collaborative measure framework based on anupdated belief system,which is also capable of improving the accuracy andefficiency of preferen-ce measure algorithms.Firstly,the distance of rules andthe average internal distance of rulesets are proposed for specifying therelationship between the rules.For discovering the most representativepreferences that are common in all users,namely common preference,a algorithmbased on average internal distance of ruleset,PRA algorithm,is proposed,whichaims to finish the discoveryprocess with minimum information lossrate.Furthermore,the concept of Common belief is proposed to update the beliefsystem,and the common preferences are the evidences of updated beliefsystem.Then,under the belief system,the proposed belief degree and deviationdegree are used to determine whether a rule confirms the belief system or notand classify the preference rules into two kinds(generalized orpersonalized),and eventually filters out Top-K interesting rules relying onbelief degree and deviation degree.Based on above,a scalable interestingnesscalculation framework that can apply various formulas is proposed foraccurately calculating interestingness in different conditions.At last,IMCosalgorithm and IMCov algorithm are proposed as exemplars to verify the accuracyand efficiency of the framework by using weighted cosine similarity andcorrelation coefficients as belief degree.In experiments,the proposedalgorithms are compared to two state-of-the-art algorithms and the results showthat IMCos and IMCov outperform than the other two in most aspects.
  </details>

- **[PAARS: Persona Aligned Agentic Retail Shoppers](http://arxiv.org/abs/2503.24228v1)**  `arXiv:2503.24228`  
  _Saab Mansour, Leonardo Perelli, Lorenzo Mainetti, George Davidson, Stefano D'Amato_
  <details><summary>Abstract</summary>
  In e-commerce, behavioral data is collected for decision making which can becostly and slow. Simulation with LLM powered agents is emerging as a promisingalternative for representing human population behavior. However, LLMs are knownto exhibit certain biases, such as brand bias, review rating bias and limitedrepresentation of certain groups in the population, hence they need to becarefully benchmarked and aligned to user behavior. Ultimately, our goal is tosynthesise an agent population and verify that it collectively approximates areal sample of humans. To this end, we propose a framework that: (i) createssynthetic shopping agents by automatically mining personas from anonymisedhistorical shopping data, (ii) equips agents with retail-specific tools tosynthesise shopping sessions and (iii) introduces a novel alignment suitemeasuring distributional differences between humans and shopping agents at thegroup (i.e. population) level rather than the traditional "individual" level.Experimental results demonstrate that using personas improves performance onthe alignment suite, though a gap remains to human behaviour. We showcase aninitial application of our framework for automated agentic A/B testing andcompare the findings to human results. Finally, we discuss applications,limitations and challenges setting the stage for impactful future work.
  </details>

- **[All You Need is Sally-Anne: ToM in AI Strongly Supported After Surpassing Tests for 3-Year-Olds](http://arxiv.org/abs/2503.24215v1)**  `arXiv:2503.24215`  
  _Nitay Alon, Joseph Barnby, Reuth Mirsky, Stefan Sarkadi_
  <details><summary>Abstract</summary>
  Theory of Mind (ToM) is a hallmark of human cognition, allowing individualsto reason about others' beliefs and intentions. Engineers behind recentadvances in Artificial Intelligence (AI) have claimed to demonstrate comparablecapabilities. This paper presents a model that surpasses traditional ToM testsdesigned for 3-year-old children, providing strong support for the presence ofToM in AI systems.
  </details>

- **[Agent-Based Simulations of Online Political Discussions: A Case Study on Elections in Germany](http://arxiv.org/abs/2503.24199v1)**  `arXiv:2503.24199`  
  _Abdul Sittar, Simon M√ºnker, Fabio Sartori, Andreas Reitenbach, Achim Rettinger, Michael M√§s, et al._
  <details><summary>Abstract</summary>
  User engagement on social media platforms is influenced by historicalcontext, time constraints, and reward-driven interactions. This study presentsan agent-based simulation approach that models user interactions, consideringpast conversation history, motivation, and resource constraints. UtilizingGerman Twitter data on political discourse, we fine-tune AI models to generateposts and replies, incorporating sentiment analysis, irony detection, andoffensiveness classification. The simulation employs a myopic best-responsemodel to govern agent behavior, accounting for decision-making based onexpected rewards. Our results highlight the impact of historical context onAI-generated responses and demonstrate how engagement evolves under varyingconstraints.
  </details>

- **[Grounding Agent Reasoning in Image Schemas: A Neurosymbolic Approach to Embodied Cognition](http://arxiv.org/abs/2503.24110v1)**  `arXiv:2503.24110`  
  _Fran√ßois Olivier, Zied Bouraoui_
  <details><summary>Abstract</summary>
  Despite advances in embodied AI, agent reasoning systems still struggle tocapture the fundamental conceptual structures that humans naturally use tounderstand and interact with their environment. To address this, we propose anovel framework that bridges embodied cognition theory and agent systems byleveraging a formal characterization of image schemas, which are defined asrecurring patterns of sensorimotor experience that structure human cognition.By customizing LLMs to translate natural language descriptions into formalrepresentations based on these sensorimotor patterns, we will be able to createa neurosymbolic system that grounds the agent's understanding in fundamentalconceptual structures. We argue that such an approach enhances both efficiencyand interpretability while enabling more intuitive human-agent interactionsthrough shared embodied understanding.
  </details>

- **[Towards Scientific Intelligence: A Survey of LLM-based Scientific Agents](http://arxiv.org/abs/2503.24047v1)**  `arXiv:2503.24047`  
  _Shuo Ren, Pu Jian, Zhenjiang Ren, Chunlin Leng, Can Xie, Jiajun Zhang_
  <details><summary>Abstract</summary>
  As scientific research becomes increasingly complex, innovative tools areneeded to manage vast data, facilitate interdisciplinary collaboration, andaccelerate discovery. Large language models (LLMs) are now evolving intoLLM-based scientific agents that automate critical tasks, ranging fromhypothesis generation and experiment design to data analysis and simulation.Unlike general-purpose LLMs, these specialized agents integrate domain-specificknowledge, advanced tool sets, and robust validation mechanisms, enabling themto handle complex data types, ensure reproducibility, and drive scientificbreakthroughs. This survey provides a focused review of the architectures,design, benchmarks, applications, and ethical considerations surroundingLLM-based scientific agents. We highlight why they differ from general agentsand the ways in which they advance research across various scientific fields.By examining their development and challenges, this survey offers acomprehensive roadmap for researchers and practitioners to harness these agentsfor more efficient, reliable, and ethically sound scientific discovery.
  </details>

- **[Pay More Attention to the Robustness of Prompt for Instruction Data Mining](http://arxiv.org/abs/2503.24028v1)**  `arXiv:2503.24028`  
  _Qiang Wang, Dawei Feng, Xu Zhang, Ao Shen, Yang Xu, Bo Ding, et al._
  <details><summary>Abstract</summary>
  Instruction tuning has emerged as a paramount method for tailoring thebehaviors of LLMs. Recent work has unveiled the potential for LLMs to achievehigh performance through fine-tuning with a limited quantity of high-qualityinstruction data. Building upon this approach, we further explore the impact ofprompt's robustness on the selection of high-quality instruction data. Thispaper proposes a pioneering framework of high-quality online instruction datamining for instruction tuning, focusing on the impact of prompt's robustness onthe data mining process. Our notable innovation, is to generate the adversarialinstruction data by conducting the attack for the prompt of online instructiondata. Then, we introduce an Adversarial Instruction-Following Difficulty metricto measure how much help the adversarial instruction data can provide to thegeneration of the corresponding response. Apart from it, we propose a novelAdversarial Instruction Output Embedding Consistency approach to selecthigh-quality online instruction data. We conduct extensive experiments on twobenchmark datasets to assess the performance. The experimental results serve tounderscore the effectiveness of our proposed two methods. Moreover, the resultsunderscore the critical practical significance of considering prompt'srobustness.
  </details>

- **[AI2Agent: An End-to-End Framework for Deploying AI Projects as Autonomous Agents](http://arxiv.org/abs/2503.23948v1)**  `arXiv:2503.23948`  
  _Jiaxiang Chen, Jingwei Shi, Lei Gan, Jiale Zhang, Qingyu Zhang, Dongqian Zhang, et al._
  <details><summary>Abstract</summary>
  As AI technology advances, it is driving innovation across industries,increasing the demand for scalable AI project deployment. However, deploymentremains a critical challenge due to complex environment configurations,dependency conflicts, cross-platform adaptation, and debugging difficulties,which hinder automation and adoption. This paper introduces AI2Agent, anend-to-end framework that automates AI project deployment throughguideline-driven execution, self-adaptive debugging, and case \& solutionaccumulation. AI2Agent dynamically analyzes deployment challenges, learns frompast cases, and iteratively refines its approach, significantly reducing humanintervention. To evaluate its effectiveness, we conducted experiments on 30 AIdeployment cases, covering TTS, text-to-image generation, image editing, andother AI applications. Results show that AI2Agent significantly reducesdeployment time and improves success rates. The code and demo video are nowpublicly accessible.
  </details>

- **[What the F*ck Is Artificial General Intelligence?](http://arxiv.org/abs/2503.23923v1)**  `arXiv:2503.23923`  
  _Michael Timothy Bennett_
  <details><summary>Abstract</summary>
  Artificial general intelligence (AGI) is an established field of research.Yet Melanie Mitchell and others have questioned if the term still has meaning.AGI has been subject to so much hype and speculation it has become something ofa Rorschach test. Mitchell points out that the debate will only be settledthrough long term, scientific investigation. To that end here is a short,accessible and provocative overview of AGI. I compare definitions ofintelligence, settling on intelligence in terms of adaptation and AGI as anartificial scientist. Taking my queue from Sutton's Bitter Lesson I describetwo foundational tools used to build adaptive systems: search andapproximation. I compare pros, cons, hybrids and architectures like o3,AlphaGo, AERA, NARS and Hyperon. I then discuss overall meta-approaches tomaking systems behave more intelligently. I divide them into scale-maxing,simp-maxing, w-maxing based on the Bitter Lesson, Ockham's and Bennett'sRazors. These maximise resources, simplicity of form, and the weakness ofconstraints on functionality. I discuss examples including AIXI, the freeenergy principle and The Embiggening of language models. I conclude that thoughscale-maxed approximation dominates, AGI will be a fusion of tools andmeta-approaches. The Embiggening was enabled by improvements in hardware. Nowthe bottlenecks are sample and energy efficiency.
  </details>

- **[DebFlow: Automating Agent Creation via Agent Debate](http://arxiv.org/abs/2503.23781v1)**  `arXiv:2503.23781`  
  _Jinwei Su, Yinghui Xia, Ronghua Shi, Jianhui Wang, Jianuo Huang, Yijin Wang, et al._
  <details><summary>Abstract</summary>
  Large language models (LLMs) have demonstrated strong potential andimpressive performance in automating the generation and optimization ofworkflows. However, existing approaches are marked by limited reasoningcapabilities, high computational demands, and significant resourcerequirements. To address these issues, we propose DebFlow, a framework thatemploys a debate mechanism to optimize workflows and integrates reflexion toimprove based on previous experiences. We evaluated our method across sixbenchmark datasets, including HotpotQA, MATH, and ALFWorld. Our approachachieved a 3\% average performance improvement over the latest baselines,demonstrating its effectiveness in diverse problem domains. In particular,during training, our framework reduces resource consumption by 37\% compared tothe state-of-the-art baselines. Additionally, we performed ablation studies.Removing the Debate component resulted in a 4\% performance drop across twobenchmark datasets, significantly greater than the 2\% drop observed when theReflection component was removed. These findings strongly demonstrate thecritical role of Debate in enhancing framework performance, while alsohighlighting the auxiliary contribution of reflexion to overall optimization.
  </details>

- **[ActionStudio: A Lightweight Framework for Data and Training of Large Action Models](http://arxiv.org/abs/2503.22673v2)**  `arXiv:2503.22673`  
  _Jianguo Zhang, Thai Hoang, Ming Zhu, Zuxin Liu, Shiyu Wang, Tulika Awalgaonkar, et al._
  <details><summary>Abstract</summary>
  Action models are essential for enabling autonomous agents to perform complextasks. However, training large action models remains challenging due to thediversity of agent environments and the complexity of agentic data. Despitegrowing interest, existing infrastructure provides limited support forscalable, agent-specific fine-tuning. We present ActionStudio, a lightweightand extensible data and training framework designed for large action models.ActionStudio unifies heterogeneous agent trajectories through a standardizedformat, supports diverse training paradigms including LoRA, full fine-tuning,and distributed setups, and integrates robust preprocessing and verificationtools. We validate its effectiveness across both public and realistic industrybenchmarks, demonstrating strong performance and practical scalability. Weopen-sourced code and data at https://github.com/SalesforceAIResearch/xLAM tofacilitate research in the community.
  </details>

- **[Agent-Centric Personalized Multiple Clustering with Multi-Modal LLMs](http://arxiv.org/abs/2503.22241v2)**  `arXiv:2503.22241`  
  _Ziye Chen, Yiqun Duan, Riheng Zhu, Zhenbang Sun, Mingming Gong_
  <details><summary>Abstract</summary>
  Personalized multiple clustering aims to generate diverse partitions of adataset based on different user-specific aspects, rather than a singleclustering. It has recently drawn research interest for accommodating varyinguser preferences. Recent approaches primarily use CLIP embeddings with proxylearning to extract representations biased toward user clustering preferences.However, CLIP primarily focuses on coarse image-text alignment, lacking a deepcontextual understanding of user interests. To overcome these limitations, wepropose an agent-centric personalized clustering framework that leveragesmulti-modal large language models (MLLMs) as agents to comprehensively traversea relational graph to search for clusters based on user interests. Due to theadvanced reasoning mechanism of MLLMs, the obtained clusters align more closelywith user-defined criteria than those obtained from CLIP-based representations.To reduce computational overhead, we shorten the agents' traversal path byconstructing a relational graph using user-interest-biased embeddings extractedby MLLMs. A large number of weakly connected edges can be filtered out based onembedding similarity, facilitating an efficient traversal search for agents.Experimental results show that the proposed method achieves NMI scores of0.9667 and 0.9481 on the Card Order and Card Suits benchmarks, respectively,largely improving the SOTA model by over 140%.
  </details>

- **[Quantifying the Capability Boundary of DeepSeek Models: An Application-Driven Performance Analysis](http://arxiv.org/abs/2502.11164v4)**  `arXiv:2502.11164`  
  _Kaikai Zhao, Zhaoxiang Liu, Xuejiao Lei, Jiaojiao Zhao, Zhenhong Long, Zipeng Wang, et al._
  <details><summary>Abstract</summary>
  DeepSeek-R1, known for its low training cost and exceptional reasoningcapabilities, has achieved state-of-the-art performance on various benchmarks.However, detailed evaluations for DeepSeek Series models from the perspectiveof real-world applications are lacking, making it challenging for users toselect the most suitable DeepSeek models for their specific needs. To addressthis gap, we conduct a systematic evaluation of the DeepSeek-V3, DeepSeek-R1,DeepSeek-R1-Distill-Qwen series, DeepSeek-R1-Distill-Llama series, theircorresponding 4-bit quantized models, and the reasoning model QwQ-32B using theenhanced A-Eval benchmark, A-Eval-2.0. Through a comparative analysis oforiginal instruction-tuned models and their distilled counterparts, weinvestigate how reasoning enhancements impact performance across diversepractical tasks. To assist users in model selection, we quantify the capabilityboundary of DeepSeek models through performance tier classifications. Based onthe quantification results, we develop a model selection handbook that clearlyillustrates the relation among models, their capabilities and practicalapplications. This handbook enables users to select the most cost-effectivemodels without efforts, ensuring optimal performance and resource efficiency inreal-world applications. It should be noted that, despite our efforts toestablish a comprehensive, objective, and authoritative evaluation benchmark,the selection of test samples, characteristics of data distribution, and thesetting of evaluation criteria may inevitably introduce certain biases into theevaluation results. We will continuously optimize the evaluation benchmarks andperiodically update this paper to provide more comprehensive and accurateevaluation results. Please refer to the latest version of the paper for themost current results and conclusions.
  </details>

- **[PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models](http://arxiv.org/abs/2502.01584v3)**  `arXiv:2502.01584`  
  _Zixuan Wu, Francesca Lucchetti, Aleksander Boruch-Gruszecki, Jingmiao Zhao, Carolyn Jane Anderson, Joydeep Biswas, et al._
  <details><summary>Abstract</summary>
  Existing benchmarks for frontier models often test specialized, "PhD-level"knowledge that is difficult for non-experts to grasp. In contrast, we present abenchmark with 594 problems based on the NPR Sunday Puzzle Challenge thatrequires only general knowledge. Our benchmark is challenging for both humansand models; however correct solutions are easy to verify, and models' mistakesare easy to spot. As LLMs are more widely deployed in society, we believe it isuseful to develop benchmarks for frontier models that humans can understandwithout the need for deep domain expertise.  Our work reveals capability gaps that are not evident in existing benchmarks:OpenAI o1 significantly outperforms other reasoning models on our benchmark,despite being on par with other models when tested on benchmarks that testspecialized knowledge. Furthermore, our analysis of reasoning outputs uncoversnew kinds of failures. DeepSeek R1, for instance, often concedes with "I giveup" before providing an answer that it knows is wrong. R1 can also beremarkably "uncertain" in its output and in rare cases, it does not "finishthinking," which suggests the need for techniques to "wrap up" before thecontext window limit is reached. We also quantify the effectiveness ofreasoning longer to identify the point beyond which more reasoning is unlikelyto improve accuracy on our benchmark.
  </details>

- **[AI in radiological imaging of soft-tissue and bone tumours: a systematic review evaluating against CLAIM and FUTURE-AI guidelines](http://arxiv.org/abs/2408.12491v2)**  `arXiv:2408.12491`  
  _Douwe J. Spaanderman, Matthew Marzetti, Xinyi Wan, Andrew F. Scarsbrook, Philip Robinson, Edwin H. G. Oei, et al._
  <details><summary>Abstract</summary>
  Soft-tissue and bone tumours (STBT) are rare, diagnostically challenginglesions with variable clinical behaviours and treatment approaches. Thissystematic review provides an overview of Artificial Intelligence (AI) methodsusing radiological imaging for diagnosis and prognosis of these tumours,highlighting challenges in clinical translation, and evaluating study alignmentwith the Checklist for AI in Medical Imaging (CLAIM) and the FUTURE-AIinternational consensus guidelines for trustworthy and deployable AI to promotethe clinical translation of AI methods. The review covered literature fromseveral bibliographic databases, including papers published before 17/07/2024.Original research in peer-reviewed journals focused on radiology-based AI fordiagnosing or prognosing primary STBT was included. Exclusion criteria wereanimal, cadaveric, or laboratory studies, and non-English papers. Abstractswere screened by two of three independent reviewers for eligibility. Eligiblepapers were assessed against guidelines by one of three independent reviewers.The search identified 15,015 abstracts, from which 325 articles were includedfor evaluation. Most studies performed moderately on CLAIM, averaging a scoreof 28.9$\pm$7.5 out of 53, but poorly on FUTURE-AI, averaging 5.1$\pm$2.1 outof 30. Imaging-AI tools for STBT remain at the proof-of-concept stage,indicating significant room for improvement. Future efforts by AI developersshould focus on design (e.g. define unmet clinical need, intended clinicalsetting and how AI would be integrated in clinical workflow), development (e.g.build on previous work, explainability), evaluation (e.g. evaluating andaddressing biases, evaluating AI against best practices), and datareproducibility and availability (making documented code and data publiclyavailable). Following these recommendations could improve clinical translationof AI methods.
  </details>

- **[MAQA: Evaluating Uncertainty Quantification in LLMs Regarding Data Uncertainty](http://arxiv.org/abs/2408.06816v2)**  `arXiv:2408.06816`  
  _Yongjin Yang, Haneul Yoo, Hwaran Lee_
  <details><summary>Abstract</summary>
  Despite the massive advancements in large language models (LLMs), they stillsuffer from producing plausible but incorrect responses. To improve thereliability of LLMs, recent research has focused on uncertainty quantificationto predict whether a response is correct or not. However, most uncertaintyquantification methods have been evaluated on single-labeled questions, whichremoves data uncertainty: the irreducible randomness often present in userqueries, which can arise from factors like multiple possible answers. Thislimitation may cause uncertainty quantification results to be unreliable inpractical settings. In this paper, we investigate previous uncertaintyquantification methods under the presence of data uncertainty. Ourcontributions are two-fold: 1) proposing a new Multi-Answer Question Answeringdataset, MAQA, consisting of world knowledge, mathematical reasoning, andcommonsense reasoning tasks to evaluate uncertainty quantification regardingdata uncertainty, and 2) assessing 5 uncertainty quantification methods ofdiverse white- and black-box LLMs. Our findings show that previous methodsrelatively struggle compared to single-answer settings, though this variesdepending on the task. Moreover, we observe that entropy- and consistency-basedmethods effectively estimate model uncertainty, even in the presence of datauncertainty. We believe these observations will guide future work onuncertainty quantification in more realistic settings.
  </details>

- **[ShapG: new feature importance method based on the Shapley value](http://arxiv.org/abs/2407.00506v2)**  `arXiv:2407.00506`  
  _Chi Zhao, Jing Liu, Elena Parilina_
  <details><summary>Abstract</summary>
  With wide application of Artificial Intelligence (AI), it has becomeparticularly important to make decisions of AI systems explainable andtransparent. In this paper, we proposed a new Explainable ArtificialIntelligence (XAI) method called ShapG (Explanations based on Shapley value forGraphs) for measuring feature importance. ShapG is a model-agnostic globalexplanation method. At the first stage, it defines an undirected graph based onthe dataset, where nodes represent features and edges are added based oncalculation of correlation coefficients between features. At the second stage,it calculates an approximated Shapley value by sampling the data taking intoaccount this graph structure. The sampling approach of ShapG allows tocalculate the importance of features efficiently, i.e. to reduce computationalcomplexity. Comparison of ShapG with other existing XAI methods shows that itprovides more accurate explanations for two examined datasets. We also comparedother XAI methods developed based on cooperative game theory with ShapG inrunning time, and the results show that ShapG exhibits obvious advantages inits running time, which further proves efficiency of ShapG. In addition,extensive experiments demonstrate a wide range of applicability of the ShapGmethod for explaining complex models. We find ShapG an important tool inimproving explainability and transparency of AI systems and believe it can bewidely used in various fields.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Computation and Language üí¨

<details open><summary>Click to Collapse</summary>

- **[Harnessing the Reasoning Economy: A Survey of Efficient Reasoning for Large Language Models](http://arxiv.org/abs/2503.24377v1)**  `arXiv:2503.24377`  
  _Rui Wang, Hongru Wang, Boyang Xue, Jianhui Pang, Shudong Liu, Yi Chen, et al._
  <details><summary>Abstract</summary>
  Recent advancements in Large Language Models (LLMs) have significantlyenhanced their ability to perform complex reasoning tasks, transitioning fromfast and intuitive thinking (System 1) to slow and deep reasoning (System 2).While System 2 reasoning improves task accuracy, it often incurs substantialcomputational costs due to its slow thinking nature and inefficient orunnecessary reasoning behaviors. In contrast, System 1 reasoning iscomputationally efficient but leads to suboptimal performance. Consequently, itis critical to balance the trade-off between performance (benefits) andcomputational costs (budgets), giving rise to the concept of reasoning economy.In this survey, we provide a comprehensive analysis of reasoning economy inboth the post-training and test-time inference stages of LLMs, encompassing i)the cause of reasoning inefficiency, ii) behavior analysis of differentreasoning patterns, and iii) potential solutions to achieve reasoning economy.By offering actionable insights and highlighting open challenges, we aim toshed light on strategies for improving the reasoning economy of LLMs, therebyserving as a valuable resource for advancing research in this evolving area. Wealso provide a public repository to continually track developments in thisfast-evolving field.
  </details>

- **[Query and Conquer: Execution-Guided SQL Generation](http://arxiv.org/abs/2503.24364v1)**  `arXiv:2503.24364`  
  _≈Åukasz Borchmann, Marek Wydmuch_
  <details><summary>Abstract</summary>
  We propose a novel approach for generating complex outputs that significantlyimproves accuracy in text-to-SQL tasks. Our method leverages execution resultsto select the most semantically consistent query from multiple candidates,enabling smaller, cost-effective models to surpass computationally intensivereasoning methods such as o1, o3-mini, and DeepSeek R1 while reducing inferencecost by as much as 30 times. It integrates effortlessly with existing models,offering a practical and scalable pathway to state-of-the-art SQL generation.
  </details>

- **[BEATS: Bias Evaluation and Assessment Test Suite for Large Language Models](http://arxiv.org/abs/2503.24310v1)**  `arXiv:2503.24310`  
  _Alok Abhishek, Lisa Erickson, Tushar Bandopadhyay_
  <details><summary>Abstract</summary>
  In this research, we introduce BEATS, a novel framework for evaluating Bias,Ethics, Fairness, and Factuality in Large Language Models (LLMs). Building uponthe BEATS framework, we present a bias benchmark for LLMs that measureperformance across 29 distinct metrics. These metrics span a broad range ofcharacteristics, including demographic, cognitive, and social biases, as wellas measures of ethical reasoning, group fairness, and factuality relatedmisinformation risk. These metrics enable a quantitative assessment of theextent to which LLM generated responses may perpetuate societal prejudices thatreinforce or expand systemic inequities. To achieve a high score on thisbenchmark a LLM must show very equitable behavior in their responses, making ita rigorous standard for responsible AI evaluation. Empirical results based ondata from our experiment show that, 37.65\% of outputs generated by industryleading models contained some form of bias, highlighting a substantial risk ofusing these models in critical decision making systems. BEATS framework andbenchmark offer a scalable and statistically rigorous methodology to benchmarkLLMs, diagnose factors driving biases, and develop mitigation strategies. Withthe BEATS framework, our goal is to help the development of more sociallyresponsible and ethically aligned AI models.
  </details>

- **[A Systematic Evaluation of LLM Strategies for Mental Health Text Analysis: Fine-tuning vs. Prompt Engineering vs. RAG](http://arxiv.org/abs/2503.24307v1)**  `arXiv:2503.24307`  
  _Arshia Kermani, Veronica Perez-Rosas, Vangelis Metsis_
  <details><summary>Abstract</summary>
  This study presents a systematic comparison of three approaches for theanalysis of mental health text using large language models (LLMs): promptengineering, retrieval augmented generation (RAG), and fine-tuning. Using LLaMA3, we evaluate these approaches on emotion classification and mental healthcondition detection tasks across two datasets. Fine-tuning achieves the highestaccuracy (91% for emotion classification, 80% for mental health conditions) butrequires substantial computational resources and large training sets, whileprompt engineering and RAG offer more flexible deployment with moderateperformance (40-68% accuracy). Our findings provide practical insights forimplementing LLM-based solutions in mental health applications, highlightingthe trade-offs between accuracy, computational requirements, and deploymentflexibility.
  </details>

- **[Is analogy enough to draw novel adjective-noun inferences?](http://arxiv.org/abs/2503.24293v1)**  `arXiv:2503.24293`  
  _Hayley Ross, Kathryn Davidson, Najoung Kim_
  <details><summary>Abstract</summary>
  Recent work (Ross et al., 2025, 2024) has argued that the ability of humansand LLMs respectively to generalize to novel adjective-noun combinations showsthat they each have access to a compositional mechanism to determine thephrase's meaning and derive inferences. We study whether these inferences caninstead be derived by analogy to known inferences, without need forcomposition. We investigate this by (1) building a model of analogicalreasoning using similarity over lexical items, and (2) asking humanparticipants to reason by analogy. While we find that this strategy works wellfor a large proportion of the dataset of Ross et al. (2025), there are novelcombinations for which both humans and LLMs derive convergent inferences butwhich are not well handled by analogy. We thus conclude that the mechanismhumans and LLMs use to generalize in these cases cannot be fully reduced toanalogy, and likely involves composition.
  </details>

- **[Enhancing Large Language Models (LLMs) for Telecommunications using Knowledge Graphs and Retrieval-Augmented Generation](http://arxiv.org/abs/2503.24245v1)**  `arXiv:2503.24245`  
  _Dun Yuan, Hao Zhou, Di Wu, Xue Liu, Hao Chen, Yan Xin, et al._
  <details><summary>Abstract</summary>
  Large language models (LLMs) have made significant progress ingeneral-purpose natural language processing tasks. However, LLMs are stillfacing challenges when applied to domain-specific areas liketelecommunications, which demands specialized expertise and adaptability toevolving standards. This paper presents a novel framework that combinesknowledge graph (KG) and retrieval-augmented generation (RAG) techniques toenhance LLM performance in the telecom domain. The framework leverages a KG tocapture structured, domain-specific information about network protocols,standards, and other telecom-related entities, comprehensively representingtheir relationships. By integrating KG with RAG, LLMs can dynamically accessand utilize the most relevant and up-to-date knowledge during responsegeneration. This hybrid approach bridges the gap between structured knowledgerepresentation and the generative capabilities of LLMs, significantly enhancingaccuracy, adaptability, and domain-specific comprehension. Our resultsdemonstrate the effectiveness of the KG-RAG framework in addressing complextechnical queries with precision. The proposed KG-RAG model attained anaccuracy of 88% for question answering tasks on a frequently usedtelecom-specific dataset, compared to 82% for the RAG-only and 48% for theLLM-only approaches.
  </details>

- **[What, How, Where, and How Well? A Survey on Test-Time Scaling in Large Language Models](http://arxiv.org/abs/2503.24235v1)**  `arXiv:2503.24235`  
  _Qiyuan Zhang, Fuyuan Lyu, Zexu Sun, Lei Wang, Weixu Zhang, Zhihan Guo, et al._
  <details><summary>Abstract</summary>
  As enthusiasm for scaling computation (data and parameters) in thepretraining era gradually diminished, test-time scaling (TTS), also referred toas ``test-time computing'' has emerged as a prominent research focus. Recentstudies demonstrate that TTS can further elicit the problem-solvingcapabilities of large language models (LLMs), enabling significantbreakthroughs not only in specialized reasoning tasks, such as mathematics andcoding, but also in general tasks like open-ended Q&A. However, despite theexplosion of recent efforts in this area, there remains an urgent need for acomprehensive survey offering a systemic understanding. To fill this gap, wepropose a unified, multidimensional framework structured along four coredimensions of TTS research: what to scale, how to scale, where to scale, andhow well to scale. Building upon this taxonomy, we conduct an extensive reviewof methods, application scenarios, and assessment aspects, and present anorganized decomposition that highlights the unique functional roles ofindividual techniques within the broader TTS landscape. From this analysis, wedistill the major developmental trajectories of TTS to date and offer hands-onguidelines for practical deployment. Furthermore, we identify several openchallenges and offer insights into promising future directions, includingfurther scaling, clarifying the functional essence of techniques, generalizingto more tasks, and more attributions.
  </details>

- **[BAR-Analytics: A Web-based Platform for Analyzing Information Spreading Barriers in News: Comparative Analysis Across Multiple Barriers and Events](http://arxiv.org/abs/2503.24220v1)**  `arXiv:2503.24220`  
  _Abdul Sittar, Dunja Mladenic, Alenka Gucek, Marko Grobelnik_
  <details><summary>Abstract</summary>
  This paper presents BAR-Analytics, a web-based, open-source platform designedto analyze news dissemination across geographical, economic, political, andcultural boundaries. Using the Russian-Ukrainian and Israeli-Palestinianconflicts as case studies, the platform integrates four analytical methods:propagation analysis, trend analysis, sentiment analysis, and temporal topicmodeling. Over 350,000 articles were collected and analyzed, with a focus oneconomic disparities and geographical influences using metadata enrichment. Weevaluate the case studies using coherence, sentiment polarity, topic frequency,and trend shifts as key metrics. Our results show distinct patterns in newscoverage: the Israeli-Palestinian conflict tends to have more negativesentiment with a focus on human rights, while the Russia-Ukraine conflict ismore positive, emphasizing election interference. These findings highlight theinfluence of political, economic, and regional factors in shaping medianarratives across different conflicts.
  </details>

- **[Synthetic News Generation for Fake News Classification](http://arxiv.org/abs/2503.24206v1)**  `arXiv:2503.24206`  
  _Abdul Sittar, Luka Golob, Mateja Smiljanic_
  <details><summary>Abstract</summary>
  This study explores the generation and evaluation of synthetic fake newsthrough fact based manipulations using large language models (LLMs). Weintroduce a novel methodology that extracts key facts from real articles,modifies them, and regenerates content to simulate fake news while maintainingcoherence. To assess the quality of the generated content, we propose a set ofevaluation metrics coherence, dissimilarity, and correctness. The research alsoinvestigates the application of synthetic data in fake news classification,comparing traditional machine learning models with transformer based modelssuch as BERT. Our experiments demonstrate that transformer models, especiallyBERT, effectively leverage synthetic data for fake news detection, showingimprovements with smaller proportions of synthetic data. Additionally, we findthat fact verification features, which focus on identifying factualinconsistencies, provide the most promising results in distinguishing syntheticfake news. The study highlights the potential of synthetic data to enhance fakenews detection systems, offering valuable insights for future research andsuggesting that targeted improvements in synthetic data generation can furtherstrengthen detection models.
  </details>

- **[TwT: Thinking without Tokens by Habitual Reasoning Distillation with Multi-Teachers' Guidance](http://arxiv.org/abs/2503.24198v1)**  `arXiv:2503.24198`  
  _Jingxian Xu, Mengyu Zhou, Weichang Liu, Hanbing Liu, Shi Han, Dongmei Zhang_
  <details><summary>Abstract</summary>
  Large Language Models (LLMs) have made significant strides in problem-solvingby incorporating reasoning processes. However, this enhanced reasoningcapability results in an increased number of output tokens during inference,leading to higher computational costs. To address this challenge, we proposeTwT (Thinking without Tokens), a method that reduces inference-time coststhrough habitual reasoning distillation with multi-teachers' guidance, whilemaintaining high performance. Our approach introduces a Habitual ReasoningDistillation method, which internalizes explicit reasoning into the model'shabitual behavior through a Teacher-Guided compression strategy inspired byhuman cognition. Additionally, we propose Dual-Criteria Rejection Sampling(DCRS), a technique that generates a high-quality and diverse distillationdataset using multiple teacher models, making our method suitable forunsupervised scenarios. Experimental results demonstrate that TwT effectivelyreduces inference costs while preserving superior performance, achieving up toa 13.6% improvement in accuracy with fewer output tokens compared to otherdistillation methods, offering a highly practical solution for efficient LLMdeployment.
  </details>

- **[Implicit In-Context Learning: Evidence from Artificial Language Experiments](http://arxiv.org/abs/2503.24190v1)**  `arXiv:2503.24190`  
  _Xiaomeng Ma, Qihui Xu_
  <details><summary>Abstract</summary>
  Humans acquire language through implicit learning, absorbing complex patternswithout explicit awareness. While LLMs demonstrate impressive linguisticcapabilities, it remains unclear whether they exhibit human-like patternrecognition during in-context learning at inferencing level. We adapted threeclassic artificial language learning experiments spanning morphology,morphosyntax, and syntax to systematically evaluate implicit learning atinferencing level in two state-of-the-art OpenAI models: gpt-4o and o3-mini.Our results reveal linguistic domain-specific alignment between models andhuman behaviors, o3-mini aligns better in morphology while both models align insyntax.
  </details>

- **[Multi-Task Learning for Extracting Menstrual Characteristics from Clinical Notes](http://arxiv.org/abs/2503.24116v1)**  `arXiv:2503.24116`  
  _Anna Shopova, Cristoph Lippert, Leslee J. Shaw, Eugenia Alleva_
  <details><summary>Abstract</summary>
  Menstrual health is a critical yet often overlooked aspect of women'shealthcare. Despite its clinical relevance, detailed data on menstrualcharacteristics is rarely available in structured medical records. To addressthis gap, we propose a novel Natural Language Processing pipeline to extractkey menstrual cycle attributes -- dysmenorrhea, regularity, flow volume, andintermenstrual bleeding. Our approach utilizes the GatorTron model withMulti-Task Prompt-based Learning, enhanced by a hybrid retrieval preprocessingstep to identify relevant text segments. It out- performs baseline methods,achieving an average F1-score of 90% across all menstrual characteristics,despite being trained on fewer than 100 annotated clinical notes. The retrievalstep consistently improves performance across all approaches, allowing themodel to focus on the most relevant segments of lengthy clinical notes. Theseresults show that combining multi-task learning with retrieval improvesgeneralization and performance across menstrual charac- teristics, advancingautomated extraction from clinical notes and supporting women's healthresearch.
  </details>

- **[EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues](http://arxiv.org/abs/2503.21080v3)**  `arXiv:2503.21080`  
  _Yuhan Liu, Yunbo Long_
  <details><summary>Abstract</summary>
  While large language model (LLM)-based chatbots have been applied foreffective engagement in credit dialogues, their capacity for dynamic emotionalexpression remains limited. Current agents primarily rely on passive empathyrather than affective reasoning. For instance, when faced with persistentclient negativity, the agent should employ strategic emotional adaptation byexpressing measured anger to discourage counterproductive behavior and guidethe conversation toward resolution. This context-aware emotional modulation isessential for imitating the nuanced decision-making of human negotiators. Thispaper introduces an EQ-negotiator that combines emotion sensing frompre-trained language models (PLMs) with emotional reasoning based on GameTheory and Hidden Markov Models. It takes into account both the current andhistorical emotions of the client to better manage and address negativeemotions during interactions. By fine-tuning pre-trained language models (PLMs)on public emotion datasets and validating them on the credit dialogue datasets,our approach enables LLM-based agents to effectively capture shifts in clientemotions and dynamically adjust their response tone based on our emotiondecision policies in real-world financial negotiations. This EQ-negotiator canalso help credit agencies foster positive client relationships, enhancingsatisfaction in credit services.
  </details>

- **[Surgical Action Planning with Large Language Models](http://arxiv.org/abs/2503.18296v2)**  `arXiv:2503.18296`  
  _Mengya Xu, Zhongzhen Huang, Jie Zhang, Xiaofan Zhang, Qi Dou_
  <details><summary>Abstract</summary>
  In robot-assisted minimally invasive surgery, we introduce the SurgicalAction Planning (SAP) task, which generates future action plans from visualinputs to address the absence of intraoperative predictive planning in currentintelligent applications. SAP shows great potential for enhancingintraoperative guidance and automating procedures. However, it faces challengessuch as understanding instrument-action relationships and tracking surgicalprogress. Large Language Models (LLMs) show promise in understanding surgicalvideo content but remain underexplored for predictive decision-making in SAP,as they focus mainly on retrospective analysis. Challenges like data privacy,computational demands, and modality-specific constraints further highlightsignificant research gaps. To tackle these challenges, we introduce LLM-SAP, aLarge Language Models-based Surgical Action Planning framework that predictsfuture actions and generates text responses by interpreting natural languageprompts of surgical goals. The text responses potentially support surgicaleducation, intraoperative decision-making, procedure documentation, and skillanalysis. LLM-SAP integrates two novel modules: the Near-History Focus MemoryModule (NHF-MM) for modeling historical states and the prompts factory foraction planning. We evaluate LLM-SAP on our constructed CholecT50-SAP datasetusing models like Qwen2.5 and Qwen2-VL, demonstrating its effectiveness innext-action prediction. Pre-trained LLMs are tested in a zero-shot setting, andsupervised fine-tuning (SFT) with LoRA is implemented. Our experiments showthat Qwen2.5-72B-SFT surpasses Qwen2.5-72B with a 19.3% higher accuracy.
  </details>

- **[Concept Navigation and Classification via Open-Source Large Language Model Processing](http://arxiv.org/abs/2502.04756v2)**  `arXiv:2502.04756`  
  _Ma√´l Kubli_
  <details><summary>Abstract</summary>
  This paper presents a novel methodological framework for detecting andclassifying latent constructs, including frames, narratives, and topics, fromtextual data using Open-Source Large Language Models (LLMs). The proposedhybrid approach combines automated summarization with human-in-the-loopvalidation to enhance the accuracy and interpretability of constructidentification. By employing iterative sampling coupled with expert refinement,the framework guarantees methodological robustness and ensures conceptualprecision. Applied to diverse data sets, including AI policy debates, newspaperarticles on encryption, and the 20 Newsgroups data set, this approachdemonstrates its versatility in systematically analyzing complex politicaldiscourses, media framing, and topic classification tasks.
  </details>

- **[Evil twins are not that evil: Qualitative insights into machine-generated prompts](http://arxiv.org/abs/2412.08127v3)**  `arXiv:2412.08127`  
  _Nathana√´l Carraz Rakotonirina, Corentin Kervadec, Francesca Franzon, Marco Baroni_
  <details><summary>Abstract</summary>
  It has been widely observed that language models (LMs) respond in predictableways to algorithmically generated prompts that are seemingly unintelligible.This is both a sign that we lack a full understanding of how LMs work, and apractical challenge, because opaqueness can be exploited for harmful uses ofLMs, such as jailbreaking. We present the first thorough analysis of opaquemachine-generated prompts, or autoprompts, pertaining to 6 LMs of differentsizes and families. We find that machine-generated prompts are characterized bya last token that is often intelligible and strongly affects the generation. Asmall but consistent proportion of the previous tokens are prunable, probablyappearing in the prompt as a by-product of the fact that the optimizationprocess fixes the number of tokens. The remaining tokens fall into twocategories: filler tokens, which can be replaced with semantically unrelatedsubstitutes, and keywords, that tend to have at least a loose semantic relationwith the generation, although they do not engage in well-formed syntacticrelations with it. Additionally, human experts can reliably identify the mostinfluential tokens in an autoprompt a posteriori, suggesting these prompts arenot entirely opaque. Finally, some of the ablations we applied to autopromptsyield similar effects in natural language inputs, suggesting that autopromptsemerge naturally from the way LMs process linguistic inputs in general.
  </details>

- **[How Does A Text Preprocessing Pipeline Affect Ontology Syntactic Matching?](http://arxiv.org/abs/2411.03962v5)**  `arXiv:2411.03962`  
  _Zhangcheng Qiang, Kerry Taylor, Weiqing Wang_
  <details><summary>Abstract</summary>
  The classic text preprocessing pipeline, comprising Tokenisation,Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has beenimplemented in many systems for syntactic ontology matching (OM). However, thelack of standardisation in text preprocessing creates diversity in mappingresults. In this paper we investigate the effect of the text preprocessingpipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)tracks with 49 distinct alignments. We find that Phase 1 text preprocessing(Tokenisation and Normalisation) is more effective than Phase 2 textpreprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair theunwanted false mappings caused by Phase 2 text preprocessing, we propose anovel context-based pipeline repair approach that employs a post hoc check tofind common words that cause false mappings. These words are stored in areserved word set and applied in text preprocessing. The experimental resultsshow that our approach improves the matching correctness and the overallmatching performance. We then consider the broader integration of the classictext preprocessing pipeline with modern large language models (LLMs) for OM. Werecommend that (1) the text preprocessing pipeline be injected via functioncalling into LLMs to avoid the tendency towards unstable true mappings producedby LLM prompting; or (2) LLMs be used to repair non-existent andcounter-intuitive false mappings generated by the text preprocessing pipeline.
  </details>

- **[ScienceAgentBench: Toward Rigorous Assessment of Language Agents for Data-Driven Scientific Discovery](http://arxiv.org/abs/2410.05080v3)**  `arXiv:2410.05080`  
  _Ziru Chen, Shijie Chen, Yuting Ning, Qianheng Zhang, Boshi Wang, Botao Yu, et al._
  <details><summary>Abstract</summary>
  The advancements of large language models (LLMs) have piqued growing interestin developing LLM-based language agents to automate scientific discoveryend-to-end, which has sparked both excitement and skepticism about their truecapabilities. In this work, we call for rigorous assessment of agents onindividual tasks in a scientific workflow before making bold claims onend-to-end automation. To this end, we present ScienceAgentBench, a newbenchmark for evaluating language agents for data-driven scientific discovery.To ensure the scientific authenticity and real-world relevance of ourbenchmark, we extract 102 tasks from 44 peer-reviewed publications in fourdisciplines and engage nine subject matter experts to validate them. We unifythe target output for every task to a self-contained Python program file andemploy an array of evaluation metrics to examine the generated programs,execution results, and costs. Each task goes through multiple rounds of manualvalidation by annotators and subject matter experts to ensure its annotationquality and scientific plausibility. We also propose two effective strategiesto mitigate data contamination concerns. Using ScienceAgentBench, we evaluatefive open-weight and proprietary LLMs, each with three frameworks: directprompting, OpenHands CodeAct, and self-debug. Given three attempts for eachtask, the best-performing agent can only solve 32.4% of the tasks independentlyand 34.3% with expert-provided knowledge. In addition, we evaluate OpenAIo1-preview with direct prompting and self-debug, which can boost theperformance to 42.2%, demonstrating the effectiveness of increasinginference-time compute but with more than 10 times the cost of other LLMs.Still, our results underscore the limitations of current language agents ingenerating code for data-driven discovery, let alone end-to-end automation forscientific research.
  </details>

- **[Cascade Reward Sampling for Efficient Decoding-Time Alignment](http://arxiv.org/abs/2406.16306v2)**  `arXiv:2406.16306`  
  _Bolian Li, Yifan Wang, Anamika Lochab, Ananth Grama, Ruqi Zhang_
  <details><summary>Abstract</summary>
  Aligning large language models (LLMs) with human preferences is essential fortheir applications. Recently, decoding-time alignment has emerged as aneffective plug-and-play technique that avoids fine-tuning model parameters.This approach retains the general utility of pretrained LLMs but often suffersfrom significant inefficiencies during decoding, primarily due to wasted tokengeneration and excessive reward evaluations. To address these challenges, weintroduce Cascade Reward Sampling (CARDS) to resolve both efficiencybottlenecks in decoding-time alignment. Specifically, we develop asegment-level rejection sampling algorithm that minimizes redundantcomputations of both LLMs and reward models (RMs). Central to CARDS is anuncertainty-based segmentation mechanism, which ensures the accuracy of RMsevaluations on incomplete segments. Furthermore, we provide a detailed analysisof reward scores on segments to elucidate the improved alignment performance.Experimental results demonstrate that CARDS significantly improves decodingefficiency, alignment quality, and general utility compared to existingdecoding-time alignment methods, achieving approximately a 70% reduction indecoding time and over 90% win-ties in utility and safety benchmarks.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Computer Vision and Pattern Recognition üì∏

<details open><summary>Click to Collapse</summary>

- **[Easi3R: Estimating Disentangled Motion from DUSt3R Without Training](http://arxiv.org/abs/2503.24391v1)**  `arXiv:2503.24391`  
  _Xingyu Chen, Yue Chen, Yuliang Xiu, Andreas Geiger, Anpei Chen_
  <details><summary>Abstract</summary>
  Recent advances in DUSt3R have enabled robust estimation of dense pointclouds and camera parameters of static scenes, leveraging Transformer networkarchitectures and direct supervision on large-scale 3D datasets. In contrast,the limited scale and diversity of available 4D datasets present a majorbottleneck for training a highly generalizable 4D model. This constraint hasdriven conventional 4D methods to fine-tune 3D models on scalable dynamic videodata with additional geometric priors such as optical flow and depths. In thiswork, we take an opposite path and introduce Easi3R, a simple yet efficienttraining-free method for 4D reconstruction. Our approach applies attentionadaptation during inference, eliminating the need for from-scratch pre-trainingor network fine-tuning. We find that the attention layers in DUSt3R inherentlyencode rich information about camera and object motion. By carefullydisentangling these attention maps, we achieve accurate dynamic regionsegmentation, camera pose estimation, and 4D dense point map reconstruction.Extensive experiments on real-world dynamic videos demonstrate that ourlightweight attention adaptation significantly outperforms previousstate-of-the-art methods that are trained or finetuned on extensive dynamicdatasets. Our code is publicly available for research purpose athttps://easi3r.github.io/
  </details>

- **[SU-YOLO: Spiking Neural Network for Efficient Underwater Object Detection](http://arxiv.org/abs/2503.24389v1)**  `arXiv:2503.24389`  
  _Chenyang Li, Wenxuan Liu, Guoqiang Gong, Xiaobo Ding, Xian Zhong_
  <details><summary>Abstract</summary>
  Underwater object detection is critical for oceanic research and industrialsafety inspections. However, the complex optical environment and the limitedresources of underwater equipment pose significant challenges to achieving highaccuracy and low power consumption. To address these issues, we propose SpikingUnderwater YOLO (SU-YOLO), a Spiking Neural Network (SNN) model. Leveraging thelightweight and energy-efficient properties of SNNs, SU-YOLO incorporates anovel spike-based underwater image denoising method based solely on integeraddition, which enhances the quality of feature maps with minimal computationaloverhead. In addition, we introduce Separated Batch Normalization (SeBN), atechnique that normalizes feature maps independently across multiple time stepsand is optimized for integration with residual structures to capture thetemporal dynamics of SNNs more effectively. The redesigned spiking residualblocks integrate the Cross Stage Partial Network (CSPNet) with the YOLOarchitecture to mitigate spike degradation and enhance the model's featureextraction capabilities. Experimental results on URPC2019 underwater datasetdemonstrate that SU-YOLO achieves mAP of 78.8% with 6.97M parameters and anenergy consumption of 2.98 mJ, surpassing mainstream SNN models in bothdetection accuracy and computational efficiency. These results underscore thepotential of SNNs for engineering applications. The code is available inhttps://github.com/lwxfight/snn-underwater.
  </details>

- **[Consistent Subject Generation via Contrastive Instantiated Concepts](http://arxiv.org/abs/2503.24387v1)**  `arXiv:2503.24387`  
  _Lee Hsin-Ying, Kelvin C. K. Chan, Ming-Hsuan Yang_
  <details><summary>Abstract</summary>
  While text-to-image generative models can synthesize diverse and faithfulcontents, subject variation across multiple creations limits the application inlong content generation. Existing approaches require time-consuming tuning,references for all subjects, or access to other creations. We introduceContrastive Concept Instantiation (CoCoIns) to effectively synthesizeconsistent subjects across multiple independent creations. The frameworkconsists of a generative model and a mapping network, which transforms inputlatent codes into pseudo-words associated with certain instances of concepts.Users can generate consistent subjects with the same latent codes. To constructsuch associations, we propose a contrastive learning approach that trains thenetwork to differentiate the combination of prompts and latent codes. Extensiveevaluations of human faces with a single subject show that CoCoIns performscomparably to existing methods while maintaining higher flexibility. We alsodemonstrate the potential of extending CoCoIns to multiple subjects and otherobject categories.
  </details>

- **[Free360: Layered Gaussian Splatting for Unbounded 360-Degree View Synthesis from Extremely Sparse and Unposed Views](http://arxiv.org/abs/2503.24382v1)**  `arXiv:2503.24382`  
  _Chong Bao, Xiyu Zhang, Zehao Yu, Jiale Shi, Guofeng Zhang, Songyou Peng, et al._
  <details><summary>Abstract</summary>
  Neural rendering has demonstrated remarkable success in high-quality 3Dneural reconstruction and novel view synthesis with dense input views andaccurate poses. However, applying it to extremely sparse, unposed views inunbounded 360{\deg} scenes remains a challenging problem. In this paper, wepropose a novel neural rendering framework to accomplish the unposed andextremely sparse-view 3D reconstruction in unbounded 360{\deg} scenes. Toresolve the spatial ambiguity inherent in unbounded scenes with sparse inputviews, we propose a layered Gaussian-based representation to effectively modelthe scene with distinct spatial layers. By employing a dense stereoreconstruction model to recover coarse geometry, we introduce a layer-specificbootstrap optimization to refine the noise and fill occluded regions in thereconstruction. Furthermore, we propose an iterative fusion of reconstructionand generation alongside an uncertainty-aware training approach to facilitatemutual conditioning and enhancement between these two processes. Comprehensiveexperiments show that our approach outperforms existing state-of-the-artmethods in terms of rendering quality and surface reconstruction accuracy.Project page: https://zju3dv.github.io/free360/
  </details>

- **[UniOcc: A Unified Benchmark for Occupancy Forecasting and Prediction in Autonomous Driving](http://arxiv.org/abs/2503.24381v1)**  `arXiv:2503.24381`  
  _Yuping Wang, Xiangyu Huang, Xiaokang Sun, Mingxuan Yan, Shuo Xing, Zhengzhong Tu, et al._
  <details><summary>Abstract</summary>
  We introduce UniOcc, a comprehensive, unified benchmark for occupancyforecasting (i.e., predicting future occupancies based on historicalinformation) and current-frame occupancy prediction from camera images. UniOccunifies data from multiple real-world datasets (i.e., nuScenes, Waymo) andhigh-fidelity driving simulators (i.e., CARLA, OpenCOOD), which provides 2D/3Doccupancy labels with per-voxel flow annotations and support for cooperativeautonomous driving. In terms of evaluation, unlike existing studies that relyon suboptimal pseudo labels for evaluation, UniOcc incorporates novel metricsthat do not depend on ground-truth occupancy, enabling robust assessment ofadditional aspects of occupancy quality. Through extensive experiments onstate-of-the-art models, we demonstrate that large-scale, diverse training dataand explicit flow information significantly enhance occupancy prediction andforecasting performance.
  </details>

- **[Any2Caption:Interpreting Any Condition to Caption for Controllable Video Generation](http://arxiv.org/abs/2503.24379v1)**  `arXiv:2503.24379`  
  _Shengqiong Wu, Weicai Ye, Jiahao Wang, Quande Liu, Xintao Wang, Pengfei Wan, et al._
  <details><summary>Abstract</summary>
  To address the bottleneck of accurate user intent interpretation within thecurrent video generation community, we present Any2Caption, a novel frameworkfor controllable video generation under any condition. The key idea is todecouple various condition interpretation steps from the video synthesis step.By leveraging modern multimodal large language models (MLLMs), Any2Captioninterprets diverse inputs--text, images, videos, and specialized cues such asregion, motion, and camera poses--into dense, structured captions that offerbackbone video generators with better guidance. We also introduce Any2CapIns, alarge-scale dataset with 337K instances and 407K conditions forany-condition-to-caption instruction tuning. Comprehensive evaluationsdemonstrate significant improvements of our system in controllability and videoquality across various aspects of existing video generation models. ProjectPage: https://sqwu.top/Any2Cap/
  </details>

- **[Exploring the Effect of Reinforcement Learning on Video Understanding: Insights from SEED-Bench-R1](http://arxiv.org/abs/2503.24376v1)**  `arXiv:2503.24376`  
  _Yi Chen, Yuying Ge, Rui Wang, Yixiao Ge, Lu Qiu, Ying Shan, et al._
  <details><summary>Abstract</summary>
  Recent advancements in Chain of Thought (COT) generation have significantlyimproved the reasoning capabilities of Large Language Models (LLMs), withreinforcement learning (RL) emerging as an effective post-training approach.Multimodal Large Language Models (MLLMs) inherit this reasoning potential butremain underexplored in tasks requiring both perception and logical reasoning.To address this, we introduce SEED-Bench-R1, a benchmark designed tosystematically evaluate post-training methods for MLLMs in video understanding.It includes intricate real-world videos and complex everyday planning tasks inthe format of multiple-choice questions, requiring sophisticated perception andreasoning. SEED-Bench-R1 assesses generalization through a three-levelhierarchy: in-distribution, cross-environment, and cross-environment-taskscenarios, equipped with a large-scale training dataset with easily verifiableground-truth answers. Using Qwen2-VL-Instruct-7B as a base model, we compare RLwith supervised fine-tuning (SFT), demonstrating RL's data efficiency andsuperior performance on both in-distribution and out-of-distribution tasks,even outperforming SFT on general video understanding benchmarks likeLongVideoBench. Our detailed analysis reveals that RL enhances visualperception but often produces less logically coherent reasoning chains. Weidentify key limitations such as inconsistent reasoning and overlooked visualcues, and suggest future improvements in base model reasoning, reward modeling,and RL robustness against noisy signals.
  </details>

- **[ERUPT: Efficient Rendering with Unposed Patch Transformer](http://arxiv.org/abs/2503.24374v1)**  `arXiv:2503.24374`  
  _Maxim V. Shugaev, Vincent Chen, Maxim Karrenbach, Kyle Ashley, Bridget Kennedy, Naresh P. Cuntoor_
  <details><summary>Abstract</summary>
  This work addresses the problem of novel view synthesis in diverse scenesfrom small collections of RGB images. We propose ERUPT (Efficient Renderingwith Unposed Patch Transformer) a state-of-the-art scene reconstruction modelcapable of efficient scene rendering using unposed imagery. We introducepatch-based querying, in contrast to existing pixel-based queries, to reducethe compute required to render a target view. This makes our model highlyefficient both during training and at inference, capable of rendering at 600fps on commercial hardware. Notably, our model is designed to use a learnedlatent camera pose which allows for training using unposed targets in datasetswith sparse or inaccurate ground truth camera pose. We show that our approachcan generalize on large real-world data and introduce a new benchmark dataset(MSVS-1M) for latent view synthesis using street-view imagery collected fromMapillary. In contrast to NeRF and Gaussian Splatting, which require denseimagery and precise metadata, ERUPT can render novel views of arbitrary sceneswith as few as five unposed input images. ERUPT achieves better rendered imagequality than current state-of-the-art methods for unposed image synthesistasks, reduces labeled data requirements by ~95\% and decreases computationalrequirements by an order of magnitude, providing efficient novel view synthesisfor diverse real-world scenes.
  </details>

- **[Adapting Vision Foundation Models for Real-time Ultrasound Image Segmentation](http://arxiv.org/abs/2503.24368v1)**  `arXiv:2503.24368`  
  _Xiaoran Zhang, Eric Z. Chen, Lin Zhao, Xiao Chen, Yikang Liu, Boris Maihe, et al._
  <details><summary>Abstract</summary>
  We propose a novel approach that adapts hierarchical vision foundation modelsfor real-time ultrasound image segmentation. Existing ultrasound segmentationmethods often struggle with adaptability to new tasks, relying on costly manualannotations, while real-time approaches generally fail to matchstate-of-the-art performance. To overcome these limitations, we introduce anadaptive framework that leverages the vision foundation model Hiera to extractmulti-scale features, interleaved with DINOv2 representations to enhance visualexpressiveness. These enriched features are then decoded to produce precise androbust segmentation. We conduct extensive evaluations on six public datasetsand one in-house dataset, covering both cardiac and thyroid ultrasoundsegmentation. Experiments show that our approach outperforms state-of-the-artmethods across multiple datasets and excels with limited supervision,surpassing nnUNet by over 20\% on average in the 1\% and 10\% data settings.Our method achieves $\sim$77 FPS inference speed with TensorRT on a single GPU,enabling real-time clinical applications.
  </details>

- **[StochasticSplats: Stochastic Rasterization for Sorting-Free 3D Gaussian Splatting](http://arxiv.org/abs/2503.24366v1)**  `arXiv:2503.24366`  
  _Shakiba Kheradmand, Delio Vicini, George Kopanas, Dmitry Lagun, Kwang Moo Yi, Mark Matthews, et al._
  <details><summary>Abstract</summary>
  3D Gaussian splatting (3DGS) is a popular radiance field method, with manyapplication-specific extensions. Most variants rely on the same core algorithm:depth-sorting of Gaussian splats then rasterizing in primitive order. Thisensures correct alpha compositing, but can cause rendering artifacts due tobuilt-in approximations. Moreover, for a fixed representation, sorted renderingoffers little control over render cost and visual fidelity. For example, andcounter-intuitively, rendering a lower-resolution image is not necessarilyfaster. In this work, we address the above limitations by combining 3D Gaussiansplatting with stochastic rasterization. Concretely, we leverage an unbiasedMonte Carlo estimator of the volume rendering equation. This removes the needfor sorting, and allows for accurate 3D blending of overlapping Gaussians. Thenumber of Monte Carlo samples further imbues 3DGS with a way to trade offcomputation time and quality. We implement our method using OpenGL shaders,enabling efficient rendering on modern GPU hardware. At a reasonable visualquality, our method renders more than four times faster than sortedrasterization.
  </details>

- **[InstructRestore: Region-Customized Image Restoration with Human Instructions](http://arxiv.org/abs/2503.24357v1)**  `arXiv:2503.24357`  
  _Shuaizheng Liu, Jianqi Ma, Lingchen Sun, Xiangtao Kong, Lei Zhang_
  <details><summary>Abstract</summary>
  Despite the significant progress in diffusion prior-based image restoration,most existing methods apply uniform processing to the entire image, lacking thecapability to perform region-customized image restoration according to userinstructions. In this work, we propose a new framework, namely InstructRestore,to perform region-adjustable image restoration following human instructions. Toachieve this, we first develop a data generation engine to produce trainingtriplets, each consisting of a high-quality image, the target regiondescription, and the corresponding region mask. With this engine and carefuldata screening, we construct a comprehensive dataset comprising 536,945triplets to support the training and evaluation of this task. We then examinehow to integrate the low-quality image features under the ControlNetarchitecture to adjust the degree of image details enhancement. Consequently,we develop a ControlNet-like model to identify the target region and allocatedifferent integration scales to the target and surrounding regions, enablingregion-customized image restoration that aligns with user instructions.Experimental results demonstrate that our proposed InstructRestore approachenables effective human-instructed image restoration, such as images with bokeheffects and user-instructed local enhancement. Our work advances theinvestigation of interactive image restoration and enhancement techniques.Data, code, and models will be found athttps://github.com/shuaizhengliu/InstructRestore.git.
  </details>

- **[PathOrchestra: A Comprehensive Foundation Model for Computational Pathology with Over 100 Diverse Clinical-Grade Tasks](http://arxiv.org/abs/2503.24345v1)**  `arXiv:2503.24345`  
  _Fang Yan, Jianfeng Wu, Jiawen Li, Wei Wang, Jiaxuan Lu, Wen Chen, et al._
  <details><summary>Abstract</summary>
  The complexity and variability inherent in high-resolution pathologicalimages present significant challenges in computational pathology. Whilepathology foundation models leveraging AI have catalyzed transformativeadvancements, their development demands large-scale datasets, considerablestorage capacity, and substantial computational resources. Furthermore,ensuring their clinical applicability and generalizability requires rigorousvalidation across a broad spectrum of clinical tasks. Here, we presentPathOrchestra, a versatile pathology foundation model trained viaself-supervised learning on a dataset comprising 300K pathological slides from20 tissue and organ types across multiple centers. The model was rigorouslyevaluated on 112 clinical tasks using a combination of 61 private and 51 publicdatasets. These tasks encompass digital slide preprocessing, pan-cancerclassification, lesion identification, multi-cancer subtype classification,biomarker assessment, gene expression prediction, and the generation ofstructured reports. PathOrchestra demonstrated exceptional performance across27,755 WSIs and 9,415,729 ROIs, achieving over 0.950 accuracy in 47 tasks,including pan-cancer classification across various organs, lymphoma subtypediagnosis, and bladder cancer screening. Notably, it is the first model togenerate structured reports for high-incidence colorectal cancer anddiagnostically complex lymphoma-areas that are infrequently addressed byfoundational models but hold immense clinical potential. Overall, PathOrchestraexemplifies the feasibility and efficacy of a large-scale, self-supervisedpathology foundation model, validated across a broad range of clinical-gradetasks. Its high accuracy and reduced reliance on extensive data annotationunderline its potential for clinical integration, offering a pathway towardmore efficient and high-quality medical services.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Machine Learning üìä

<details open><summary>Click to Collapse</summary>

- **[Effectively Controlling Reasoning Models through Thinking Intervention](http://arxiv.org/abs/2503.24370v1)**  `arXiv:2503.24370`  
  _Tong Wu, Chong Xiang, Jiachen T. Wang, Prateek Mittal_
  <details><summary>Abstract</summary>
  Reasoning-enhanced large language models (LLMs) explicitly generateintermediate reasoning steps prior to generating final answers, helping themodel excel in complex problem-solving. In this paper, we demonstrate that thisemerging generation framework offers a unique opportunity for more fine-grainedcontrol over model behavior. We propose Thinking Intervention, a novel paradigmdesigned to explicitly guide the internal reasoning processes of LLMs bystrategically inserting or revising specific thinking tokens. We conductcomprehensive evaluations across multiple tasks, including instructionfollowing on IFEval, instruction hierarchy on SEP, and safety alignment onXSTest and SORRY-Bench. Our results demonstrate that Thinking Interventionsignificantly outperforms baseline prompting approaches, achieving up to 6.7%accuracy gains in instruction-following scenarios, 15.4% improvements inreasoning about instruction hierarchies, and a 40.0% increase in refusal ratesfor unsafe prompts using open-source DeepSeek R1 models. Overall, our workopens a promising new research avenue for controlling reasoning LLMs.
  </details>

- **[Which LIME should I trust? Concepts, Challenges, and Solutions](http://arxiv.org/abs/2503.24365v1)**  `arXiv:2503.24365`  
  _Patrick Knab, Sascha Marton, Udo Schlegel, Christian Bartelt_
  <details><summary>Abstract</summary>
  As neural networks become dominant in essential systems, ExplainableArtificial Intelligence (XAI) plays a crucial role in fostering trust anddetecting potential misbehavior of opaque models. LIME (Local InterpretableModel-agnostic Explanations) is among the most prominent model-agnosticapproaches, generating explanations by approximating the behavior of black-boxmodels around specific instances. Despite its popularity, LIME faces challengesrelated to fidelity, stability, and applicability to domain-specific problems.Numerous adaptations and enhancements have been proposed to address theseissues, but the growing number of developments can be overwhelming,complicating efforts to navigate LIME-related research. To the best of ourknowledge, this is the first survey to comprehensively explore and collectLIME's foundational concepts and known limitations. We categorize and compareits various enhancements, offering a structured taxonomy based on intermediatesteps and key issues. Our analysis provides a holistic overview of advancementsin LIME, guiding future research and helping practitioners identify suitableapproaches. Additionally, we provide a continuously updated interactive website(https://patrick-knab.github.io/which-lime-to-trust/), offering a concise andaccessible overview of the survey.
  </details>

- **[SQuat: Subspace-orthogonal KV Cache Quantization](http://arxiv.org/abs/2503.24358v1)**  `arXiv:2503.24358`  
  _Hao Wang, Ligong Han, Kai Xu, Akash Srivastava_
  <details><summary>Abstract</summary>
  The key-value (KV) cache accelerates LLMs decoding by storing KV tensors frompreviously generated tokens. It reduces redundant computation at the cost ofincreased memory usage. To mitigate this overhead, existing approaches compressKV tensors into lower-bit representations; however, quantization errors canaccumulate as more tokens are generated, potentially resulting in undesiredoutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cachequantization). It first constructs a subspace spanned by query tensors tocapture the most critical task-related information. During key tensorquantization, it enforces that the difference between the (de)quantized andoriginal keys remains orthogonal to this subspace, minimizing the impact ofquantization errors on the attention mechanism's outputs. SQuat requires nomodel fine-tuning, no additional calibration dataset for offline learning, andis grounded in a theoretical framework we develop. Through numericalexperiments, we show that our method reduces peak memory by 2.17 to 2.82,improves throughput by 2.45 to 3.60, and achieves more favorable benchmarkscores than existing KV cache quantization algorithms.
  </details>

- **[ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion](http://arxiv.org/abs/2503.24354v1)**  `arXiv:2503.24354`  
  _Rana Muhammad Shahroz Khan, Dongwen Tang, Pingzhi Li, Kai Wang, Tianlong Chen_
  <details><summary>Abstract</summary>
  Parameter generation has emerged as a novel paradigm for neural networkdevelopment, offering an alternative to traditional neural network training bysynthesizing high-quality model weights directly. In the context of Low-RankAdaptation (LoRA) for evolving ($\textit{i.e.}$, constantly updated) largelanguage models (LLMs), this approach promises efficient adaptation withoutcostly retraining. However, existing methods face critical limitations insimultaneously achieving scalability and controllability. In this paper, weintroduce $\texttt{ORAL}$, a novel $\textbf{conditional recurrent diffusion}$framework that addresses these challenges. $\texttt{ORAL}$ incorporates a novelconditioning mechanism that integrates model architecture and textual taskspecifications, enabling the generation of task-specific LoRA parameters thatcan seamlessly transfer across evolving foundation models. Our approachsuccessfully scales to billions-of-parameter LLMs and maintainscontrollability. Through extensive experiments across seven language tasks,four vision tasks, and three multimodal tasks using five pre-trained LLMs, wedemonstrate that $\texttt{ORAL}$ generates high-quality LoRA parameters thatachieve comparable or superior performance to vanilla trained counterparts.
  </details>

- **[NoProp: Training Neural Networks without Back-propagation or Forward-propagation](http://arxiv.org/abs/2503.24322v1)**  `arXiv:2503.24322`  
  _Qinyu Li, Yee Whye Teh, Razvan Pascanu_
  <details><summary>Abstract</summary>
  The canonical deep learning approach for learning requires computing agradient term at each layer by back-propagating the error signal from theoutput towards each learnable parameter. Given the stacked structure of neuralnetworks, where each layer builds on the representation of the layer below,this approach leads to hierarchical representations. More abstract featureslive on the top layers of the model, while features on lower layers areexpected to be less abstract. In contrast to this, we introduce a new learningmethod named NoProp, which does not rely on either forward or backwardspropagation. Instead, NoProp takes inspiration from diffusion and flow matchingmethods, where each layer independently learns to denoise a noisy target. Webelieve this work takes a first step towards introducing a new family ofgradient-free learning methods, that does not learn hierarchicalrepresentations -- at least not in the usual sense. NoProp needs to fix therepresentation at each layer beforehand to a noised version of the target,learning a local denoising process that can then be exploited at inference. Wedemonstrate the effectiveness of our method on MNIST, CIFAR-10, and CIFAR-100image classification benchmarks. Our results show that NoProp is a viablelearning algorithm which achieves superior accuracy, is easier to use andcomputationally more efficient compared to other existing back-propagation-freemethods. By departing from the traditional gradient based learning paradigm,NoProp alters how credit assignment is done within the network, enabling moreefficient distributed learning as well as potentially impacting othercharacteristics of the learning process.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Multiagent Systems üåê

<details open><summary>Click to Collapse</summary>

- **[A Constrained Multi-Agent Reinforcement Learning Approach to Autonomous Traffic Signal Control](http://arxiv.org/abs/2503.23626v1)**  `arXiv:2503.23626`  
  _Anirudh Satheesh, Keenan Powell_
  <details><summary>Abstract</summary>
  Traffic congestion in modern cities is exacerbated by the limitations oftraditional fixed-time traffic signal systems, which fail to adapt to dynamictraffic patterns. Adaptive Traffic Signal Control (ATSC) algorithms haveemerged as a solution by dynamically adjusting signal timing based on real-timetraffic conditions. However, the main limitation of such methods is that theyare not transferable to environments under real-world constraints, such asbalancing efficiency, minimizing collisions, and ensuring fairness acrossintersections. In this paper, we view the ATSC problem as a constrainedmulti-agent reinforcement learning (MARL) problem and propose a novel algorithmnamed Multi-Agent Proximal Policy Optimization with Lagrange Cost Estimator(MAPPO-LCE) to produce effective traffic signal control policies. Our approachintegrates the Lagrange multipliers method to balance rewards and constraints,with a cost estimator for stable adjustment. We also introduce threeconstraints on the traffic network: GreenTime, GreenSkip, and PhaseSkip, whichpenalize traffic policies that do not conform to real-world scenarios. Ourexperimental results on three real-world datasets demonstrate that MAPPO-LCEoutperforms three baseline MARL algorithms by across all environments andtraffic constraints (improving on MAPPO by 12.60%, IPPO by 10.29%, and QTRAN by13.10%). Our results show that constrained MARL is a valuable tool for trafficplanners to deploy scalable and efficient ATSC methods in real-world trafficnetworks. We provide code at https://github.com/Asatheesh6561/MAPPO-LCE.
  </details>

- **[Teams of LLM Agents can Exploit Zero-Day Vulnerabilities](http://arxiv.org/abs/2406.01637v2)**  `arXiv:2406.01637`  
  _Yuxuan Zhu, Antony Kellermann, Akul Gupta, Philip Li, Richard Fang, Rohan Bindu, et al._
  <details><summary>Abstract</summary>
  LLM agents have become increasingly sophisticated, especially in the realm ofcybersecurity. Researchers have shown that LLM agents can exploit real-worldvulnerabilities when given a description of the vulnerability and toycapture-the-flag problems. However, these agents still perform poorly onreal-world vulnerabilities that are unknown to the agent ahead of time(zero-day vulnerabilities).  In this work, we show that teams of LLM agents can exploit real-world,zero-day vulnerabilities. Prior agents struggle with exploring many differentvulnerabilities and long-range planning when used alone. To resolve this, weintroduce HPTSA, a system of agents with a planning agent that can launchsubagents. The planning agent explores the system and determines whichsubagents to call, resolving long-term planning issues when trying differentvulnerabilities. We construct a benchmark of 14 real-world vulnerabilities andshow that our team of agents improve over prior agent frameworks by up to 4.3X.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Robotics ü§ñ

<details open><summary>Click to Collapse</summary>

- **[Sim-and-Real Co-Training: A Simple Recipe for Vision-Based Robotic Manipulation](http://arxiv.org/abs/2503.24361v1)**  `arXiv:2503.24361`  
  _Abhiram Maddukuri, Zhenyu Jiang, Lawrence Yunliang Chen, Soroush Nasiriany, Yuqi Xie, Yu Fang, et al._
  <details><summary>Abstract</summary>
  Large real-world robot datasets hold great potential to train generalistrobot models, but scaling real-world human data collection is time-consumingand resource-intensive. Simulation has great potential in supplementinglarge-scale data, especially with recent advances in generative AI andautomated data generation tools that enable scalable creation of robot behaviordatasets. However, training a policy solely in simulation and transferring itto the real world often demands substantial human effort to bridge the realitygap. A compelling alternative is to co-train the policy on a mixture ofsimulation and real-world datasets. Preliminary studies have recently shownthis strategy to substantially improve the performance of a policy over onetrained on a limited amount of real-world data. Nonetheless, the communitylacks a systematic understanding of sim-and-real co-training and what it takesto reap the benefits of simulation data for real-robot learning. This workpresents a simple yet effective recipe for utilizing simulation data to solvevision-based robotic manipulation tasks. We derive this recipe fromcomprehensive experiments that validate the co-training strategy on varioussimulation and real-world datasets. Using two domains--a robot arm and ahumanoid--across diverse tasks, we demonstrate that simulation data can enhancereal-world task performance by an average of 38%, even with notable differencesbetween the simulation and real-world data. Videos and additional results canbe found at https://co-training.github.io/
  </details>

- **[Pro-Routing: Proactive Routing of Autonomous Multi-Capacity Robots for Pickup-and-Delivery Tasks](http://arxiv.org/abs/2503.24325v1)**  `arXiv:2503.24325`  
  _Daniel Garces, Stephanie Gil_
  <details><summary>Abstract</summary>
  We consider a multi-robot setting, where we have a fleet of multi-capacityautonomous robots that must service spatially distributed pickup-and-deliveryrequests with fixed maximum wait times. Requests can be either scheduled aheadof time or they can enter the system in real-time. In this setting, stabilityfor a routing policy is defined as the cost of the policy being uniformlybounded over time. Most previous work either solve the problem offline totheoretically maintain stability or they consider dynamically arriving requestsat the expense of the theoretical guarantees on stability. In this paper, weaim to bridge this gap by proposing a novel proactive rollout-based routingframework that adapts to real-time demand while still provably maintaining thestability of the learned routing policy. We derive provable stabilityguarantees for our method by proposing a fleet sizing algorithm that obtains asufficiently large fleet that ensures stability by construction. To validateour theoretical results, we consider a case study on real ride requests forHarvard's evening Van System. We also evaluate the performance of our frameworkusing the currently deployed smaller fleet size. In this smaller setup, wecompare against the currently deployed routing algorithm, greedy heuristics,and Monte-Carlo-Tree-Search-based algorithms. Our empirical results show thatour framework maintains stability when we use the sufficiently large fleet sizefound in our theoretical results. For the smaller currently deployed fleetsize, our method services 6% more requests than the closest baseline whilereducing median passenger wait times by 33%.
  </details>

- **[AutoEval: Autonomous Evaluation of Generalist Robot Manipulation Policies in the Real World](http://arxiv.org/abs/2503.24278v1)**  `arXiv:2503.24278`  
  _Zhiyuan Zhou, Pranav Atreya, You Liang Tan, Karl Pertsch, Sergey Levine_
  <details><summary>Abstract</summary>
  Scalable and reproducible policy evaluation has been a long-standingchallenge in robot learning. Evaluations are critical to assess progress andbuild better policies, but evaluation in the real world, especially at a scalethat would provide statistically reliable results, is costly in terms of humantime and hard to obtain. Evaluation of increasingly generalist robot policiesrequires an increasingly diverse repertoire of evaluation environments, makingthe evaluation bottleneck even more pronounced. To make real-world evaluationof robotic policies more practical, we propose AutoEval, a system toautonomously evaluate generalist robot policies around the clock with minimalhuman intervention. Users interact with AutoEval by submitting evaluation jobsto the AutoEval queue, much like how software jobs are submitted with a clusterscheduling system, and AutoEval will schedule the policies for evaluationwithin a framework supplying automatic success detection and automatic sceneresets. We show that AutoEval can nearly fully eliminate human involvement inthe evaluation process, permitting around the clock evaluations, and theevaluation results correspond closely to ground truth evaluations conducted byhand. To facilitate the evaluation of generalist policies in the roboticscommunity, we provide public access to multiple AutoEval scenes in the popularBridgeData robot setup with WidowX robot arms. In the future, we hope thatAutoEval scenes can be set up across institutions to form a diverse anddistributed evaluation network.
  </details>

- **[Pseudo-Random UAV Test Generation Using Low-Fidelity Path Simulator](http://arxiv.org/abs/2503.24172v1)**  `arXiv:2503.24172`  
  _Anas Shrinah, Kerstin Eder_
  <details><summary>Abstract</summary>
  Simulation-based testing provides a safe and cost-effective environment forverifying the safety of Uncrewed Aerial Vehicles (UAVs). However, simulationcan be resource-consuming, especially when High-Fidelity Simulators (HFS) areused. To optimise simulation resources, we propose a pseudo-random testgenerator that uses a Low-Fidelity Simulator (LFS) to estimate UAV flightpaths. This work simplifies the PX4 autopilot HFS to develop a LFS, whichoperates one order of magnitude faster than the HFS.Test cases predicted tocause safety violations in the LFS are subsequently validated using the HFS.
  </details>

- **[HACTS: a Human-As-Copilot Teleoperation System for Robot Learning](http://arxiv.org/abs/2503.24070v1)**  `arXiv:2503.24070`  
  _Zhiyuan Xu, Yinuo Zhao, Kun Wu, Ning Liu, Junjie Ji, Zhengping Che, et al._
  <details><summary>Abstract</summary>
  Teleoperation is essential for autonomous robot learning, especially inmanipulation tasks that require human demonstrations or corrections. However,most existing systems only offer unilateral robot control and lack the abilityto synchronize the robot's status with the teleoperation hardware, preventingreal-time, flexible intervention. In this work, we introduce HACTS(Human-As-Copilot Teleoperation System), a novel system that establishesbilateral, real-time joint synchronization between a robot arm andteleoperation hardware. This simple yet effective feedback mechanism, akin to asteering wheel in autonomous vehicles, enables the human copilot to interveneseamlessly while collecting action-correction data for future learning.Implemented using 3D-printed components and low-cost, off-the-shelf motors,HACTS is both accessible and scalable. Our experiments show that HACTSsignificantly enhances performance in imitation learning (IL) and reinforcementlearning (RL) tasks, boosting IL recovery capabilities and data efficiency, andfacilitating human-in-the-loop RL. HACTS paves the way for more effective andinteractive human-robot collaboration and data-collection, advancing thecapabilities of robot manipulation.
  </details>

- **[Toward Anxiety-Reducing Pocket Robots for Children](http://arxiv.org/abs/2503.24041v1)**  `arXiv:2503.24041`  
  _Morten Roed Frederiksen, Kasper St√∏y, Maja Matariƒá_
  <details><summary>Abstract</summary>
  A common denominator for most therapy treatments for children who suffer froman anxiety disorder is daily practice routines to learn techniques needed toovercome anxiety. However, applying those techniques while experiencing anxietycan be highly challenging. This paper presents the design, implementation, andpilot study of a tactile hand-held pocket robot AffectaPocket, designed to workalongside therapy as a focus object to facilitate coping during an anxietyattack. The robot does not require daily practice to be used, has a small formfactor, and has been designed for children 7 to 12 years old. The pocket robotworks by sensing when it is being held and attempts to shift the child's focusby presenting them with a simple three-note rhythm-matching game. We conducteda pilot study of the pocket robot involving four children aged 7 to 10 years,and then a main study with 18 children aged 6 to 8 years; neither studyinvolved children with anxiety. Both studies aimed to assess the reliability ofthe robot's sensor configuration, its design, and the effectiveness of the usertutorial. The results indicate that the morphology and sensor setup performedadequately and the tutorial process enabled the children to use the robot withlittle practice. This work demonstrates that the presented pocket robot couldrepresent a step toward developing low-cost accessible technologies to helpchildren suffering from anxiety disorders.
  </details>

- **[A Reactive Framework for Whole-Body Motion Planning of Mobile Manipulators Combining Reinforcement Learning and SDF-Constrained Quadratic Programmi](http://arxiv.org/abs/2503.23975v1)**  `arXiv:2503.23975`  
  _Chenyu Zhang, Shiying Sun, Kuan Liu, Chuanbao Zhou, Xiaoguang Zhao, Min Tan, et al._
  <details><summary>Abstract</summary>
  As an important branch of embodied artificial intelligence, mobilemanipulators are increasingly applied in intelligent services, but theirredundant degrees of freedom also limit efficient motion planning in clutteredenvironments. To address this issue, this paper proposes a hybrid learning andoptimization framework for reactive whole-body motion planning of mobilemanipulators. We develop the Bayesian distributional soft actor-critic(Bayes-DSAC) algorithm to improve the quality of value estimation and theconvergence performance of the learning. Additionally, we introduce a quadraticprogramming method constrained by the signed distance field to enhance thesafety of the obstacle avoidance motion. We conduct experiments and makecomparison with standard benchmark. The experimental results verify that ourproposed framework significantly improves the efficiency of reactive whole-bodymotion planning, reduces the planning time, and improves the success rate ofmotion planning. Additionally, the proposed reinforcement learning methodensures a rapid learning process in the whole-body planning task. The novelframework allows mobile manipulators to adapt to complex environments moresafely and efficiently.
  </details>

- **[MAER-Nav: Bidirectional Motion Learning Through Mirror-Augmented Experience Replay for Robot Navigation](http://arxiv.org/abs/2503.23908v1)**  `arXiv:2503.23908`  
  _Shanze Wang, Mingao Tan, Zhibo Yang, Biao Huang, Xiaoyu Shen, Hailong Huang, et al._
  <details><summary>Abstract</summary>
  Deep Reinforcement Learning (DRL) based navigation methods have demonstratedpromising results for mobile robots, but suffer from limited action flexibilityin confined spaces. Conventional DRL approaches predominantly learnforward-motion policies, causing robots to become trapped in complexenvironments where backward maneuvers are necessary for recovery. This paperpresents MAER-Nav (Mirror-Augmented Experience Replay for Robot Navigation), anovel framework that enables bidirectional motion learning without requiringexplicit failure-driven hindsight experience replay or reward functionmodifications. Our approach integrates a mirror-augmented experience replaymechanism with curriculum learning to generate synthetic backward navigationexperiences from successful trajectories. Experimental results in bothsimulation and real-world environments demonstrate that MAER-Nav significantlyoutperforms state-of-the-art methods while maintaining strong forwardnavigation capabilities. The framework effectively bridges the gap between thecomprehensive action space utilization of traditional planning methods and theenvironmental adaptability of learning-based approaches, enabling robustnavigation in scenarios where conventional DRL methods consistently fail.
  </details>

- **[Less is More: Contextual Sampling for Nonlinear Data-Enabled Predictive Control](http://arxiv.org/abs/2503.23890v1)**  `arXiv:2503.23890`  
  _Julius Beerwerth, Bassam Alrifaee_
  <details><summary>Abstract</summary>
  Data-enabled Predictive Control (DeePC) is a powerful data-driven approachfor predictive control without requiring an explicit system model. However, itshigh computational cost limits its applicability to real-time robotic systems.For robotic applications such as motion planning and trajectory tracking,real-time control is crucial. Nonlinear DeePC either relies on large datasetsor learning the nonlinearities to ensure predictive accuracy, leading to highcomputational complexity. This work introduces contextual sampling, a noveldata selection strategy to handle nonlinearities for DeePC by dynamicallyselecting the most relevant data at each time step. By reducing the datasetsize while preserving prediction accuracy, our method improves computationalefficiency, of DeePC for real-time robotic applications. We validate ourapproach for autonomous vehicle motion planning. For a dataset size of 100sub-trajectories, Contextual sampling DeePC reduces tracking error by 53.2 %compared to Leverage Score sampling. Additionally, Contextual sampling reducesmax computation time by 87.2 % compared to using the full dataset of 491sub-trajectories while achieving comparable tracking performance. These resultshighlight the potential of Contextual sampling to enable real-time, data-drivencontrol for robotic systems.
  </details>

- **[ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos](http://arxiv.org/abs/2503.23877v1)**  `arXiv:2503.23877`  
  _Junyao Shi, Zhuolun Zhao, Tianyou Wang, Ian Pedroza, Amy Luo, Jie Wang, et al._
  <details><summary>Abstract</summary>
  Many recent advances in robotic manipulation have come through imitationlearning, yet these rely largely on mimicking a particularly hard-to-acquireform of demonstrations: those collected on the same robot in the same room withthe same objects as the trained policy must handle at test time. In contrast,large pre-recorded human video datasets demonstrating manipulation skillsin-the-wild already exist, which contain valuable information for robots. Is itpossible to distill a repository of useful robotic skill policies out of suchdata without any additional requirements on robot-specific demonstrations orexploration? We present the first such system ZeroMimic, that generatesimmediately deployable image goal-conditioned skill policies for several commoncategories of manipulation tasks (opening, closing, pouring, pick&place,cutting, and stirring) each capable of acting upon diverse objects and acrossdiverse unseen task setups. ZeroMimic is carefully designed to exploit recentadvances in semantic and geometric visual understanding of human videos,together with modern grasp affordance detectors and imitation policy classes.After training ZeroMimic on the popular EpicKitchens dataset of ego-centrichuman videos, we evaluate its out-of-the-box performance in varied real-worldand simulated kitchen settings with two different robot embodiments,demonstrating its impressive abilities to handle these varied tasks. To enableplug-and-play reuse of ZeroMimic policies on other task setups and robots, werelease software and policy checkpoints of our skill policies.
  </details>

- **[GenSwarm: Scalable Multi-Robot Code-Policy Generation and Deployment via Language Models](http://arxiv.org/abs/2503.23875v1)**  `arXiv:2503.23875`  
  _Wenkang Ji, Huaben Chen, Mingyang Chen, Guobin Zhu, Lufeng Xu, Roderich Gro√ü, et al._
  <details><summary>Abstract</summary>
  The development of control policies for multi-robot systems traditionallyfollows a complex and labor-intensive process, often lacking the flexibility toadapt to dynamic tasks. This has motivated research on methods to automaticallycreate control policies. However, these methods require iterative processes ofmanually crafting and refining objective functions, thereby prolonging thedevelopment cycle. This work introduces \textit{GenSwarm}, an end-to-end systemthat leverages large language models to automatically generate and deploycontrol policies for multi-robot tasks based on simple user instructions innatural language. As a multi-language-agent system, GenSwarm achieves zero-shotlearning, enabling rapid adaptation to altered or unseen tasks. The white-boxnature of the code policies ensures strong reproducibility andinterpretability. With its scalable software and hardware architectures,GenSwarm supports efficient policy deployment on both simulated and real-worldmulti-robot systems, realizing an instruction-to-execution end-to-endfunctionality that could prove valuable for robotics specialists andnon-specialists alike.The code of the proposed GenSwarm system is availableonline: https://github.com/WindyLab/GenSwarm.
  </details>

- **[Disambiguate Gripper State in Grasp-Based Tasks: Pseudo-Tactile as Feedback Enables Pure Simulation Learning](http://arxiv.org/abs/2503.23835v1)**  `arXiv:2503.23835`  
  _Yifei Yang, Lu Chen, Zherui Song, Yenan Chen, Wentao Sun, Zhongxiang Zhou, et al._
  <details><summary>Abstract</summary>
  Grasp-based manipulation tasks are fundamental to robots interacting withtheir environments, yet gripper state ambiguity significantly reduces therobustness of imitation learning policies for these tasks. Data-drivensolutions face the challenge of high real-world data costs, while simulationdata, despite its low costs, is limited by the sim-to-real gap. We identify theroot cause of gripper state ambiguity as the lack of tactile feedback. Toaddress this, we propose a novel approach employing pseudo-tactile as feedback,inspired by the idea of using a force-controlled gripper as a tactile sensor.This method enhances policy robustness without additional data collection andhardware involvement, while providing a noise-free binary gripper stateobservation for the policy and thus facilitating pure simulation learning tounleash the power of simulation. Experimental results across three real-worldgrasp-based tasks demonstrate the necessity, effectiveness, and efficiency ofour approach.
  </details>

- **[Trajectory Planning for Automated Driving using Target Funnels](http://arxiv.org/abs/2503.23795v1)**  `arXiv:2503.23795`  
  _Benjamin Bogenberger, Johannes B√ºrger, Vladislav Nenchev_
  <details><summary>Abstract</summary>
  Self-driving vehicles rely on sensory input to monitor their surroundings andcontinuously adapt to the most likely future road course. Predictive trajectoryplanning is based on snapshots of the (uncertain) road course as a key input.Under noisy perception data, estimates of the road course can varysignificantly, leading to indecisive and erratic steering behavior. To overcomethis issue, this paper introduces a predictive trajectory planning algorithmwith a novel objective function: instead of targeting a single referencetrajectory based on the most likely road course, tracking a series of targetreference sets, called a target funnel, is considered. The proposed planningalgorithm integrates probabilistic information about the road course, and thusimplicitly considers regular updates to road perception. Our solution isassessed in a case study using real driving data collected from a prototypevehicle. The results demonstrate that the algorithm maintains tracking accuracyand substantially reduces undesirable steering commands in the presence ofnoisy road perception, achieving a 56% reduction in input costs compared to acertainty equivalent formulation.
  </details>

- **[Towards a cognitive architecture to enable natural language interaction in co-constructive task learning](http://arxiv.org/abs/2503.23760v1)**  `arXiv:2503.23760`  
  _Manuel Scheibl, Birte Richter, Alissa M√ºller, Michael Beetz, Britta Wrede_
  <details><summary>Abstract</summary>
  This research addresses the question, which characteristics a cognitivearchitecture must have to leverage the benefits of natural language inCo-Constructive Task Learning (CCTL). To provide context, we first discussInteractive Task Learning (ITL), the mechanisms of the human memory system, andthe significance of natural language and multi-modality. Next, we examine thecurrent state of cognitive architectures, analyzing their capabilities toinform a concept of CCTL grounded in multiple sources. We then integrateinsights from various research domains to develop a unified framework. Finally,we conclude by identifying the remaining challenges and requirements necessaryto achieve CCTL in Human-Robot Interaction (HRI).
  </details>

- **[Towards Benchmarking and Assessing the Safety and Robustness of Autonomous Driving on Safety-critical Scenarios](http://arxiv.org/abs/2503.23708v1)**  `arXiv:2503.23708`  
  _Jingzheng Li, Xianglong Liu, Shikui Wei, Zhijun Chen, Bing Li, Qing Guo, et al._
  <details><summary>Abstract</summary>
  Autonomous driving has made significant progress in both academia andindustry, including performance improvements in perception task and thedevelopment of end-to-end autonomous driving systems. However, the safety androbustness assessment of autonomous driving has not received sufficientattention. Current evaluations of autonomous driving are typically conducted innatural driving scenarios. However, many accidents often occur in edge cases,also known as safety-critical scenarios. These safety-critical scenarios aredifficult to collect, and there is currently no clear definition of whatconstitutes a safety-critical scenario. In this work, we explore the safety androbustness of autonomous driving in safety-critical scenarios. First, weprovide a definition of safety-critical scenarios, including static trafficscenarios such as adversarial attack scenarios and natural distribution shifts,as well as dynamic traffic scenarios such as accident scenarios. Then, wedevelop an autonomous driving safety testing platform to comprehensivelyevaluate autonomous driving systems, encompassing not only the assessment ofperception modules but also system-level evaluations. Our work systematicallyconstructs a safety verification process for autonomous driving, providingtechnical support for the industry to establish standardized test framework andreduce risks in real-world road deployment.
  </details>

- **[Grasping a Handful: Sequential Multi-Object Dexterous Grasp Generation](http://arxiv.org/abs/2503.22370v2)**  `arXiv:2503.22370`  
  _Haofei Lu, Yifei Dong, Zehang Weng, Jens Lundell, Danica Kragic_
  <details><summary>Abstract</summary>
  We introduce the sequential multi-object robotic grasp sampling algorithmSeqGrasp that can robustly synthesize stable grasps on diverse objects usingthe robotic hand's partial Degrees of Freedom (DoF). We use SeqGrasp toconstruct the large-scale Allegro Hand sequential grasping dataset SeqDatasetand use it for training the diffusion-based sequential grasp generatorSeqDiffuser. We experimentally evaluate SeqGrasp and SeqDiffuser against thestate-of-the-art non-sequential multi-object grasp generation method MultiGraspin simulation and on a real robot. The experimental results demonstrate thatSeqGrasp and SeqDiffuser reach an 8.71%-43.33% higher grasp success rate thanMultiGrasp. Furthermore, SeqDiffuser is approximately 1000 times faster atgenerating grasps than SeqGrasp and MultiGrasp.
  </details>

- **[CALMM-Drive: Confidence-Aware Autonomous Driving with Large Multimodal Model](http://arxiv.org/abs/2412.04209v2)**  `arXiv:2412.04209`  
  _Ruoyu Yao, Yubin Wang, Haichao Liu, Rui Yang, Zengqi Peng, Lei Zhu, et al._
  <details><summary>Abstract</summary>
  Decision-making and motion planning constitute critical components forensuring the safety and efficiency of autonomous vehicles (AVs). Existingmethodologies typically adopt two paradigms: decision then planning orgeneration then scoring. However, the former architecture often suffers fromdecision-planning misalignment that incurs risky situations. Meanwhile, thelatter struggles to balance short-term operational metrics (e.g., immediatemotion smoothness) with long-term tactical goals (e.g., route efficiency),resulting in myopic or overly conservative behaviors. To address these issues,we introduce CALMM-Drive, a novel Confidence-Aware Large Multimodal Model (LMM)empowered Autonomous Driving framework. Our approach integrates drivingtask-oriented Chain-of-Thought (CoT) reasoning coupled with Top-K confidenceelicitation, which facilitates high-level reasoning to generate multiplecandidate decisions with their confidence levels. Furthermore, we propose anovel planning module that integrates a diffusion model for trajectorygeneration and a hierarchical refinement process to find the optimaltrajectory. This framework enables the selection over trajectory candidatesaccounting for both low-level solution quality and high-level tacticalconfidence, which avoids the risks within one-shot decisions and overcomes thelimitations in short-sighted scoring mechanisms. Comprehensive evaluations innuPlan closed-loop simulation environments demonstrate the competitiveperformance of CALMM-Drive across both common and long-tail benchmarks,showcasing a significant advancement in the integration of uncertainty inLMM-empowered AVs. The code will be released upon acceptance.
  </details>

- **[Dynamic High-Order Control Barrier Functions with Diffuser for Safety-Critical Trajectory Planning at Signal-Free Intersections](http://arxiv.org/abs/2412.00162v2)**  `arXiv:2412.00162`  
  _Di Chen, Ruiguo Zhong, Kehua Chen, Zhiwei Shang, Meixin Zhu, Edward Chung_
  <details><summary>Abstract</summary>
  Planning safe and efficient trajectories through signal-free intersectionspresents significant challenges for autonomous vehicles (AVs), particularly indynamic, multi-task environments with unpredictable interactions and anincreased possibility of conflicts. This study aims to address these challengesby developing a unified, robust, adaptive framework to ensure safety andefficiency across three distinct intersection movements: left-turn, right-turn,and straight-ahead. Existing methods often struggle to reliably ensure safetyand effectively learn multi-task behaviors from demonstrations in suchenvironments. This study proposes a safety-critical planning method thatintegrates Dynamic High-Order Control Barrier Functions (DHOCBF) with adiffusion-based model, called Dynamic Safety-Critical Diffuser (DSC-Diffuser).The DSC-Diffuser leverages task-guided planning to enhance efficiency, allowingthe simultaneous learning of multiple driving tasks from real-world expertdemonstrations. Moreover, the incorporation of goal-oriented constraintssignificantly reduces displacement errors, ensuring precise trajectoryexecution. To further ensure driving safety in dynamic environments, theproposed DHOCBF framework dynamically adjusts to account for the movements ofsurrounding vehicles, offering enhanced adaptability and reduce theconservatism compared to traditional control barrier functions. Validityevaluations of DHOCBF, conducted through numerical simulations, demonstrate itsrobustness in adapting to variations in obstacle velocities, sizes,uncertainties, and locations, effectively maintaining driving safety across awide range of complex and uncertain scenarios. Comprehensive performanceevaluations demonstrate that DSC-Diffuser generates realistic, stable, andgeneralizable policies, providing flexibility and reliable safety assurance incomplex multi-task driving scenarios.
  </details>

- **[Robust Nonprehensile Object Transportation with Uncertain Inertial Parameters](http://arxiv.org/abs/2411.07079v3)**  `arXiv:2411.07079`  
  _Adam Heins, Angela P. Schoellig_
  <details><summary>Abstract</summary>
  We consider the nonprehensile object transportation task known as thewaiter's problem - in which a robot must move an object on a tray from onelocation to another - when the transported object has uncertain inertialparameters. In contrast to existing approaches that completely ignoreuncertainty in the inertia matrix or which only consider small parametererrors, we are interested in pushing the limits of the amount of inertialparameter uncertainty that can be handled. We first show how constraints thatare robust to inertial parameter uncertainty can be incorporated into anoptimization-based motion planning framework to transport objects while movingquickly. Next, we develop necessary conditions for the inertial parameters tobe realizable on a bounding shape based on moment relaxations, allowing us toverify whether a trajectory will violate the constraints for any realizableinertial parameters. Finally, we demonstrate our approach on a mobilemanipulator in simulations and real hardware experiments: our proposed robustconstraints consistently successfully transport a 56 cm tall object withsubstantial inertial parameter uncertainty in the real world, while thebaseline approaches drop the object while transporting it.
  </details>

- **[Fast Online Learning of CLiFF-maps in Changing Environments](http://arxiv.org/abs/2410.12237v2)**  `arXiv:2410.12237`  
  _Yufei Zhu, Andrey Rudenko, Luigi Palmieri, Lukas Heuer, Achim J. Lilienthal, Martin Magnusson_
  <details><summary>Abstract</summary>
  Maps of dynamics are effective representations of motion patterns learnedfrom prior observations, with recent research demonstrating their ability toenhance various downstream tasks such as human-aware robot navigation,long-term human motion prediction, and robot localization. Current advancementshave primarily concentrated on methods for learning maps of human flow inenvironments where the flow is static, i.e., not assumed to change over time.In this paper we propose an online update method of the CLiFF-map (an advancedmap of dynamics type that models motion patterns as velocity and orientationmixtures) to actively detect and adapt to human flow changes. As newobservations are collected, our goal is to update a CLiFF-map to effectivelyand accurately integrate them, while retaining relevant historic motionpatterns. The proposed online update method maintains a probabilisticrepresentation in each observed location, updating parameters by continuouslytracking sufficient statistics. In experiments using both synthetic andreal-world datasets, we show that our method is able to maintain accuraterepresentations of human motion dynamics, contributing to high performanceflow-compliant planning downstream tasks, while being orders of magnitudefaster than the comparable baselines.
  </details>

- **[Joint Moment Estimation for Hip Exoskeleton Control: A Generalized Moment Feature Generation Method](http://arxiv.org/abs/2410.00462v2)**  `arXiv:2410.00462`  
  _Yuanwen Zhang, Jingfeng Xiong, Haolan Xian, Chuheng Chen, Xinxing Chen, Chenglong Fu, et al._
  <details><summary>Abstract</summary>
  Hip joint moments during walking are the key foundation for hip exoskeletonassistance control. Most recent studies have shown estimating hip joint momentsinstantaneously offers a lot of advantages compared to generating assistivetorque profiles based on gait estimation, such as simple sensor requirementsand adaptability to variable walking speeds. However, existing joint momentestimation methods still suffer from a lack of personalization, leading toestimation accuracy degradation for new users. To address the challenges, thispaper proposes a hip joint moment estimation method based on generalized momentfeatures (GMF). A GMF generator is constructed to learn GMF of the joint momentwhich is invariant to individual variations while remaining decodable intojoint moments through a dedicated decoder. Utilizing this well-featuredrepresentation, a GRU-based neural network is used to predict GMF with jointkinematics data, which can easily be acquired by hip exoskeleton encoders. Theproposed estimation method achieves a root mean square error of 0.1180 Nm/kgunder 28 walking speed conditions on a treadmill dataset, improved by 6.5%compared to the model without body parameter fusion, and by 8.3% for theconventional fusion model with body parameter. Furthermore, the proposed methodwas employed on a hip exoskeleton with only encoder sensors and achieved anaverage 20.5% metabolic reduction (p<0.01) for users compared to assist-offcondition in level-ground walking.
  </details>

- **[Fast and Accurate Task Planning using Neuro-Symbolic Language Models and Multi-level Goal Decomposition](http://arxiv.org/abs/2409.19250v2)**  `arXiv:2409.19250`  
  _Minseo Kwon, Yaesol Kim, Young J. Kim_
  <details><summary>Abstract</summary>
  In robotic task planning, symbolic planners using rule-based representationslike PDDL are effective but struggle with long-sequential tasks in complicatedenvironments due to exponentially increasing search space. Meanwhile, LLM-basedapproaches, which are grounded in artificial neural networks, offer fasterinference and commonsense reasoning but suffer from lower success rates. Toaddress the limitations of the current symbolic (slow speed) or LLM-basedapproaches (low accuracy), we propose a novel neuro-symbolic task planner thatdecomposes complex tasks into subgoals using LLM and carries out task planningfor each subgoal using either symbolic or MCTS-based LLM planners, depending onthe subgoal complexity. This decomposition reduces planning time and improvessuccess rates by narrowing the search space and enabling LLMs to focus on moremanageable tasks. Our method significantly reduces planning time whilemaintaining high success rates across task planning domains, as well asreal-world and simulated robotics environments. More details are available athttp://graphics.ewha.ac.kr/LLMTAMP/.
  </details>

- **[Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models](http://arxiv.org/abs/2409.16663v3)**  `arXiv:2409.16663`  
  _Alexander Popov, Alperen Degirmenci, David Wehr, Shashank Hegde, Ryan Oldja, Alexey Kamenev, et al._
  <details><summary>Abstract</summary>
  We propose the use of latent space generative world models to address thecovariate shift problem in autonomous driving. A world model is a neuralnetwork capable of predicting an agent's next state given past states andactions. By leveraging a world model during training, the driving policyeffectively mitigates covariate shift without requiring an excessive amount oftraining data. During end-to-end training, our policy learns how to recoverfrom errors by aligning with states observed in human demonstrations, so thatat runtime it can recover from perturbations outside the training distribution.Additionally, we introduce a novel transformer-based perception encoder thatemploys multi-view cross-attention and a learned scene query. We presentqualitative and quantitative results, demonstrating significant improvementsupon prior state of the art in closed-loop testing in the CARLA simulator, aswell as showing the ability to handle perturbations in both CARLA and NVIDIA'sDRIVE Sim.
  </details>

- **[Tactile Ergodic Coverage on Curved Surfaces](http://arxiv.org/abs/2402.04862v3)**  `arXiv:2402.04862`  
  _Cem Bilaloglu, Tobias L√∂w, Sylvain Calinon_
  <details><summary>Abstract</summary>
  In this article, we present a feedback control method for tactile coveragetasks, such as cleaning or surface inspection. These tasks are challenging toplan due to complex continuous physical interactions. In these tasks, thecoverage target and progress can be easily measured using a camera and encodedin a point cloud. We propose an ergodic coverage method that operates directlyon point clouds, guiding the robot to spend more time on regions requiring morecoverage. For robot control and contact behavior, we use geometric algebra toformulate a task-space impedance controller that tracks a line whilesimultaneously exerting a desired force along that line. We evaluate theperformance of our method in kinematic simulations and demonstrate itsapplicability in real-world experiments on kitchenware. Our source codes,experimental data, and videos are available as open access athttps://sites.google.com/view/tactile-ergodic-control/
  </details>

- **[Scalable Multi-modal Model Predictive Control via Duality-based Interaction Predictions](http://arxiv.org/abs/2402.01116v5)**  `arXiv:2402.01116`  
  _Hansung Kim, Siddharth H. Nair, Francesco Borrelli_
  <details><summary>Abstract</summary>
  We propose a hierarchical architecture designed for scalable real-time ModelPredictive Control (MPC) in complex, multi-modal traffic scenarios. Thisarchitecture comprises two key components: 1) RAID-Net, a novel attention-basedRecurrent Neural Network that predicts relevant interactions along the MPCprediction horizon between the autonomous vehicle and the surrounding vehiclesusing Lagrangian duality, and 2) a reduced Stochastic MPC problem thateliminates irrelevant collision avoidance constraints, enhancing computationalefficiency. Our approach is demonstrated in a simulated traffic intersectionwith interactive surrounding vehicles, showcasing a 12x speed-up in solving themotion planning problem. A video demonstrating the proposed architecture inmultiple complex traffic scenarios can be found here:https://youtu.be/-pRiOnPb9_c. GitHub:https://github.com/MPC-Berkeley/hmpc_raidnet
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

