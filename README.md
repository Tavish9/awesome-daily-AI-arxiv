# üöÄ Daily AI arXiv Digest

[![Total Papers](https://img.shields.io/badge/paper_today-384+-red)]()
[![Last Updated](https://img.shields.io/badge/dynamic/json?url=https://api.github.com/repos/tavish9/awesome-daily-AI-arxiv/commits/main&query=%24.commit.author.date&label=updated&color=orange)](https://github.com/Tavish9/awesome-daily-AI-arxiv/commits/main/)
[![arXiv API](https://img.shields.io/badge/powered_by-arXiv_API-009688)](https://arxiv.org/help/api)
[![License](https://img.shields.io/badge/license-CC_BY--SA_4.0-3989c9)](LICENSE)


üìå ‚Äã**Tracking Breakthroughs in**: `AI` ‚Ä¢ `NLP` ‚Ä¢ `CV` ‚Ä¢ `ML` ‚Ä¢ `Robotics`  
‚è∞ ‚Äã**Update Schedule**: [UTC 02:00](https://time.is/UTC) | [GMT+8 10:00](https://time.is/China)

## üåü Today's Highlights

- üî• Hot Topic
  - [Benchmark](hot_topic/Benchmark.md)
  - [LLM](hot_topic/LLM.md)
  - [Reasoning](hot_topic/Reasoning.md)
  - [3D_Reconstruction](hot_topic/3D_Reconstruction.md)
  - [Diffusion](hot_topic/Diffusion.md)
  - [MLLM](hot_topic/MLLM.md)
  - [3D_Generation](hot_topic/3D_Generation.md)
  - [Embodied_AI](hot_topic/Embodied_AI.md)
- üí´ Active Platform
  - [Huggingface](https://huggingface.co/papers)
  - [LlamaFactory](https://www.llamafactory.cn/daily-paper/)
  - [X (Twitter)](https://x.com/arxiv_daily)
  - [Paper Reading](https://paperreading.club/)
  - [Paper Digest](https://www.paperdigest.org/arxiv/)
  

## üìå Full Archive

| Category                                                                                | Count |
| --------------------------------------------------------------------------------------- | ----- |
| [Artificial Intelligence üß†](#artificial-intelligence-) | 31    |
| [Computation and Language üí¨](#computation-and-language-) | 67    |
| [Computer Vision and Pattern Recognition üì∏](#computer-vision-and-pattern-recognition-) | 176   |
| [Machine Learning üìä](#machine-learning-) | 80    |
| [Multiagent Systems üåê](#multiagent-systems-) | 1     |
| [Robotics ü§ñ](#robotics-) | 29    |

### Artificial Intelligence üß†

<details open><summary>Click to Collapse</summary>

- **[LLM-Gomoku: A Large Language Model-Based System for Strategic Gomoku with Self-Play and Reinforcement Learning](http://arxiv.org/abs/2503.21683v1)**  `arXiv:2503.21683`  
  _Hui Wang_
  <details><summary>Abstract</summary>
  In recent years, large language models (LLMs) have shown significantadvancements in natural language processing (NLP), with strong capa-bilities ingeneration, comprehension, and rea-soning. These models have found applicationsin education, intelligent decision-making, and gaming. However, effectivelyutilizing LLMs for strategic planning and decision-making in the game of Gomokuremains a challenge. This study aims to develop a Gomoku AI system based onLLMs, simulating the human learning process of playing chess. The system isde-signed to understand and apply Gomoku strat-egies and logic to make rationaldecisions. The research methods include enabling the model to "read the board,""understand the rules," "select strategies," and "evaluate positions," whileen-hancing its abilities through self-play and rein-forcement learning. Theresults demonstrate that this approach significantly improves the se-lection ofmove positions, resolves the issue of generating illegal positions, and reducespro-cess time through parallel position evaluation. After extensive self-playtraining, the model's Gomoku-playing capabilities have been notably enhanced.
  </details>

- **[Cognitive Science-Inspired Evaluation of Core Capabilities for Object Understanding in AI](http://arxiv.org/abs/2503.21668v1)**  `arXiv:2503.21668`  
  _Danaja Rutar, Alva Markelius, Konstantinos Voudouris, Jos√© Hern√°ndez-Orallo, Lucy Cheke_
  <details><summary>Abstract</summary>
  One of the core components of our world models is 'intuitive physics' - anunderstanding of objects, space, and causality. This capability enables us topredict events, plan action and navigate environments, all of which rely on acomposite sense of objecthood. Despite its importance, there is no single,unified account of objecthood, though multiple theoretical frameworks provideinsights. In the first part of this paper, we present a comprehensive overviewof the main theoretical frameworks in objecthood research - Gestalt psychology,enactive cognition, and developmental psychology - and identify the corecapabilities each framework attributes to object understanding, as well as whatfunctional roles they play in shaping world models in biological agents. Giventhe foundational role of objecthood in world modelling, understandingobjecthood is also essential in AI. In the second part of the paper, weevaluate how current AI paradigms approach and test objecthood capabilitiescompared to those in cognitive science. We define an AI paradigm as acombination of how objecthood is conceptualised, the methods used for studyingobjecthood, the data utilised, and the evaluation techniques. We find that,whilst benchmarks can detect that AI systems model isolated aspects ofobjecthood, the benchmarks cannot detect when AI systems lack functionalintegration across these capabilities, not solving the objecthood challengefully. Finally, we explore novel evaluation approaches that align with theintegrated vision of objecthood outlined in this paper. These methods arepromising candidates for advancing from isolated object capabilities towardgeneral-purpose AI with genuine object understanding in real-world contexts.
  </details>

- **[Unlocking the Potential of Past Research: Using Generative AI to Reconstruct Healthcare Simulation Models](http://arxiv.org/abs/2503.21646v1)**  `arXiv:2503.21646`  
  _Thomas Monks, Alison Harper, Amy Heather_
  <details><summary>Abstract</summary>
  Discrete-event simulation (DES) is widely used in healthcare OperationsResearch, but the models themselves are rarely shared. This limits theirpotential for reuse and long-term impact in the modelling and healthcarecommunities. This study explores the feasibility of using generative artificialintelligence (AI) to recreate published models using Free and Open SourceSoftware (FOSS), based on the descriptions provided in an academic journal.Using a structured methodology, we successfully generated, tested andinternally reproduced two DES models, including user interfaces. The reportedresults were replicated for one model, but not the other, likely due to missinginformation on distributions. These models are substantially more complex thanAI-generated DES models published to date. Given the challenges we faced inprompt engineering, code generation, and model testing, we conclude that ouriterative approach to model development, systematic comparison and testing, andthe expertise of our team were necessary to the success of our recreatedsimulation models.
  </details>

- **[Towards Fully Automated Decision-Making Systems for Greenhouse Control: Challenges and Opportunities](http://arxiv.org/abs/2503.21640v1)**  `arXiv:2503.21640`  
  _Yongshuai Liu, Taeyeong Choi, Xin Liu_
  <details><summary>Abstract</summary>
  Machine learning has been successful in building control policies to drive acomplex system to desired states in various applications (e.g. games, robotics,etc.). To be specific, a number of parameters of policy can be automaticallyoptimized from the observations of environment to be able to generate asequence of decisions leading to the best performance. In this survey paper, weparticularly explore such policy-learning techniques for another unique,practical use-case scenario--farming, in which critical decisions (e.g., watersupply, heating, etc.) must be made in a timely manner to minimize risks (e.g.,damage to plants) while maximizing the revenue (e.g., healthy crops) in theend. We first provide a broad overview of latest studies on it to identify notonly domain-specific challenges but opportunities with potential solutions,some of which are suggested as promising directions for future research. Also,we then introduce our successful approach to being ranked second among 46 teamsat the ''3rd Autonomous Greenhouse Challenge'' to use this specific example todiscuss the lessons learned about important considerations for design to createautonomous farm-management systems.
  </details>

- **[UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning](http://arxiv.org/abs/2503.21620v1)**  `arXiv:2503.21620`  
  _Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, et al._
  <details><summary>Abstract</summary>
  The recent DeepSeek-R1 has showcased the emergence of reasoning capabilitiesin LLMs through reinforcement learning (RL) with rule-based rewards. Buildingon this idea, we are the first to explore how rule-based RL can enhance thereasoning capabilities of multimodal large language models (MLLMs) for graphicuser interface (GUI) action prediction tasks. To this end, we curate a smallyet high-quality dataset of 136 challenging tasks, encompassing five commonaction types on mobile devices. We also introduce a unified rule-based actionreward, enabling model optimization via policy-based algorithms such as GroupRelative Policy Optimization (GRPO). Experimental results demonstrate that ourproposed data-efficient model, UI-R1-3B, achieves substantial improvements onboth in-domain (ID) and out-of-domain (OOD) tasks. Specifically, on the IDbenchmark AndroidControl, the action type accuracy improves by 15%, whilegrounding accuracy increases by 10.3%, compared with the base model (i.e.Qwen2.5-VL-3B). On the OOD GUI grounding benchmark ScreenSpot-Pro, our modelsurpasses the base model by 6.0% and achieves competitive performance withlarger models (e.g., OS-Atlas-7B), which are trained via supervised fine-tuning(SFT) on 76K data. These results underscore the potential of rule-basedreinforcement learning to advance GUI understanding and control, paving the wayfor future research in this domain.
  </details>

- **[GenEdit: Compounding Operators and Continuous Improvement to Tackle Text-to-SQL in the Enterprise](http://arxiv.org/abs/2503.21602v1)**  `arXiv:2503.21602`  
  _Karime Maamari, Connor Landy, Amine Mhedhbi_
  <details><summary>Abstract</summary>
  Recent advancements in Text-to-SQL, driven by large language models, aredemocratizing data access. Despite these advancements, enterprise deploymentsremain challenging due to the need to capture business-specific knowledge,handle complex queries, and meet expectations of continuous improvements. Toaddress these issues, we designed and implemented GenEdit: our Text-to-SQLgeneration system that improves with user feedback. GenEdit builds andmaintains a company-specific knowledge set, employs a pipeline of operatorsdecomposing SQL generation, and uses feedback to update its knowledge set toimprove future SQL generations.  We describe GenEdit's architecture made of two core modules: (i) decomposedSQL generation; and (ii) knowledge set edits based on user feedback. Forgeneration, GenEdit leverages compounding operators to improve knowledgeretrieval and to create a plan as chain-of-thought steps that guidesgeneration. GenEdit first retrieves relevant examples in an initial retrievalstage where original SQL queries are decomposed into sub-statements, clauses orsub-queries. It then also retrieves instructions and schema elements. Using theretrieved contextual information, GenEdit then generates step-by-step plan innatural language on how to produce the query. Finally, GenEdit uses the plan togenerate SQL, minimizing the need for model reasoning, which enhances complexSQL generation. If necessary, GenEdit regenerates the query based on syntacticand semantic errors. The knowledge set edits are recommended through aninteractive copilot, allowing users to iterate on their feedback and toregenerate SQL queries as needed. Each generation uses staged edits whichupdate the generation prompt. Once the feedback is submitted, it gets mergedafter passing regression testing and obtaining an approval, improving futuregenerations.
  </details>

- **[debug-gym: A Text-Based Environment for Interactive Debugging](http://arxiv.org/abs/2503.21557v1)**  `arXiv:2503.21557`  
  _Xingdi Yuan, Morgane M Moss, Charbel El Feghali, Chinmay Singh, Darya Moldavskaya, Drew MacPhee, et al._
  <details><summary>Abstract</summary>
  Large Language Models (LLMs) are increasingly relied upon for coding tasks,yet in most scenarios it is assumed that all relevant information can be eitheraccessed in context or matches their training data. We posit that LLMs canbenefit from the ability to interactively explore a codebase to gather theinformation relevant to their task. To achieve this, we present a textualenvironment, namely debug-gym, for developing LLM-based agents in aninteractive coding setting. Our environment is lightweight and provides apreset of useful tools, such as a Python debugger (pdb), designed to facilitatean LLM-based agent's interactive debugging. Beyond coding and debugging tasks,this approach can be generalized to other tasks that would benefit frominformation-seeking behavior by an LLM agent.
  </details>

- **[The Procedural Content Generation Benchmark: An Open-source Testbed for Generative Challenges in Games](http://arxiv.org/abs/2503.21474v1)**  `arXiv:2503.21474`  
  _Ahmed Khalifa, Roberto Gallotta, Matthew Barthet, Antonios Liapis, Julian Togelius, Georgios N. Yannakakis_
  <details><summary>Abstract</summary>
  This paper introduces the Procedural Content Generation Benchmark forevaluating generative algorithms on different game content creation tasks. Thebenchmark comes with 12 game-related problems with multiple variants on eachproblem. Problems vary from creating levels of different kinds to creating rulesets for simple arcade games. Each problem has its own content representation,control parameters, and evaluation metrics for quality, diversity, andcontrollability. This benchmark is intended as a first step towards astandardized way of comparing generative algorithms. We use the benchmark toscore three baseline algorithms: a random generator, an evolution strategy, anda genetic algorithm. Results show that some problems are easier to solve thanothers, as well as the impact the chosen objective has on quality, diversity,and controllability of the generated artifacts.
  </details>

- **[Graph-to-Vision: Multi-graph Understanding and Reasoning using Vision-Language Models](http://arxiv.org/abs/2503.21435v1)**  `arXiv:2503.21435`  
  _Ruizhou Li, Haiyun Jiang_
  <details><summary>Abstract</summary>
  Graph Neural Networks (GNNs), as the dominant paradigm for graph-structuredlearning, have long faced dual challenges of exponentially escalatingcomputational complexity and inadequate cross-scenario generalizationcapability. With the rapid advancement of multimodal learning, Vision-LanguageModels (VLMs) have demonstrated exceptional cross-modal relational reasoningcapabilities and generalization capacities, thereby opening up novel pathwaysfor overcoming the inherent limitations of conventional graph learningparadigms. However, current research predominantly concentrates oninvestigating the single-graph reasoning capabilities of VLMs, whichfundamentally fails to address the critical requirement for coordinatedreasoning across multiple heterogeneous graph data in real-world applicationscenarios. To address these limitations, we propose the first multi-graph jointreasoning benchmark for VLMs. Our benchmark encompasses four graph categories:knowledge graphs, flowcharts, mind maps, and route maps,with each graph groupaccompanied by three progressively challenging instruction-response pairs.Leveraging this benchmark, we conducted comprehensive capability assessments ofstate-of-the-art VLMs and performed fine-tuning on open-source models. Thisstudy not only addresses the underexplored evaluation gap in multi-graphreasoning for VLMs but also empirically validates their generalizationsuperiority in graph-structured learning.
  </details>

- **[Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In \& Out Learning](http://arxiv.org/abs/2503.21419v1)**  `arXiv:2503.21419`  
  _Yupei Li, Manuel Milling, Bj√∂rn W. Schuller_
  <details><summary>Abstract</summary>
  Artificial Intelligence (AI) has achieved new levels of performance andspread in public usage with the rise of deep neural networks (DNNs). Initiallyinspired by human neurons and their connections, NNs have become the foundationof AI models for many advanced architectures. However, some of the mostintegral processes in the human brain, particularly neurogenesis andneuroplasticity in addition to the more spread neuroapoptosis have largely beenignored in DNN architecture design. Instead, contemporary AI developmentpredominantly focuses on constructing advanced frameworks, such as largelanguage models, which retain a static structure of neural connections duringtraining and inference. In this light, we explore how neurogenesis,neuroapoptosis, and neuroplasticity can inspire future AI advances.Specifically, we examine analogous activities in artificial NNs, introducingthe concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' andstructural pruning for neuroapoptosis. We additionally suggest neuroplasticitycombining the two for future large NNs in ``life-long learning'' settingsfollowing the biological inspiration. We conclude by advocating for greaterresearch efforts in this interdisciplinary domain and identifying promisingdirections for future exploration.
  </details>

- **[Federated Intelligence: When Large AI Models Meet Federated Fine-Tuning and Collaborative Reasoning at the Network Edge](http://arxiv.org/abs/2503.21412v1)**  `arXiv:2503.21412`  
  _Wanli Ni, Haofeng Sun, Huiqing Ao, Hui Tian_
  <details><summary>Abstract</summary>
  Large artificial intelligence (AI) models exhibit remarkable capabilities invarious application scenarios, but deploying them at the network edge posessignificant challenges due to issues such as data privacy, computationalresources, and latency. In this paper, we explore federated fine-tuning andcollaborative reasoning techniques to facilitate the implementation of large AImodels in resource-constrained wireless networks. Firstly, promisingapplications of large AI models within specific domains are discussed.Subsequently, federated fine-tuning methods are proposed to adapt large AImodels to specific tasks or environments at the network edge, effectivelyaddressing the challenges associated with communication overhead and enhancingcommunication efficiency. These methodologies follow clustered, hierarchical,and asynchronous paradigms to effectively tackle privacy issues and eliminatedata silos. Furthermore, to enhance operational efficiency and reduce latency,efficient frameworks for model collaborative reasoning are developed, whichinclude decentralized horizontal collaboration, cloud-edge-end verticalcollaboration, and multi-access collaboration. Next, simulation resultsdemonstrate the effectiveness of our proposed methods in reducing thefine-tuning loss of large AI models across various downstream tasks. Finally,several open challenges and research opportunities are outlined.
  </details>

- **[Exploring the Roles of Large Language Models in Reshaping Transportation Systems: A Survey, Framework, and Roadmap](http://arxiv.org/abs/2503.21411v1)**  `arXiv:2503.21411`  
  _Tong Nie, Jian Sun, Wei Ma_
  <details><summary>Abstract</summary>
  Modern transportation systems face pressing challenges due to increasingdemand, dynamic environments, and heterogeneous information integration. Therapid evolution of Large Language Models (LLMs) offers transformative potentialto address these challenges. Extensive knowledge and high-level capabilitiesderived from pretraining evolve the default role of LLMs as text generators tobecome versatile, knowledge-driven task solvers for intelligent transportationsystems. This survey first presents LLM4TR, a novel conceptual framework thatsystematically categorizes the roles of LLMs in transportation into foursynergetic dimensions: information processors, knowledge encoders, componentgenerators, and decision facilitators. Through a unified taxonomy, wesystematically elucidate how LLMs bridge fragmented data pipelines, enhancepredictive analytics, simulate human-like reasoning, and enable closed-loopinteractions across sensing, learning, modeling, and managing tasks intransportation systems. For each role, our review spans diverse applications,from traffic prediction and autonomous driving to safety analytics and urbanmobility optimization, highlighting how emergent capabilities of LLMs such asin-context learning and step-by-step reasoning can enhance the operation andmanagement of transportation systems. We further curate practical guidance,including available resources and computational guidelines, to supportreal-world deployment. By identifying challenges in existing LLM-basedsolutions, this survey charts a roadmap for advancing LLM-driven transportationresearch, positioning LLMs as central actors in the next generation ofcyber-physical-social mobility ecosystems. Online resources can be found in theproject page: https://github.com/tongnie/awesome-llm4tr.
  </details>

- **[Neuro-Symbolic Imitation Learning: Discovering Symbolic Abstractions for Skill Learning](http://arxiv.org/abs/2503.21406v1)**  `arXiv:2503.21406`  
  _Leon Keller, Daniel Tanneberg, Jan Peters_
  <details><summary>Abstract</summary>
  Imitation learning is a popular method for teaching robots new behaviors.However, most existing methods focus on teaching short, isolated skills ratherthan long, multi-step tasks. To bridge this gap, imitation learning algorithmsmust not only learn individual skills but also an abstract understanding of howto sequence these skills to perform extended tasks effectively. This paperaddresses this challenge by proposing a neuro-symbolic imitation learningframework. Using task demonstrations, the system first learns a symbolicrepresentation that abstracts the low-level state-action space. The learnedrepresentation decomposes a task into easier subtasks and allows the system toleverage symbolic planning to generate abstract plans. Subsequently, the systemutilizes this task decomposition to learn a set of neural skills capable ofrefining abstract plans into actionable robot commands. Experimental results inthree simulated robotic environments demonstrate that, compared to baselines,our neuro-symbolic approach increases data efficiency, improves generalizationcapabilities, and facilitates interpretability.
  </details>

- **[HybridoNet-Adapt: A Domain-Adapted Framework for Accurate Lithium-Ion Battery RUL Prediction](http://arxiv.org/abs/2503.21392v1)**  `arXiv:2503.21392`  
  _Khoa Tran, Bao Huynh, Tri Le, Lam Pham, Vy-Rin Nguyen_
  <details><summary>Abstract</summary>
  Accurate prediction of the remaining useful life (RUL) in Lithium-ion battery(LIB) health management systems is crucial for ensuring reliability and safety.Current methods typically assume that training and testing data share the samedistribution, overlooking the benefits of incorporating diverse data sources toenhance model performance. To address this limitation, we introduce adata-independent RUL prediction framework along with its domain adaptation (DA)approach, which leverages heterogeneous data sources for improved targetpredictions. Our approach integrates comprehensive data preprocessing,including feature extraction, denoising, and normalization, with adata-independent prediction model that combines Long Short-Term Memory (LSTM),Multihead Attention, and a Neural Ordinary Differential Equation (NODE) block,termed HybridoNet. The domain-adapted version, HybridoNet Adapt, is trainedusing a novel technique inspired by the Domain-Adversarial Neural Network(DANN) framework, a regression ensemble method, and Maximum Mean Discrepancy(MMD) to learn domain-invariant features from labeled cycling data in thesource and target domains. Experimental results demonstrate that our approachoutperforms state-of-the-art techniques, providing reliable RUL predictions forreal-world applications.
  </details>

- **[Using large language models to produce literature reviews: Usages and systematic biases of microphysics parametrizations in 2699 publications](http://arxiv.org/abs/2503.21352v1)**  `arXiv:2503.21352`  
  _Tianhang Zhang, Shengnan Fu, David M. Schultz, Zhonghua Zheng_
  <details><summary>Abstract</summary>
  Large language models afford opportunities for using computers for intensivetasks, realizing research opportunities that have not been considered before.One such opportunity could be a systematic interrogation of the scientificliterature. Here, we show how a large language model can be used to construct aliterature review of 2699 publications associated with microphysicsparametrizations in the Weather and Research Forecasting (WRF) model, with thegoal of learning how they were used and their systematic biases, whensimulating precipitation. The database was constructed of publicationsidentified from Web of Science and Scopus searches. The large language modelGPT-4 Turbo was used to extract information about model configurations andperformance from the text of 2699 publications. Our results reveal thelandscape of how nine of the most popular microphysics parameterizations havebeen used around the world: Lin, Ferrier, WRF Single-Moment, Goddard CumulusEnsemble, Morrison, Thompson, and WRF Double-Moment. More studies usedone-moment parameterizations before 2020 and two-moment parameterizations after2020. Seven out of nine parameterizations tended to overestimate precipitation.However, systematic biases of parameterizations differed in various regions.Except simulations using the Lin, Ferrier, and Goddard parameterizations thattended to underestimate precipitation over almost all locations, the remainingsix parameterizations tended to overestimate, particularly over China,southeast Asia, western United States, and central Africa. This method could beused by other researchers to help understand how the increasingly massive bodyof scientific literature can be harnessed through the power of artificialintelligence to solve their research problems.
  </details>

- **[HyperGraphRAG: Retrieval-Augmented Generation with Hypergraph-Structured Knowledge Representation](http://arxiv.org/abs/2503.21322v1)**  `arXiv:2503.21322`  
  _Haoran Luo, Haihong E, Guanting Chen, Yandan Zheng, Xiaobao Wu, Yikai Guo, et al._
  <details><summary>Abstract</summary>
  While standard Retrieval-Augmented Generation (RAG) based on chunks, GraphRAGstructures knowledge as graphs to leverage the relations among entities.However, previous GraphRAG methods are limited by binary relations: one edge inthe graph only connects two entities, which cannot well model the n-aryrelations among more than two entities that widely exist in reality. To addressthis limitation, we propose HyperGraphRAG, a novel hypergraph-based RAG methodthat represents n-ary relational facts via hyperedges, modeling the complicatedn-ary relations in the real world. To retrieve and generate over hypergraphs,we introduce a complete pipeline with a hypergraph construction method, ahypergraph retrieval strategy, and a hypergraph-guided generation mechanism.Experiments across medicine, agriculture, computer science, and law demonstratethat HyperGraphRAG outperforms standard RAG and GraphRAG in accuracy andgeneration quality.
  </details>

- **[Reinforced Model Merging](http://arxiv.org/abs/2503.21272v1)**  `arXiv:2503.21272`  
  _Jiaqi Han, Jingwen Ye, Shunyu Liu, Haofei Zhang, Jie Song, Zunlei Feng, et al._
  <details><summary>Abstract</summary>
  The success of large language models has garnered widespread attention formodel merging techniques, especially training-free methods which combine modelcapabilities within the parameter space. However, two challenges remain: (1)uniform treatment of all parameters leads to performance degradation; (2)search-based algorithms are often inefficient. In this paper, we present aninnovative framework termed Reinforced Model Merging (RMM), which encompassesan environment and agent tailored for merging tasks. These components interactto execute layer-wise merging actions, aiming to search the optimal mergingarchitecture. Notably, RMM operates without any gradient computations on theoriginal models, rendering it feasible for edge devices. Furthermore, byutilizing data subsets during the evaluation process, we addressed thebottleneck in the reward feedback phase, thereby accelerating RMM by up to 100times. Extensive experiments demonstrate that RMM achieves state-of-the-artperformance across various vision and NLP datasets and effectively overcomesthe limitations of the existing baseline methods. Our code is available athttps://github.com/WuDiHJQ/Reinforced-Model-Merging.
  </details>

- **[Knowledge Graphs as World Models for Semantic Material-Aware Obstacle Handling in Autonomous Vehicles](http://arxiv.org/abs/2503.21232v1)**  `arXiv:2503.21232`  
  _Ayush Bheemaiah, Seungyong Yang_
  <details><summary>Abstract</summary>
  The inability of autonomous vehicles (AVs) to infer the material propertiesof obstacles limits their decision-making capacity. While AVs rely on sensorsystems such as cameras, LiDAR, and radar to detect obstacles, this studysuggests combining sensors with a knowledge graph (KG)-based world model toimprove AVs' comprehension of physical material qualities. Beyond sensor data,AVs can infer qualities such as malleability, density, and elasticity using asemantic KG that depicts the relationships between obstacles and theirattributes. Using the CARLA autonomous driving simulator, we evaluated AVperformance with and without KG integration. The findings demonstrate that theKG-based method improves obstacle management, which allows AVs to use materialqualities to make better decisions about when to change lanes or applyemergency braking. For example, the KG-integrated AV changed lanes for hardimpediments like traffic cones and successfully avoided collisions withflexible items such as plastic bags by passing over them. Compared to thecontrol system, the KG framework demonstrated improved responsiveness toobstacles by resolving conflicting sensor data, causing emergency stops for13.3% more cases. In addition, our method exhibits a 6.6% higher success ratein lane-changing maneuvers in experimental scenarios, particularly for larger,high-impact obstacles. While we focus particularly on autonomous driving, ourwork demonstrates the potential of KG-based world models to improvedecision-making in embodied AI systems and scale to other domains, includingrobotics, healthcare, and environmental simulation.
  </details>

- **[Integrating Large Language Models For Monte Carlo Simulation of Chemical Reaction Networks](http://arxiv.org/abs/2503.21178v1)**  `arXiv:2503.21178`  
  _Sadikshya Gyawali, Ashwini Mandal, Manish Dahal, Manish Awale, Sanjay Rijal, Shital Adhikari, et al._
  <details><summary>Abstract</summary>
  Chemical reaction network is an important method for modeling and exploringcomplex biological processes, bio-chemical interactions and the behavior ofdifferent dynamics in system biology. But, formulating such reaction kineticstakes considerable time. In this paper, we leverage the efficiency of modernlarge language models to automate the stochastic monte carlo simulation ofchemical reaction networks and enable the simulation through the reactiondescription provided in the form of natural languages. We also integrate thisprocess into widely used simulation tool Copasi to further give the edge andease to the modelers and researchers. In this work, we show the efficacy andlimitations of the modern large language models to parse and create reactionkinetics for modelling complex chemical reaction processes.
  </details>

- **[A computational theory of evaluation for parameterisable subject](http://arxiv.org/abs/2503.21138v1)**  `arXiv:2503.21138`  
  _Hedong Yan_
  <details><summary>Abstract</summary>
  Evaluation is critical to advance decision making across domains, yetexisting methodologies often struggle to balance theoretical rigor andpractical scalability. In order to reduce the cost of experimental evaluation,we introduce a computational theory of evaluation for parameterisable subjects.We prove upper bounds of generalized evaluation error and generalized causaleffect error of evaluation metric on subject. We also prove efficiency, andconsistency to estimated causal effect of subject on metric by prediction. Tooptimize evaluation models, we propose a meta-learner to handle heterogeneousevaluation subjects space. Comparing with other computational approaches, our(conditional) evaluation model reduced 24.1%-99.0% evaluation errors across 12scenes, including individual medicine, scientific simulation, businessactivities, and quantum trade. The evaluation time is reduced 3-7 order ofmagnitude comparing with experiments or simulations.
  </details>

- **[AskSport: Web Application for Sports Question-Answering](http://arxiv.org/abs/2503.21067v1)**  `arXiv:2503.21067`  
  _Enzo B Onofre, Leonardo M P Moraes, Cristina D Aguiar_
  <details><summary>Abstract</summary>
  This paper introduces AskSport, a question-answering web application aboutsports. It allows users to ask questions using natural language and retrievethe three most relevant answers, including related information and documents.The paper describes the characteristics and functionalities of the application,including use cases demonstrating its ability to return names and numericalvalues. AskSport and its implementation are available for public access onHuggingFace.
  </details>

- **[ReSearch: Learning to Reason with Search for LLMs via Reinforcement Learning](http://arxiv.org/abs/2503.19470v2)**  `arXiv:2503.19470`  
  _Mingyang Chen, Tianpeng Li, Haoze Sun, Yijie Zhou, Chenzheng Zhu, Haofen Wang, et al._
  <details><summary>Abstract</summary>
  Large Language Models (LLMs) have shown remarkable capabilities in reasoning,exemplified by the success of OpenAI-o1 and DeepSeek-R1. However, integratingreasoning with external search processes remains challenging, especially forcomplex multi-hop questions requiring multiple retrieval steps. We proposeReSearch, a novel framework that trains LLMs to Reason with Search viareinforcement learning without using any supervised data on reasoning steps.Our approach treats search operations as integral components of the reasoningchain, where when and how to perform searches is guided by text-based thinking,and search results subsequently influence further reasoning. We train ReSearchon Qwen2.5-7B(-Instruct) and Qwen2.5-32B(-Instruct) models and conductextensive experiments. Despite being trained on only one dataset, our modelsdemonstrate strong generalizability across various benchmarks. Analysis revealsthat ReSearch naturally elicits advanced reasoning capabilities such asreflection and self-correction during the reinforcement learning process.
  </details>

- **[Robust Counterfactual Inference in Markov Decision Processes](http://arxiv.org/abs/2502.13731v2)**  `arXiv:2502.13731`  
  _Jessica Lally, Milad Kazemi, Nicola Paoletti_
  <details><summary>Abstract</summary>
  This paper addresses a key limitation in existing counterfactual inferencemethods for Markov Decision Processes (MDPs). Current approaches assume aspecific causal model to make counterfactuals identifiable. However, there areusually many causal models that align with the observational and interventionaldistributions of an MDP, each yielding different counterfactual distributions,so fixing a particular causal model limits the validity (and usefulness) ofcounterfactual inference. We propose a novel non-parametric approach thatcomputes tight bounds on counterfactual transition probabilities across allcompatible causal models. Unlike previous methods that require solvingprohibitively large optimisation problems (with variables that growexponentially in the size of the MDP), our approach provides closed-formexpressions for these bounds, making computation highly efficient and scalablefor non-trivial MDPs. Once such an interval counterfactual MDP is constructed,our method identifies robust counterfactual policies that optimise theworst-case reward w.r.t. the uncertain interval MDP probabilities. We evaluateour method on various case studies, demonstrating improved robustness overexisting methods.
  </details>

- **[VERA: Explainable Video Anomaly Detection via Verbalized Learning of Vision-Language Models](http://arxiv.org/abs/2412.01095v2)**  `arXiv:2412.01095`  
  _Muchao Ye, Weiyang Liu, Pan He_
  <details><summary>Abstract</summary>
  The rapid advancement of vision-language models (VLMs) has established a newparadigm in video anomaly detection (VAD): leveraging VLMs to simultaneouslydetect anomalies and provide comprehendible explanations for the decisions.Existing work in this direction often assumes the complex reasoning requiredfor VAD exceeds the capabilities of pretrained VLMs. Consequently, theseapproaches either incorporate specialized reasoning modules during inference orrely on instruction tuning datasets through additional training to adapt VLMsfor VAD. However, such strategies often incur substantial computational costsor data annotation overhead. To address these challenges in explainable VAD, weintroduce a verbalized learning framework named VERA that enables VLMs toperform VAD without model parameter modifications. Specifically, VERAautomatically decomposes the complex reasoning required for VAD intoreflections on simpler, more focused guiding questions capturing distinctabnormal patterns. It treats these reflective questions as learnable parametersand optimizes them through data-driven verbal interactions between learner andoptimizer VLMs, using coarsely labeled training data. During inference, VERAembeds the learned questions into model prompts to guide VLMs in generatingsegment-level anomaly scores, which are then refined into frame-level scoresvia the fusion of scene and temporal contexts. Experimental results onchallenging benchmarks demonstrate that the learned questions of VERA arehighly adaptable, significantly improving both detection performance andexplainability of VLMs for VAD.
  </details>

- **[Rethinking Training for De-biasing Text-to-Image Generation: Unlocking the Potential of Stable Diffusion](http://arxiv.org/abs/2408.12692v2)**  `arXiv:2408.12692`  
  _Eunji Kim, Siwon Kim, Minjun Park, Rahim Entezari, Sungroh Yoon_
  <details><summary>Abstract</summary>
  Recent advancements in text-to-image models, such as Stable Diffusion, showsignificant demographic biases. Existing de-biasing techniques rely heavily onadditional training, which imposes high computational costs and risks ofcompromising core image generation functionality. This hinders them from beingwidely adopted to real-world applications. In this paper, we explore StableDiffusion's overlooked potential to reduce bias without requiring additionaltraining. Through our analysis, we uncover that initial noises associated withminority attributes form "minority regions" rather than scattered. We viewthese "minority regions" as opportunities in SD to reduce bias. To unlock thepotential, we propose a novel de-biasing method called 'weak guidance,'carefully designed to guide a random noise to the minority regions withoutcompromising semantic integrity. Through analysis and experiments on variousversions of SD, we demonstrate that our proposed approach effectively reducesbias without additional training, achieving both efficiency and preservation ofcore image generation functionality.
  </details>

- **[A Survey on Self-play Methods in Reinforcement Learning](http://arxiv.org/abs/2408.01072v3)**  `arXiv:2408.01072`  
  _Ruize Zhang, Zelai Xu, Chengdong Ma, Chao Yu, Wei-Wei Tu, Wenhao Tang, et al._
  <details><summary>Abstract</summary>
  Self-play, characterized by agents' interactions with copies or past versionsof themselves, has recently gained prominence in reinforcement learning (RL).This paper first clarifies the preliminaries of self-play, including themulti-agent reinforcement learning framework and basic game theory concepts.Then, it provides a unified framework and classifies existing self-playalgorithms within this framework. Moreover, the paper bridges the gap betweenthe algorithms and their practical implications by illustrating the role ofself-play in different scenarios. Finally, the survey highlights openchallenges and future research directions in self-play. This paper is anessential guide map for understanding the multifaceted landscape of self-playin RL.
  </details>

- **[Vision language models are blind: Failing to translate detailed visual features into words](http://arxiv.org/abs/2407.06581v6)**  `arXiv:2407.06581`  
  _Pooyan Rahmanzadehgervi, Logan Bolton, Mohammad Reza Taesiri, Anh Totti Nguyen_
  <details><summary>Abstract</summary>
  While large language models with vision capabilities (VLMs), e.g., GPT-4o andGemini 1.5 Pro, score high on many vision-understanding benchmarks, they arestill struggling with low-level vision tasks that are easy to humans.Specifically, on BlindTest, our suite of 7 very simple tasks, includingidentifying (a) whether two circles overlap; (b) how many times two linesintersect; (c) which letter is being circled in a word; and (d) the number ofcircles in an Olympic-like logo, four state-of-the-art VLMs are only 58.07%accurate on average. Claude 3.5 Sonnet performs the best at 77.84% accuracy,far from the human expected accuracy of 100%. Across different imageresolutions and line widths, VLMs including slow-thinking models consistentlystruggle with those tasks that require precise spatial information whengeometric primitives overlap or are close. Yet, VLMs perform at near-100%accuracy when much more space is added to separate shapes and letters. Linearprobing experiments show that vision encoders contain sufficient visualinformation to solve BlindTest and that language models fail to decode thisinformation into correct answers. Code and data are at:https://vlmsareblind.github.io
  </details>

- **[CleanGen: Mitigating Backdoor Attacks for Generation Tasks in Large Language Models](http://arxiv.org/abs/2406.12257v3)**  `arXiv:2406.12257`  
  _Yuetai Li, Zhangchen Xu, Fengqing Jiang, Luyao Niu, Dinuka Sahabandu, Bhaskar Ramasubramanian, et al._
  <details><summary>Abstract</summary>
  The remarkable performance of large language models (LLMs) in generationtasks has enabled practitioners to leverage publicly available models to powercustom applications, such as chatbots and virtual assistants. However, the dataused to train or fine-tune these LLMs is often undisclosed, allowing anattacker to compromise the data and inject backdoors into the models. In thispaper, we develop a novel inference time defense, named CLEANGEN, to mitigatebackdoor attacks for generation tasks in LLMs. CLEANGEN is a lightweight andeffective decoding strategy that is compatible with the state-of-the-art (SOTA)LLMs. Our insight behind CLEANGEN is that compared to other LLMs, backdooredLLMs assign significantly higher probabilities to tokens representing theattacker-desired contents. These discrepancies in token probabilities enableCLEANGEN to identify suspicious tokens favored by the attacker and replace themwith tokens generated by another LLM that is not compromised by the sameattacker, thereby avoiding generation of attacker-desired content. We evaluateCLEANGEN against five SOTA backdoor attacks. Our results show that CLEANGENachieves lower attack success rates (ASR) compared to five SOTA baselinedefenses for all five backdoor attacks. Moreover, LLMs deploying CLEANGENmaintain helpfulness in their responses when serving benign user queries withminimal added computational overhead.
  </details>

- **[A Logic for Reasoning About Aggregate-Combine Graph Neural Networks](http://arxiv.org/abs/2405.00205v2)**  `arXiv:2405.00205`  
  _Pierre Nunn, Marco S√§lzer, Fran√ßois Schwarzentruber, Nicolas Troquard_
  <details><summary>Abstract</summary>
  We propose a modal logic in which counting modalities appear in linearinequalities. We show that each formula can be transformed into an equivalentgraph neural network (GNN). We also show that a broad class of GNNs can betransformed efficiently into a formula, thus significantly improving upon theliterature about the logical expressiveness of GNNs. We also show that thesatisfiability problem is PSPACE-complete. These results bring together thepromise of using standard logical methods for reasoning about GNNs and theirproperties, particularly in applications such as GNN querying, equivalencechecking, etc. We prove that such natural problems can be solved in polynomialspace.
  </details>

- **[Counterfactual Influence in Markov Decision Processes](http://arxiv.org/abs/2402.08514v2)**  `arXiv:2402.08514`  
  _Milad Kazemi, Jessica Lally, Ekaterina Tishchenko, Hana Chockler, Nicola Paoletti_
  <details><summary>Abstract</summary>
  Our work addresses a fundamental problem in the context of counterfactualinference for Markov Decision Processes (MDPs). Given an MDP path $\tau$, thiskind of inference allows us to derive counterfactual paths $\tau'$ describingwhat-if versions of $\tau$ obtained under different action sequences than thoseobserved in $\tau$. However, as the counterfactual states and actions deviatefrom the observed ones over time, the observation $\tau$ may no longerinfluence the counterfactual world, meaning that the analysis is no longertailored to the individual observation, resulting in interventional outcomesrather than counterfactual ones. Even though this issue specifically affectsthe popular Gumbel-max structural causal model used for MDP counterfactuals, ithas remained overlooked until now. In this work, we introduce a formalcharacterisation of influence based on comparing counterfactual andinterventional distributions. We devise an algorithm to constructcounterfactual models that automatically satisfy influence constraints.Leveraging such models, we derive counterfactual policies that are not justoptimal for a given reward structure but also remain tailored to the observedpath. Even though there is an unavoidable trade-off between policy optimalityand strength of influence constraints, our experiments demonstrate that it ispossible to derive (near-)optimal policies while remaining under the influenceof the observation.
  </details>

- **[Online POMDP Planning with Anytime Deterministic Guarantees](http://arxiv.org/abs/2310.01791v3)**  `arXiv:2310.01791`  
  _Moran Barenboim, Vadim Indelman_
  <details><summary>Abstract</summary>
  Decision-making under uncertainty is a critical aspect of many practicalautonomous systems due to incomplete information. Partially Observable MarkovDecision Processes (POMDPs) offer a mathematically principled framework forformulating decision-making problems under such conditions. However, finding anoptimal solution for a POMDP is generally intractable. In recent years, therehas been a significant progress of scaling approximate solvers from small tomoderately sized problems, using online tree search solvers. Often, suchapproximate solvers are limited to probabilistic or asymptotic guaranteestowards the optimal solution. In this paper, we derive a deterministicrelationship for discrete POMDPs between an approximated and the optimalsolution. We show that at any time, we can derive bounds that relate betweenthe existing solution and the optimal one. We show that our derivations providean avenue for a new set of algorithms and can be attached to existingalgorithms that have a certain structure to provide them with deterministicguarantees with marginal computational overhead. In return, not only do wecertify the solution quality, but we demonstrate that making a decision basedon the deterministic guarantee may result in superior performance compared tothe original algorithm without the deterministic certification.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Computation and Language üí¨

<details open><summary>Click to Collapse</summary>

- **[MemInsight: Autonomous Memory Augmentation for LLM Agents](http://arxiv.org/abs/2503.21760v1)**  `arXiv:2503.21760`  
  _Rana Salama, Jason Cai, Michelle Yuan, Anna Currey, Monica Sunkara, Yi Zhang, et al._
  <details><summary>Abstract</summary>
  Large language model (LLM) agents have evolved to intelligently processinformation, make decisions, and interact with users or tools. A key capabilityis the integration of long-term memory capabilities, enabling these agents todraw upon historical interactions and knowledge. However, the growing memorysize and need for semantic structuring pose significant challenges. In thiswork, we propose an autonomous memory augmentation approach, MemInsight, toenhance semantic data representation and retrieval mechanisms. By leveragingautonomous augmentation to historical interactions, LLM agents are shown todeliver more accurate and contextualized responses. We empirically validate theefficacy of our proposed approach in three task scenarios; conversationalrecommendation, question answering and event summarization. On the LLM-REDIALdataset, MemInsight boosts persuasiveness of recommendations by up to 14%.Moreover, it outperforms a RAG baseline by 34% in recall for LoCoMo retrieval.Our empirical results show the potential of MemInsight to enhance thecontextual performance of LLM agents across multiple tasks.
  </details>

- **[Effective Skill Unlearning through Intervention and Abstention](http://arxiv.org/abs/2503.21730v1)**  `arXiv:2503.21730`  
  _Yongce Li, Chung-En Sun, Tsui-Wei Weng_
  <details><summary>Abstract</summary>
  Large language Models (LLMs) have demonstrated remarkable skills acrossvarious domains. Understanding the mechanisms behind their abilities andimplementing controls over them is becoming increasingly important fordeveloping better models. In this paper, we focus on skill unlearning in LLMs,specifically unlearning a particular skill while retaining their overallcapabilities. We introduce two lightweight, training-free machine skillunlearning techniques for LLMs. First, we observe that the pre-activationdistribution of neurons in each Feed-Forward Layer (FFL) differs when the modeldemonstrates different skills. Additionally, we find that queries triggeringthe same skill cluster within the FFL key space and can be separated from otherqueries using a hypercube. Based on these observations, we propose twolightweight, training-free skill unlearning methods via \textit{intervention}and \textit{abstention} respectively: \texttt{Neuron Adjust} and \texttt{KeySpace Detection}. We evaluate our methods on unlearning math-solving,Python-coding, and comprehension skills across seven different languages. Theresults demonstrate their strong unlearning capabilities for the designatedskills. Specifically, \texttt{Key Space Detection} achieves over 80\% relativeperformance drop on the forgetting skill and less than 10\% relativeperformance drop on other skills and the model's general knowledge (MMLU) formost unlearning tasks. Our code is available athttps://github.com/Trustworthy-ML-Lab/effective_skill_unlearning
  </details>

- **[ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation](http://arxiv.org/abs/2503.21729v1)**  `arXiv:2503.21729`  
  _Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, et al._
  <details><summary>Abstract</summary>
  Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but relyprimarily on parametric knowledge, limiting factual accuracy. While recentworks equip reinforcement learning (RL)-based LRMs with retrieval capabilities,they suffer from overthinking and lack robustness in reasoning, reducing theireffectiveness in question answering (QA) tasks. To address this, we proposeReaRAG, a factuality-enhanced reasoning model that explores diverse querieswithout excessive iterations. Our solution includes a novel data constructionframework with an upper bound on the reasoning chain length. Specifically, wefirst leverage an LRM to generate deliberate thinking, then select an actionfrom a predefined action space (Search and Finish). For Search action, a queryis executed against the RAG engine, where the result is returned as observationto guide reasoning steps later. This process iterates until a Finish action ischosen. Benefiting from ReaRAG's strong reasoning capabilities, our approachoutperforms existing baselines on multi-hop QA. Further analysis highlights itsstrong reflective ability to recognize errors and refine its reasoningtrajectory. Our study enhances LRMs' factuality while effectively integratingrobust reasoning for Retrieval-Augmented Generation (RAG).
  </details>

- **[Collab: Controlled Decoding using Mixture of Agents for LLM Alignment](http://arxiv.org/abs/2503.21720v1)**  `arXiv:2503.21720`  
  _Souradip Chakraborty, Sujay Bhatt, Udari Madhushani Sehwag, Soumya Suvra Ghosal, Jiahao Qiu, Mengdi Wang, et al._
  <details><summary>Abstract</summary>
  Alignment of Large Language models (LLMs) is crucial for safe and trustworthydeployment in applications. Reinforcement learning from human feedback (RLHF)has emerged as an effective technique to align LLMs to human preferences andbroader utilities, but it requires updating billions of model parameters, whichis computationally expensive. Controlled Decoding, by contrast, provides amechanism for aligning a model at inference time without retraining. However,single-agent decoding approaches often struggle to adapt to diverse tasks dueto the complexity and variability inherent in these tasks. To strengthen thetest-time performance w.r.t the target task, we propose a mixture ofagent-based decoding strategies leveraging the existing off-the-shelf alignedLLM policies. Treating each prior policy as an agent in the spirit of mixtureof agent collaboration, we develop a decoding method that allows forinference-time alignment through a token-level selection strategy amongmultiple agents. For each token, the most suitable LLM is dynamically chosenfrom a pool of models based on a long-term utility metric. Thispolicy-switching mechanism ensures optimal model selection at each step,enabling efficient collaboration and alignment among LLMs during decoding.Theoretical analysis of our proposed algorithm establishes optimal performancewith respect to the target task represented via a target reward for the givenoff-the-shelf models. We conduct comprehensive empirical evaluations withopen-source aligned models on diverse tasks and preferences, which demonstratesthe merits of this approach over single-agent decoding baselines. Notably,Collab surpasses the current SoTA decoding strategy, achieving an improvementof up to 1.56x in average reward and 71.89% in GPT-4 based win-tie rate.
  </details>

- **[Outlier dimensions favor frequent tokens in language model](http://arxiv.org/abs/2503.21718v1)**  `arXiv:2503.21718`  
  _Iuri Macocco, Nora Graichen, Gemma Boleda, Marco Baroni_
  <details><summary>Abstract</summary>
  We study last-layer outlier dimensions, i.e.dimensions that display extremeactivations for the majority of inputs. We show that outlier dimensions arisein many different modern language models, and trace their function back to theheuristic of constantly predicting frequent words. We further show how a modelcan block this heuristic when it is not contextually appropriate, by assigninga counterbalancing weight mass to the remaining dimensions, and we investigatewhich model parameters boost outlier dimensions and when they arise duringtraining. We conclude that outlier dimensions are a specialized mechanismdiscovered by many distinct models to implement a useful token predictionheuristic.
  </details>

- **[CLAIMCHECK: How Grounded are LLM Critiques of Scientific Papers?](http://arxiv.org/abs/2503.21717v1)**  `arXiv:2503.21717`  
  _Jiefu Ou, William Gantt Walden, Kate Sanders, Zhengping Jiang, Kaiser Sun, Jeffrey Cheng, et al._
  <details><summary>Abstract</summary>
  A core part of scientific peer review involves providing expert critiquesthat directly assess the scientific claims a paper makes. While it is nowpossible to automatically generate plausible (if generic) reviews, ensuringthat these reviews are sound and grounded in the papers' claims remainschallenging. To facilitate LLM benchmarking on these challenges, we introduceCLAIMCHECK, an annotated dataset of NeurIPS 2023 and 2024 submissions andreviews mined from OpenReview. CLAIMCHECK is richly annotated by ML experts forweakness statements in the reviews and the paper claims that they dispute, aswell as fine-grained labels of the validity, objectivity, and type of theidentified weaknesses. We benchmark several LLMs on three claim-centric taskssupported by CLAIMCHECK, requiring models to (1) associate weaknesses with theclaims they dispute, (2) predict fine-grained labels for weaknesses and rewritethe weaknesses to enhance their specificity, and (3) verify a paper's claimswith grounded reasoning. Our experiments reveal that cutting-edge LLMs, whilecapable of predicting weakness labels in (2), continue to underperform relativeto human experts on all other tasks.
  </details>

- **[As easy as PIE: understanding when pruning causes language models to disagree](http://arxiv.org/abs/2503.21714v1)**  `arXiv:2503.21714`  
  _Pietro Tropeano, Maria Maistro, Tuukka Ruotsalo, Christina Lioma_
  <details><summary>Abstract</summary>
  Language Model (LM) pruning compresses the model by removing weights, nodes,or other parts of its architecture. Typically, pruning focuses on the resultingefficiency gains at the cost of effectiveness. However, when looking at howindividual data points are affected by pruning, it turns out that a particularsubset of data points always bears most of the brunt (in terms of reducedaccuracy) when pruning, but this effect goes unnoticed when reporting the meanaccuracy of all data points. These data points are called PIEs and have beenstudied in image processing, but not in NLP. In a study of various NLPdatasets, pruning methods, and levels of compression, we find that PIEs impactinference quality considerably, regardless of class frequency, and that BERT ismore prone to this than BiLSTM. We also find that PIEs contain a high amount ofdata points that have the largest influence on how well the model generalisesto unseen data. This means that when pruning, with seemingly moderate loss toaccuracy across all data points, we in fact hurt tremendously those data pointsthat matter the most. We trace what makes PIEs both hard and impactful toinference to their overall longer and more semantically complex text. Thesefindings are novel and contribute to understanding how LMs are affected bypruning. The code is available at: https://github.com/pietrotrope/AsEasyAsPIE
  </details>

- **[Embodied-Reasoner: Synergizing Visual Search, Reasoning, and Action for Embodied Interactive Tasks](http://arxiv.org/abs/2503.21696v1)**  `arXiv:2503.21696`  
  _Wenqi Zhang, Mengna Wang, Gangao Liu, Xu Huixin, Yiwei Jiang, Yongliang Shen, et al._
  <details><summary>Abstract</summary>
  Recent advances in deep thinking models have demonstrated remarkablereasoning capabilities on mathematical and coding tasks. However, theireffectiveness in embodied domains which require continuous interaction withenvironments through image action interleaved trajectories remains largely-unexplored. We present Embodied Reasoner, a model that extends o1 stylereasoning to interactive embodied search tasks. Unlike mathematical reasoningthat relies primarily on logical deduction, embodied scenarios demand spatialunderstanding, temporal reasoning, and ongoing self-reflection based oninteraction history. To address these challenges, we synthesize 9.3k coherentObservation-Thought-Action trajectories containing 64k interactive images and90k diverse thinking processes (analysis, spatial reasoning, reflection,planning, and verification). We develop a three-stage training pipeline thatprogressively enhances the model's capabilities through imitation learning,self-exploration via rejection sampling, and self-correction through reflectiontuning. The evaluation shows that our model significantly outperforms thoseadvanced visual reasoning models, e.g., it exceeds OpenAI o1, o3-mini, andClaude-3.7 by +9\%, 24\%, and +13\%. Analysis reveals our model exhibits fewerrepeated searches and logical inconsistencies, with particular advantages incomplex long-horizon tasks. Real-world environments also show our superioritywhile exhibiting fewer repeated searches and logical inconsistency cases.
  </details>

- **[JiraiBench: A Bilingual Benchmark for Evaluating Large Language Models' Detection of Human Self-Destructive Behavior Content in Jirai Community](http://arxiv.org/abs/2503.21679v1)**  `arXiv:2503.21679`  
  _Yunze Xiao, Tingyu He, Lionel Z. Wang, Yiming Ma, Xingyu Song, Xiaohang Xu, et al._
  <details><summary>Abstract</summary>
  This paper introduces JiraiBench, the first bilingual benchmark forevaluating large language models' effectiveness in detecting self-destructivecontent across Chinese and Japanese social media communities. Focusing on thetransnational "Jirai" (landmine) online subculture that encompasses multipleforms of self-destructive behaviors including drug overdose, eating disorders,and self-harm, we present a comprehensive evaluation framework incorporatingboth linguistic and cultural dimensions. Our dataset comprises 10,419 Chineseposts and 5,000 Japanese posts with multidimensional annotation along threebehavioral categories, achieving substantial inter-annotator agreement.Experimental evaluations across four state-of-the-art models reveal significantperformance variations based on instructional language, with Japanese promptsunexpectedly outperforming Chinese prompts when processing Chinese content.This emergent cross-cultural transfer suggests that cultural proximity cansometimes outweigh linguistic similarity in detection tasks. Cross-lingualtransfer experiments with fine-tuned models further demonstrate the potentialfor knowledge transfer between these language systems without explicit targetlanguage training. These findings highlight the need for culturally-informedapproaches to multilingual content moderation and provide empirical evidencefor the importance of cultural context in developing more effective detectionsystems for vulnerable online communities.
  </details>

- **[How do language models learn facts? Dynamics, curricula and hallucinations](http://arxiv.org/abs/2503.21676v1)**  `arXiv:2503.21676`  
  _Nicolas Zucchet, J√∂rg Bornschein, Stephanie Chan, Andrew Lampinen, Razvan Pascanu, Soham De_
  <details><summary>Abstract</summary>
  Large language models accumulate vast knowledge during pre-training, yet thedynamics governing this acquisition remain poorly understood. This workinvestigates the learning dynamics of language models on a synthetic factualrecall task, uncovering three key findings: First, language models learn inthree phases, exhibiting a performance plateau before acquiring precise factualknowledge. Mechanistically, this plateau coincides with the formation ofattention-based circuits that support recall. Second, the training datadistribution significantly impacts learning dynamics, as imbalanceddistributions lead to shorter plateaus. Finally, hallucinations emergesimultaneously with knowledge, and integrating new knowledge into the modelthrough fine-tuning is challenging, as it quickly corrupts its existingparametric memories. Our results emphasize the importance of data distributionin knowledge acquisition and suggest novel data scheduling strategies toaccelerate neural network training.
  </details>

- **[COMI-LINGUA: Expert Annotated Large-Scale Dataset for Multitask NLP in Hindi-English Code-Mixing](http://arxiv.org/abs/2503.21670v1)**  `arXiv:2503.21670`  
  _Rajvee Sheth, Himanshu Beniwal, Mayank Singh_
  <details><summary>Abstract</summary>
  The rapid growth of digital communication has driven the widespread use ofcode-mixing, particularly Hindi-English, in multilingual communities. Existingdatasets often focus on romanized text, have limited scope, or rely onsynthetic data, which fails to capture realworld language nuances. Humanannotations are crucial for assessing the naturalness and acceptability ofcode-mixed text. To address these challenges, We introduce COMI-LINGUA, thelargest manually annotated dataset for code-mixed text, comprising 100,970instances evaluated by three expert annotators in both Devanagari and Romanscripts. The dataset supports five fundamental NLP tasks: LanguageIdentification, Matrix Language Identification, Part-of-Speech Tagging, NamedEntity Recognition, and Translation. We evaluate LLMs on these tasks usingCOMILINGUA, revealing limitations in current multilingual modeling strategiesand emphasizing the need for improved code-mixed text processing capabilities.COMI-LINGUA is publically availabe at:https://huggingface.co/datasets/LingoIITGN/COMI-LINGUA.
  </details>

- **[A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond](http://arxiv.org/abs/2503.21614v1)**  `arXiv:2503.21614`  
  _Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, et al._
  <details><summary>Abstract</summary>
  Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, havedemonstrated strong performance gains by scaling up the length ofChain-of-Thought (CoT) reasoning during inference. However, a growing concernlies in their tendency to produce excessively long reasoning traces, which areoften filled with redundant content (e.g., repeated definitions), over-analysisof simple problems, and superficial exploration of multiple reasoning paths forharder tasks. This inefficiency introduces significant challenges for training,inference, and real-world deployment (e.g., in agent-based systems), wheretoken economy is critical. In this survey, we provide a comprehensive overviewof recent efforts aimed at improving reasoning efficiency in LRMs, with aparticular focus on the unique challenges that arise in this new paradigm. Weidentify common patterns of inefficiency, examine methods proposed across theLRM lifecycle, i.e., from pretraining to inference, and discuss promisingfuture directions for research. To support ongoing development, we alsomaintain a real-time GitHub repository tracking recent progress in the field.We hope this survey serves as a foundation for further exploration and inspiresinnovation in this rapidly evolving area.
  </details>

- **[Evaluating book summaries from internal knowledge in Large Language Models: a cross-model and semantic consistency approach](http://arxiv.org/abs/2503.21613v1)**  `arXiv:2503.21613`  
  _Javier Coronado-Bl√°zquez_
  <details><summary>Abstract</summary>
  We study the ability of large language models (LLMs) to generatecomprehensive and accurate book summaries solely from their internal knowledge,without recourse to the original text. Employing a diverse set of books andmultiple LLM architectures, we examine whether these models can synthesizemeaningful narratives that align with established human interpretations.Evaluation is performed with a LLM-as-a-judge paradigm: each AI-generatedsummary is compared against a high-quality, human-written summary via across-model assessment, where all participating LLMs evaluate not only theirown outputs but also those produced by others. This methodology enables theidentification of potential biases, such as the proclivity for models to favortheir own summarization style over others. In addition, alignment between thehuman-crafted and LLM-generated summaries is quantified using ROUGE andBERTScore metrics, assessing the depth of grammatical and semanticcorrespondence. The results reveal nuanced variations in content representationand stylistic preferences among the models, highlighting both strengths andlimitations inherent in relying on internal knowledge for summarization tasks.These findings contribute to a deeper understanding of LLM internal encodingsof factual information and the dynamics of cross-model evaluation, withimplications for the development of more robust natural language generativesystems.
  </details>

- **[SWI: Speaking with Intent in Large Language Models](http://arxiv.org/abs/2503.21544v1)**  `arXiv:2503.21544`  
  _Yuwei Yin, EunJeong Hwang, Giuseppe Carenini_
  <details><summary>Abstract</summary>
  Intent, typically clearly formulated and planned, functions as a cognitiveframework for reasoning and problem-solving. This paper introduces the conceptof Speaking with Intent (SWI) in large language models (LLMs), where theexplicitly generated intent encapsulates the model's underlying intention andprovides high-level planning to guide subsequent analysis and communication. Byemulating deliberate and purposeful thoughts in the human mind, SWI ishypothesized to enhance the reasoning capabilities and generation quality ofLLMs. Extensive experiments on mathematical reasoning benchmarks consistentlydemonstrate the superiority of Speaking with Intent over Baseline (i.e.,generation without explicit intent). Moreover, SWI outperforms answer-triggerprompting methods Chain-of-Thought and Plan-and-Solve and maintains competitiveperformance with the strong method ARR (Analyzing, Retrieving, and Reasoning).Additionally, the effectiveness and generalizability of SWI are solidified onreasoning-intensive question answering (QA) and text summarization benchmarks,where SWI brings consistent improvement to the Baseline generation. In textsummarization, SWI-generated summaries exhibit greater accuracy, conciseness,and factual correctness, with fewer hallucinations. Furthermore, humanevaluations verify the coherence, effectiveness, and interpretability of theintent produced by SWI. This proof-of-concept study creates a novel avenue forenhancing LLMs' reasoning abilities with cognitive notions.
  </details>

- **[Low-Resource Transliteration for Roman-Urdu and Urdu Using Transformer-Based Models](http://arxiv.org/abs/2503.21530v1)**  `arXiv:2503.21530`  
  _Umer Butt, Stalin Veranasi, G√ºnter Neumann_
  <details><summary>Abstract</summary>
  As the Information Retrieval (IR) field increasingly recognizes theimportance of inclusivity, addressing the needs of low-resource languagesremains a significant challenge. Transliteration between Urdu and its Romanizedform, Roman Urdu, remains underexplored despite the widespread use of bothscripts in South Asia. Prior work using RNNs on the Roman-Urdu-Parl datasetshowed promising results but suffered from poor domain adaptability and limitedevaluation. We propose a transformer-based approach using the m2m100multilingual translation model, enhanced with masked language modeling (MLM)pretraining and fine-tuning on both Roman-Urdu-Parl and the domain-diverseDakshina dataset. To address previous evaluation flaws, we introduce rigorousdataset splits and assess performance using BLEU, character-level BLEU, andCHRF. Our model achieves strong transliteration performance, with Char-BLEUscores of 96.37 for Urdu->Roman-Urdu and 97.44 for Roman-Urdu->Urdu. Theseresults outperform both RNN baselines and GPT-4o Mini and demonstrate theeffectiveness of multilingual transfer learning for low-resourcetransliteration tasks.
  </details>

- **[Datasets for Depression Modeling in Social Media: An Overview](http://arxiv.org/abs/2503.21513v1)**  `arXiv:2503.21513`  
  _Ana-Maria Bucur, Andreea-Codrina Moldovan, Krutika Parvatikar, Marcos Zampieri, Ashiqur R. KhudaBukhsh, Liviu P. Dinu_
  <details><summary>Abstract</summary>
  Depression is the most common mental health disorder, and its prevalenceincreased during the COVID-19 pandemic. As one of the most extensivelyresearched psychological conditions, recent research has increasingly focusedon leveraging social media data to enhance traditional methods of depressionscreening. This paper addresses the growing interest in interdisciplinaryresearch on depression, and aims to support early-career researchers byproviding a comprehensive and up-to-date list of datasets for analyzing andpredicting depression through social media data. We present an overview ofdatasets published between 2019 and 2024. We also make the comprehensive listof datasets available online as a continuously updated resource, with the hopethat it will facilitate further interdisciplinary research into the linguisticexpressions of depression on social media.
  </details>

- **[Fine-Grained Evaluation of Large Vision-Language Models in Autonomous Driving](http://arxiv.org/abs/2503.21505v1)**  `arXiv:2503.21505`  
  _Yue Li, Meng Tian, Zhenyu Lin, Jiangtong Zhu, Dechang Zhu, Haiqiang Liu, et al._
  <details><summary>Abstract</summary>
  Existing benchmarks for Vision-Language Model (VLM) on autonomous driving(AD) primarily assess interpretability through open-form visual questionanswering (QA) within coarse-grained tasks, which remain insufficient to assesscapabilities in complex driving scenarios. To this end, we introduce$\textbf{VLADBench}$, a challenging and fine-grained dataset featuringclose-form QAs that progress from static foundational knowledge and elements toadvanced reasoning for dynamic on-road situations. The elaborate$\textbf{VLADBench}$ spans 5 key domains: Traffic Knowledge Understanding,General Element Recognition, Traffic Graph Generation, Target AttributeComprehension, and Ego Decision-Making and Planning. These domains are furtherbroken down into 11 secondary aspects and 29 tertiary tasks for a granularevaluation. A thorough assessment of general and domain-specific (DS) VLMs onthis benchmark reveals both their strengths and critical limitations in ADcontexts. To further exploit the cognitive and reasoning interactions among the5 domains for AD understanding, we start from a small-scale VLM and train theDS models on individual domain datasets (collected from 1.4M DS QAs acrosspublic sources). The experimental results demonstrate that the proposedbenchmark provides a crucial step toward a more comprehensive assessment ofVLMs in AD, paving the way for the development of more cognitivelysophisticated and reasoning-capable AD systems.
  </details>

- **[Keyword-Oriented Multimodal Modeling for Euphemism Identification](http://arxiv.org/abs/2503.21504v1)**  `arXiv:2503.21504`  
  _Yuxue Hu, Junsong Li, Meixuan Chen, Dongyu Su, Tongguan Wang, Ying Sha_
  <details><summary>Abstract</summary>
  Euphemism identification deciphers the true meaning of euphemisms, such aslinking "weed" (euphemism) to "marijuana" (target keyword) in illicit texts,aiding content moderation and combating underground markets. While existingmethods are primarily text-based, the rise of social media highlights the needfor multimodal analysis, incorporating text, images, and audio. However, thelack of multimodal datasets for euphemisms limits further research. To addressthis, we regard euphemisms and their corresponding target keywords as keywordsand first introduce a keyword-oriented multimodal corpus of euphemisms(KOM-Euph), involving three datasets (Drug, Weapon, and Sexuality), includingtext, images, and speech. We further propose a keyword-oriented multimodaleuphemism identification method (KOM-EI), which uses cross-modal featurealignment and dynamic fusion modules to explicitly utilize the visual and audiofeatures of the keywords for efficient euphemism identification. Extensiveexperiments demonstrate that KOM-EI outperforms state-of-the-art models andlarge language models, and show the importance of our multimodal datasets.
  </details>

- **[OpenHuEval: Evaluating Large Language Model on Hungarian Specifics](http://arxiv.org/abs/2503.21500v1)**  `arXiv:2503.21500`  
  _Haote Yang, Xingjian Wei, Jiang Wu, No√©mi Ligeti-Nagy, Jiaxing Sun, Yinfan Wang, et al._
  <details><summary>Abstract</summary>
  We introduce OpenHuEval, the first benchmark for LLMs focusing on theHungarian language and specifics. OpenHuEval is constructed from a vastcollection of Hungarian-specific materials sourced from multiple origins. Inthe construction, we incorporated the latest design principles for evaluatingLLMs, such as using real user queries from the internet, emphasizing theassessment of LLMs' generative capabilities, and employing LLM-as-judge toenhance the multidimensionality and accuracy of evaluations. Ultimately,OpenHuEval encompasses eight Hungarian-specific dimensions, featuring fivetasks and 3953 questions. Consequently, OpenHuEval provides the comprehensive,in-depth, and scientifically accurate assessment of LLM performance in thecontext of the Hungarian language and its specifics. We evaluated currentmainstream LLMs, including both traditional LLMs and recently developed LargeReasoning Models. The results demonstrate the significant necessity forevaluation and model optimization tailored to the Hungarian language andspecifics. We also established the framework for analyzing the thinkingprocesses of LRMs with OpenHuEval, revealing intrinsic patterns and mechanismsof these models in non-English languages, with Hungarian serving as arepresentative example. We will release OpenHuEval athttps://github.com/opendatalab/OpenHuEval .
  </details>

- **[OmniVox: Zero-Shot Emotion Recognition with Omni-LLMs](http://arxiv.org/abs/2503.21480v1)**  `arXiv:2503.21480`  
  _John Murzaku, Owen Rambow_
  <details><summary>Abstract</summary>
  The use of omni-LLMs (large language models that accept any modality asinput), particularly for multimodal cognitive state tasks involving speech, isunderstudied. We present OmniVox, the first systematic evaluation of fouromni-LLMs on the zero-shot emotion recognition task. We evaluate on two widelyused multimodal emotion benchmarks: IEMOCAP and MELD, and find zero-shotomni-LLMs outperform or are competitive with fine-tuned audio models. Alongsideour audio-only evaluation, we also evaluate omni-LLMs on text only and text andaudio. We present acoustic prompting, an audio-specific prompting strategy foromni-LLMs which focuses on acoustic feature analysis, conversation contextanalysis, and step-by-step reasoning. We compare our acoustic prompting tominimal prompting and full chain-of-thought prompting techniques. We perform acontext window analysis on IEMOCAP and MELD, and find that using context helps,especially on IEMOCAP. We conclude with an error analysis on the generatedacoustic reasoning outputs from the omni-LLMs.
  </details>

- **[Harnessing Chain-of-Thought Metadata for Task Routing and Adversarial Prompt Detection](http://arxiv.org/abs/2503.21464v1)**  `arXiv:2503.21464`  
  _Ryan Marinelli, Josef Pichlmeier, Tamas Bisztray_
  <details><summary>Abstract</summary>
  In this work, we propose a metric called Number of Thoughts (NofT) todetermine the difficulty of tasks pre-prompting and support Large LanguageModels (LLMs) in production contexts. By setting thresholds based on the numberof thoughts, this metric can discern the difficulty of prompts and support moreeffective prompt routing. A 2% decrease in latency is achieved when routingprompts from the MathInstruct dataset through quantized, distilled versions ofDeepseek with 1.7 billion, 7 billion, and 14 billion parameters. Moreover, thismetric can be used to detect adversarial prompts used in prompt injectionattacks with high efficacy. The Number of Thoughts can inform a classifier thatachieves 95% accuracy in adversarial prompt detection. Our experiments addatasets used are available on our GitHub page:https://github.com/rymarinelli/Number_Of_Thoughts/tree/main.
  </details>

- **[Large Language Model Agent: A Survey on Methodology, Applications and Challenges](http://arxiv.org/abs/2503.21460v1)**  `arXiv:2503.21460`  
  _Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, et al._
  <details><summary>Abstract</summary>
  The era of intelligent agents is upon us, driven by revolutionaryadvancements in large language models. Large Language Model (LLM) agents, withgoal-driven behaviors and dynamic adaptation capabilities, potentiallyrepresent a critical pathway toward artificial general intelligence. Thissurvey systematically deconstructs LLM agent systems through amethodology-centered taxonomy, linking architectural foundations, collaborationmechanisms, and evolutionary pathways. We unify fragmented research threads byrevealing fundamental connections between agent design principles and theiremergent behaviors in complex environments. Our work provides a unifiedarchitectural perspective, examining how agents are constructed, how theycollaborate, and how they evolve over time, while also addressing evaluationmethodologies, tool applications, practical challenges, and diverse applicationdomains. By surveying the latest developments in this rapidly evolving field,we offer researchers a structured taxonomy for understanding LLM agents andidentify promising directions for future research. The collection is availableat https://github.com/luo-junyu/Awesome-Agent-Papers.
  </details>

- **[An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses](http://arxiv.org/abs/2503.21393v1)**  `arXiv:2503.21393`  
  _Rohitash Chandra, Aryan Chaudhary, Yeshwanth Rayavarapu_
  <details><summary>Abstract</summary>
  Large Language models (LLMs) have been prominent for language translation,including low-resource languages. There has been limited study about theassessment of the quality of translations generated by LLMs, including Gemini,GPT and Google Translate. In this study, we address this limitation by usingsemantic and sentiment analysis of selected LLMs for Indian languages,including Sanskrit, Telugu and Hindi. We select prominent texts that have beenwell translated by experts and use LLMs to generate their translations toEnglish, and then we provide a comparison with selected expert (human)translations. Our findings suggest that while LLMs have made significantprogress in translation accuracy, challenges remain in preserving sentiment andsemantic integrity, especially in figurative and philosophical contexts. Thesentiment analysis revealed that GPT-4o and GPT-3.5 are better at preservingthe sentiments for the Bhagavad Gita (Sanskrit-English) translations whencompared to Google Translate. We observed a similar trend for the case of Tamas(Hindi-English) and Maha P (Telugu-English) translations. GPT-4o performssimilarly to GPT-3.5 in the translation in terms of sentiments for the threelanguages. We found that LLMs are generally better at translation for capturingsentiments when compared to Google Translate.
  </details>

- **[Controlling Large Language Model with Latent Actions](http://arxiv.org/abs/2503.21383v1)**  `arXiv:2503.21383`  
  _Chengxing Jia, Ziniu Li, Pengyuan Wang, Yi-Chen Li, Zhenyu Hou, Yuxiao Dong, et al._
  <details><summary>Abstract</summary>
  Adapting Large Language Models (LLMs) to downstream tasks using ReinforcementLearning (RL) has proven to be an effective approach. However, LLMs do notinherently define the structure of an agent for RL training, particularly interms of defining the action space. This paper studies learning a compactlatent action space to enhance the controllability and exploration of RL forLLMs. We propose Controlling Large Language Models with Latent Actions (CoLA),a framework that integrates a latent action space into pre-trained LLMs. Weapply CoLA to the Llama-3.1-8B model. Our experiments demonstrate that,compared to RL with token-level actions, CoLA's latent action enables greatersemantic diversity in text generation. For enhancing downstream tasks, we showthat CoLA with RL achieves a score of 42.4 on the math500 benchmark, surpassingthe baseline score of 38.2, and reaches 68.2 when augmented with a Monte CarloTree Search variant. Furthermore, CoLA with RL consistently improvesperformance on agent-based tasks without degrading the pre-trained LLM'scapabilities, unlike the baseline. Finally, CoLA reduces computation time byhalf in tasks involving enhanced thinking prompts for LLMs by RL. These resultshighlight CoLA's potential to advance RL-based adaptation of LLMs fordownstream applications.
  </details>

- **[Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models](http://arxiv.org/abs/2503.21380v1)**  `arXiv:2503.21380`  
  _Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Zheng Liu, Zhongyuan Wang, et al._
  <details><summary>Abstract</summary>
  In recent years, the rapid development of large reasoning models has resultedin the saturation of existing benchmarks for evaluating mathematical reasoning,highlighting the urgent need for more challenging and rigorous evaluationframeworks. To address this gap, we introduce OlymMATH, a novel Olympiad-levelmathematical benchmark, designed to rigorously test the complex reasoningcapabilities of LLMs. OlymMATH features 200 meticulously curated problems, eachmanually verified and available in parallel English and Chinese versions. Theproblems are systematically organized into two distinct difficulty tiers: (1)AIME-level problems (easy) that establish a baseline for mathematical reasoningassessment, and (2) significantly more challenging problems (hard) designed topush the boundaries of current state-of-the-art models. In our benchmark, theseproblems span four core mathematical fields, each including a verifiablenumerical solution to enable objective, rule-based evaluation. Empiricalresults underscore the significant challenge presented by OlymMATH, withstate-of-the-art models including DeepSeek-R1 and OpenAI's o3-minidemonstrating notably limited accuracy on the hard subset. Furthermore, thebenchmark facilitates comprehensive bilingual assessment of mathematicalreasoning abilities-a critical dimension that remains largely unaddressed inmainstream mathematical reasoning benchmarks. We release the OlymMATH benchmarkat the STILL project: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.
  </details>

- **[Retrieving Time-Series Differences Using Natural Language Queries](http://arxiv.org/abs/2503.21378v1)**  `arXiv:2503.21378`  
  _Kota Dohi, Tomoya Nishida, Harsh Purohit, Takashi Endo, Yohei Kawaguchi_
  <details><summary>Abstract</summary>
  Effectively searching time-series data is essential for system analysis;however, traditional methods often require domain expertise to define searchcriteria. Recent advancements have enabled natural language-based search, butthese methods struggle to handle differences between time-series data. Toaddress this limitation, we propose a natural language query-based approach forretrieving pairs of time-series data based on differences specified in thequery. Specifically, we define six key characteristics of differences,construct a corresponding dataset, and develop a contrastive learning-basedmodel to align differences between time-series data with query texts.Experimental results demonstrate that our model achieves an overall mAP scoreof 0.994 in retrieving time-series pairs.
  </details>

- **[From User Preferences to Optimization Constraints Using Large Language Models](http://arxiv.org/abs/2503.21360v1)**  `arXiv:2503.21360`  
  _Manuela Sanguinetti, Alessandra Perniciano, Luca Zedda, Andrea Loddo, Cecilia Di Ruberto, Maurizio Atzori_
  <details><summary>Abstract</summary>
  This work explores using Large Language Models (LLMs) to translate userpreferences into energy optimization constraints for home appliances. Wedescribe a task where natural language user utterances are converted intoformal constraints for smart appliances, within the broader context of arenewable energy community (REC) and in the Italian scenario. We evaluate theeffectiveness of various LLMs currently available for Italian in translatingthese preferences resorting to classical zero-shot, one-shot, and few-shotlearning settings, using a pilot dataset of Italian user requests paired withcorresponding formal constraint representation. Our contributions includeestablishing a baseline performance for this task, publicly releasing thedataset and code for further research, and providing insights on observed bestpractices and limitations of LLMs in this particular domain
  </details>

- **[Fine-Tuning LLMs on Small Medical Datasets: Text Classification and Normalization Effectiveness on Cardiology reports and Discharge records](http://arxiv.org/abs/2503.21349v1)**  `arXiv:2503.21349`  
  _Noah Losch, Lucas Plagwitz, Antonius B√ºscher, Julian Varghese_
  <details><summary>Abstract</summary>
  We investigate the effectiveness of fine-tuning large language models (LLMs)on small medical datasets for text classification and named entity recognitiontasks. Using a German cardiology report dataset and the i2b2 Smoking Challengedataset, we demonstrate that fine-tuning small LLMs locally on limited trainingdata can improve performance achieving comparable results to larger models. Ourexperiments show that fine-tuning improves performance on both tasks, withnotable gains observed with as few as 200-300 training examples. Overall, thestudy highlights the potential of task-specific fine-tuning of LLMs forautomating clinical workflows and efficiently extracting structured data fromunstructured medical text.
  </details>

- **[ReFeed: Multi-dimensional Summarization Refinement with Reflective Reasoning on Feedback](http://arxiv.org/abs/2503.21332v1)**  `arXiv:2503.21332`  
  _Taewon Yun, Jihwan Oh, Hyangsuk Min, Yuho Lee, Jihwan Bang, Jason Cai, et al._
  <details><summary>Abstract</summary>
  Summarization refinement faces challenges when extending to multi-dimension.In this paper, we introduce ReFeed, a powerful summarization refinementpipeline that enhances multiple dimensions through reflective reasoning onfeedback. To achieve this, we release SumFeed-CoT, a large-scale Long-CoT-baseddataset optimized for training a lightweight model with reflective reasoning.Our experiments reveal how the number of dimensions, feedback exposure, andreasoning policy influence refinement performance, highlighting reflectivereasoning and simultaneously addressing multiple feedback is crucial tomitigate trade-off between dimensions. Furthermore, ReFeed is robust to noisyfeedback and feedback order. Lastly, our finding emphasizes that creating datawith a proper goal and guideline constitutes a fundamental pillar of effectivereasoning. The dataset and model will be released.
  </details>

- **[R-PRM: Reasoning-Driven Process Reward Modeling](http://arxiv.org/abs/2503.21295v1)**  `arXiv:2503.21295`  
  _Shuaijie She, Junxiao Liu, Yifeng Liu, Jiajun Chen, Xin Huang, Shujian Huang_
  <details><summary>Abstract</summary>
  Large language models (LLMs) inevitably make mistakes when performingstep-by-step mathematical reasoning. Process Reward Models (PRMs) have emergedas a promising solution by evaluating each reasoning step. However, existingPRMs typically output evaluation scores directly, limiting both learningefficiency and evaluation accuracy, which is further exacerbated by thescarcity of annotated data. To address these issues, we proposeReasoning-Driven Process Reward Modeling (R-PRM). First, we leverage strongerLLMs to generate seed data from limited annotations, effectively bootstrappingour model's reasoning capabilities and enabling comprehensive step-by-stepevaluation. Second, we further enhance performance through preferenceoptimization, without requiring additional annotated data. Third, we introduceinference-time scaling to fully harness the model's reasoning potential.Extensive experiments demonstrate R-PRM's effectiveness: on ProcessBench andPRMBench, it surpasses strong baselines by 11.9 and 8.5 points in F1 scores,respectively. When applied to guide mathematical reasoning, R-PRM achievesconsistent accuracy improvements of over 8.5 points across six challengingdatasets. Further analysis reveals that R-PRM exhibits more comprehensiveevaluation and stronger generalization capabilities, thereby highlighting itssignificant potential.
  </details>

- **[Cultivating Game Sense for Yourself: Making VLMs Gaming Experts](http://arxiv.org/abs/2503.21263v1)**  `arXiv:2503.21263`  
  _Wenxuan Lu, Jiangyang He, Zhanqiu Zhang, Yiwen Guo, Tianning Zang_
  <details><summary>Abstract</summary>
  Developing agents capable of fluid gameplay in first/third-person gameswithout API access remains a critical challenge in Artificial GeneralIntelligence (AGI). Recent efforts leverage Vision Language Models (VLMs) asdirect controllers, frequently pausing the game to analyze screens and planaction through language reasoning. However, this inefficient paradigmfundamentally restricts agents to basic and non-fluent interactions: relying onisolated VLM reasoning for each action makes it impossible to handle tasksrequiring high reactivity (e.g., FPS shooting) or dynamic adaptability (e.g.,ACT combat). To handle this, we propose a paradigm shift in gameplay agentdesign: instead of directly controlling gameplay, VLM develops specializedexecution modules tailored for tasks like shooting and combat. These moduleshandle real-time game interactions, elevating VLM to a high-level developer.Building upon this paradigm, we introduce GameSense, a gameplay agent frameworkwhere VLM develops task-specific game sense modules by observing task executionand leveraging vision tools and neural network training pipelines. Thesemodules encapsulate action-feedback logic, ranging from direct action rules toneural network-based decisions. Experiments demonstrate that our framework isthe first to achieve fluent gameplay in diverse genres, including ACT, FPS, andFlappy Bird, setting a new benchmark for game-playing agents.
  </details>

- **[ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition](http://arxiv.org/abs/2503.21248v1)**  `arXiv:2503.21248`  
  _Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, et al._
  <details><summary>Abstract</summary>
  Large language models (LLMs) have demonstrated potential in assistingscientific research, yet their ability to discover high-quality researchhypotheses remains unexamined due to the lack of a dedicated benchmark. Toaddress this gap, we introduce the first large-scale benchmark for evaluatingLLMs with a near-sufficient set of sub-tasks of scientific discovery:inspiration retrieval, hypothesis composition, and hypothesis ranking. Wedevelop an automated framework that extracts critical components - researchquestions, background surveys, inspirations, and hypotheses - from scientificpapers across 12 disciplines, with expert validation confirming its accuracy.To prevent data contamination, we focus exclusively on papers published in2024, ensuring minimal overlap with LLM pretraining data. Our evaluationreveals that LLMs perform well in retrieving inspirations, anout-of-distribution task, suggesting their ability to surface novel knowledgeassociations. This positions LLMs as "research hypothesis mines", capable offacilitating automated scientific discovery by generating innovative hypothesesat scale with minimal human intervention.
  </details>

- **[LLaVA-CMoE: Towards Continual Mixture of Experts for Large Vision-Language Models](http://arxiv.org/abs/2503.21227v1)**  `arXiv:2503.21227`  
  _Hengyuan Zhao, Ziqin Wang, Qixin Sun, Kaiyou Song, Yilin Li, Xiaolin Hu, et al._
  <details><summary>Abstract</summary>
  Although applying Mixture of Experts to large language models for learningnew tasks is widely regarded as an effective strategy for continuous learning,there still remain two major challenges: (1) As the number of tasks grows,simple parameter expansion strategies can lead to excessively large models. (2)Modifying the parameters of the existing router results in the erosion ofpreviously acquired knowledge. In this paper, we present an innovativeframework named LLaVA-CMoE, which is a continuous Mixture of Experts (MoE)architecture without any replay data. Specifically, we have developed a methodcalled Probe-Guided Knowledge Extension (PGKE), which employs probe experts toassess whether additional knowledge is required for a specific layer. Thisapproach enables the model to adaptively expand its network parameters based ontask distribution, thereby significantly improving the efficiency of parameterexpansion. Additionally, we introduce a hierarchical routing algorithm calledProbabilistic Task Locator (PTL), where high-level routing captures inter-taskinformation and low-level routing focuses on intra-task details, ensuring thatnew task experts do not interfere with existing ones. Our experiments showsthat our efficient architecture has substantially improved model performance onthe Coin benchmark while maintaining a reasonable parameter count.
  </details>

- **[UGen: Unified Autoregressive Multimodal Model with Progressive Vocabulary Learning](http://arxiv.org/abs/2503.21193v1)**  `arXiv:2503.21193`  
  _Hongxuan Tang, Hao Liu, Xinyan Xiao_
  <details><summary>Abstract</summary>
  We introduce UGen, a unified autoregressive multimodal model thatdemonstrates strong performance across text processing, image understanding,and image generation tasks simultaneously. UGen converts both texts and imagesinto discrete token sequences and utilizes a single transformer to generatethem uniformly in an autoregressive manner. To address the challengesassociated with unified multimodal learning, UGen is trained using a novelmechanism, namely progressive vocabulary learning. In this process, visualtoken IDs are incrementally activated and integrated into the training phase,ultimately enhancing the effectiveness of unified multimodal learning.Experiments on comprehensive text and image tasks show that UGen achieves asignificant overall performance improvement of 13.3% compared to the vanillaunified autoregressive method, and it also delivers competitive results acrossall tasks against several task-specific models.
  </details>

- **[Collaborative Evolution: Multi-Round Learning Between Large and Small Language Models for Emergent Fake News Detection](http://arxiv.org/abs/2503.21127v1)**  `arXiv:2503.21127`  
  _Ziyi Zhou, Xiaoming Zhang, Shenghan Tan, Litian Zhang, Chaozhuo Li_
  <details><summary>Abstract</summary>
  The proliferation of fake news on social media platforms has exerted asubstantial influence on society, leading to discernible impacts anddeleterious consequences. Conventional deep learning methodologies employingsmall language models (SLMs) suffer from the necessity for extensive supervisedtraining and the challenge of adapting to rapidly evolving circumstances. Largelanguage models (LLMs), despite their robust zero-shot capabilities, havefallen short in effectively identifying fake news due to a lack of pertinentdemonstrations and the dynamic nature of knowledge. In this paper, a novelframework Multi-Round Collaboration Detection (MRCD) is proposed to addressthese aforementioned limitations. The MRCD framework is capable of enjoying themerits from both LLMs and SLMs by integrating their generalization abilitiesand specialized functionalities, respectively. Our approach features atwo-stage retrieval module that selects relevant and up-to-date demonstrationsand knowledge, enhancing in-context learning for better detection of emergingnews events. We further design a multi-round learning framework to ensure morereliable detection results. Our framework MRCD achieves SOTA results on tworeal-world datasets Pheme and Twitter16, with accuracy improvements of 7.4\%and 12.8\% compared to using only SLMs, which effectively addresses thelimitations of current models and improves the detection of emergent fake news.
  </details>

- **[Leveraging Large Language Models for Risk Assessment in Hyperconnected Logistic Hub Network Deployment](http://arxiv.org/abs/2503.21115v1)**  `arXiv:2503.21115`  
  _Yinzhu Quan, Yujia Xu, Guanlin Chen, Frederick Benaben, Benoit Montreuil_
  <details><summary>Abstract</summary>
  The growing emphasis on energy efficiency and environmental sustainability inglobal supply chains introduces new challenges in the deployment ofhyperconnected logistic hub networks. In current volatile, uncertain, complex,and ambiguous (VUCA) environments, dynamic risk assessment becomes essential toensure successful hub deployment. However, traditional methods often struggleto effectively capture and analyze unstructured information. In this paper, wedesign an Large Language Model (LLM)-driven risk assessment pipeline integratedwith multiple analytical tools to evaluate logistic hub deployment. Thisframework enables LLMs to systematically identify potential risks by analyzingunstructured data, such as geopolitical instability, financial trends,historical storm events, traffic conditions, and emerging risks from newssources. These data are processed through a suite of analytical tools, whichare automatically called by LLMs to support a structured and data-drivendecision-making process for logistic hub selection. In addition, we designprompts that instruct LLMs to leverage these tools for assessing thefeasibility of hub selection by evaluating various risk types and levels.Through risk-based similarity analysis, LLMs cluster logistic hubs withcomparable risk profiles, enabling a structured approach to risk assessment. Inconclusion, the framework incorporates scalability with long-term memory andenhances decision-making through explanation and interpretation, enablingcomprehensive risk assessments for logistic hub deployment in hyperconnectedsupply chain networks.
  </details>

- **[Function Alignment: A New Theory for Mind and Intelligence, Part I: Foundations](http://arxiv.org/abs/2503.21106v1)**  `arXiv:2503.21106`  
  _Gus G. Xia_
  <details><summary>Abstract</summary>
  This paper introduces function alignment, a novel theory of mind andintelligence that is both intuitively compelling and structurally grounded. Itexplicitly models how meaning, interpretation, and analogy emerge frominteractions among layered representations, forming a coherent frameworkcapable not only of modeling minds but also of serving as a blueprint forbuilding them. One of the key theoretical insights derived from functionalignment is bounded interpretability, which provides a unified explanation forpreviously fragmented ideas in cognitive science, such as bounded rationality,symbol grounding, and analogy-making. Beyond modeling, the function alignmentframework bridges disciplines often kept apart, linking computationalarchitecture, psychological theory, and even contemplative traditions such asZen. Rather than building on any philosophical systems, it offers a structuralfoundation upon which multiple ways of understanding the mind may bereconstructed.
  </details>

- **[ZJUKLAB at SemEval-2025 Task 4: Unlearning via Model Merging](http://arxiv.org/abs/2503.21088v1)**  `arXiv:2503.21088`  
  _Haoming Xu, Shuxun Wang, Yanqiu Zhao, Yi Zhong, Ziyan Jiang, Ningyuan Zhao, et al._
  <details><summary>Abstract</summary>
  This paper presents the ZJUKLAB team's submission for SemEval-2025 Task 4:Unlearning Sensitive Content from Large Language Models. This task aims toselectively erase sensitive knowledge from large language models, avoiding bothover-forgetting and under-forgetting issues. We propose an unlearning systemthat leverages Model Merging (specifically TIES-Merging), combining twospecialized models into a more balanced unlearned model. Our system achievescompetitive results, ranking second among 26 teams, with an online score of0.944 for Task Aggregate and 0.487 for overall Aggregate. In this paper, wealso conduct local experiments and perform a comprehensive analysis of theunlearning process, examining performance trajectories, loss dynamics, andweight perspectives, along with several supplementary experiments, tounderstand the effectiveness of our method. Furthermore, we analyze theshortcomings of our method and evaluation metrics, emphasizing that MIA scoresand ROUGE-based metrics alone are insufficient to fully evaluate successfulunlearning. Finally, we emphasize the need for more comprehensive evaluationmethodologies and rethinking of unlearning objectives in future research. Codeis available at https://github.com/zjunlp/unlearn/tree/main/semeval25.
  </details>

- **[EQ-Negotiator: An Emotion-Reasoning LLM Agent in Credit Dialogues](http://arxiv.org/abs/2503.21080v1)**  `arXiv:2503.21080`  
  _Yuhan Liu, Yunbo Long_
  <details><summary>Abstract</summary>
  While large language model (LLM)-based chatbots have been applied foreffective engagement in credit dialogues, their capacity for dynamic emotionalexpression remains limited. Current agents primarily rely on passive empathyrather than affective reasoning. For instance, when faced with persistentclient negativity, the agent should employ strategic emotional adaptation byexpressing measured anger to discourage counterproductive behavior and guidethe conversation toward resolution. This context-aware emotional modulation isessential for imitating the nuanced decision-making of human negotiators. Thispaper introduces an EQ-negotiator that combines emotion sensing frompre-trained language models (PLMs) with emotional reasoning based on GameTheory and Hidden Markov Models. It takes into account both the current andhistorical emotions of the client to better manage and address negativeemotions during interactions. By fine-tuning pre-trained language models (PLMs)on public emotion datasets and validating them on the credit dialogue datasets,our approach enables LLM-based agents to effectively capture shifts in clientemotions and dynamically adjust their response tone based on our emotiondecision policies in real-world financial negotiations. This EQ-negotiator canalso help credit agencies foster positive client relationships, enhancingsatisfaction in credit services.
  </details>

- **[Shared Global and Local Geometry of Language Model Embeddings](http://arxiv.org/abs/2503.21073v1)**  `arXiv:2503.21073`  
  _Andrew Lee, Melanie Weber, Fernanda Vi√©gas, Martin Wattenberg_
  <details><summary>Abstract</summary>
  Researchers have recently suggested that models share common representations.In this work, we find that the token embeddings of language models exhibitcommon geometric structure. First, we find ``global'' similarities: tokenembeddings often share similar relative orientations. Next, we characterizelocal geometry in two ways: (1) by using Locally Linear Embeddings, and (2) bydefining a simple measure for the intrinsic dimension of each token embedding.Our intrinsic dimension measure demonstrates that token embeddings lie on alower dimensional manifold. We qualitatively show that tokens with lowerintrinsic dimensions often have semantically coherent clusters, while thosewith higher intrinsic dimensions do not. Both characterizations allow us tofind similarities in the local geometry of token embeddings. Perhaps mostsurprisingly, we find that alignment in token embeddings persists through thehidden states of language models, allowing us to develop an application forinterpretability. Namely, we empirically demonstrate that steering vectors fromone language model can be transferred to another, despite the two models havingdifferent dimensions.
  </details>

- **[Beyond Believability: Accurate Human Behavior Simulation with Fine-Tuned LLMs](http://arxiv.org/abs/2503.20749v2)**  `arXiv:2503.20749`  
  _Yuxuan Lu, Jing Huang, Yan Han, Bennet Bei, Yaochen Xie, Dakuo Wang, et al._
  <details><summary>Abstract</summary>
  Recent research shows that LLMs can simulate ``believable'' human behaviorsto power LLM agents via prompt-only methods. In this work, we focus onevaluating and improving LLM's objective ``accuracy'' rather than thesubjective ``believability'' in the web action generation task, leveraging alarge-scale, real-world dataset collected from online shopping human actions.We present the first comprehensive quantitative evaluation of state-of-the-artLLMs (e.g., DeepSeek-R1, Llama, and Claude) on the task of web actiongeneration. Our results show that fine-tuning LLMs on real-world behavioraldata substantially improves their ability to generate actions compared toprompt-only methods. Furthermore, incorporating synthesized reasoning tracesinto model training leads to additional performance gains, demonstrating thevalue of explicit rationale in behavior modeling. This work establishes a newbenchmark for evaluating LLMs in behavior simulation and offers actionableinsights into how real-world action data and reasoning augmentation can enhancethe fidelity of LLM agents.
  </details>

- **[PVLens: Enhancing Pharmacovigilance Through Automated Label Extraction](http://arxiv.org/abs/2503.20639v2)**  `arXiv:2503.20639`  
  _Jeffery L Painter, Gregory E Powell, Andrew Bate_
  <details><summary>Abstract</summary>
  Reliable drug safety reference databases are essential for pharmacovigilance,yet existing resources like SIDER are outdated and static. We introduce PVLens,an automated system that extracts labeled safety information from FDAStructured Product Labels (SPLs) and maps terms to MedDRA. PVLens integratesautomation with expert oversight through a web-based review tool. In validationagainst 97 drug labels, PVLens achieved an F1 score of 0.882, with high recall(0.983) and moderate precision (0.799). By offering a scalable, more accurateand continuously updated alternative to SIDER, PVLens enhances real-timepharamcovigilance with improved accuracy and contemporaneous insights.
  </details>

- **[Cross-Tokenizer Distillation via Approximate Likelihood Matching](http://arxiv.org/abs/2503.20083v2)**  `arXiv:2503.20083`  
  _Benjamin Minixhofer, Ivan Vuliƒá, Edoardo Maria Ponti_
  <details><summary>Abstract</summary>
  Distillation has shown remarkable success in transferring knowledge from aLarge Language Model (LLM) teacher to a student LLM. However, currentdistillation methods predominantly require the same tokenizer between theteacher and the student, restricting their applicability to only a small subsetof teacher-student pairs. In this work, we develop a cross-tokenizerdistillation method to solve this crucial deficiency. Our method is the firstto enable cross-tokenizer distillation without a next-token prediction loss asthe main objective, instead purely maximizing the student predictions'similarity to the teacher's predictions (known as pure distillation), whilealso being robust to large mismatches between the teacher and the studenttokenizer function and vocabulary. Empirically, our method enablessubstantially improved performance as tested on two use cases. First, we showthat viewing tokenizer transfer as self-distillation enables unprecedentlyeffective transfer across tokenizers. We transfer (subword-level) Llama andGemma models to byte-level tokenization more effectively than prior methodstransfer to a similar subword tokenizer under a comparable training budget.Transferring different base models to the same tokenizer also enablesensembling them (e.g., via averaging their predicted probabilities) whichboosts performance. Second, we use our cross-tokenizer distillation method todistil a large maths-specialized LLM into a smaller model, achievingcompetitive maths problem-solving performance. Overall, our results makesubstantial strides toward better adaptability and enhanced interaction betweendifferent LLMs.
  </details>

- **[AlphaSpace: Enabling Robotic Actions through Semantic Tokenization and Symbolic Reasoning](http://arxiv.org/abs/2503.18769v2)**  `arXiv:2503.18769`  
  _Alan Dao, Dinh Bach Vu, Bui Quang Huy_
  <details><summary>Abstract</summary>
  This paper presents AlphaSpace, a novel methodology designed to enhance thespatial reasoning capabilities of language models for robotic manipulation in3D Cartesian space. AlphaSpace employs a hierarchical semantics-basedtokenization strategy that encodes spatial information at both coarse andfine-grained levels. Our approach represents objects with their attributes,positions, and height information through structured tokens, enabling precisespatial reasoning without relying on traditional vision-based embeddings. Thisapproach enables LLMs to accurately manipulate objects by positioning them atspecific (x, y, z) coordinates. Experimental results suggest that AlphaSpacedemonstrates promising potential for improving manipulation tasks, achieving atotal accuracy of 66.67%, compared to 37.5% for GPT-4o and 29.17% for Claude3.5 Sonnet. These results demonstrate the potential of structured spatialencoding for manipulation tasks and warrant further exploration.
  </details>

- **[WindowKV: Task-Adaptive Group-Wise KV Cache Window Selection for Efficient LLM Inference](http://arxiv.org/abs/2503.17922v2)**  `arXiv:2503.17922`  
  _Youhui Zuo, Sibo Wei, Chen Zhang, Zhuorui Liu, Wenpeng Lu, Dawei Song_
  <details><summary>Abstract</summary>
  With the advancements in long-context inference capabilities of largelanguage models (LLMs), the KV cache has become one of the foundationalcomponents. However, its substantial GPU memory consumption makes KV cachecompression a key technique for enabling efficient LLM inference in industrialscenarios. While recent studies have focused on optimizing the memory occupiedby the KV cache, they overlook two critical factors: preserving semanticcoherence and considering task-specific characteristic during compression. Toaddress these limitations, we propose a novel task-adaptive KV cache windowselection method, WindowKV. WindowKV dynamically selects local semantic windowsconsisting of consecutive tokens, according to task-specific characteristics,ensuring the retained KV cache captures continuous, essential context.Additionally, we introduce an intra-group layer KV cache indices sharingstrategy to reduce computational overhead, achieving a balance betweenperformance and efficiency. We rigorously evaluate WindowKV on the LongBenchbenchmark, and the results demonstrate that it maintains a performancecomparable to full KV cache retention while using only 12% of the original KVcache, significantly reducing memory requirements. Furthermore, our method alsoachieves state-of-the-art results in the Needle-in-a-Haystack evaluation,highlighting its effectiveness and robustness.
  </details>

- **[Accelerating Antibiotic Discovery with Large Language Models and Knowledge Graphs](http://arxiv.org/abs/2503.16655v2)**  `arXiv:2503.16655`  
  _Maxime Delmas, Magdalena Wysocka, Danilo Gusicuma, Andr√© Freitas_
  <details><summary>Abstract</summary>
  The discovery of novel antibiotics is critical to address the growingantimicrobial resistance (AMR). However, pharmaceutical industries face highcosts (over $1 billion), long timelines, and a high failure rate, worsened bythe rediscovery of known compounds. We propose an LLM-based pipeline that actsas an alarm system, detecting prior evidence of antibiotic activity to preventcostly rediscoveries. The system integrates organism and chemical literatureinto a Knowledge Graph (KG), ensuring taxonomic resolution, synonym handling,and multi-level evidence classification. We tested the pipeline on a privatelist of 73 potential antibiotic-producing organisms, disclosing 12 negativehits for evaluation. The results highlight the effectiveness of the pipelinefor evidence reviewing, reducing false negatives, and acceleratingdecision-making. The KG for negative hits and the user interface forinteractive exploration will be made publicly available.
  </details>

- **[Dynamic Bi-Elman Attention Networks: A Dual-Directional Context-Aware Test-Time Learning for Text Classification](http://arxiv.org/abs/2503.15469v3)**  `arXiv:2503.15469`  
  _ZhengLin Lai, MengYao Liao, Dong Xu_
  <details><summary>Abstract</summary>
  Text classification, a fundamental task in natural language processing, aimsto categorize textual data into predefined labels. Traditional methodsstruggled with complex linguistic structures and semantic dependencies.However, the advent of deep learning, particularly recurrent neural networksand Transformer-based models, has significantly advanced the field by enablingnuanced feature extraction and context-aware predictions. Despite theseimprovements, existing models still exhibit limitations in balancinginterpretability, computational efficiency, and long-range contextualunderstanding. To address these challenges, this paper proposes the DynamicBidirectional Elman with Attention Network (DBEAN). DBEAN integratesbidirectional temporal modeling with self-attention mechanisms. It dynamicallyassigns weights to critical segments of input, improving contextualrepresentation while maintaining computational efficiency.
  </details>

- **[Bias Evaluation and Mitigation in Retrieval-Augmented Medical Question-Answering Systems](http://arxiv.org/abs/2503.15454v3)**  `arXiv:2503.15454`  
  _Yuelyu Ji, Hang Zhang, Yanshan Wang_
  <details><summary>Abstract</summary>
  Medical Question Answering systems based on Retrieval Augmented Generation ispromising for clinical decision support because they can integrate externalknowledge, thus reducing inaccuracies inherent in standalone large languagemodels (LLMs). However, these systems may unintentionally propagate or amplifybiases associated with sensitive demographic attributes like race, gender, andsocioeconomic factors. This study systematically evaluates demographic biaseswithin medical RAG pipelines across multiple QA benchmarks, including MedQA,MedMCQA, MMLU, and EquityMedQA. We quantify disparities in retrievalconsistency and answer correctness by generating and analyzing queriessensitive to demographic variations. We further implement and compare severalbias mitigation strategies to address identified biases, including Chain ofThought reasoning, Counterfactual filtering, Adversarial prompt refinement, andMajority Vote aggregation. Experimental results reveal significant demographicdisparities, highlighting that Majority Vote aggregation notably improvesaccuracy and fairness metrics. Our findings underscore the critical need forexplicitly fairness-aware retrieval methods and prompt engineering strategiesto develop truly equitable medical QA systems.
  </details>

- **[TLUE: A Tibetan Language Understanding Evaluation Benchmark](http://arxiv.org/abs/2503.12051v2)**  `arXiv:2503.12051`  
  _Fan Gao, Cheng Huang, Nyima Tashi, Xiangxiang Wang, Thupten Tsering, Ban Ma-bao, et al._
  <details><summary>Abstract</summary>
  Large language models (LLMs) have made tremendous progress in recent years,but low-resource languages, such as Tibetan, remain significantlyunderrepresented in their evaluation. Despite Tibetan being spoken by overseven million people, it has largely been neglected in the development andassessment of LLMs. To address this gap, we present TLUE (A Tibetan LanguageUnderstanding Evaluation Benchmark), the first large-scale benchmark forassessing LLMs' capabilities in Tibetan. TLUE comprises two major components:(1) a comprehensive multi-task understanding benchmark spanning 5 domains and67 subdomains, and (2) a safety benchmark covering 7 subdomains. We evaluate adiverse set of state-of-the-art LLMs. Experimental results demonstrate thatmost LLMs perform below the random baseline, highlighting the considerablechallenges LLMs face in processing Tibetan, a low-resource language. TLUEprovides an essential foundation for driving future research and progress inTibetan language understanding and underscores the need for greater inclusivityin LLM development.
  </details>

- **[Cognitive-Mental-LLM: Evaluating Reasoning in Large Language Models for Mental Health Prediction via Online Text](http://arxiv.org/abs/2503.10095v2)**  `arXiv:2503.10095`  
  _Avinash Patil, Amardeep Kour Gedhu_
  <details><summary>Abstract</summary>
  Large Language Models (LLMs) have demonstrated potential in predicting mentalhealth outcomes from online text, yet traditional classification methods oftenlack interpretability and robustness. This study evaluates structured reasoningtechniques-Chain-of-Thought (CoT), Self-Consistency (SC-CoT), andTree-of-Thought (ToT)-to improve classification accuracy across multiple mentalhealth datasets sourced from Reddit. We analyze reasoning-driven promptingstrategies, including Zero-shot CoT and Few-shot CoT, using key performancemetrics such as Balanced Accuracy, F1 score, and Sensitivity/Specificity. Ourfindings indicate that reasoning-enhanced techniques improve classificationperformance over direct prediction, particularly in complex cases. Compared tobaselines such as Zero Shot non-CoT Prompting, and fine-tuned pre-trainedtransformers such as BERT and Mental-RoBerta, and fine-tuned Open Source LLMssuch as Mental Alpaca and Mental-Flan-T5, reasoning-driven LLMs yield notablegains on datasets like Dreaddit (+0.52\% over M-LLM, +0.82\% over BERT) andSDCNL (+4.67\% over M-LLM, +2.17\% over BERT). However, performance declines inDepression Severity, and CSSRS predictions suggest dataset-specificlimitations, likely due to our using a more extensive test set. Among promptingstrategies, Few-shot CoT consistently outperforms others, reinforcing theeffectiveness of reasoning-driven LLMs. Nonetheless, dataset variabilityhighlights challenges in model reliability and interpretability. This studyprovides a comprehensive benchmark of reasoning-based LLM techniques for mentalhealth text classification. It offers insights into their potential forscalable clinical applications while identifying key challenges for futureimprovements.
  </details>

- **[R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs](http://arxiv.org/abs/2502.12767v4)**  `arXiv:2502.12767`  
  _Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi_
  <details><summary>Abstract</summary>
  Recent studies have combined Large Language Models (LLMs) with KnowledgeGraphs (KGs) to enhance reasoning, improving inference accuracy withoutadditional training while mitigating hallucination. However, existingframeworks are often rigid, struggling to adapt to KG or task changes. Theyalso rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.To address this, We introduce R2-KG, a plug-and-play, dual-agent framework thatseparates reasoning into two roles: an Operator (a low-capacity LLM) thatgathers evidence and a Supervisor (a high-capacity LLM) that makes finaljudgments. This design is cost-efficient for LLM inference while stillmaintaining strong reasoning accuracy. Additionally, R2-KG employs anAbstention mechanism, generating answers only when sufficient evidence iscollected from KG, which significantly enhances reliability. Experiments acrossmultiple KG-based reasoning tasks show that R2-KG consistently outperformsbaselines in both accuracy and reliability, regardless of the inherentcapability of LLMs used as the Operator. Further experiments reveal that thesingle-agent version of R2-KG, equipped with a strict self-consistencystrategy, achieves significantly higher-than-baseline reliability whilereducing inference cost. However, it also leads to a higher abstention rate incomplex KGs. Our findings establish R2-KG as a flexible and cost-effectivesolution for KG-based reasoning. It reduces reliance on high-capacity LLMswhile ensuring trustworthy inference. The code is available athttps://github.com/ekrxjwh2009/R2-KG/.
  </details>

- **[Adapting Language-Specific LLMs to a Reasoning Model in One Day via Model Merging -- An Open Recipe](http://arxiv.org/abs/2502.09056v3)**  `arXiv:2502.09056`  
  _Kunat Pipatanakul, Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai_
  <details><summary>Abstract</summary>
  This paper investigates data selection and model merging methodologies aimedat incorporating advanced reasoning capabilities such as those of DeepSeek R1into language-specific large language models (LLMs), with a particular focus onthe Thai LLM. Our goal is to enhance the reasoning capabilities oflanguage-specific LLMs while maintaining their target language abilities.DeepSeek R1 excels in reasoning but primarily benefits high-resource languagessuch as English and Chinese. However, low-resource languages remain underserveddue to the dominance of English-centric training data and model optimizations,which limit performance in these languages. This limitation results inunreliable code-switching and diminished effectiveness on tasks in low-resourcelanguages. Meanwhile, local and regional LLM initiatives have attempted tobridge this gap by developing language-specific LLMs that focus on improvinglocal linguistic fidelity. We demonstrate that, with only publicly availabledatasets and a computational budget of $120, it is possible to enhance thereasoning capabilities of language-specific LLMs to match the level of DeepSeekR1, without compromising their performance on target language tasks.
  </details>

- **[Typhoon T1: An Open Thai Reasoning Model](http://arxiv.org/abs/2502.09042v2)**  `arXiv:2502.09042`  
  _Pittawat Taveekitworachai, Potsawee Manakul, Kasima Tharnpipitchai, Kunat Pipatanakul_
  <details><summary>Abstract</summary>
  This paper introduces Typhoon T1, an open effort to develop an open Thaireasoning model. A reasoning model is a relatively new type of generative modelbuilt on top of large language models (LLMs). A reasoning model generates along chain of thought before arriving at a final answer, an approach found toimprove performance on complex tasks. However, details on developing such amodel are limited, especially for reasoning models that can generate traces ina low-resource language. Typhoon T1 presents an open effort that dives into thedetails of developing a reasoning model in a more cost-effective way byleveraging supervised fine-tuning using open datasets, instead of reinforcementlearning. This paper shares the details about synthetic data generation andtraining, as well as our dataset and model weights. Additionally, we provideinsights gained from developing a reasoning model that generalizes acrossdomains and is capable of generating reasoning traces in a low-resourcelanguage, using Thai as an example. We hope this open effort provides afoundation for further research in this field.
  </details>

- **[Tuning-Free Personalized Alignment via Trial-Error-Explain In-Context Learning](http://arxiv.org/abs/2502.08972v2)**  `arXiv:2502.08972`  
  _Hyundong Cho, Karishma Sharma, Nicolaas Jedema, Leonardo F. R. Ribeiro, Alessandro Moschitti, Ravi Krishnan, et al._
  <details><summary>Abstract</summary>
  Language models are aligned to the collective voice of many, resulting ingeneric outputs that do not align with specific users' styles. In this work, wepresent Trial-Error-Explain In-Context Learning} (ITCL), a tuning-free methodthat personalizes language models for text generation tasks with fewer than 10examples per user. TICL iteratively expands an in-context learning prompt via atrial-error-explain process, adding model-generated negative samples andexplanations that provide fine-grained guidance towards a specific user'sstyle. TICL achieves favorable win rates on pairwise comparisons withLLM-as-a-judge up to 91.5% against the previous state-of-the-art andoutperforms competitive tuning-free baselines for personalized alignment tasksof writing emails, essays and news articles. Both lexical and qualitativeanalyses show that the negative samples and explanations enable language modelsto learn stylistic context more effectively and overcome the bias towardsstructural and formal phrases observed in their zero-shot outputs. Byfront-loading inference compute to create a user-specific in-context learningprompt that does not require extra generation steps at test time, TICL presentsa novel yet simple approach for personalized alignment.
  </details>

- **[Systematic Knowledge Injection into Large Language Models via Diverse Augmentation for Domain-Specific RAG](http://arxiv.org/abs/2502.08356v3)**  `arXiv:2502.08356`  
  _Kushagra Bhushan, Yatin Nandwani, Dinesh Khandelwal, Sonam Gupta, Gaurav Pandey, Dinesh Raghu, et al._
  <details><summary>Abstract</summary>
  Retrieval-Augmented Generation (RAG) has emerged as a prominent method forincorporating domain knowledge into Large Language Models (LLMs). While RAGenhances response relevance by incorporating retrieved domain knowledge in thecontext, retrieval errors can still lead to hallucinations and incorrectanswers. To recover from retriever failures, domain knowledge is injected byfine-tuning the model to generate the correct response, even in the case ofretrieval errors. However, we observe that without systematic knowledgeaugmentation, fine-tuned LLMs may memorize new information but still fail toextract relevant domain knowledge, leading to poor performance. In this work,we present a novel framework that significantly enhances the fine-tuningprocess by augmenting the training data in two ways -- context augmentation andknowledge paraphrasing. In context augmentation, we create multiple trainingsamples for a given QA pair by varying the relevance of the retrievedinformation, teaching the model when to ignore and when to rely on retrievedcontent. In knowledge paraphrasing, we fine-tune with multiple answers to thesame question, enabling LLMs to better internalize specialized knowledge. Tomitigate catastrophic forgetting due to fine-tuning, we add a domain-specificidentifier to a question and also utilize a replay buffer containing general QApairs. Experimental results demonstrate the efficacy of our method overexisting techniques, achieving up to 10\% relative gain in token-level recallwhile preserving the LLM's generalization capabilities.
  </details>

- **[Enhancing LLM Character-Level Manipulation via Divide and Conquer](http://arxiv.org/abs/2502.08180v2)**  `arXiv:2502.08180`  
  _Zhen Xiong, Yujun Cai, Bryan Hooi, Nanyun Peng, Zhecheng Li, Yiwei Wang_
  <details><summary>Abstract</summary>
  Large Language Models (LLMs) have demonstrated strong generalizationcapabilities across a wide range of natural language processing (NLP) tasks.However, they exhibit notable weaknesses in character-level stringmanipulation, struggling with fundamental operations such as characterdeletion, insertion, and substitution. These challenges stem primarily fromtokenization constraints, despite the critical role of such operations in datapreprocessing and code generation. Through systematic analysis, we derive twokey insights: (1) LLMs face significant difficulties in leveraging intrinsictoken knowledge for character-level reasoning, and (2) atomized word structurescan substantially enhance LLMs' ability to process token-level structuralinformation. Building on these insights, we propose Character-LevelManipulation via Divide and Conquer, a novel approach designed to bridge thegap between token-level processing and character-level manipulation. Our methoddecomposes complex operations into explicit character-level subtasks coupledwith controlled token reconstruction phases, leading to significantimprovements in accuracy. Without additional training, our method significantlyimproves accuracies on the $\texttt{Deletion}$, $\texttt{Insertion}$, and$\texttt{Substitution}$ tasks. To support further research, we open-source ourimplementation and benchmarks.
  </details>

- **[Reinforced Lifelong Editing for Language Models](http://arxiv.org/abs/2502.05759v3)**  `arXiv:2502.05759`  
  _Zherui Li, Houcheng Jiang, Hao Chen, Baolong Bi, Zhenhong Zhou, Fei Sun, et al._
  <details><summary>Abstract</summary>
  Large language models (LLMs) acquire information from pre-training corpora,but their stored knowledge can become inaccurate or outdated over time. Modelediting addresses this challenge by modifying model parameters withoutretraining, and prevalent approaches leverage hypernetworks to generate theseparameter updates. However, they face significant challenges in lifelongediting due to their incompatibility with LLM parameters that dynamicallychange during the editing process. To address this, we observed thathypernetwork-based lifelong editing aligns with reinforcement learning modelingand proposed RLEdit, an RL-based editing method. By treating editing losses asrewards and optimizing hypernetwork parameters at the full knowledge sequencelevel, we enable it to precisely capture LLM changes and generate appropriateparameter updates. Our extensive empirical evaluation across several LLMsdemonstrates that RLEdit outperforms existing methods in lifelong editing withsuperior effectiveness and efficiency, achieving a 59.24% improvement whilerequiring only 2.11% of the time compared to most approaches. Our code isavailable at: https://github.com/zhrli324/RLEdit.
  </details>

- **[AnyEdit: Edit Any Knowledge Encoded in Language Models](http://arxiv.org/abs/2502.05628v2)**  `arXiv:2502.05628`  
  _Houcheng Jiang, Junfeng Fang, Ningyu Zhang, Guojun Ma, Mingyang Wan, Xiang Wang, et al._
  <details><summary>Abstract</summary>
  Large language models (LLMs) often produce incorrect or outdated information,necessitating efficient and precise knowledge updates. Current model editingmethods, however, struggle with long-form knowledge in diverse formats, such aspoetry, code snippets, and mathematical derivations. These limitations arisefrom their reliance on editing a single token's hidden state, a limitation weterm "efficacy barrier". To solve this, we propose AnyEdit, a newautoregressive editing paradigm. It decomposes long-form knowledge intosequential chunks and iteratively edits the key token in each chunk, ensuringconsistent and accurate outputs. Theoretically, we ground AnyEdit in the ChainRule of Mutual Information, showing its ability to update any knowledge withinLLMs. Empirically, it outperforms strong baselines by 21.5% on benchmarksincluding UnKEBench, AKEW, and our new EditEverything dataset for long-formdiverse-formatted knowledge. Additionally, AnyEdit serves as a plug-and-playframework, enabling current editing methods to update knowledge with arbitrarylength and format, significantly advancing the scope and practicality of LLMknowledge editing.
  </details>

- **[Group Reasoning Emission Estimation Networks](http://arxiv.org/abs/2502.06874v2)**  `arXiv:2502.06874`  
  _Yanming Guo, Xiao Qian, Kevin Credit, Jin Ma_
  <details><summary>Abstract</summary>
  Accurate greenhouse gas (GHG) emission reporting is critical for governments,businesses, and investors. However, adoption remains limited particularly amongsmall and medium enterprises due to high implementation costs, fragmentedemission factor databases, and a lack of robust sector classification methods.To address these challenges, we introduce Group Reasoning Emission EstimationNetworks (GREEN), an AI-driven carbon accounting framework that standardizesenterprise-level emission estimation, constructs a large-scale benchmarkdataset, and leverages a novel reasoning approach with large language models(LLMs). Specifically, we compile textual descriptions for 20,850 companies withvalidated North American Industry Classification System (NAICS) labels andalign these with an economic model of carbon intensity factors. By reframingsector classification as an information retrieval task, we fine-tuneSentence-BERT models using a contrastive learning loss. To overcome thelimitations of single-stage models in handling thousands of hierarchicalcategories, we propose a Group Reasoning method that ensembles LLM classifiersbased on the natural NAICS ontology, decomposing the task into multiplesub-classification steps. We theoretically prove that this approach reducesclassification uncertainty and computational complexity. Experiments on 1,114NAICS categories yield state-of-the-art performance (83.68% Top-1, 91.47%Top-10 accuracy), and case studies on 20 companies report a mean absolutepercentage error (MAPE) of 45.88%. The project is available at:https://huggingface.co/datasets/Yvnminc/ExioNAICS.
  </details>

- **[iTool: Boosting Tool Use of Large Language Models via Iterative Reinforced Fine-Tuning](http://arxiv.org/abs/2501.09766v3)**  `arXiv:2501.09766`  
  _Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, et al._
  <details><summary>Abstract</summary>
  Augmenting large language models (LLMs) with external tools is known as apromising approach to enhancing their capabilities, especially for complextasks. Synthesizing tool-use data through real-world simulations is aneffective way to achieve it. Nevertheless, our investigation reveals that (1)training gains significantly decay as synthetic data increases. The modelstruggles to benefit from more synthetic data due to potential data diversityissues, resulting in poor performance in complex scenarios. Moreover, we findthat (2) this challenge primarily manifests as minor discrepancies between themodel's output and the ground truth response (termed as deficiency), such aserrors in parameter values that require complex reasoning from the context toresolve. To this end, we propose an iterative reinforced fine-tuning strategydesigned to alleviate these challenges. This strategy involves: (1) enhancingthe diversity of synthetic data through path exploration of Monte Carlo TreeSearch. (2) iteratively identifying deficiency-related data, constructingfine-grained preference pairs to pinpoint deficiencies, and then applyingpreference optimization to optimize these deficiencies. Our experiments showthat models trained using our method achieve about 12\% better performance thanbaseline models, outperforming larger open-source and closed-source models.
  </details>

- **[Hengqin-RA-v1: Advanced Large Language Model for Diagnosis and Treatment of Rheumatoid Arthritis with Dataset based Traditional Chinese Medicine](http://arxiv.org/abs/2501.02471v2)**  `arXiv:2501.02471`  
  _Yishen Liu, Shengda Luo, Zishao Zhong, Tongtong Wu, Jianguo Zhang, Peiyao Ou, et al._
  <details><summary>Abstract</summary>
  Large language models (LLMs) primarily trained on English texts, often facebiases and inaccuracies in Chinese contexts. Their limitations are pronouncedin fields like Traditional Chinese Medicine (TCM), where cultural and clinicalsubtleties are vital, further hindered by a lack of domain-specific data, suchas rheumatoid arthritis (RA). To address these issues, this paper introducesHengqin-RA-v1, the first large language model specifically tailored for TCMwith a focus on diagnosing and treating RA. We also present HQ-GCM-RA-C1, acomprehensive RA-specific dataset curated from ancient Chinese medicalliterature, classical texts, and modern clinical studies. This dataset empowersHengqin-RA-v1 to deliver accurate and culturally informed responses,effectively bridging the gaps left by general-purpose models. Extensiveexperiments demonstrate that Hengqin-RA-v1 outperforms state-of-the-art models,even surpassing the diagnostic accuracy of TCM practitioners in certain cases.
  </details>

- **[Understanding the Logic of Direct Preference Alignment through Logic](http://arxiv.org/abs/2412.17696v2)**  `arXiv:2412.17696`  
  _Kyle Richardson, Vivek Srikumar, Ashish Sabharwal_
  <details><summary>Abstract</summary>
  Recent direct preference alignment algorithms (DPA), such as DPO, have showngreat promise in aligning large language models to human preferences. Whilethis has motivated the development of many new variants of the original DPOloss, understanding the differences between these recent proposals, as well asdeveloping new DPA loss functions, remains difficult given the lack of atechnical and conceptual framework for reasoning about the underlying semanticsof these algorithms. In this paper, we attempt to remedy this by formalizingDPA losses in terms of discrete reasoning problems. Specifically, we ask: Givenan existing DPA loss, can we systematically derive a symbolic program thatcharacterizes its semantics? We propose a novel formalism for characterizingpreference losses for single model and reference model based approaches, andidentify symbolic forms for a number of commonly used DPA variants. Further, weshow how this formal view of preference learning sheds new light on both thesize and structure of the DPA loss landscape, making it possible to not onlyrigorously characterize the relationships between recent loss proposals butalso to systematically explore the landscape and derive new loss functions fromfirst principles. We hope our framework and findings will help provide usefulguidance to those working on human AI alignment.
  </details>

- **[Towards Controllable Speech Synthesis in the Era of Large Language Models: A Survey](http://arxiv.org/abs/2412.06602v2)**  `arXiv:2412.06602`  
  _Tianxin Xie, Yan Rong, Pengfei Zhang, Wenwu Wang, Li Liu_
  <details><summary>Abstract</summary>
  Text-to-speech (TTS), also known as speech synthesis, is a prominent researcharea that aims to generate natural-sounding human speech from text. Recently,with the increasing industrial demand, TTS technologies have evolved beyondsynthesizing human-like speech to enabling controllable speech generation. Thisincludes fine-grained control over various attributes of synthesized speechsuch as emotion, prosody, timbre, and duration. In addition, advancements indeep learning, such as diffusion and large language models, have significantlyenhanced controllable TTS over the past several years. In this work, we conducta comprehensive survey of controllable TTS, covering approaches ranging frombasic control techniques to methods utilizing natural language prompts, aimingto provide a clear understanding of the current state of research. We examinethe general controllable TTS pipeline, challenges, model architectures, andcontrol strategies, offering a comprehensive and clear taxonomy of existingmethods. Additionally, we provide a detailed summary of datasets and evaluationmetrics and shed some light on the applications and future directions ofcontrollable TTS. To the best of our knowledge, this survey paper provides thefirst comprehensive review of emerging controllable TTS methods, which canserve as a beneficial resource for both academic researchers and industrialpractitioners.
  </details>

- **[Does RAG Introduce Unfairness in LLMs? Evaluating Fairness in Retrieval-Augmented Generation Systems](http://arxiv.org/abs/2409.19804v2)**  `arXiv:2409.19804`  
  _Xuyang Wu, Shuowei Li, Hsin-Tai Wu, Zhiqiang Tao, Yi Fang_
  <details><summary>Abstract</summary>
  Retrieval-Augmented Generation (RAG) has recently gained significantattention for its enhanced ability to integrate external knowledge sources intoopen-domain question answering (QA) tasks. However, it remains unclear howthese models address fairness concerns, particularly with respect to sensitiveattributes such as gender, geographic location, and other demographic factors.First, as language models evolve to prioritize utility, like improving exactmatch accuracy, fairness considerations may have been largely overlooked.Second, the complex, multi-component architecture of RAG methods poseschallenges in identifying and mitigating biases, as each component is optimizedfor distinct objectives. In this paper, we aim to empirically evaluate fairnessin several RAG methods. We propose a fairness evaluation framework tailored toRAG, using scenario-based questions and analyzing disparities acrossdemographic attributes. Our experimental results indicate that, despite recentadvances in utility-driven optimization, fairness issues persist in both theretrieval and generation stages. These findings underscore the need fortargeted interventions to address fairness concerns throughout the RAGpipeline. The dataset and code used in this study are publicly available atthis GitHub Repository https://github.com/elviswxy/RAG_fairness .
  </details>

- **[OmniBench: Towards The Future of Universal Omni-Language Models](http://arxiv.org/abs/2409.15272v4)**  `arXiv:2409.15272`  
  _Yizhi Li, Ge Zhang, Yinghao Ma, Ruibin Yuan, Kang Zhu, Hangyu Guo, et al._
  <details><summary>Abstract</summary>
  Recent advancements in multimodal large language models (MLLMs) have focusedon integrating multiple modalities, yet their ability to simultaneously processand reason across different inputs remains underexplored. We introduceOmniBench, a novel benchmark designed to evaluate models' ability to recognize,interpret, and reason across visual, acoustic, and textual inputssimultaneously. We define language models capable of such tri-modal processingas omni-language models (OLMs). OmniBench features high-quality humanannotations that require integrated understanding across all modalities. Ourevaluation reveals that: i) open-source OLMs show significant limitations ininstruction-following and reasoning in tri-modal contexts; and ii) mostbaseline models perform poorly (around 50% accuracy) even with textualalternatives to image/audio inputs. To address these limitations, we developOmniInstruct, an 96K-sample instruction tuning dataset for training OLMs. Weadvocate for developing more robust tri-modal integration techniques andtraining strategies to enhance OLM performance. Codes and data could be foundat our repo (https://github.com/multimodal-art-projection/OmniBench).
  </details>

- **[A Context-Aware Approach for Enhancing Data Imputation with Pre-trained Language Models](http://arxiv.org/abs/2405.17712v2)**  `arXiv:2405.17712`  
  _Ahatsham Hayat, Mohammad Rashedul Hasan_
  <details><summary>Abstract</summary>
  This paper presents a novel approach named \textbf{C}ontextually\textbf{R}elevant \textbf{I}mputation leveraging pre-trained \textbf{L}anguage\textbf{M}odels (\textbf{CRILM}) for handling missing data in tabular datasets.Instead of relying on traditional numerical estimations, CRILM uses pre-trainedlanguage models (LMs) to create contextually relevant descriptors for missingvalues. This method aligns datasets with LMs' strengths, allowing large LMs togenerate these descriptors and small LMs to be fine-tuned on the enricheddatasets for enhanced downstream task performance. Our evaluations demonstrateCRILM's superior performance and robustness across MCAR, MAR, and challengingMNAR scenarios, with up to a 10\% improvement over the best-performingbaselines. By mitigating biases, particularly in MNAR settings, CRILM improvesdownstream task performance and offers a cost-effective solution forresource-constrained environments.
  </details>

- **[Achieving >97% on GSM8K: Deeply Understanding the Problems Makes LLMs Better Solvers for Math Word Problems](http://arxiv.org/abs/2404.14963v5)**  `arXiv:2404.14963`  
  _Qihuang Zhong, Kang Wang, Ziyang Xu, Juhua Liu, Liang Ding, Bo Du_
  <details><summary>Abstract</summary>
  Chain-of-Thought (CoT) prompting has enhanced the performance of LargeLanguage Models (LLMs) across various reasoning tasks. However, CoT still fallsshort in dealing with complex math word problems, as it usually suffers fromthree pitfalls: semantic misunderstanding errors, calculation errors, andstep-missing errors. Prior studies involve addressing the calculation errorsand step-missing errors, but neglect the semantic misunderstanding errors,which is the major factor limiting the reasoning performance of LLMs. To thisend, we propose a simple-yet-effective method, namely Deeply Understanding theProblems (DUP), to improve the LLMs' math problem-solving ability by addressingsemantic misunderstanding errors. The core of our method is to encourage theLLMs to deeply understand the problems and extract the key problem-solvinginformation used for better reasoning. Extensive experiments on 10 diversereasoning benchmarks show that our DUP method consistently outperforms theother counterparts by a large margin. More encouragingly, DUP achieves a newSOTA result on the GSM8K benchmark, with an accuracy of 97.1% under thezero-shot setting.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Computer Vision and Pattern Recognition üì∏

<details open><summary>Click to Collapse</summary>

- **[Semantic Library Adaptation: LoRA Retrieval and Fusion for Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2503.21780v1)**  `arXiv:2503.21780`  
  _Reza Qorbani, Gianluca Villani, Theodoros Panagiotakopoulos, Marc Botet Colomer, Linus H√§renstam-Nielsen, Mattia Segu, et al._
  <details><summary>Abstract</summary>
  Open-vocabulary semantic segmentation models associate vision and text tolabel pixels from an undefined set of classes using textual queries, providingversatile performance on novel datasets. However, large shifts between trainingand test domains degrade their performance, requiring fine-tuning for effectivereal-world applications. We introduce Semantic Library Adaptation (SemLA), anovel framework for training-free, test-time domain adaptation. SemLA leveragesa library of LoRA-based adapters indexed with CLIP embeddings, dynamicallymerging the most relevant adapters based on proximity to the target domain inthe embedding space. This approach constructs an ad-hoc model tailored to eachspecific input without additional training. Our method scales efficiently,enhances explainability by tracking adapter contributions, and inherentlyprotects data privacy, making it ideal for sensitive applications.Comprehensive experiments on a 20-domain benchmark built over 10 standarddatasets demonstrate SemLA's superior adaptability and performance acrossdiverse settings, establishing a new standard in domain adaptation foropen-vocabulary semantic segmentation.
  </details>

- **[VideoMage: Multi-Subject and Motion Customization of Text-to-Video Diffusion Models](http://arxiv.org/abs/2503.21781v1)**  `arXiv:2503.21781`  
  _Chi-Pin Huang, Yen-Siang Wu, Hung-Kai Chung, Kai-Po Chang, Fu-En Yang, Yu-Chiang Frank Wang_
  <details><summary>Abstract</summary>
  Customized text-to-video generation aims to produce high-quality videos thatincorporate user-specified subject identities or motion patterns. However,existing methods mainly focus on personalizing a single concept, either subjectidentity or motion pattern, limiting their effectiveness for multiple subjectswith the desired motion patterns. To tackle this challenge, we propose aunified framework VideoMage for video customization over both multiple subjectsand their interactive motions. VideoMage employs subject and motion LoRAs tocapture personalized content from user-provided images and videos, along withan appearance-agnostic motion learning approach to disentangle motion patternsfrom visual appearance. Furthermore, we develop a spatial-temporal compositionscheme to guide interactions among subjects within the desired motion patterns.Extensive experiments demonstrate that VideoMage outperforms existing methods,generating coherent, user-controlled videos with consistent subject identitiesand interactions.
  </details>

- **[Mobile-VideoGPT: Fast and Accurate Video Understanding Language Model](http://arxiv.org/abs/2503.21782v1)**  `arXiv:2503.21782`  
  _Abdelrahman Shaker, Muhammad Maaz, Chenhui Gou, Hamid Rezatofighi, Salman Khan, Fahad Shahbaz Khan_
  <details><summary>Abstract</summary>
  Video understanding models often struggle with high computationalrequirements, extensive parameter counts, and slow inference speed, making theminefficient for practical use. To tackle these challenges, we proposeMobile-VideoGPT, an efficient multimodal framework designed to operate withfewer than a billion parameters. Unlike traditional video large multimodalmodels (LMMs), Mobile-VideoGPT consists of lightweight dual visual encoders,efficient projectors, and a small language model (SLM), enabling real-timethroughput. To further improve efficiency, we present an Attention-Based FrameScoring mechanism to select the key-frames, along with an efficient tokenprojector that prunes redundant visual tokens and preserves essentialcontextual cues. We evaluate our model across well-established six videounderstanding benchmarks (e.g., MVBench, EgoSchema, NextQA, and PercepTest).Our results show that Mobile-VideoGPT-0.5B can generate up to 46 tokens persecond while outperforming existing state-of-the-art 0.5B-parameter models by 6points on average with 40% fewer parameters and more than 2x higher throughput.Our code and models are publicly available at:https://github.com/Amshaker/Mobile-VideoGPT.
  </details>

- **[X$^{2}$-Gaussian: 4D Radiative Gaussian Splatting for Continuous-time Tomographic Reconstruction](http://arxiv.org/abs/2503.21779v1)**  `arXiv:2503.21779`  
  _Weihao Yu, Yuanhao Cai, Ruyi Zha, Zhiwen Fan, Chenxin Li, Yixuan Yuan_
  <details><summary>Abstract</summary>
  Four-dimensional computed tomography (4D CT) reconstruction is crucial forcapturing dynamic anatomical changes but faces inherent limitations fromconventional phase-binning workflows. Current methods discretize temporalresolution into fixed phases with respiratory gating devices, introducingmotion misalignment and restricting clinical practicality. In this paper, Wepropose X$^2$-Gaussian, a novel framework that enables continuous-time 4D-CTreconstruction by integrating dynamic radiative Gaussian splatting withself-supervised respiratory motion learning. Our approach models anatomicaldynamics through a spatiotemporal encoder-decoder architecture that predictstime-varying Gaussian deformations, eliminating phase discretization. To removedependency on external gating devices, we introduce a physiology-drivenperiodic consistency loss that learns patient-specific breathing cyclesdirectly from projections via differentiable optimization. Extensiveexperiments demonstrate state-of-the-art performance, achieving a 9.93 dB PSNRgain over traditional methods and 2.25 dB improvement against prior Gaussiansplatting techniques. By unifying continuous motion modeling with hardware-freeperiod learning, X$^2$-Gaussian advances high-fidelity 4D CT reconstruction fordynamic clinical imaging. Project website at: https://x2-gaussian.github.io/.
  </details>

- **[HS-SLAM: Hybrid Representation with Structural Supervision for Improved Dense SLAM](http://arxiv.org/abs/2503.21778v1)**  `arXiv:2503.21778`  
  _Ziren Gong, Fabio Tosi, Youmin Zhang, Stefano Mattoccia, Matteo Poggi_
  <details><summary>Abstract</summary>
  NeRF-based SLAM has recently achieved promising results in tracking andreconstruction. However, existing methods face challenges in providingsufficient scene representation, capturing structural information, andmaintaining global consistency in scenes emerging significant movement or beingforgotten. To this end, we present HS-SLAM to tackle these problems. To enhancescene representation capacity, we propose a hybrid encoding network thatcombines the complementary strengths of hash-grid, tri-planes, and one-blob,improving the completeness and smoothness of reconstruction. Additionally, weintroduce structural supervision by sampling patches of non-local pixels ratherthan individual rays to better capture the scene structure. To ensure globalconsistency, we implement an active global bundle adjustment (BA) to eliminatecamera drifts and mitigate accumulative errors. Experimental resultsdemonstrate that HS-SLAM outperforms the baselines in tracking andreconstruction accuracy while maintaining the efficiency required for robotics.
  </details>

- **[Test-Time Visual In-Context Tuning](http://arxiv.org/abs/2503.21777v1)**  `arXiv:2503.21777`  
  _Jiahao Xie, Alessio Tonioni, Nathalie Rauschmayr, Federico Tombari, Bernt Schiele_
  <details><summary>Abstract</summary>
  Visual in-context learning (VICL), as a new paradigm in computer vision,allows the model to rapidly adapt to various tasks with only a handful ofprompts and examples. While effective, the existing VICL paradigm exhibits poorgeneralizability under distribution shifts. In this work, we propose test-timeVisual In-Context Tuning (VICT), a method that can adapt VICL models on the flywith a single test sample. Specifically, we flip the role between the taskprompts and the test sample and use a cycle consistency loss to reconstruct theoriginal task prompt output. Our key insight is that a model should be aware ofa new test distribution if it can successfully recover the original taskprompts. Extensive experiments on six representative vision tasks ranging fromhigh-level visual understanding to low-level image processing, with 15 commoncorruptions, demonstrate that our VICT can improve the generalizability of VICLto unseen new domains. In addition, we show the potential of applying VICT forunseen tasks at test time. Code: https://github.com/Jiahao000/VICT.
  </details>

- **[Video-R1: Reinforcing Video Reasoning in MLLMs](http://arxiv.org/abs/2503.21776v1)**  `arXiv:2503.21776`  
  _Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, et al._
  <details><summary>Abstract</summary>
  Inspired by DeepSeek-R1's success in eliciting reasoning abilities throughrule-based reinforcement learning (RL), we introduce Video-R1 as the firstattempt to systematically explore the R1 paradigm for eliciting video reasoningwithin multimodal large language models (MLLMs). However, directly applying RLtraining with the GRPO algorithm to video reasoning presents two primarychallenges: (i) a lack of temporal modeling for video reasoning, and (ii) thescarcity of high-quality video-reasoning data. To address these issues, wefirst propose the T-GRPO algorithm, which encourages models to utilize temporalinformation in videos for reasoning. Additionally, instead of relying solely onvideo data, we incorporate high-quality image-reasoning data into the trainingprocess. We have constructed two datasets: Video-R1-COT-165k for SFT cold startand Video-R1-260k for RL training, both comprising image and video data.Experimental results demonstrate that Video-R1 achieves significantimprovements on video reasoning benchmarks such as VideoMMMU and VSI-Bench, aswell as on general video benchmarks including MVBench and TempCompass, etc.Notably, Video-R1-7B attains a 35.8% accuracy on video spatial reasoningbenchmark VSI-bench, surpassing the commercial proprietary model GPT-4o. Allcodes, models, data are released.
  </details>

- **[Optimal Stepsize for Diffusion Sampling](http://arxiv.org/abs/2503.21774v1)**  `arXiv:2503.21774`  
  _Jianning Pei, Han Hu, Shuyang Gu_
  <details><summary>Abstract</summary>
  Diffusion models achieve remarkable generation quality but suffer fromcomputational intensive sampling due to suboptimal step discretization. Whileexisting works focus on optimizing denoising directions, we address theprincipled design of stepsize schedules. This paper proposes Optimal StepsizeDistillation, a dynamic programming framework that extracts theoreticallyoptimal schedules by distilling knowledge from reference trajectories. Byreformulating stepsize optimization as recursive error minimization, our methodguarantees global discretization bounds through optimal substructureexploitation. Crucially, the distilled schedules demonstrate strong robustnessacross architectures, ODE solvers, and noise schedules. Experiments show 10xaccelerated text-to-image generation while preserving 99.4% performance onGenEval. Our code is available at https://github.com/bebebe666/OptimalSteps.
  </details>

- **[StyleMotif: Multi-Modal Motion Stylization using Style-Content Cross Fusion](http://arxiv.org/abs/2503.21775v1)**  `arXiv:2503.21775`  
  _Ziyu Guo, Young Yoon Lee, Joseph Liu, Yizhak Ben-Shabat, Victor Zordan, Mubbasir Kapadia_
  <details><summary>Abstract</summary>
  We present StyleMotif, a novel Stylized Motion Latent Diffusion model,generating motion conditioned on both content and style from multiplemodalities. Unlike existing approaches that either focus on generating diversemotion content or transferring style from sequences, StyleMotif seamlesslysynthesizes motion across a wide range of content while incorporating stylisticcues from multi-modal inputs, including motion, text, image, video, and audio.To achieve this, we introduce a style-content cross fusion mechanism and aligna style encoder with a pre-trained multi-modal model, ensuring that thegenerated motion accurately captures the reference style while preservingrealism. Extensive experiments demonstrate that our framework surpassesexisting methods in stylized motion generation and exhibits emergentcapabilities for multi-modal motion stylization, enabling more nuanced motionsynthesis. Source code and pre-trained models will be released upon acceptance.Project Page: https://stylemotif.github.io
  </details>

- **[LOCORE: Image Re-ranking with Long-Context Sequence Modeling](http://arxiv.org/abs/2503.21772v1)**  `arXiv:2503.21772`  
  _Zilin Xiao, Pavel Suma, Ayush Sachdeva, Hao-Jen Wang, Giorgos Kordopatis-Zilos, Giorgos Tolias, et al._
  <details><summary>Abstract</summary>
  We introduce LOCORE, Long-Context Re-ranker, a model that takes as inputlocal descriptors corresponding to an image query and a list of gallery imagesand outputs similarity scores between the query and each gallery image. Thismodel is used for image retrieval, where typically a first ranking is performedwith an efficient similarity measure, and then a shortlist of top-ranked imagesis re-ranked based on a more fine-grained similarity measure. Compared toexisting methods that perform pair-wise similarity estimation with localdescriptors or list-wise re-ranking with global descriptors, LOCORE is thefirst method to perform list-wise re-ranking with local descriptors. To achievethis, we leverage efficient long-context sequence models to effectively capturethe dependencies between query and gallery images at the local-descriptorlevel. During testing, we process long shortlists with a sliding windowstrategy that is tailored to overcome the context size limitations of sequencemodels. Our approach achieves superior performance compared with otherre-rankers on established image retrieval benchmarks of landmarks (ROxf andRPar), products (SOP), fashion items (In-Shop), and bird species (CUB-200)while having comparable latency to the pair-wise local descriptor re-rankers.
  </details>

- **[A Unified Image-Dense Annotation Generation Model for Underwater Scenes](http://arxiv.org/abs/2503.21771v1)**  `arXiv:2503.21771`  
  _Hongkai Lin, Dingkang Liang, Zhenghao Qi, Xiang Bai_
  <details><summary>Abstract</summary>
  Underwater dense prediction, especially depth estimation and semanticsegmentation, is crucial for gaining a comprehensive understanding ofunderwater scenes. Nevertheless, high-quality and large-scale underwaterdatasets with dense annotations remain scarce because of the complexenvironment and the exorbitant data collection costs. This paper proposes aunified Text-to-Image and DEnse annotation generation method (TIDE) forunderwater scenes. It relies solely on text as input to simultaneously generaterealistic underwater images and multiple highly consistent dense annotations.Specifically, we unify the generation of text-to-image and text-to-denseannotations within a single model. The Implicit Layout Sharing mechanism (ILS)and cross-modal interaction method called Time Adaptive Normalization (TAN) areintroduced to jointly optimize the consistency between image and denseannotations. We synthesize a large-scale underwater dataset using TIDE tovalidate the effectiveness of our method in underwater dense prediction tasks.The results demonstrate that our method effectively improves the performance ofexisting underwater dense prediction models and mitigates the scarcity ofunderwater data with dense annotations. We hope our method can offer newperspectives on alleviating data scarcity issues in other fields. The code isavailable at https: //github.com/HongkLin/TIDE.
  </details>

- **[Visual Jenga: Discovering Object Dependencies via Counterfactual Inpainting](http://arxiv.org/abs/2503.21770v1)**  `arXiv:2503.21770`  
  _Anand Bhattad, Konpat Preechakul, Alexei A. Efros_
  <details><summary>Abstract</summary>
  This paper proposes a novel scene understanding task called Visual Jenga.Drawing inspiration from the game Jenga, the proposed task involvesprogressively removing objects from a single image until only the backgroundremains. Just as Jenga players must understand structural dependencies tomaintain tower stability, our task reveals the intrinsic relationships betweenscene elements by systematically exploring which objects can be removed whilepreserving scene coherence in both physical and geometric sense. As a startingpoint for tackling the Visual Jenga task, we propose a simple, data-driven,training-free approach that is surprisingly effective on a range of real-worldimages. The principle behind our approach is to utilize the asymmetry in thepairwise relationships between objects within a scene and employ a largeinpainting model to generate a set of counterfactuals to quantify theasymmetry.
  </details>

- **[Semantic Consistent Language Gaussian Splatting for Point-Level Open-vocabulary Querying](http://arxiv.org/abs/2503.21767v1)**  `arXiv:2503.21767`  
  _Hairong Yin, Huangying Zhan, Yi Xu, Raymond A. Yeh_
  <details><summary>Abstract</summary>
  Open-vocabulary querying in 3D Gaussian Splatting aims to identifysemantically relevant regions within a 3D Gaussian representation based on agiven text query. Prior work, such as LangSplat, addressed this task byretrieving these regions in the form of segmentation masks on 2D renderings.More recently, OpenGaussian introduced point-level querying, which directlyselects a subset of 3D Gaussians. In this work, we propose a point-levelquerying method that builds upon LangSplat's framework. Our approach improvesthe framework in two key ways: (a) we leverage masklets from the SegmentAnything Model 2 (SAM2) to establish semantic consistent ground-truth fordistilling the language Gaussians; (b) we introduces a novel two-step queryingapproach that first retrieves the distilled ground-truth and subsequently usesthe ground-truth to query the individual Gaussians. Experimental evaluations onthree benchmark datasets demonstrate that the proposed method achieves betterperformance compared to state-of-the-art approaches. For instance, our methodachieves an mIoU improvement of +20.42 on the 3D-OVS dataset.
  </details>

- **[Stable-SCore: A Stable Registration-based Framework for 3D Shape Correspondence](http://arxiv.org/abs/2503.21766v1)**  `arXiv:2503.21766`  
  _Haolin Liu, Xiaohang Zhan, Zizheng Yan, Zhongjin Luo, Yuxin Wen, Xiaoguang Han_
  <details><summary>Abstract</summary>
  Establishing character shape correspondence is a critical and fundamentaltask in computer vision and graphics, with diverse applications includingre-topology, attribute transfer, and shape interpolation. Current dominantfunctional map methods, while effective in controlled scenarios, struggle inreal situations with more complex challenges such as non-isometric shapediscrepancies. In response, we revisit registration-for-correspondence methodsand tap their potential for more stable shape correspondence estimation. Toovercome their common issues including unstable deformations and the necessityfor careful pre-alignment or high-quality initial 3D correspondences, weintroduce Stable-SCore: A Stable Registration-based Framework for 3D ShapeCorrespondence. We first re-purpose a foundation model for 2D charactercorrespondence that ensures reliable and stable 2D mappings. Crucially, wepropose a novel Semantic Flow Guided Registration approach that leverages 2Dcorrespondence to guide mesh deformations. Our framework significantlysurpasses existing methods in challenging scenarios, and brings possibilitiesfor a wide array of real applications, as demonstrated in our results.
  </details>

- **[Exploring the Evolution of Physics Cognition in Video Generation: A Survey](http://arxiv.org/abs/2503.21765v1)**  `arXiv:2503.21765`  
  _Minghui Lin, Xiang Wang, Yishan Wang, Shu Wang, Fengqi Dai, Pengxiang Ding, et al._
  <details><summary>Abstract</summary>
  Recent advancements in video generation have witnessed significant progress,especially with the rapid advancement of diffusion models. Despite this, theirdeficiencies in physical cognition have gradually received widespread attention- generated content often violates the fundamental laws of physics, fallinginto the dilemma of ''visual realism but physical absurdity". Researchers beganto increasingly recognize the importance of physical fidelity in videogeneration and attempted to integrate heuristic physical cognition such asmotion representations and physical knowledge into generative systems tosimulate real-world dynamic scenarios. Considering the lack of a systematicoverview in this field, this survey aims to provide a comprehensive summary ofarchitecture designs and their applications to fill this gap. Specifically, wediscuss and organize the evolutionary process of physical cognition in videogeneration from a cognitive science perspective, while proposing a three-tiertaxonomy: 1) basic schema perception for generation, 2) passive cognition ofphysical knowledge for generation, and 3) active cognition for worldsimulation, encompassing state-of-the-art methods, classical paradigms, andbenchmarks. Subsequently, we emphasize the inherent key challenges in thisdomain and delineate potential pathways for future research, contributing toadvancing the frontiers of discussion in both academia and industry. Throughstructured review and interdisciplinary analysis, this survey aims to providedirectional guidance for developing interpretable, controllable, and physicallyconsistent video generation paradigms, thereby propelling generative modelsfrom the stage of ''visual mimicry'' towards a new phase of ''human-likephysical comprehension''.
  </details>

- **[Uni4D: Unifying Visual Foundation Models for 4D Modeling from a Single Video](http://arxiv.org/abs/2503.21761v1)**  `arXiv:2503.21761`  
  _David Yifan Yao, Albert J. Zhai, Shenlong Wang_
  <details><summary>Abstract</summary>
  This paper presents a unified approach to understanding dynamic scenes fromcasual videos. Large pretrained vision foundation models, such asvision-language, video depth prediction, motion tracking, and segmentationmodels, offer promising capabilities. However, training a single model forcomprehensive 4D understanding remains challenging. We introduce Uni4D, amulti-stage optimization framework that harnesses multiple pretrained models toadvance dynamic 3D modeling, including static/dynamic reconstruction, camerapose estimation, and dense 3D motion tracking. Our results showstate-of-the-art performance in dynamic 4D modeling with superior visualquality. Notably, Uni4D requires no retraining or fine-tuning, highlighting theeffectiveness of repurposing visual foundation models for 4D understanding.
  </details>

- **[Fwd2Bot: LVLM Visual Token Compression with Double Forward Bottleneck](http://arxiv.org/abs/2503.21757v1)**  `arXiv:2503.21757`  
  _Adrian Bulat, Yassine Ouali, Georgios Tzimiropoulos_
  <details><summary>Abstract</summary>
  In this work, we aim to compress the vision tokens of a Large Vision LanguageModel (LVLM) into a representation that is simultaneously suitable for (a)generative and (b) discriminative tasks, (c) is nearly lossless, and (d) isstorage-efficient. We propose a novel compression approach, called Fwd2Bot,that uses the LVLM itself to compress the visual information in a task-agnosticmanner. At the core of Fwd2bot there exists a "double-forward pass" trainingstrategy, whereby, during the first forward pass, the LLM (of the LVLM) createsa bottleneck by condensing the visual information into a small number ofsummary tokens. Then, using the same LLM, the second forward pass processes thelanguage instruction(s) alongside the summary tokens, used as a directreplacement for the image ones. The training signal is provided by two losses:an autoregressive one applied after the second pass that provides a directoptimization objective for compression, and a contrastive loss, applied afterthe first pass, that further boosts the representation strength, especially fordiscriminative tasks. The training is further enhanced by stage-specificadapters. We accompany the proposed method by an in-depth ablation study.Overall, Fwd2Bot results in highly-informative compressed representationssuitable for both generative and discriminative tasks. For generative tasks, weoffer a 2x higher compression rate without compromising the generativecapabilities, setting a new state-of-the-art result. For discriminative tasks,we set a new state-of-the-art on image retrieval and compositionality.
  </details>

- **[Lumina-Image 2.0: A Unified and Efficient Image Generative Framework](http://arxiv.org/abs/2503.21758v1)**  `arXiv:2503.21758`  
  _Qi Qin, Le Zhuo, Yi Xin, Ruoyi Du, Zhen Li, Bin Fu, et al._
  <details><summary>Abstract</summary>
  We introduce Lumina-Image 2.0, an advanced text-to-image generation frameworkthat achieves significant progress compared to previous work, Lumina-Next.Lumina-Image 2.0 is built upon two key principles: (1) Unification - it adoptsa unified architecture (Unified Next-DiT) that treats text and image tokens asa joint sequence, enabling natural cross-modal interactions and allowingseamless task expansion. Besides, since high-quality captioners can providesemantically well-aligned text-image training pairs, we introduce a unifiedcaptioning system, Unified Captioner (UniCap), specifically designed for T2Igeneration tasks. UniCap excels at generating comprehensive and accuratecaptions, accelerating convergence and enhancing prompt adherence. (2)Efficiency - to improve the efficiency of our proposed model, we developmulti-stage progressive training strategies and introduce inferenceacceleration techniques without compromising image quality. Extensiveevaluations on academic benchmarks and public text-to-image arenas show thatLumina-Image 2.0 delivers strong performances even with only 2.6B parameters,highlighting its scalability and design efficiency. We have released ourtraining details, code, and models athttps://github.com/Alpha-VLLM/Lumina-Image-2.0.
  </details>

- **[VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness](http://arxiv.org/abs/2503.21755v1)**  `arXiv:2503.21755`  
  _Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, et al._
  <details><summary>Abstract</summary>
  Video generation has advanced significantly, evolving from producingunrealistic outputs to generating videos that appear visually convincing andtemporally coherent. To evaluate these video generative models, benchmarks suchas VBench have been developed to assess their faithfulness, measuring factorslike per-frame aesthetics, temporal consistency, and basic prompt adherence.However, these aspects mainly represent superficial faithfulness, which focuson whether the video appears visually convincing rather than whether it adheresto real-world principles. While recent models perform increasingly well onthese metrics, they still struggle to generate videos that are not justvisually plausible but fundamentally realistic. To achieve real "world models"through video generation, the next frontier lies in intrinsic faithfulness toensure that generated videos adhere to physical laws, commonsense reasoning,anatomical correctness, and compositional integrity. Achieving this level ofrealism is essential for applications such as AI-assisted filmmaking andsimulated world modeling. To bridge this gap, we introduce VBench-2.0, anext-generation benchmark designed to automatically evaluate video generativemodels for their intrinsic faithfulness. VBench-2.0 assesses five keydimensions: Human Fidelity, Controllability, Creativity, Physics, andCommonsense, each further broken down into fine-grained capabilities. Tailoredfor individual dimensions, our evaluation framework integrates generalists suchas state-of-the-art VLMs and LLMs, and specialists, including anomaly detectionmethods proposed for video generation. We conduct extensive annotations toensure alignment with human judgment. By pushing beyond superficialfaithfulness toward intrinsic faithfulness, VBench-2.0 aims to set a newstandard for the next generation of video generative models in pursuit ofintrinsic faithfulness.
  </details>

- **[Reconstructing Humans with a Biomechanically Accurate Skeleton](http://arxiv.org/abs/2503.21751v1)**  `arXiv:2503.21751`  
  _Yan Xia, Xiaowei Zhou, Etienne Vouga, Qixing Huang, Georgios Pavlakos_
  <details><summary>Abstract</summary>
  In this paper, we introduce a method for reconstructing 3D humans from asingle image using a biomechanically accurate skeleton model. To achieve this,we train a transformer that takes an image as input and estimates theparameters of the model. Due to the lack of training data for this task, webuild a pipeline to produce pseudo ground truth model parameters for singleimages and implement a training procedure that iteratively refines these pseudolabels. Compared to state-of-the-art methods for 3D human mesh recovery, ourmodel achieves competitive performance on standard benchmarks, while itsignificantly outperforms them in settings with extreme 3D poses andviewpoints. Additionally, we show that previous reconstruction methodsfrequently violate joint angle limits, leading to unnatural rotations. Incontrast, our approach leverages the biomechanically plausible degrees offreedom making more realistic joint rotation estimates. We validate ourapproach across multiple human pose estimation benchmarks. We make the code,models and data available at: https://isshikihugh.github.io/HSMR/
  </details>

- **[LeX-Art: Rethinking Text Generation via Scalable High-Quality Data Synthesis](http://arxiv.org/abs/2503.21749v1)**  `arXiv:2503.21749`  
  _Shitian Zhao, Qilong Wu, Xinyue Li, Bo Zhang, Ming Li, Qi Qin, et al._
  <details><summary>Abstract</summary>
  We introduce LeX-Art, a comprehensive suite for high-quality text-imagesynthesis that systematically bridges the gap between prompt expressiveness andtext rendering fidelity. Our approach follows a data-centric paradigm,constructing a high-quality data synthesis pipeline based on Deepseek-R1 tocurate LeX-10K, a dataset of 10K high-resolution, aesthetically refined1024$\times$1024 images. Beyond dataset construction, we develop LeX-Enhancer,a robust prompt enrichment model, and train two text-to-image models, LeX-FLUXand LeX-Lumina, achieving state-of-the-art text rendering performance. Tosystematically evaluate visual text generation, we introduce LeX-Bench, abenchmark that assesses fidelity, aesthetics, and alignment, complemented byPairwise Normalized Edit Distance (PNED), a novel metric for robust textaccuracy evaluation. Experiments demonstrate significant improvements, withLeX-Lumina achieving a 79.81% PNED gain on CreateBench, and LeX-FLUXoutperforming baselines in color (+3.18%), positional (+4.45%), and fontaccuracy (+3.81%). Our codes, models, datasets, and demo are publiclyavailable.
  </details>

- **[CTRL-O: Language-Controllable Object-Centric Visual Representation Learning](http://arxiv.org/abs/2503.21747v1)**  `arXiv:2503.21747`  
  _Aniket Didolkar, Andrii Zadaianchuk, Rabiul Awal, Maximilian Seitzer, Efstratios Gavves, Aishwarya Agrawal_
  <details><summary>Abstract</summary>
  Object-centric representation learning aims to decompose visual scenes intofixed-size vectors called "slots" or "object files", where each slot captures adistinct object. Current state-of-the-art object-centric models have shownremarkable success in object discovery in diverse domains, including complexreal-world scenes. However, these models suffer from a key limitation: theylack controllability. Specifically, current object-centric models learnrepresentations based on their preconceived understanding of objects, withoutallowing user input to guide which objects are represented. Introducingcontrollability into object-centric models could unlock a range of usefulcapabilities, such as the ability to extract instance-specific representationsfrom a scene. In this work, we propose a novel approach for user-directedcontrol over slot representations by conditioning slots on languagedescriptions. The proposed ConTRoLlable Object-centric representation learningapproach, which we term CTRL-O, achieves targeted object-language binding incomplex real-world scenes without requiring mask supervision. Next, we applythese controllable slot representations on two downstream vision languagetasks: text-to-image generation and visual question answering. The proposedapproach enables instance-specific text-to-image generation and also achievesstrong performance on visual question answering.
  </details>

- **[3DGen-Bench: Comprehensive Benchmark Suite for 3D Generative Models](http://arxiv.org/abs/2503.21745v1)**  `arXiv:2503.21745`  
  _Yuhan Zhang, Mengchen Zhang, Tong Wu, Tengfei Wang, Gordon Wetzstein, Dahua Lin, et al._
  <details><summary>Abstract</summary>
  3D generation is experiencing rapid advancements, while the development of 3Devaluation has not kept pace. How to keep automatic evaluation equitablyaligned with human perception has become a well-recognized challenge. Recentadvances in the field of language and image generation have explored humanpreferences and showcased respectable fitting ability. However, the 3D domainstill lacks such a comprehensive preference dataset over generative models. Tomitigate this absence, we develop 3DGen-Arena, an integrated platform in abattle manner. Then, we carefully design diverse text and image prompts andleverage the arena platform to gather human preferences from both public usersand expert annotators, resulting in a large-scale multi-dimension humanpreference dataset 3DGen-Bench. Using this dataset, we further train aCLIP-based scoring model, 3DGen-Score, and a MLLM-based automatic evaluator,3DGen-Eval. These two models innovatively unify the quality evaluation oftext-to-3D and image-to-3D generation, and jointly form our automatedevaluation system with their respective strengths. Extensive experimentsdemonstrate the efficacy of our scoring model in predicting human preferences,exhibiting a superior correlation with human ranks compared to existingmetrics. We believe that our 3DGen-Bench dataset and automated evaluationsystem will foster a more equitable evaluation in the field of 3D generation,further promoting the development of 3D generative models and their downstreamapplications.
  </details>

- **[SparseFlex: High-Resolution and Arbitrary-Topology 3D Shape Modeling](http://arxiv.org/abs/2503.21732v1)**  `arXiv:2503.21732`  
  _Xianglong He, Zi-Xin Zou, Chia-Hao Chen, Yuan-Chen Guo, Ding Liang, Chun Yuan, et al._
  <details><summary>Abstract</summary>
  Creating high-fidelity 3D meshes with arbitrary topology, including opensurfaces and complex interiors, remains a significant challenge. Existingimplicit field methods often require costly and detail-degrading watertightconversion, while other approaches struggle with high resolutions. This paperintroduces SparseFlex, a novel sparse-structured isosurface representation thatenables differentiable mesh reconstruction at resolutions up to $1024^3$directly from rendering losses. SparseFlex combines the accuracy of Flexicubeswith a sparse voxel structure, focusing computation on surface-adjacent regionsand efficiently handling open surfaces. Crucially, we introduce a frustum-awaresectional voxel training strategy that activates only relevant voxels duringrendering, dramatically reducing memory consumption and enablinghigh-resolution training. This also allows, for the first time, thereconstruction of mesh interiors using only rendering supervision. Buildingupon this, we demonstrate a complete shape modeling pipeline by training avariational autoencoder (VAE) and a rectified flow transformer for high-quality3D shape generation. Our experiments show state-of-the-art reconstructionaccuracy, with a ~82% reduction in Chamfer Distance and a ~88% increase inF-score compared to previous methods, and demonstrate the generation ofhigh-resolution, detailed 3D shapes with arbitrary topology. By enablinghigh-resolution, differentiable mesh reconstruction and generation withrendering losses, SparseFlex significantly advances the state-of-the-art in 3Dshape representation and modeling.
  </details>

- **[OccRobNet : Occlusion Robust Network for Accurate 3D Interacting Hand-Object Pose Estimation](http://arxiv.org/abs/2503.21723v1)**  `arXiv:2503.21723`  
  _Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan_
  <details><summary>Abstract</summary>
  Occlusion is one of the challenging issues when estimating 3D hand pose. Thisproblem becomes more prominent when hand interacts with an object or two handsare involved. In the past works, much attention has not been given to theseoccluded regions. But these regions contain important and beneficialinformation that is vital for 3D hand pose estimation. Thus, in this paper, wepropose an occlusion robust and accurate method for the estimation of 3Dhand-object pose from the input RGB image. Our method includes first localisingthe hand joints using a CNN based model and then refining them by extractingcontextual information. The self attention transformer then identifies thespecific joints along with the hand identity. This helps the model to identifythe hand belongingness of a particular joint which helps to detect the jointeven in the occluded region. Further, these joints with hand identity are thenused to estimate the pose using cross attention mechanism. Thus, by identifyingthe joints in the occluded region, the obtained network becomes robust toocclusion. Hence, this network achieves state-of-the-art results when evaluatedon the InterHand2.6M, HO3D and H$_2$O3D datasets.
  </details>

- **[Evaluating Text-to-Image Synthesis with a Conditional Fr√©chet Distance](http://arxiv.org/abs/2503.21721v1)**  `arXiv:2503.21721`  
  _Jaywon Koo, Jefferson Hernandez, Moayed Haji-Ali, Ziyan Yang, Vicente Ordonez_
  <details><summary>Abstract</summary>
  Evaluating text-to-image synthesis is challenging due to misalignment betweenestablished metrics and human preferences. We propose cFreD, a metric based onthe notion of Conditional Fr\'echet Distance that explicitly accounts for bothvisual fidelity and text-prompt alignment. Existing metrics such as InceptionScore (IS), Fr\'echet Inception Distance (FID) and CLIPScore assess eitherimage quality or image-text alignment but not both which limits theircorrelation with human preferences. Scoring models explicitly trained toreplicate human preferences require constant updates and may not generalize tonovel generation techniques or out-of-domain inputs. Through extensiveexperiments across multiple recently proposed text-to-image models and diverseprompt datasets, we demonstrate that cFreD exhibits a higher correlation withhuman judgments compared to statistical metrics, including metrics trained withhuman preferences. Our findings validate cFreD as a robust, future-proof metricfor the systematic evaluation of text-to-image models, standardizingbenchmarking in this rapidly evolving field. We release our evaluation toolkitand benchmark in the appendix.
  </details>

- **[AMA-SAM: Adversarial Multi-Domain Alignment of Segment Anything Model for High-Fidelity Histology Nuclei Segmentation](http://arxiv.org/abs/2503.21695v1)**  `arXiv:2503.21695`  
  _Jiahe Qian, Yaoyu Fang, Jinkui Hao, Bo Zhou_
  <details><summary>Abstract</summary>
  Accurate segmentation of cell nuclei in histopathology images is essentialfor numerous biomedical research and clinical applications. However, existingcell nucleus segmentation methods only consider a single dataset (i.e., primarydomain), while neglecting to leverage supplementary data from diverse sources(i.e., auxiliary domains) to reduce overfitting and enhance the performance.Although incorporating multiple datasets could alleviate overfitting, it oftenexacerbates performance drops caused by domain shifts. In this work, weintroduce Adversarial Multi-domain Alignment of Segment Anything Model(AMA-SAM) that extends the Segment Anything Model (SAM) to overcome theseobstacles through two key innovations. First, we propose a Conditional GradientReversal Layer (CGRL), a multi-domain alignment module that harmonizes featuresfrom diverse domains to promote domain-invariant representation learning whilepreserving crucial discriminative features for the primary dataset. Second, weaddress SAM's inherent low-resolution output by designing a High-ResolutionDecoder (HR-Decoder), which directly produces fine-grained segmentation maps inorder to capture intricate nuclei boundaries in high-resolution histologyimages. To the best of our knowledge, this is the first attempt to adapt SAMfor multi-dataset learning with application to histology nuclei segmentation.We validate our method on several publicly available datasets, demonstratingconsistent and significant improvements over state-of-the-art approaches.
  </details>

- **[RapidPoseTriangulation: Multi-view Multi-person Whole-body Human Pose Triangulation in a Millisecond](http://arxiv.org/abs/2503.21692v1)**  `arXiv:2503.21692`  
  _Daniel Bermuth, Alexander Poeppel, Wolfgang Reif_
  <details><summary>Abstract</summary>
  The integration of multi-view imaging and pose estimation represents asignificant advance in computer vision applications, offering new possibilitiesfor understanding human movement and interactions. This work presents a newalgorithm that improves multi-view multi-person pose estimation, focusing onfast triangulation speeds and good generalization capabilities. The approachextends to whole-body pose estimation, capturing details from facialexpressions to finger movements across multiple individuals and viewpoints.Adaptability to different settings is demonstrated through strong performanceacross unseen datasets and configurations. To support further progress in thisfield, all of this work is publicly accessible.
  </details>

- **[CMED: A Child Micro-Expression Dataset](http://arxiv.org/abs/2503.21690v1)**  `arXiv:2503.21690`  
  _Nikin~Matharaarachchi, Muhammad~Fermi Pasha, Sonya~Coleman, Kah PengWong_
  <details><summary>Abstract</summary>
  Micro-expressions are short bursts of emotion that are difficult to hide.Their detection in children is an important cue to assist psychotherapists inconducting better therapy. However, existing research on the detection ofmicro-expressions has focused on adults, whose expressions differ in theircharacteristics from those of children. The lack of research is a directconsequence of the lack of a child-based micro-expressions dataset as it ismuch more challenging to capture children's facial expressions due to the lackof predictability and controllability. This study compiles a dataset ofspontaneous child micro-expression videos, the first of its kind, to the bestof the authors knowledge. The dataset is captured in the wild using videoconferencing software. This dataset enables us to then explore key features anddifferences between adult and child micro-expressions. This study alsoestablishes a baseline for the automated spotting and recognition ofmicro-expressions in children using three approaches comprising of hand-createdand learning-based approaches.
  </details>

- **[InteractionMap: Improving Online Vectorized HDMap Construction with Interaction](http://arxiv.org/abs/2503.21659v1)**  `arXiv:2503.21659`  
  _Kuang Wu, Chuan Yang, Zhanbin Li_
  <details><summary>Abstract</summary>
  Vectorized high-definition (HD) maps are essential for an autonomous drivingsystem. Recently, state-of-the-art map vectorization methods are mainly basedon DETR-like framework to generate HD maps in an end-to-end manner. In thispaper, we propose InteractionMap, which improves previous map vectorizationmethods by fully leveraging local-to-global information interaction in bothtime and space. Firstly, we explore enhancing DETR-like detectors by explicitposition relation prior from point-level to instance-level, since map elementscontain strong shape priors. Secondly, we propose a key-frame-basedhierarchical temporal fusion module, which interacts temporal information fromlocal to global. Lastly, the separate classification branch and regressionbranch lead to the problem of misalignment in the output distribution. Weinteract semantic information with geometric information by introducing a novelgeometric-aware classification loss in optimization and a geometric-awarematching cost in label assignment. InteractionMap achieves state-of-the-artperformance on both nuScenes and Argoverse2 benchmarks.
  </details>

- **[The MVTec AD 2 Dataset: Advanced Scenarios for Unsupervised Anomaly Detection](http://arxiv.org/abs/2503.21622v1)**  `arXiv:2503.21622`  
  _Lars Heckler-Kram, Jan-Hendrik Neudeck, Ulla Scheler, Rebecca K√∂nig, Carsten Steger_
  <details><summary>Abstract</summary>
  In recent years, performance on existing anomaly detection benchmarks likeMVTec AD and VisA has started to saturate in terms of segmentation AU-PRO, withstate-of-the-art models often competing in the range of less than onepercentage point. This lack of discriminatory power prevents a meaningfulcomparison of models and thus hinders progress of the field, especially whenconsidering the inherent stochastic nature of machine learning results. Wepresent MVTec AD 2, a collection of eight anomaly detection scenarios with morethan 8000 high-resolution images. It comprises challenging and highly relevantindustrial inspection use cases that have not been considered in previousdatasets, including transparent and overlapping objects, dark-field and backlight illumination, objects with high variance in the normal data, andextremely small defects. We provide comprehensive evaluations ofstate-of-the-art methods and show that their performance remains below 60%average AU-PRO. Additionally, our dataset provides test scenarios with lightingcondition changes to assess the robustness of methods under real-worlddistribution shifts. We host a publicly accessible evaluation server that holdsthe pixel-precise ground truth of the test set (https://benchmark.mvtec.com/).All image data is available athttps://www.mvtec.com/company/research/datasets/mvtec-ad-2.
  </details>

- **[Audio-driven Gesture Generation via Deviation Feature in the Latent Space](http://arxiv.org/abs/2503.21616v1)**  `arXiv:2503.21616`  
  _Jiahui Chen, Yang Huan, Runhua Shi, Chanfan Ding, Xiaoqi Mo, Siyu Xiong, et al._
  <details><summary>Abstract</summary>
  Gestures are essential for enhancing co-speech communication, offering visualemphasis and complementing verbal interactions. While prior work hasconcentrated on point-level motion or fully supervised data-driven methods, wefocus on co-speech gestures, advocating for weakly supervised learning andpixel-level motion deviations. We introduce a weakly supervised framework thatlearns latent representation deviations, tailored for co-speech gesture videogeneration. Our approach employs a diffusion model to integrate latent motionfeatures, enabling more precise and nuanced gesture representation. Byleveraging weakly supervised deviations in latent space, we effectivelygenerate hand gestures and mouth movements, crucial for realistic videoproduction. Experiments show our method significantly improves video quality,surpassing current state-of-the-art techniques.
  </details>

- **[FusionSegReID: Advancing Person Re-Identification with Multimodal Retrieval and Precise Segmentation](http://arxiv.org/abs/2503.21595v1)**  `arXiv:2503.21595`  
  _Jincheng Yan, Yun Wang, Xiaoyan Luo, Yu-Wing Tai_
  <details><summary>Abstract</summary>
  Person re-identification (ReID) plays a critical role in applications likesecurity surveillance and criminal investigations by matching individualsacross large image galleries captured by non-overlapping cameras. TraditionalReID methods rely on unimodal inputs, typically images, but face limitationsdue to challenges like occlusions, lighting changes, and pose variations. Whileadvancements in image-based and text-based ReID systems have been made, theintegration of both modalities has remained under-explored. This paper presentsFusionSegReID, a multimodal model that combines both image and text inputs forenhanced ReID performance. By leveraging the complementary strengths of thesemodalities, our model improves matching accuracy and robustness, particularlyin complex, real-world scenarios where one modality may struggle. Ourexperiments show significant improvements in Top-1 accuracy and mean AveragePrecision (mAP) for ReID, as well as better segmentation results in challengingscenarios like occlusion and low-quality images. Ablation studies furtherconfirm that multimodal fusion and segmentation modules contribute to enhancedre-identification and mask accuracy. The results show that FusionSegReIDoutperforms traditional unimodal models, offering a more robust and flexiblesolution for real-world person ReID tasks.
  </details>

- **[AlignDiff: Learning Physically-Grounded Camera Alignment via Diffusion](http://arxiv.org/abs/2503.21581v1)**  `arXiv:2503.21581`  
  _Liuyue Xie, Jiancong Guo, Ozan Cakmakci, Andre Araujo, Laszlo A. Jeni, Zhiheng Jia_
  <details><summary>Abstract</summary>
  Accurate camera calibration is a fundamental task for 3D perception,especially when dealing with real-world, in-the-wild environments where complexoptical distortions are common. Existing methods often rely on pre-rectifiedimages or calibration patterns, which limits their applicability andflexibility. In this work, we introduce a novel framework that addresses thesechallenges by jointly modeling camera intrinsic and extrinsic parameters usinga generic ray camera model. Unlike previous approaches, AlignDiff shifts focusfrom semantic to geometric features, enabling more accurate modeling of localdistortions. We propose AlignDiff, a diffusion model conditioned on geometricpriors, enabling the simultaneous estimation of camera distortions and scenegeometry. To enhance distortion prediction, we incorporate edge-awareattention, focusing the model on geometric features around image edges, ratherthan semantic content. Furthermore, to enhance generalizability to real-worldcaptures, we incorporate a large database of ray-traced lenses containing overthree thousand samples. This database characterizes the distortion inherent ina diverse variety of lens forms. Our experiments demonstrate that the proposedmethod significantly reduces the angular error of estimated ray bundles by ~8.2degrees and overall calibration accuracy, outperforming existing approaches onchallenging, real-world datasets.
  </details>

- **[Bearing fault diagnosis based on multi-scale spectral images and convolutional neural network](http://arxiv.org/abs/2503.21566v1)**  `arXiv:2503.21566`  
  _Tongchao Luo, Mingquan Qiu, Zhenyu Wu, Zebo Zhao, Dingyou Zhang_
  <details><summary>Abstract</summary>
  To address the challenges of low diagnostic accuracy in traditional bearingfault diagnosis methods, this paper proposes a novel fault diagnosis approachbased on multi-scale spectrum feature images and deep learning. Firstly, thevibration signal are preprocessed through mean removal and then converted tomulti-length spectrum with fast Fourier transforms (FFT). Secondly, a novelfeature called multi-scale spectral image (MSSI) is constructed by multi-lengthspectrum paving scheme. Finally, a deep learning framework, convolutionalneural network (CNN), is formulated to diagnose the bearing faults. Twoexperimental cases are utilized to verify the effectiveness of the proposedmethod. Experimental results demonstrate that the proposed method significantlyimproves the accuracy of fault diagnosis.
  </details>

- **[uLayout: Unified Room Layout Estimation for Perspective and Panoramic Images](http://arxiv.org/abs/2503.21562v1)**  `arXiv:2503.21562`  
  _Jonathan Lee, Bolivar Solarte, Chin-Hsuan Wu, Jin-Cheng Jhang, Fu-En Wang, Yi-Hsuan Tsai, et al._
  <details><summary>Abstract</summary>
  We present uLayout, a unified model for estimating room layout geometriesfrom both perspective and panoramic images, whereas traditional solutionsrequire different model designs for each image type. The key idea of oursolution is to unify both domains into the equirectangular projection,particularly, allocating perspective images into the most suitable latitudecoordinate to effectively exploit both domains seamlessly. To address theField-of-View (FoV) difference between the input domains, we design uLayoutwith a shared feature extractor with an extra 1D-Convolution layer to conditioneach domain input differently. This conditioning allows us to efficientlyformulate a column-wise feature regression problem regardless of the FoV input.This simple yet effective approach achieves competitive performance withcurrent state-of-the-art solutions and shows for the first time a singleend-to-end model for both domains. Extensive experiments in the real-worlddatasets, LSUN, Matterport3D, PanoContext, and Stanford 2D-3D evidence thecontribution of our approach. Code is available athttps://github.com/JonathanLee112/uLayout.
  </details>

- **[LOCATEdit: Graph Laplacian Optimized Cross Attention for Localized Text-Guided Image Editing](http://arxiv.org/abs/2503.21541v1)**  `arXiv:2503.21541`  
  _Achint Soni, Meet Soni, Sirisha Rambhatla_
  <details><summary>Abstract</summary>
  Text-guided image editing aims to modify specific regions of an imageaccording to natural language instructions while maintaining the generalstructure and the background fidelity. Existing methods utilize masks derivedfrom cross-attention maps generated from diffusion models to identify thetarget regions for modification. However, since cross-attention mechanismsfocus on semantic relevance, they struggle to maintain the image integrity. Asa result, these methods often lack spatial consistency, leading to editingartifacts and distortions. In this work, we address these limitations andintroduce LOCATEdit, which enhances cross-attention maps through a graph-basedapproach utilizing self-attention-derived patch relationships to maintainsmooth, coherent attention across image regions, ensuring that alterations arelimited to the designated items while retaining the surrounding structure.\method consistently and substantially outperforms existing baselines onPIE-Bench, demonstrating its state-of-the-art performance and effectiveness onvarious editing tasks. Code can be found onhttps://github.com/LOCATEdit/LOCATEdit/
  </details>

- **[ICG-MVSNet: Learning Intra-view and Cross-view Relationships for Guidance in Multi-View Stereo](http://arxiv.org/abs/2503.21525v1)**  `arXiv:2503.21525`  
  _Yuxi Hu, Jun Zhang, Zhe Zhang, Rafael Weilharter, Yuchen Rao, Kuangyi Chen, et al._
  <details><summary>Abstract</summary>
  Multi-view Stereo (MVS) aims to estimate depth and reconstruct 3D pointclouds from a series of overlapping images. Recent learning-based MVSframeworks overlook the geometric information embedded in features andcorrelations, leading to weak cost matching. In this paper, we proposeICG-MVSNet, which explicitly integrates intra-view and cross-view relationshipsfor depth estimation. Specifically, we develop an intra-view feature fusionmodule that leverages the feature coordinate correlations within a single imageto enhance robust cost matching. Additionally, we introduce a lightweightcross-view aggregation module that efficiently utilizes the contextualinformation from volume correlations to guide regularization. Our method isevaluated on the DTU dataset and Tanks and Temples benchmark, consistentlyachieving competitive performance against state-of-the-art works, whilerequiring lower computational resources.
  </details>

- **[Shape Modeling of Longitudinal Medical Images: From Diffeomorphic Metric Mapping to Deep Learning](http://arxiv.org/abs/2503.21489v1)**  `arXiv:2503.21489`  
  _Edwin Tay, Nazli T√ºmer, Amir A. Zadpoor_
  <details><summary>Abstract</summary>
  Living biological tissue is a complex system, constantly growing and changingin response to external and internal stimuli. These processes lead toremarkable and intricate changes in shape. Modeling and understanding bothnatural and pathological (or abnormal) changes in the shape of anatomicalstructures is highly relevant, with applications in diagnostic, prognostic, andtherapeutic healthcare. Nevertheless, modeling the longitudinal shape change ofbiological tissue is a non-trivial task due to its inherent nonlinear nature.In this review, we highlight several existing methodologies and tools formodeling longitudinal shape change (i.e., spatiotemporal shape modeling). Thesemethods range from diffeomorphic metric mapping to deep-learning basedapproaches (e.g., autoencoders, generative networks, recurrent neural networks,etc.). We discuss the synergistic combinations of existing technologies andpotential directions for future research, underscoring key deficiencies in thecurrent research landscape.
  </details>

- **[Invert2Restore: Zero-Shot Degradation-Blind Image Restoration](http://arxiv.org/abs/2503.21486v1)**  `arXiv:2503.21486`  
  _Hamadi Chihaoui, Paolo Favaro_
  <details><summary>Abstract</summary>
  Two of the main challenges of image restoration in real-world scenarios arethe accurate characterization of an image prior and the precise modeling of theimage degradation operator. Pre-trained diffusion models have been verysuccessfully used as image priors in zero-shot image restoration methods.However, how to best handle the degradation operator is still an open problem.In real-world data, methods that rely on specific parametric assumptions aboutthe degradation model often face limitations in their applicability. To addressthis, we introduce Invert2Restore, a zero-shot, training-free method thatoperates in both fully blind and partially blind settings -- requiring no priorknowledge of the degradation model or only partial knowledge of its parametricform without known parameters. Despite this, Invert2Restore achieveshigh-fidelity results and generalizes well across various types of imagedegradation. It leverages a pre-trained diffusion model as a deterministicmapping between normal samples and undistorted image samples. The key insightis that the input noise mapped by a diffusion model to a degraded image lies ina low-probability density region of the standard normal distribution. Thus, wecan restore the degraded image by carefully guiding its input noise toward ahigher-density region. We experimentally validate Invert2Restore across severalimage restoration tasks, demonstrating that it achieves state-of-the-artperformance in scenarios where the degradation operator is either unknown orpartially known.
  </details>

- **[BOLT: Boost Large Vision-Language Model Without Training for Long-form Video Understanding](http://arxiv.org/abs/2503.21483v1)**  `arXiv:2503.21483`  
  _Shuming Liu, Chen Zhao, Tianqi Xu, Bernard Ghanem_
  <details><summary>Abstract</summary>
  Large video-language models (VLMs) have demonstrated promising progress invarious video understanding tasks. However, their effectiveness in long-formvideo analysis is constrained by limited context windows. Traditionalapproaches, such as uniform frame sampling, often inevitably allocate resourcesto irrelevant content, diminishing their effectiveness in real-world scenarios.In this paper, we introduce BOLT, a method to BOost Large VLMs withoutadditional Training through a comprehensive study of frame selectionstrategies. First, to enable a more realistic evaluation of VLMs in long-formvideo understanding, we propose a multi-source retrieval evaluation setting.Our findings reveal that uniform sampling performs poorly in noisy contexts,underscoring the importance of selecting the right frames. Second, we exploreseveral frame selection strategies based on query-frame similarity and analyzetheir effectiveness at inference time. Our results show that inverse transformsampling yields the most significant performance improvement, increasingaccuracy on the Video-MME benchmark from 53.8% to 56.1% and MLVU benchmark from58.9% to 63.4%. Our code is available at https://github.com/sming256/BOLT.
  </details>

- **[Fine-Grained Behavior and Lane Constraints Guided Trajectory Prediction Method](http://arxiv.org/abs/2503.21477v1)**  `arXiv:2503.21477`  
  _Wenyi Xiong, Jian Chen, Ziheng Qi_
  <details><summary>Abstract</summary>
  Trajectory prediction, as a critical component of autonomous driving systems,has attracted the attention of many researchers. Existing prediction algorithmsfocus on extracting more detailed scene features or selecting more reasonabletrajectory destinations. However, in the face of dynamic and evolving futuremovements of the target vehicle, these algorithms cannot provide a fine-grainedand continuous description of future behaviors and lane constraints, whichdegrades the prediction accuracy. To address this challenge, we present BLNet,a novel dualstream architecture that synergistically integrates behavioralintention recognition and lane constraint modeling through parallel attentionmechanisms. The framework generates fine-grained behavior state queries(capturing spatial-temporal movement patterns) and lane queries (encoding lanetopology constraints), supervised by two auxiliary losses, respectively.Subsequently, a two-stage decoder first produces trajectory proposals, thenperforms point-level refinement by jointly incorporating both the continuity ofpassed lanes and future motion features. Extensive experiments on two largedatasets, nuScenes and Argoverse, show that our network exhibits significantperformance gains over existing direct regression and goal-based algorithms.
  </details>

- **[Retinal Fundus Multi-Disease Image Classification using Hybrid CNN-Transformer-Ensemble Architectures](http://arxiv.org/abs/2503.21465v1)**  `arXiv:2503.21465`  
  _Deependra Singh, Saksham Agarwal, Subhankar Mishra_
  <details><summary>Abstract</summary>
  Our research is motivated by the urgent global issue of a large populationaffected by retinal diseases, which are evenly distributed but underserved byspecialized medical expertise, particularly in non-urban areas. Our primaryobjective is to bridge this healthcare gap by developing a comprehensivediagnostic system capable of accurately predicting retinal diseases solely fromfundus images. However, we faced significant challenges due to limited, diversedatasets and imbalanced class distributions. To overcome these issues, we havedevised innovative strategies. Our research introduces novel approaches,utilizing hybrid models combining deeper Convolutional Neural Networks (CNNs),Transformer encoders, and ensemble architectures sequentially and in parallelto classify retinal fundus images into 20 disease labels. Our overarching goalis to assess these advanced models' potential in practical applications, with astrong focus on enhancing retinal disease diagnosis accuracy across a broaderspectrum of conditions. Importantly, our efforts have surpassed baseline modelresults, with the C-Tran ensemble model emerging as the leader, achieving aremarkable model score of 0.9166, surpassing the baseline score of 0.9.Additionally, experiments with the IEViT model showcased equally promisingoutcomes with improved computational efficiency. We've also demonstrated theeffectiveness of dynamic patch extraction and the integration of domainknowledge in computer vision tasks. In summary, our research strives tocontribute significantly to retinal disease diagnosis, addressing the criticalneed for accessible healthcare solutions in underserved regions while aimingfor comprehensive and accurate disease prediction.
  </details>

- **[RoadSocial: A Diverse VideoQA Dataset and Benchmark for Road Event Understanding from Social Video Narratives](http://arxiv.org/abs/2503.21459v1)**  `arXiv:2503.21459`  
  _Chirag Parikh, Deepti Rawat, Rakshitha R. T., Tathagata Ghosh, Ravi Kiran Sarvadevabhatla_
  <details><summary>Abstract</summary>
  We introduce RoadSocial, a large-scale, diverse VideoQA dataset tailored forgeneric road event understanding from social media narratives. Unlike existingdatasets limited by regional bias, viewpoint bias and expert-drivenannotations, RoadSocial captures the global complexity of road events withvaried geographies, camera viewpoints (CCTV, handheld, drones) and rich socialdiscourse. Our scalable semi-automatic annotation framework leverages Text LLMsand Video LLMs to generate comprehensive question-answer pairs across 12challenging QA tasks, pushing the boundaries of road event understanding.RoadSocial is derived from social media videos spanning 14M frames and 414Ksocial comments, resulting in a dataset with 13.2K videos, 674 tags and 260Khigh-quality QA pairs. We evaluate 18 Video LLMs (open-source and proprietary,driving-specific and general-purpose) on our road event understandingbenchmark. We also demonstrate RoadSocial's utility in improving road eventunderstanding capabilities of general-purpose Video LLMs.
  </details>

- **[FaceBench: A Multi-View Multi-Level Facial Attribute VQA Dataset for Benchmarking Face Perception MLLMs](http://arxiv.org/abs/2503.21457v1)**  `arXiv:2503.21457`  
  _Xiaoqin Wang, Xusen Ma, Xianxu Hou, Meidan Ding, Yudong Li, Junliang Chen, et al._
  <details><summary>Abstract</summary>
  Multimodal large language models (MLLMs) have demonstrated remarkablecapabilities in various tasks. However, effectively evaluating these MLLMs onface perception remains largely unexplored. To address this gap, we introduceFaceBench, a dataset featuring hierarchical multi-view and multi-levelattributes specifically designed to assess the comprehensive face perceptionabilities of MLLMs. Initially, we construct a hierarchical facial attributestructure, which encompasses five views with up to three levels of attributes,totaling over 210 attributes and 700 attribute values. Based on the structure,the proposed FaceBench consists of 49,919 visual question-answering (VQA) pairsfor evaluation and 23,841 pairs for fine-tuning. Moreover, we further develop arobust face perception MLLM baseline, Face-LLaVA, by training with our proposedface VQA data. Extensive experiments on various mainstream MLLMs and Face-LLaVAare conducted to test their face perception ability, with results also comparedagainst human performance. The results reveal that, the existing MLLMs are farfrom satisfactory in understanding the fine-grained facial attributes, whileour Face-LLaVA significantly outperforms existing open-source models with asmall amount of training data and is comparable to commercial ones like GPT-4oand Gemini. The dataset will be released athttps://github.com/CVI-SZU/FaceBench.
  </details>

- **[Towards Generating Realistic 3D Semantic Training Data for Autonomous Driving](http://arxiv.org/abs/2503.21449v1)**  `arXiv:2503.21449`  
  _Lucas Nunes, Rodrigo Marcuzzi, Jens Behley, Cyrill Stachniss_
  <details><summary>Abstract</summary>
  Semantic scene understanding is crucial for robotics and computer visionapplications. In autonomous driving, 3D semantic segmentation plays animportant role for enabling safe navigation. Despite significant advances inthe field, the complexity of collecting and annotating 3D data is a bottleneckin this developments. To overcome that data annotation limitation, syntheticsimulated data has been used to generate annotated data on demand. There isstill however a domain gap between real and simulated data. More recently,diffusion models have been in the spotlight, enabling close-to-real datasynthesis. Those generative models have been recently applied to the 3D datadomain for generating scene-scale data with semantic annotations. Still, thosemethods either rely on image projection or decoupled models trained withdifferent resolutions in a coarse-to-fine manner. Such intermediaryrepresentations impact the generated data quality due to errors added in thosetransformations. In this work, we propose a novel approach able to generate 3Dsemantic scene-scale data without relying on any projection or decoupledtrained multi-resolution models, achieving more realistic semantic scene datageneration compared to previous state-of-the-art methods. Besides improving 3Dsemantic scene-scale data synthesis, we thoroughly evaluate the use of thesynthetic scene samples as labeled data to train a semantic segmentationnetwork. In our experiments, we show that using the synthetic annotated datagenerated by our method as training data together with the real semanticsegmentation labels, leads to an improvement in the semantic segmentation modelperformance. Our results show the potential of generated scene-scale pointclouds to generate more training data to extend existing datasets, reducing thedata annotation effort. Our code is available athttps://github.com/PRBonn/3DiSS.
  </details>

- **[Dual-Task Learning for Dead Tree Detection and Segmentation with Hybrid Self-Attention U-Nets in Aerial Imagery](http://arxiv.org/abs/2503.21438v1)**  `arXiv:2503.21438`  
  _Anis Ur Rahman, Einari Heinaro, Mete Ahishali, Samuli Junttila_
  <details><summary>Abstract</summary>
  Mapping standing dead trees is critical for assessing forest health,monitoring biodiversity, and mitigating wildfire risks, for which aerialimagery has proven useful. However, dense canopy structures, spectral overlapsbetween living and dead vegetation, and over-segmentation errors limit thereliability of existing methods. This study introduces a hybrid postprocessingframework that refines deep learning-based tree segmentation by integratingwatershed algorithms with adaptive filtering, enhancing boundary delineation,and reducing false positives in complex forest environments. Tested onhigh-resolution aerial imagery from boreal forests, the framework improvedinstance-level segmentation accuracy by 41.5% and reduced positional errors by57%, demonstrating robust performance in densely vegetated regions. Bybalancing detection accuracy and over-segmentation artifacts, the methodenabled the precise identification of individual dead trees, which is criticalfor ecological monitoring. The framework's computational efficiency supportsscalable applications, such as wall-to-wall tree mortality mapping over largegeographic regions using aerial or satellite imagery. These capabilitiesdirectly benefit wildfire risk assessment (identifying fuel accumulations),carbon stock estimation (tracking emissions from decaying biomass), andprecision forestry (targeting salvage loggings). By bridging advanced remotesensing techniques with practical forest management needs, this work advancestools for large-scale ecological conservation and climate resilience planning.
  </details>

- **[Diffusion Image Prior](http://arxiv.org/abs/2503.21410v1)**  `arXiv:2503.21410`  
  _Hamadi Chihaoui, Paolo Favaro_
  <details><summary>Abstract</summary>
  Zero-shot image restoration (IR) methods based on pretrained diffusion modelshave recently achieved significant success. These methods typically require atleast a parametric form of the degradation model. However, in real-worldscenarios, the degradation may be too complex to define explicitly. To handlethis general case, we introduce the Diffusion Image Prior (DIIP). We takeinspiration from the Deep Image Prior (DIP)[16], since it can be used to removeartifacts without the need for an explicit degradation model. However, incontrast to DIP, we find that pretrained diffusion models offer a much strongerprior, despite being trained without knowledge from corrupted data. We showthat, the optimization process in DIIP first reconstructs a clean version ofthe image before eventually overfitting to the degraded input, but it does sofor a broader range of degradations than DIP. In light of this result, wepropose a blind image restoration (IR) method based on early stopping, whichdoes not require prior knowledge of the degradation model. We validate DIIP onvarious degradation-blind IR tasks, including JPEG artifact removal, waterdropremoval, denoising and super-resolution with state-of-the-art results.
  </details>

- **[VALLR: Visual ASR Language Model for Lip Reading](http://arxiv.org/abs/2503.21408v1)**  `arXiv:2503.21408`  
  _Marshall Thomas, Edward Fish, Richard Bowden_
  <details><summary>Abstract</summary>
  Lip Reading, or Visual Automatic Speech Recognition (V-ASR), is a complextask requiring the interpretation of spoken language exclusively from visualcues, primarily lip movements and facial expressions. This task is especiallychallenging due to the absence of auditory information and the inherentambiguity when visually distinguishing phonemes that have overlapping visemeswhere different phonemes appear identical on the lips. Current methodstypically attempt to predict words or characters directly from these visualcues, but this approach frequently encounters high error rates due tocoarticulation effects and viseme ambiguity. We propose a novel two-stage,phoneme-centric framework for Visual Automatic Speech Recognition (V-ASR) thataddresses these longstanding challenges. First, our model predicts a compactsequence of phonemes from visual inputs using a Video Transformer with a CTChead, thereby reducing the task complexity and achieving robust speakerinvariance. This phoneme output then serves as the input to a fine-tuned LargeLanguage Model (LLM), which reconstructs coherent words and sentences byleveraging broader linguistic context. Unlike existing methods that eitherpredict words directly-often faltering on visually similar phonemes-or rely onlarge-scale multimodal pre-training, our approach explicitly encodesintermediate linguistic structure while remaining highly data efficient. Wedemonstrate state-of-the-art performance on two challenging datasets, LRS2 andLRS3, where our method achieves significant reductions in Word Error Rate (WER)achieving a SOTA WER of 18.7 on LRS3 despite using 99.4% less labelled datathan the next best approach.
  </details>

- **[Unsupervised Real-World Denoising: Sparsity is All You Need](http://arxiv.org/abs/2503.21377v1)**  `arXiv:2503.21377`  
  _Hamadi Chihaoui, Paolo Favaro_
  <details><summary>Abstract</summary>
  Supervised training for real-world denoising presents challenges due to thedifficulty of collecting large datasets of paired noisy and clean images.Recent methods have attempted to address this by utilizing unpaired datasets ofclean and noisy images. Some approaches leverage such unpaired data to traindenoisers in a supervised manner by generating synthetic clean-noisy pairs.However, these methods often fall short due to the distribution gap betweensynthetic and real noisy images. To mitigate this issue, we propose a solutionbased on input sparsification, specifically using random input masking. Ourmethod, which we refer to as Mask, Inpaint and Denoise (MID), trains a denoiserto simultaneously denoise and inpaint synthetic clean-noisy pairs. On one hand,input sparsification reduces the gap between synthetic and real noisy images.On the other hand, an inpainter trained in a supervised manner can stillaccurately reconstruct sparse inputs by predicting missing clean pixels usingthe remaining unmasked pixels. Our approach begins with a synthetic Gaussiannoise sampler and iteratively refines it using a noise dataset derived from thedenoiser's predictions. The noise dataset is created by subtracting predictedpseudo-clean images from real noisy images at each iteration. The coreintuition is that improving the denoiser results in a more accurate noisedataset and, consequently, a better noise sampler. We validate our methodthrough extensive experiments on real-world noisy image datasets, demonstratingcompetitive performance compared to existing unsupervised denoising methods.
  </details>

- **[Multimodal surface defect detection from wooden logs for sawing optimization](http://arxiv.org/abs/2503.21367v1)**  `arXiv:2503.21367`  
  _Bo≈ôek Reich, Matej Kunda, Fedor Zolotarev, Tuomas Eerola, Pavel Zemƒç√≠k, Tomi Kauppi_
  <details><summary>Abstract</summary>
  We propose a novel, good-quality, and less demanding method for detectingknots on the surface of wooden logs using multimodal data fusion. Knots are aprimary factor affecting the quality of sawn timber, making their detectionfundamental to any timber grading or cutting optimization system. While X-raycomputed tomography provides accurate knot locations and internal structures,it is often too slow or expensive for practical use. An attractive alternativeis to use fast and cost-effective log surface measurements, such as laserscanners or RGB cameras, to detect surface knots and estimate the internalstructure of wood. However, due to the small size of knots and noise caused byfactors, such as bark and other natural variations, detection accuracy oftenremains low when only one measurement modality is used. In this paper, wedemonstrate that by using a data fusion pipeline consisting of separate streamsfor RGB and point cloud data, combined by a late fusion module, higher knotdetection accuracy can be achieved compared to using either modality alone. Wefurther propose a simple yet efficient sawing angle optimization method thatutilizes surface knot detections and cross-correlation to minimize the amountof unwanted arris knots, demonstrating its benefits over randomized sawingangles.
  </details>

- **[LandMarkSystem Technical Report](http://arxiv.org/abs/2503.21364v1)**  `arXiv:2503.21364`  
  _Zhenxiang Ma, Zhenyu Yang, Miao Tao, Yuanzhen Zhou, Zeyu He, Yuchang Zhang, et al._
  <details><summary>Abstract</summary>
  3D reconstruction is vital for applications in autonomous driving, virtualreality, augmented reality, and the metaverse. Recent advancements such asNeural Radiance Fields(NeRF) and 3D Gaussian Splatting (3DGS) have transformedthe field, yet traditional deep learning frameworks struggle to meet theincreasing demands for scene quality and scale. This paper introducesLandMarkSystem, a novel computing framework designed to enhance multi-scalescene reconstruction and rendering. By leveraging a componentized modeladaptation layer, LandMarkSystem supports various NeRF and 3DGS structureswhile optimizing computational efficiency through distributed parallelcomputing and model parameter offloading. Our system addresses the limitationsof existing frameworks, providing dedicated operators for complex 3D sparsecomputations, thus facilitating efficient training and rapid inference overextensive scenes. Key contributions include a modular architecture, a dynamicloading strategy for limited resources, and proven capabilities across multiplerepresentative algorithms.This comprehensive solution aims to advance theefficiency and effectiveness of 3D reconstruction tasks.To facilitate furtherresearch and collaboration, the source code and documentation for theLandMarkSystem project are publicly available in an open-source repository,accessing the repository at: https://github.com/InternLandMark/LandMarkSystem.
  </details>

- **[UGNA-VPR: A Novel Training Paradigm for Visual Place Recognition Based on Uncertainty-Guided NeRF Augmentation](http://arxiv.org/abs/2503.21338v1)**  `arXiv:2503.21338`  
  _Yehui Shen, Lei Zhang, Qingqiu Li, Xiongwei Zhao, Yue Wang, Huimin Lu, et al._
  <details><summary>Abstract</summary>
  Visual place recognition (VPR) is crucial for robots to identify previouslyvisited locations, playing an important role in autonomous navigation in bothindoor and outdoor environments. However, most existing VPR datasets arelimited to single-viewpoint scenarios, leading to reduced recognition accuracy,particularly in multi-directional driving or feature-sparse scenes. Moreover,obtaining additional data to mitigate these limitations is often expensive.This paper introduces a novel training paradigm to improve the performance ofexisting VPR networks by enhancing multi-view diversity within current datasetsthrough uncertainty estimation and NeRF-based data augmentation. Specifically,we initially train NeRF using the existing VPR dataset. Then, our devisedself-supervised uncertainty estimation network identifies places with highuncertainty. The poses of these uncertain places are input into NeRF togenerate new synthetic observations for further training of VPR networks.Additionally, we propose an improved storage method for efficient organizationof augmented and original training data. We conducted extensive experiments onthree datasets and tested three different VPR backbone networks. The resultsdemonstrate that our proposed training paradigm significantly improves VPRperformance by fully utilizing existing data, outperforming other trainingapproaches. We further validated the effectiveness of our approach onself-recorded indoor and outdoor datasets, consistently demonstrating superiorresults. Our dataset and code have been released at\href{https://github.com/nubot-nudt/UGNA-VPR}{https://github.com/nubot-nudt/UGNA-VPR}.
  </details>

- **[DuckSegmentation: A segmentation model based on the AnYue Hemp Duck Dataset](http://arxiv.org/abs/2503.21323v1)**  `arXiv:2503.21323`  
  _Ling Feng, Tianyu Xie, Wei Ma, Ruijie Fu, Yingxiao Zhang, Jun Li, et al._
  <details><summary>Abstract</summary>
  The modernization of smart farming is a way to improve agriculturalproduction efficiency, and improve the agricultural production environment.Although many large models have achieved high accuracy in the task of objectrecognition and segmentation, they cannot really be put into use in the farmingindustry due to their own poor interpretability and limitations incomputational volume. In this paper, we built AnYue Shelduck Dateset, whichcontains a total of 1951 Shelduck datasets, and performed target detection andsegmentation annotation with the help of professional annotators. Based onAnYue ShelduckDateset, this paper describes DuckProcessing, an efficient andpowerful module for duck identification based on real shelduckfarms. First ofall, using the YOLOv8 module designed to divide the mahjong between them,Precision reached 98.10%, Recall reached 96.53% and F1 score reached 0.95 onthe test set. Again using the DuckSegmentation segmentation model,DuckSegmentation reached 96.43% mIoU. Finally, the excellent DuckSegmentationwas used as the teacher model, and through knowledge distillation, Deeplabv3r50 was used as the student model, and the final student model achieved 94.49%mIoU on the test set. The method provides a new way of thinking in practicalsisal duck smart farming.
  </details>

- **[HORT: Monocular Hand-held Objects Reconstruction with Transformers](http://arxiv.org/abs/2503.21313v1)**  `arXiv:2503.21313`  
  _Zerui Chen, Rolandos Alexandros Potamias, Shizhe Chen, Cordelia Schmid_
  <details><summary>Abstract</summary>
  Reconstructing hand-held objects in 3D from monocular images remains asignificant challenge in computer vision. Most existing approaches rely onimplicit 3D representations, which produce overly smooth reconstructions andare time-consuming to generate explicit 3D shapes. While more recent methodsdirectly reconstruct point clouds with diffusion models, the multi-stepdenoising makes high-resolution reconstruction inefficient. To address theselimitations, we propose a transformer-based model to efficiently reconstructdense 3D point clouds of hand-held objects. Our method follows a coarse-to-finestrategy, first generating a sparse point cloud from the image andprogressively refining it into a dense representation using pixel-aligned imagefeatures. To enhance reconstruction accuracy, we integrate image features with3D hand geometry to jointly predict the object point cloud and its poserelative to the hand. Our model is trained end-to-end for optimal performance.Experimental results on both synthetic and real datasets demonstrate that ourmethod achieves state-of-the-art accuracy with much faster inference speed,while generalizing well to in-the-wild images.
  </details>

- **[FineCIR: Explicit Parsing of Fine-Grained Modification Semantics for Composed Image Retrieval](http://arxiv.org/abs/2503.21309v1)**  `arXiv:2503.21309`  
  _Zixu Li, Zhiheng Fu, Yupeng Hu, Zhiwei Chen, Haokun Wen, Liqiang Nie_
  <details><summary>Abstract</summary>
  Composed Image Retrieval (CIR) facilitates image retrieval through amultimodal query consisting of a reference image and modification text. Thereference image defines the retrieval context, while the modification textspecifies desired alterations. However, existing CIR datasets predominantlyemploy coarse-grained modification text (CoarseMT), which inadequately capturesfine-grained retrieval intents. This limitation introduces two key challenges:(1) ignoring detailed differences leads to imprecise positive samples, and (2)greater ambiguity arises when retrieving visually similar images. These issuesdegrade retrieval accuracy, necessitating manual result filtering or repeatedqueries. To address these limitations, we develop a robust fine-grained CIRdata annotation pipeline that minimizes imprecise positive samples and enhancesCIR systems' ability to discern modification intents accurately. Using thispipeline, we refine the FashionIQ and CIRR datasets to create two fine-grainedCIR datasets: Fine-FashionIQ and Fine-CIRR. Furthermore, we introduce FineCIR,the first CIR framework explicitly designed to parse the modification text.FineCIR effectively captures fine-grained modification semantics and alignsthem with ambiguous visual entities, enhancing retrieval precision. Extensiveexperiments demonstrate that FineCIR consistently outperforms state-of-the-artCIR baselines on both fine-grained and traditional CIR benchmark datasets. OurFineCIR code and fine-grained CIR datasets are available athttps://github.com/SDU-L/FineCIR.git.
  </details>

- **[InternVL-X: Advancing and Accelerating InternVL Series with Efficient Visual Token Compression](http://arxiv.org/abs/2503.21307v1)**  `arXiv:2503.21307`  
  _Dongchen Lu, Yuyao Sun, Zilu Zhang, Leping Huang, Jianliang Zeng, Mao Shu, et al._
  <details><summary>Abstract</summary>
  Most multimodal large language models (MLLMs) treat visual tokens as "asequence of text", integrating them with text tokens into a large languagemodel (LLM). However, a great quantity of visual tokens significantly increasesthe demand for computational resources and time. In this paper, we proposeInternVL-X, which outperforms the InternVL model in both performance andefficiency by incorporating three visual token compression methods. First, wepropose a novel vision-language projector, PVTC. This component integratesadjacent visual embeddings to form a local query and utilizes the transformedCLS token as a global query, then performs point-to-region cross-attentionthrough these local and global queries to more effectively convert visualfeatures. Second, we present a layer-wise visual token compression module,LVTC, which compresses tokens in the LLM shallow layers and then expands themthrough upsampling and residual connections in the deeper layers. Thissignificantly enhances the model computational efficiency. Futhermore, wepropose an efficient high resolution slicing method, RVTC, which dynamicallyadjusts the number of visual tokens based on image area or length filtering.RVTC greatly enhances training efficiency with only a slight reduction inperformance. By utilizing 20% or fewer visual tokens, InternVL-X achievesstate-of-the-art performance on 7 public MLLM benchmarks, and improves theaverage metric by 2.34% across 12 tasks.
  </details>

- **[Multi-Scale Invertible Neural Network for Wide-Range Variable-Rate Learned Image Compression](http://arxiv.org/abs/2503.21284v1)**  `arXiv:2503.21284`  
  _Hanyue Tu, Siqi Wu, Li Li, Wengang Zhou, Houqiang Li_
  <details><summary>Abstract</summary>
  Autoencoder-based structures have dominated recent learned image compressionmethods. However, the inherent information loss associated with autoencoderslimits their rate-distortion performance at high bit rates and restricts theirflexibility of rate adaptation. In this paper, we present a variable-rate imagecompression model based on invertible transform to overcome these limitations.Specifically, we design a lightweight multi-scale invertible neural network,which bijectively maps the input image into multi-scale latent representations.To improve the compression efficiency, a multi-scale spatial-channel contextmodel with extended gain units is devised to estimate the entropy of the latentrepresentation from high to low levels. Experimental results demonstrate thatthe proposed method achieves state-of-the-art performance compared to existingvariable-rate methods, and remains competitive with recent multi-modelapproaches. Notably, our method is the first learned image compression solutionthat outperforms VVC across a very wide range of bit rates using a singlemodel, especially at high bit rates.The source code is available at\href{https://github.com/hytu99/MSINN-VRLIC}{https://github.com/hytu99/MSINN-VRLIC}.
  </details>

- **[Zero-Shot Visual Concept Blending Without Text Guidance](http://arxiv.org/abs/2503.21277v1)**  `arXiv:2503.21277`  
  _Hiroya Makino, Takahiro Yamaguchi, Hiroyuki Sakai_
  <details><summary>Abstract</summary>
  We propose a novel, zero-shot image generation technique called "VisualConcept Blending" that provides fine-grained control over which features frommultiple reference images are transferred to a source image. If only a singlereference image is available, it is difficult to isolate which specificelements should be transferred. However, using multiple reference images, theproposed approach distinguishes between common and unique features byselectively incorporating them into a generated output. By operating within apartially disentangled Contrastive Language-Image Pre-training (CLIP) embeddingspace (from IP-Adapter), our method enables the flexible transfer of texture,shape, motion, style, and more abstract conceptual transformations withoutrequiring additional training or text prompts. We demonstrate its effectivenessacross a diverse range of tasks, including style transfer, form metamorphosis,and conceptual transformations, showing how subtle or abstract attributes(e.g., brushstroke style, aerodynamic lines, and dynamism) can be seamlesslycombined into a new image. In a user study, participants accurately recognizedwhich features were intended to be transferred. Its simplicity, flexibility,and high-level control make Visual Concept Blending valuable for creativefields such as art, design, and content creation, where combining specificvisual qualities from multiple inspirations is crucial.
  </details>

- **[Delving Deep into Semantic Relation Distillation](http://arxiv.org/abs/2503.21269v1)**  `arXiv:2503.21269`  
  _Zhaoyi Yan, Kangjun Liu, Qixiang Ye_
  <details><summary>Abstract</summary>
  Knowledge distillation has become a cornerstone technique in deep learning,facilitating the transfer of knowledge from complex models to lightweightcounterparts. Traditional distillation approaches focus on transferringknowledge at the instance level, but fail to capture nuanced semanticrelationships within the data. In response, this paper introduces a novelmethodology, Semantics-based Relation Knowledge Distillation (SeRKD), whichreimagines knowledge distillation through a semantics-relation lens among eachsample. By leveraging semantic components, \ie, superpixels, SeRKD enables amore comprehensive and context-aware transfer of knowledge, which skillfullyintegrates superpixel-based semantic extraction with relation-based knowledgedistillation for a sophisticated model compression and distillation.Particularly, the proposed method is naturally relevant in the domain of VisionTransformers (ViTs), where visual tokens serve as fundamental units ofrepresentation. Experimental evaluations on benchmark datasets demonstrate thesuperiority of SeRKD over existing methods, underscoring its efficacy inenhancing model performance and generalization capabilities.
  </details>

- **[ClimbingCap: Multi-Modal Dataset and Method for Rock Climbing in World Coordinate](http://arxiv.org/abs/2503.21268v1)**  `arXiv:2503.21268`  
  _Ming Yan, Xincheng Lin, Yuhua Luo, Shuqi Fan, Yudi Dai, Qixin Zhong, et al._
  <details><summary>Abstract</summary>
  Human Motion Recovery (HMR) research mainly focuses on ground-based motionssuch as running. The study on capturing climbing motion, an off-ground motion,is sparse. This is partly due to the limited availability of climbing motiondatasets, especially large-scale and challenging 3D labeled datasets. Toaddress the insufficiency of climbing motion datasets, we collect AscendMotion,a large-scale well-annotated, and challenging climbing motion dataset. Itconsists of 412k RGB, LiDAR frames, and IMU measurements, including thechallenging climbing motions of 22 skilled climbing coaches across 12 differentrock walls. Capturing the climbing motions is challenging as it requiresprecise recovery of not only the complex pose but also the global position ofclimbers. Although multiple global HMR methods have been proposed, they cannotfaithfully capture climbing motions. To address the limitations of HMR methodsfor climbing, we propose ClimbingCap, a motion recovery method thatreconstructs continuous 3D human climbing motion in a global coordinate system.One key insight is to use the RGB and LiDAR modalities to separatelyreconstruct motions in camera coordinates and global coordinates and tooptimize them jointly. We demonstrate the quality of the AscendMotion datasetand present promising results from ClimbingCap. The AscendMotion dataset andsource code release publicly at \href{thislink}{http://www.lidarhumanmotion.net/climbingcap/}
  </details>

- **[vGamba: Attentive State Space Bottleneck for efficient Long-range Dependencies in Visual Recognition](http://arxiv.org/abs/2503.21262v1)**  `arXiv:2503.21262`  
  _Yunusa Haruna, Adamu Lawan_
  <details><summary>Abstract</summary>
  Capturing long-range dependencies efficiently is essential for visualrecognition tasks, yet existing methods face limitations. Convolutional neuralnetworks (CNNs) struggle with restricted receptive fields, while VisionTransformers (ViTs) achieve global context and long-range modeling at a highcomputational cost. State-space models (SSMs) offer an alternative, but theirapplication in vision remains underexplored. This work introduces vGamba, ahybrid vision backbone that integrates SSMs with attention mechanisms toenhance efficiency and expressiveness. At its core, the Gamba bottleneck blockthat includes, Gamba Cell, an adaptation of Mamba for 2D spatial structures,alongside a Multi-Head Self-Attention (MHSA) mechanism and a Gated FusionModule for effective feature representation. The interplay of these componentsensures that vGamba leverages the low computational demands of SSMs whilemaintaining the accuracy of attention mechanisms for modeling long-rangedependencies in vision tasks. Additionally, the Fusion module enables seamlessinteraction between these components. Extensive experiments on classification,detection, and segmentation tasks demonstrate that vGamba achieves a superiortrade-off between accuracy and computational efficiency, outperforming severalexisting models.
  </details>

- **[Reducing CT Metal Artifacts by Learning Latent Space Alignment with Gemstone Spectral Imaging Data](http://arxiv.org/abs/2503.21259v1)**  `arXiv:2503.21259`  
  _Wencheng Han, Dongqian Guo, Xiao Chen, Pang Lyu, Yi Jin, Jianbing Shen_
  <details><summary>Abstract</summary>
  Metal artifacts in CT slices have long posed challenges in medicaldiagnostics. These artifacts degrade image quality, resulting in suboptimalvisualization and complicating the accurate interpretation of tissues adjacentto metal implants. To address these issues, we introduce the Latent GemstoneSpectral Imaging (GSI) Alignment Framework, which effectively reduces metalartifacts while avoiding the introduction of noise information. Our work isbased on a key finding that even artifact-affected ordinary CT sequencescontain sufficient information to discern detailed structures. The challengelies in the inability to clearly represent this information. To address thisissue, we developed an Alignment Framework that adjusts the representation ofordinary CT images to match GSI CT sequences. GSI is an advanced imagingtechnique using multiple energy levels to mitigate artifacts caused by metalimplants. By aligning the representation to GSI data, we can effectivelysuppress metal artifacts while clearly revealing detailed structure, withoutintroducing extraneous information into CT sequences. To facilitate theapplication, we propose a new dataset, Artifacts-GSI, captured from realpatients with metal implants, and establish a new benchmark based on thisdataset. Experimental results show that our method significantly reduces metalartifacts and greatly enhances the readability of CT slices. All our code anddata are available at: https://um-lab.github.io/GSI-MAR/
  </details>

- **[Learn by Reasoning: Analogical Weight Generation for Few-Shot Class-Incremental Learning](http://arxiv.org/abs/2503.21258v1)**  `arXiv:2503.21258`  
  _Jizhou Han, Chenhao Ding, Yuhang He, Songlin Dong, Qiang Wang, Xinyuan Gao, et al._
  <details><summary>Abstract</summary>
  Few-shot class-incremental Learning (FSCIL) enables models to learn newclasses from limited data while retaining performance on previously learnedclasses. Traditional FSCIL methods often require fine-tuning parameters withlimited new class data and suffer from a separation between learning newclasses and utilizing old knowledge. Inspired by the analogical learningmechanisms of the human brain, we propose a novel analogical generative method.Our approach includes the Brain-Inspired Analogical Generator (BiAG), whichderives new class weights from existing classes without parameter fine-tuningduring incremental stages. BiAG consists of three components: WeightSelf-Attention Module (WSA), Weight & Prototype Analogical Attention Module(WPAA), and Semantic Conversion Module (SCM). SCM uses Neural Collapse theoryfor semantic conversion, WSA supplements new class weights, and WPAA computesanalogies to generate new class weights. Experiments on miniImageNet, CUB-200,and CIFAR-100 datasets demonstrate that our method achieves higher final andaverage accuracy compared to SOTA methods.
  </details>

- **[Vision-to-Music Generation: A Survey](http://arxiv.org/abs/2503.21254v1)**  `arXiv:2503.21254`  
  _Zhaokai Wang, Chenxi Bao, Le Zhuo, Jingrui Han, Yang Yue, Yihong Tang, et al._
  <details><summary>Abstract</summary>
  Vision-to-music Generation, including video-to-music and image-to-musictasks, is a significant branch of multimodal artificial intelligencedemonstrating vast application prospects in fields such as film scoring, shortvideo creation, and dance music synthesis. However, compared to the rapiddevelopment of modalities like text and images, research in vision-to-music isstill in its preliminary stage due to its complex internal structure and thedifficulty of modeling dynamic relationships with video. Existing surveys focuson general music generation without comprehensive discussion onvision-to-music. In this paper, we systematically review the research progressin the field of vision-to-music generation. We first analyze the technicalcharacteristics and core challenges for three input types: general videos,human movement videos, and images, as well as two output types of symbolicmusic and audio music. We then summarize the existing methodologies onvision-to-music generation from the architecture perspective. A detailed reviewof common datasets and evaluation metrics is provided. Finally, we discusscurrent challenges and promising directions for future research. We hope oursurvey can inspire further innovation in vision-to-music generation and thebroader field of multimodal generation in academic research and industrialapplications. To follow latest works and foster further innovation in thisfield, we are continuously maintaining a GitHub repository athttps://github.com/wzk1015/Awesome-Vision-to-Music-Generation.
  </details>

- **[Orange Quality Grading with Deep Learning](http://arxiv.org/abs/2503.21250v1)**  `arXiv:2503.21250`  
  _Mohamed Lamine Mekhalfi, Paul Chippendale, Francisco Fraile, Marcos Rico_
  <details><summary>Abstract</summary>
  Orange grading is a crucial step in the fruit industry, as it helps to sortoranges according to different criteria such as size, quality, ripeness, andhealth condition, ensuring safety for human consumption and better priceallocation and client satisfaction. Automated grading enables fasterprocessing, precision, and reduced human labor. In this paper, we implement adeep learning-based solution for orange grading via machine vision. Unliketypical grading systems that analyze fruits from a single view, we capturemultiview images of each single orange in order to enable a richerrepresentation. Afterwards, we compose the acquired images into one collage.This enables the analysis of the whole orange skin. We train a convolutionalneural network (CNN) on the composed images to grade the oranges into threeclasses, namely good, bad, and undefined. We also evaluate the performance withtwo different CNNs (ResNet-18 and SqueezeNet). We show experimentally thatmulti-view grading is superior to single view grading.
  </details>

- **[DynamiCtrl: Rethinking the Basic Structure and the Role of Text for High-quality Human Image Animation](http://arxiv.org/abs/2503.21246v1)**  `arXiv:2503.21246`  
  _Haoyu Zhao, Zhongang Qi, Cong Wang, Qingping Zheng, Guansong Lu, Fei Chen, et al._
  <details><summary>Abstract</summary>
  Human image animation has recently gained significant attention due toadvancements in generative models. However, existing methods still face twomajor challenges: (1) architectural limitations, most models rely on U-Net,which underperforms compared to the MM-DiT; and (2) the neglect of textualinformation, which can enhance controllability. In this work, we introduceDynamiCtrl, a novel framework that not only explores different pose-guidedcontrol structures in MM-DiT, but also reemphasizes the crucial role of text inthis task. Specifically, we employ a Shared VAE encoder for both referenceimages and driving pose videos, eliminating the need for an additional poseencoder and simplifying the overall framework. To incorporate pose featuresinto the full attention blocks, we propose Pose-adaptive Layer Norm (PadaLN),which utilizes adaptive layer normalization to encode sparse pose features. Theencoded features are directly added to the visual input, preserving thespatiotemporal consistency of the backbone while effectively introducing posecontrol into MM-DiT. Furthermore, within the full attention mechanism, we aligntextual and visual features to enhance controllability. By leveraging text, wenot only enable fine-grained control over the generated content, but also, forthe first time, achieve simultaneous control over both background and motion.Experimental results verify the superiority of DynamiCtrl on benchmarkdatasets, demonstrating its strong identity preservation, heterogeneouscharacter driving, background controllability, and high-quality synthesis. Theproject page is available at https://gulucaptain.github.io/DynamiCtrl/.
  </details>

- **[Clean Image May be Dangerous: Data Poisoning Attacks Against Deep Hashing](http://arxiv.org/abs/2503.21236v1)**  `arXiv:2503.21236`  
  _Shuai Li, Jie Zhang, Yuang Qi, Kejiang Chen, Tianwei Zhang, Weiming Zhang, et al._
  <details><summary>Abstract</summary>
  Large-scale image retrieval using deep hashing has become increasinglypopular due to the exponential growth of image data and the remarkable featureextraction capabilities of deep neural networks (DNNs). However, deep hashingmethods are vulnerable to malicious attacks, including adversarial and backdoorattacks. It is worth noting that these attacks typically involve altering thequery images, which is not a practical concern in real-world scenarios. In thispaper, we point out that even clean query images can be dangerous, inducingmalicious target retrieval results, like undesired or illegal images. To thebest of our knowledge, we are the first to study data \textbf{p}oisoning\textbf{a}ttacks against \textbf{d}eep \textbf{hash}ing\textbf{(\textit{PADHASH})}. Specifically, we first train a surrogate model tosimulate the behavior of the target deep hashing model. Then, a strict gradientmatching strategy is proposed to generate the poisoned images. Extensiveexperiments on different models, datasets, hash methods, and hash code lengthsdemonstrate the effectiveness and generality of our attack method.
  </details>

- **[Frequency-Aware Gaussian Splatting Decomposition](http://arxiv.org/abs/2503.21226v1)**  `arXiv:2503.21226`  
  _Yishai Lavi, Leo Segre, Shai Avidan_
  <details><summary>Abstract</summary>
  3D Gaussian Splatting (3D-GS) has revolutionized novel view synthesis withits efficient, explicit representation. However, it lacks frequencyinterpretability, making it difficult to separate low-frequency structures fromfine details. We introduce a frequency-decomposed 3D-GS framework that groups3D Gaussians that correspond to subbands in the Laplacian Pyrmaids of the inputimages. Our approach enforces coherence within each subband (i.e., group of 3DGaussians) through dedicated regularization, ensuring well-separated frequencycomponents. We extend color values to both positive and negative ranges,allowing higher-frequency layers to add or subtract residual details. Tostabilize optimization, we employ a progressive training scheme that refinesdetails in a coarse-to-fine manner. Beyond interpretability, thisfrequency-aware design unlocks a range of practical benefits. Explicitfrequency separation enables advanced 3D editing and stylization, allowingprecise manipulation of specific frequency bands. It also supports dynamiclevel-of-detail control for progressive rendering, streaming, foveatedrendering and fast geometry interaction. Through extensive experiments, wedemonstrate that our method provides improved control and flexibility foremerging applications in scene editing and interactive rendering. Our code willbe made publicly available.
  </details>

- **[GenFusion: Closing the Loop between Reconstruction and Generation via Videos](http://arxiv.org/abs/2503.21219v1)**  `arXiv:2503.21219`  
  _Sibo Wu, Congrong Xu, Binbin Huang, Andreas Geiger, Anpei Chen_
  <details><summary>Abstract</summary>
  Recently, 3D reconstruction and generation have demonstrated impressive novelview synthesis results, achieving high fidelity and efficiency. However, anotable conditioning gap can be observed between these two fields, e.g.,scalable 3D scene reconstruction often requires densely captured views, whereas3D generation typically relies on a single or no input view, whichsignificantly limits their applications. We found that the source of thisphenomenon lies in the misalignment between 3D constraints and generativepriors. To address this problem, we propose a reconstruction-driven videodiffusion model that learns to condition video frames on artifact-prone RGB-Drenderings. Moreover, we propose a cyclical fusion pipeline that iterativelyadds restoration frames from the generative model to the training set, enablingprogressive expansion and addressing the viewpoint saturation limitations seenin previous reconstruction and generation pipelines. Our evaluation, includingview synthesis from sparse view and masked input, validates the effectivenessof our approach.
  </details>

- **[VoxRep: Enhancing 3D Spatial Understanding in 2D Vision-Language Models via Voxel Representation](http://arxiv.org/abs/2503.21214v1)**  `arXiv:2503.21214`  
  _Alan Dao, Norapat Buppodom_
  <details><summary>Abstract</summary>
  Comprehending 3D environments is vital for intelligent systems in domainslike robotics and autonomous navigation. Voxel grids offer a structuredrepresentation of 3D space, but extracting high-level semantic meaning remainschallenging. This paper proposes a novel approach utilizing a Vision-LanguageModel (VLM) to extract "voxel semantics"-object identity, color, andlocation-from voxel data. Critically, instead of employing complex 3D networks,our method processes the voxel space by systematically slicing it along aprimary axis (e.g., the Z-axis, analogous to CT scan slices). These 2D slicesare then formatted and sequentially fed into the image encoder of a standardVLM. The model learns to aggregate information across slices and correlatespatial patterns with semantic concepts provided by the language component.This slice-based strategy aims to leverage the power of pre-trained 2D VLMs forefficient 3D semantic understanding directly from voxel representations.
  </details>

- **[FakeReasoning: Towards Generalizable Forgery Detection and Reasoning](http://arxiv.org/abs/2503.21210v1)**  `arXiv:2503.21210`  
  _Yueying Gao, Dongliang Chang, Bingyao Yu, Haotian Qin, Lei Chen, Kongming Liang, et al._
  <details><summary>Abstract</summary>
  Accurate and interpretable detection of AI-generated images is essential formitigating risks associated with AI misuse. However, the substantial domain gapamong generative models makes it challenging to develop a generalizable forgerydetection model. Moreover, since every pixel in an AI-generated image issynthesized, traditional saliency-based forgery explanation methods are notwell suited for this task. To address these challenges, we propose modelingAI-generated image detection and explanation as a Forgery Detection andReasoning task (FDR-Task), leveraging vision-language models (VLMs) to provideaccurate detection through structured and reliable reasoning over forgeryattributes. To facilitate this task, we introduce the Multi-Modal ForgeryReasoning dataset (MMFR-Dataset), a large-scale dataset containing 100K imagesacross 10 generative models, with 10 types of forgery reasoning annotations,enabling comprehensive evaluation of FDR-Task. Additionally, we proposeFakeReasoning, a forgery detection and reasoning framework with two keycomponents. First, Forgery-Aligned Contrastive Learning enhances VLMs'understanding of forgery-related semantics through both cross-modal andintra-modal contrastive learning between images and forgery attributereasoning. Second, a Classification Probability Mapper bridges the optimizationgap between forgery detection and language modeling by mapping the outputlogits of VLMs to calibrated binary classification probabilities. Experimentsacross multiple generative models demonstrate that FakeReasoning not onlyachieves robust generalization but also outperforms state-of-the-art methods onboth detection and reasoning tasks.
  </details>

- **[An improved EfficientNetV2 for garbage classification](http://arxiv.org/abs/2503.21208v1)**  `arXiv:2503.21208`  
  _Wenxuan Qiu, Chengxin Xie, Jingui Huang_
  <details><summary>Abstract</summary>
  This paper presents an enhanced waste classification framework based onEfficientNetV2 to address challenges in data acquisition cost, generalization,and real-time performance. We propose a Channel-Efficient Attention(CE-Attention) module that mitigates feature loss during global pooling withoutintroducing dimensional scaling, effectively enhancing critical featureextraction. Additionally, a lightweight multi-scale spatial feature extractionmodule (SAFM) is developed by integrating depthwise separable convolutions,significantly reducing model complexity. Comprehensive data augmentationstrategies are further employed to improve generalization. Experiments on theHuawei Cloud waste classification dataset demonstrate that our method achievesa classification accuracy of 95.4\%, surpassing the baseline by 3.2\% andoutperforming mainstream models. The results validate the effectiveness of ourapproach in balancing accuracy and efficiency for practical wasteclassification scenarios.
  </details>

- **[Leveraging LLMs with Iterative Loop Structure for Enhanced Social Intelligence in Video Question Answering](http://arxiv.org/abs/2503.21190v1)**  `arXiv:2503.21190`  
  _Erika Mori, Yue Qiu, Hirokatsu Kataoka, Yoshimitsu Aoki_
  <details><summary>Abstract</summary>
  Social intelligence, the ability to interpret emotions, intentions, andbehaviors, is essential for effective communication and adaptive responses. Asrobots and AI systems become more prevalent in caregiving, healthcare, andeducation, the demand for AI that can interact naturally with humans grows.However, creating AI that seamlessly integrates multiple modalities, such asvision and speech, remains a challenge. Current video-based methods for socialintelligence rely on general video recognition or emotion recognitiontechniques, often overlook the unique elements inherent in human interactions.To address this, we propose the Looped Video Debating (LVD) framework, whichintegrates Large Language Models (LLMs) with visual information, such as facialexpressions and body movements, to enhance the transparency and reliability ofquestion-answering tasks involving human interaction videos. Our results on theSocial-IQ 2.0 benchmark show that LVD achieves state-of-the-art performancewithout fine-tuning. Furthermore, supplementary human annotations on existingdatasets provide insights into the model's accuracy, guiding futureimprovements in AI-driven social intelligence.
  </details>

- **[DGSUnet: An Improved Unet Model with DINO-Guided SAM2 for Multi-Scale Feature Collaboration](http://arxiv.org/abs/2503.21187v1)**  `arXiv:2503.21187`  
  _Yimin Xu_
  <details><summary>Abstract</summary>
  Despite the significant advancements in general image segmentation achievedby large-scale pre-trained foundation models (such as Meta's Segment Any-thingModel (SAM) series and DINOv2), their performance in specialized fields remainslimited by two critical issues: the excessive training costs due to large modelparameters, and the insufficient ability to represent specific domaincharacteristics. This paper proposes a multi-scale feature collabora-tionframework guided by DINOv2 for SAM2, with core innovations in three aspects:(1) Establishing a feature collaboration mechanism between DINOv2 and SAM2backbones, where high-dimensional semantic features extracted by theself-supervised model guide multi-scale feature fusion; (2) Designinglightweight adapter modules and cross-modal, cross-layer feature fusion unitsto inject cross-domain knowledge while freezing the base model parameters; (3)Constructing a U-shaped network structure based on U-net, which utilizesattention mechanisms to achieve adaptive aggregation decoding ofmulti-granularity features. This framework surpasses existing state-of-the-artmeth-ods in downstream tasks such as camouflage target detection and salientob-ject detection, without requiring costly training processes. It provides atech-nical pathway for efficient deployment of visual image segmentation,demon-strating significant application value in a wide range of downstreamtasks and specialized fields within image segmentation.Project page:https://github.com/CheneyXuYiMin/SAM2DINO-Seg
  </details>

- **[Model as a Game: On Numerical and Spatial Consistency for Generative Games](http://arxiv.org/abs/2503.21172v1)**  `arXiv:2503.21172`  
  _Jingye Chen, Yuzhong Zhao, Yupan Huang, Lei Cui, Li Dong, Tengchao Lv, et al._
  <details><summary>Abstract</summary>
  Recent advances in generative models have significantly impacted gamegeneration. However, despite producing high-quality graphics and adequatelyreceiving player input, existing models often fail to maintain fundamental gameproperties such as numerical and spatial consistency. Numerical consistencyensures gameplay mechanics correctly reflect score changes and otherquantitative elements, while spatial consistency prevents jarring scenetransitions, providing seamless player experiences. In this paper, we revisitthe paradigm of generative games to explore what truly constitutes a Model as aGame (MaaG) with a well-developed mechanism. We begin with an empirical studyon ``Traveler'', a 2D game created by an LLM featuring minimalist rules yetchallenging generative models in maintaining consistency. Based on the DiTarchitecture, we design two specialized modules: (1) a numerical module thatintegrates a LogicNet to determine event triggers, with calculations processedexternally as conditions for image generation; and (2) a spatial module thatmaintains a map of explored areas, retrieving location-specific informationduring generation and linking new observations to ensure continuity.Experiments across three games demonstrate that our integrated modulessignificantly enhance performance on consistency metrics compared to baselines,while incurring minimal time overhead during inference.
  </details>

- **[VADMamba: Exploring State Space Models for Fast Video Anomaly Detection](http://arxiv.org/abs/2503.21169v1)**  `arXiv:2503.21169`  
  _Jiahao Lyu, Minghua Zhao, Jing Hu, Xuewen Huang, Yifei Chen, Shuangli Du_
  <details><summary>Abstract</summary>
  Video anomaly detection (VAD) methods are mostly CNN-based orTransformer-based, achieving impressive results, but the focus on detectionaccuracy often comes at the expense of inference speed. The emergence of statespace models in computer vision, exemplified by the Mamba model, demonstratesimproved computational efficiency through selective scans and showcases thegreat potential for long-range modeling. Our study pioneers the application ofMamba to VAD, dubbed VADMamba, which is based on multi-task learning for frameprediction and optical flow reconstruction. Specifically, we propose theVQ-Mamba Unet (VQ-MaU) framework, which incorporates a Vector Quantization (VQ)layer and Mamba-based Non-negative Visual State Space (NVSS) block.Furthermore, two individual VQ-MaU networks separately predict frames andreconstruct corresponding optical flows, further boosting accuracy through aclip-level fusion evaluation strategy. Experimental results validate theefficacy of the proposed VADMamba across three benchmark datasets,demonstrating superior performance in inference speed compared to previouswork. Code is available at https://github.com/jLooo/VADMamba.
  </details>

- **[Adversarial Wear and Tear: Exploiting Natural Damage for Generating Physical-World Adversarial Examples](http://arxiv.org/abs/2503.21164v1)**  `arXiv:2503.21164`  
  _Samra Irshad, Seungkyu Lee, Nassir Navab, Hong Joo Lee, Seong Tae Kim_
  <details><summary>Abstract</summary>
  The presence of adversarial examples in the physical world poses significantchallenges to the deployment of Deep Neural Networks in safety-criticalapplications such as autonomous driving. Most existing methods for craftingphysical-world adversarial examples are ad-hoc, relying on temporarymodifications like shadows, laser beams, or stickers that are tailored tospecific scenarios. In this paper, we introduce a new class of physical-worldadversarial examples, AdvWT, which draws inspiration from the naturallyoccurring phenomenon of `wear and tear', an inherent property of physicalobjects. Unlike manually crafted perturbations, `wear and tear' emergesorganically over time due to environmental degradation, as seen in the gradualdeterioration of outdoor signboards. To achieve this, AdvWT follows a two-stepapproach. First, a GAN-based, unsupervised image-to-image translation networkis employed to model these naturally occurring damages, particularly in thecontext of outdoor signboards. The translation network encodes thecharacteristics of damaged signs into a latent `damage style code'. In thesecond step, we introduce adversarial perturbations into the style code,strategically optimizing its transformation process. This manipulation subtlyalters the damage style representation, guiding the network to generateadversarial images where the appearance of damages remains perceptuallyrealistic, while simultaneously ensuring their effectiveness in misleadingneural networks. Through comprehensive experiments on two traffic signdatasets, we show that AdvWT effectively misleads DNNs in both digital andphysical domains. AdvWT achieves an effective attack success rate, greaterrobustness, and a more natural appearance compared to existing physical-worldadversarial examples. Additionally, integrating AdvWT into training enhances amodel's generalizability to real-world damaged signs.
  </details>

- **[Integrating Travel Behavior Forecasting and Generative Modeling for Predicting Future Urban Mobility and Spatial Transformations](http://arxiv.org/abs/2503.21158v1)**  `arXiv:2503.21158`  
  _Eugene Denteh, Andrews Danyo, Joshua Kofi Asamoah, Blessing Agyei Kyem, Twitchell Addai, Armstrong Aboah_
  <details><summary>Abstract</summary>
  Transportation planning plays a critical role in shaping urban development,economic mobility, and infrastructure sustainability. However, traditionalplanning methods often struggle to accurately predict long-term urban growthand transportation demands. This may sometimes result in infrastructuredemolition to make room for current transportation planning demands. This studyintegrates a Temporal Fusion Transformer to predict travel patterns fromdemographic data with a Generative Adversarial Network to predict future urbansettings through satellite imagery. The framework achieved a 0.76 R-squarescore in travel behavior prediction and generated high-fidelity satelliteimages with a Structural Similarity Index of 0.81. The results demonstrate thatintegrating predictive analytics and spatial visualization can significantlyimprove the decision-making process, fostering more sustainable and efficienturban development. This research highlights the importance of data-drivenmethodologies in modern transportation planning and presents a step towardoptimizing infrastructure placement, capacity, and long-term viability.
  </details>

- **[The Devil is in Low-Level Features for Cross-Domain Few-Shot Segmentation](http://arxiv.org/abs/2503.21150v1)**  `arXiv:2503.21150`  
  _Yuhan Liu, Yixiong Zou, Yuhua Li, Ruixuan Li_
  <details><summary>Abstract</summary>
  Cross-Domain Few-Shot Segmentation (CDFSS) is proposed to transfer thepixel-level segmentation capabilities learned from large-scale source-domaindatasets to downstream target-domain datasets, with only a few annotated imagesper class. In this paper, we focus on a well-observed but unresolved phenomenonin CDFSS: for target domains, particularly those distant from the sourcedomain, segmentation performance peaks at the very early epochs, and declinessharply as the source-domain training proceeds. We delve into this phenomenonfor an interpretation: low-level features are vulnerable to domain shifts,leading to sharper loss landscapes during the source-domain training, which isthe devil of CDFSS. Based on this phenomenon and interpretation, we furtherpropose a method that includes two plug-and-play modules: one to flatten theloss landscapes for low-level features during source-domain training as a novelsharpness-aware minimization method, and the other to directly supplementtarget-domain information to the model during target-domain testing bylow-level-based calibration. Extensive experiments on four target datasetsvalidate our rationale and demonstrate that our method surpasses thestate-of-the-art method in CDFSS signifcantly by 3.71% and 5.34% average MIoUin 1-shot and 5-shot scenarios, respectively.
  </details>

- **[ChatAnyone: Stylized Real-time Portrait Video Generation with Hierarchical Motion Diffusion Model](http://arxiv.org/abs/2503.21144v1)**  `arXiv:2503.21144`  
  _Jinwei Qi, Chaonan Ji, Sheng Xu, Peng Zhang, Bang Zhang, Liefeng Bo_
  <details><summary>Abstract</summary>
  Real-time interactive video-chat portraits have been increasingly recognizedas the future trend, particularly due to the remarkable progress made in textand voice chat technologies. However, existing methods primarily focus onreal-time generation of head movements, but struggle to produce synchronizedbody motions that match these head actions. Additionally, achievingfine-grained control over the speaking style and nuances of facial expressionsremains a challenge. To address these limitations, we introduce a novelframework for stylized real-time portrait video generation, enabling expressiveand flexible video chat that extends from talking head to upper-bodyinteraction. Our approach consists of the following two stages. The first stageinvolves efficient hierarchical motion diffusion models, that take bothexplicit and implicit motion representations into account based on audioinputs, which can generate a diverse range of facial expressions with stylisticcontrol and synchronization between head and body movements. The second stageaims to generate portrait video featuring upper-body movements, including handgestures. We inject explicit hand control signals into the generator to producemore detailed hand movements, and further perform face refinement to enhancethe overall realism and expressiveness of the portrait video. Additionally, ourapproach supports efficient and continuous generation of upper-body portraitvideo in maximum 512 * 768 resolution at up to 30fps on 4090 GPU, supportinginteractive video-chat in real-time. Experimental results demonstrate thecapability of our approach to produce portrait videos with rich expressivenessand natural upper-body movements.
  </details>

- **[Recurrent Feature Mining and Keypoint Mixup Padding for Category-Agnostic Pose Estimation](http://arxiv.org/abs/2503.21140v1)**  `arXiv:2503.21140`  
  _Junjie Chen, Weilong Chen, Yifan Zuo, Yuming Fang_
  <details><summary>Abstract</summary>
  Category-agnostic pose estimation aims to locate keypoints on query imagesaccording to a few annotated support images for arbitrary novel classes.Existing methods generally extract support features via heatmap pooling, andobtain interacted features from support and query via cross-attention. Hence,these works neglect to mine fine-grained and structure-aware (FGSA) featuresfrom both support and query images, which are crucial for pixel-level keypointlocalization. To this end, we propose a novel yet concise framework, whichrecurrently mines FGSA features from both support and query images.Specifically, we design a FGSA mining module based on deformable attentionmechanism. On the one hand, we mine fine-grained features by applyingdeformable attention head over multi-scale feature maps. On the other hand, wemine structure-aware features by offsetting the reference points of keypointsto their linked keypoints. By means of above module, we recurrently mine FGSAfeatures from support and query images, and thus obtain better support featuresand query estimations. In addition, we propose to use mixup keypoints to padvarious classes to a unified keypoint number, which could provide richersupervision than the zero padding used in existing works. We conduct extensiveexperiments and in-depth studies on large-scale MP-100 dataset, and outperformSOTA method dramatically (+3.2\%PCK@0.05). Code is avaiable athttps://github.com/chenbys/FMMP.
  </details>

- **[Omni-AD: Learning to Reconstruct Global and Local Features for Multi-class Anomaly Detection](http://arxiv.org/abs/2503.21125v1)**  `arXiv:2503.21125`  
  _Jiajie Quan, Ao Tong, Yuxuan Cai, Xinwei He, Yulong Wang, Yang Zhou_
  <details><summary>Abstract</summary>
  In multi-class unsupervised anomaly detection(MUAD), reconstruction-basedmethods learn to map input images to normal patterns to identify anomalouspixels. However, this strategy easily falls into the well-known "learningshortcut" issue when decoders fail to capture normal patterns and reconstructboth normal and abnormal samples naively. To address that, we propose to learnthe input features in global and local manners, forcing the network to memorizethe normal patterns more comprehensively. Specifically, we design a two-branchdecoder block, named Omni-block. One branch corresponds to global featurelearning, where we serialize two self-attention blocks but replace the queryand (key, value) with learnable tokens, respectively, thus capturing globalfeatures of normal patterns concisely and thoroughly. The local branchcomprises depth-separable convolutions, whose locality enables effective andefficient learning of local features for normal patterns. By stackingOmni-blocks, we build a framework, Omni-AD, to learn normal patterns ofdifferent granularity and reconstruct them progressively. Comprehensiveexperiments on public anomaly detection benchmarks show that our methodoutperforms state-of-the-art approaches in MUAD. Code is available athttps://github.com/easyoo/Omni-AD.git.
  </details>

- **[AdaMHF: Adaptive Multimodal Hierarchical Fusion for Survival Prediction](http://arxiv.org/abs/2503.21124v1)**  `arXiv:2503.21124`  
  _Shuaiyu Zhang, Xun Lin, Rongxiang Zhang, Yu Bai, Yong Xu, Tao Tan, et al._
  <details><summary>Abstract</summary>
  The integration of pathologic images and genomic data for survival analysishas gained increasing attention with advances in multimodal learning. However,current methods often ignore biological characteristics, such as heterogeneityand sparsity, both within and across modalities, ultimately limiting theiradaptability to clinical practice. To address these challenges, we proposeAdaMHF: Adaptive Multimodal Hierarchical Fusion, a framework designed forefficient, comprehensive, and tailored feature extraction and fusion. AdaMHF isspecifically adapted to the uniqueness of medical data, enabling accuratepredictions with minimal resource consumption, even under challenging scenarioswith missing modalities. Initially, AdaMHF employs an experts expansion andresidual structure to activate specialized experts for extracting heterogeneousand sparse features. Extracted tokens undergo refinement via selection andaggregation, reducing the weight of non-dominant features while preservingcomprehensive information. Subsequently, the encoded features arehierarchically fused, allowing multi-grained interactions across modalities tobe captured. Furthermore, we introduce a survival prediction benchmark designedto resolve scenarios with missing modalities, mirroring real-world clinicalconditions. Extensive experiments on TCGA datasets demonstrate that AdaMHFsurpasses current state-of-the-art (SOTA) methods, showcasing exceptionalperformance in both complete and incomplete modality settings.
  </details>

- **[One Snapshot is All You Need: A Generalized Method for mmWave Signal Generation](http://arxiv.org/abs/2503.21122v1)**  `arXiv:2503.21122`  
  _Teng Huang, Han Ding, Wenxin Sun, Cui Zhao, Ge Wang, Fei Wang, et al._
  <details><summary>Abstract</summary>
  Wireless sensing systems, particularly those using mmWave technology, offerdistinct advantages over traditional vision-based approaches, such as enhancedprivacy and effectiveness in poor lighting conditions. These systems,leveraging FMCW signals, have shown success in human-centric applications likelocalization, gesture recognition, and so on. However, comprehensive mmWavedatasets for diverse applications are scarce, often constrained bypre-processed signatures (e.g., point clouds or RA heatmaps) and inconsistentannotation formats. To overcome these limitations, we propose mmGen, a noveland generalized framework tailored for full-scene mmWave signal generation. Byconstructing physical signal transmission models, mmGen synthesizeshuman-reflected and environment-reflected mmWave signals from the constructed3D meshes. Additionally, we incorporate methods to account for materialproperties, antenna gains, and multipath reflections, enhancing the realism ofthe synthesized signals. We conduct extensive experiments using a prototypesystem with commercial mmWave devices and Kinect sensors. The results show thatthe average similarity of Range-Angle and micro-Doppler signatures between thesynthesized and real-captured signals across three different environmentsexceeds 0.91 and 0.89, respectively, demonstrating the effectiveness andpractical applicability of mmGen.
  </details>

- **[StyledStreets: Multi-style Street Simulator with Spatial and Temporal Consistency](http://arxiv.org/abs/2503.21104v1)**  `arXiv:2503.21104`  
  _Yuyin Chen, Yida Wang, Xueyang Zhang, Kun Zhan, Peng Jia, Yifei Zhan, et al._
  <details><summary>Abstract</summary>
  Urban scene reconstruction requires modeling both static infrastructure anddynamic elements while supporting diverse environmental conditions. We present\textbf{StyledStreets}, a multi-style street simulator that achievesinstruction-driven scene editing with guaranteed spatial and temporalconsistency. Building on a state-of-the-art Gaussian Splatting framework forstreet scenarios enhanced by our proposed pose optimization and multi-viewtraining, our method enables photorealistic style transfers across seasons,weather conditions, and camera setups through three key innovations: First, ahybrid embedding scheme disentangles persistent scene geometry from transientstyle attributes, allowing realistic environmental edits while preservingstructural integrity. Second, uncertainty-aware rendering mitigates supervisionnoise from diffusion priors, enabling robust training across extreme stylevariations. Third, a unified parametric model prevents geometric drift throughregularized updates, maintaining multi-view consistency across sevenvehicle-mounted cameras.  Our framework preserves the original scene's motion patterns and geometricrelationships. Qualitative results demonstrate plausible transitions betweendiverse conditions (snow, sandstorm, night), while quantitative evaluationsshow state-of-the-art geometric accuracy under style transfers. The approachestablishes new capabilities for urban simulation, with applications inautonomous vehicle testing and augmented reality systems requiring reliableenvironmental consistency. Codes will be publicly available upon publication.
  </details>

- **[Learning Class Prototypes for Unified Sparse Supervised 3D Object Detection](http://arxiv.org/abs/2503.21099v1)**  `arXiv:2503.21099`  
  _Yun Zhu, Le Hui, Hang Yang, Jianjun Qian, Jin Xie, Jian Yang_
  <details><summary>Abstract</summary>
  Both indoor and outdoor scene perceptions are essential for embodiedintelligence. However, current sparse supervised 3D object detection methodsfocus solely on outdoor scenes without considering indoor settings. To thisend, we propose a unified sparse supervised 3D object detection method for bothindoor and outdoor scenes through learning class prototypes to effectivelyutilize unlabeled objects. Specifically, we first propose a prototype-basedobject mining module that converts the unlabeled object mining into a matchingproblem between class prototypes and unlabeled features. By using optimaltransport matching results, we assign prototype labels to high-confidencefeatures, thereby achieving the mining of unlabeled objects. We then present amulti-label cooperative refinement module to effectively recover misseddetections through pseudo label quality control and prototype labelcooperation. Experiments show that our method achieves state-of-the-artperformance under the one object per scene sparse supervised setting acrossindoor and outdoor datasets. With only one labeled object per scene, our methodachieves about 78%, 90%, and 96% performance compared to the fully superviseddetector on ScanNet V2, SUN RGB-D, and KITTI, respectively, highlighting thescalability of our method. Code is available athttps://github.com/zyrant/CPDet3D.
  </details>

- **[Can Video Diffusion Model Reconstruct 4D Geometry?](http://arxiv.org/abs/2503.21082v1)**  `arXiv:2503.21082`  
  _Jinjie Mai, Wenxuan Zhu, Haozhe Liu, Bing Li, Cheng Zheng, J√ºrgen Schmidhuber, et al._
  <details><summary>Abstract</summary>
  Reconstructing dynamic 3D scenes (i.e., 4D geometry) from monocular video isan important yet challenging problem. Conventional multiview geometry-basedapproaches often struggle with dynamic motion, whereas recent learning-basedmethods either require specialized 4D representation or sophisticatedoptimization. In this paper, we present Sora3R, a novel framework that tapsinto the rich spatiotemporal priors of large-scale video diffusion models todirectly infer 4D pointmaps from casual videos. Sora3R follows a two-stagepipeline: (1) we adapt a pointmap VAE from a pretrained video VAE, ensuringcompatibility between the geometry and video latent spaces; (2) we finetune adiffusion backbone in combined video and pointmap latent space to generatecoherent 4D pointmaps for every frame. Sora3R operates in a fully feedforwardmanner, requiring no external modules (e.g., depth, optical flow, orsegmentation) or iterative global alignment. Extensive experiments demonstratethat Sora3R reliably recovers both camera poses and detailed scene geometry,achieving performance on par with state-of-the-art methods for dynamic 4Dreconstruction across diverse scenarios.
  </details>

- **[KAC: Kolmogorov-Arnold Classifier for Continual Learning](http://arxiv.org/abs/2503.21076v1)**  `arXiv:2503.21076`  
  _Yusong Hu, Zichen Liang, Fei Yang, Qibin Hou, Xialei Liu, Ming-Ming Cheng_
  <details><summary>Abstract</summary>
  Continual learning requires models to train continuously across consecutivetasks without forgetting. Most existing methods utilize linear classifiers,which struggle to maintain a stable classification space while learning newtasks. Inspired by the success of Kolmogorov-Arnold Networks (KAN) inpreserving learning stability during simple continual regression tasks, we setout to explore their potential in more complex continual learning scenarios. Inthis paper, we introduce the Kolmogorov-Arnold Classifier (KAC), a novelclassifier developed for continual learning based on the KAN structure. Wedelve into the impact of KAN's spline functions and introduce Radial BasisFunctions (RBF) for improved compatibility with continual learning. We replacelinear classifiers with KAC in several recent approaches and conductexperiments across various continual learning benchmarks, all of whichdemonstrate performance improvements, highlighting the effectiveness androbustness of KAC in continual learning. The code is available athttps://github.com/Ethanhuhuhu/KAC.
  </details>

- **[Rerouting Connection: Hybrid Computer Vision Analysis Reveals Visual Similarity Between Indus and Tibetan-Yi Corridor Writing Systems](http://arxiv.org/abs/2503.21074v1)**  `arXiv:2503.21074`  
  _Ooha Lakkadi Reddy_
  <details><summary>Abstract</summary>
  This thesis employs a hybrid CNN-Transformer architecture, in conjunctionwith a detailed anthropological framework, to investigate potential historicalconnections between the visual morphology of the Indus Valley script andpictographic systems of the Tibetan-Yi Corridor. Through an ensemblemethodology of three target scripts across 15 independently trained models, wedemonstrate that Tibetan-Yi Corridor scripts exhibit approximately six-foldhigher visual similarity to the Indus script (61.7%-63.5%) than to the BronzeAge Proto-Cuneiform (10.2%-10.9%) or Proto-Elamite (7.6%-8.7%) systems.Additionally and contrarily to our current understanding of the networks of theIndus Valley Civilization, the Indus script unexpectedly maps closer toTibetan-Yi Corridor scripts, with a mean cosine similarity of 0.629, than tothe aforementioned contemporaneous West Asian signaries, both of which recordedmean cosine similarities of 0.104 and 0.080 despite their close geographicproximity and evident trade relations. Across various dimensionality reductionpractices and clustering methodologies, the Indus script consistently clustersclosest to Tibetan-Yi Corridor scripts. Our computational results align withqualitative observations of specific pictorial parallels in numeral systems,gender markers, and key iconographic elements; this is further supported byarchaeological evidence of sustained contact networks along the ancientShu-Shendu road in tandem with the Indus Valley Civilization's decline,providing a plausible transmission pathway. While alternative explanationscannot be ruled out, the specificity and consistency of observed similaritieschallenge conventional narratives of isolated script development and suggestmore complex ancient cultural transmission networks between South and East Asiathan previously recognized.
  </details>

- **[HSLiNets: Evaluating Band Ordering Strategies in Hyperspectral and LiDAR Fusion](http://arxiv.org/abs/2503.21072v1)**  `arXiv:2503.21072`  
  _Judy X Yang, Jing Wang, Zhuanfeng, Li, Chenhong Sui Zekun Long, Jun Zhou_
  <details><summary>Abstract</summary>
  The integration of hyperspectral imaging (HSI) and Light Detection andRanging (LiDAR) data provides complementary spectral and spatial informationfor remote sensing applications. While previous studies have explored the roleof band selection and grouping in HSI classification, little attention has beengiven to how the spectral sequence or band order affects classificationoutcomes when fused with LiDAR. In this work, we systematically investigate theinfluence of band order on HSI-LiDAR fusion performance. Through extensiveexperiments, we demonstrate that band order significantly impactsclassification accuracy, revealing a previously overlooked factor infusion-based models. Motivated by this observation, we propose a novel fusionarchitecture that not only integrates HSI and LiDAR data but also learns frommultiple band order configurations. The proposed method enhances featurerepresentation by adaptively fusing different spectral sequences, leading toimproved classification accuracy. Experimental results on the Houston 2013 andTrento datasets show that our approach outperforms state-of-the-art fusionmodels. Data and code are available at https://github.com/Judyxyang/HSLiNets.
  </details>

- **[Efficient Multi-Instance Generation with Janus-Pro-Dirven Prompt Parsing](http://arxiv.org/abs/2503.21069v1)**  `arXiv:2503.21069`  
  _Fan Qi, Yu Duan, Changsheng Xu_
  <details><summary>Abstract</summary>
  Recent advances in text-guided diffusion models have revolutionizedconditional image generation, yet they struggle to synthesize complex sceneswith multiple objects due to imprecise spatial grounding and limitedscalability. We address these challenges through two key modules: 1)Janus-Pro-driven Prompt Parsing, a prompt-layout parsing module that bridgestext understanding and layout generation via a compact 1B-parameterarchitecture, and 2) MIGLoRA, a parameter-efficient plug-in integratingLow-Rank Adaptation (LoRA) into UNet (SD1.5) and DiT (SD3) backbones. MIGLoRAis capable of preserving the base model's parameters and ensuring plug-and-playadaptability, minimizing architectural intrusion while enabling efficientfine-tuning. To support a comprehensive evaluation, we create DescripBox andDescripBox-1024, benchmarks that span diverse scenes and resolutions. Theproposed method achieves state-of-the-art performance on COCO and LVISbenchmarks while maintaining parameter efficiency, demonstrating superiorlayout fidelity and scalability for open-world synthesis.
  </details>

- **[Neural Architecture Search by Learning a Hierarchical Search Space](http://arxiv.org/abs/2503.21061v1)**  `arXiv:2503.21061`  
  _Mehraveh Javan Roshtkhari, Matthew Toews, Marco Pedersoli_
  <details><summary>Abstract</summary>
  Monte-Carlo Tree Search (MCTS) is a powerful tool for many non-differentiablesearch related problems such as adversarial games. However, the performance ofsuch approach highly depends on the order of the nodes that are considered ateach branching of the tree. If the first branches cannot distinguish betweenpromising and deceiving configurations for the final task, the efficiency ofthe search is exponentially reduced. In Neural Architecture Search (NAS), asonly the final architecture matters, the visiting order of the branching can beoptimized to improve learning. In this paper, we study the application of MCTSto NAS for image classification. We analyze several sampling methods andbranching alternatives for MCTS and propose to learn the branching byhierarchical clustering of architectures based on their similarity. Thesimilarity is measured by the pairwise distance of output vectors ofarchitectures. Extensive experiments on two challenging benchmarks on CIFAR10and ImageNet show that MCTS, if provided with a good branching hierarchy, canyield promising solutions more efficiently than other approaches for NASproblems.
  </details>

- **[Online Reasoning Video Segmentation with Just-in-Time Digital Twins](http://arxiv.org/abs/2503.21056v1)**  `arXiv:2503.21056`  
  _Yiqing Shen, Bohan Liu, Chenjia Li, Lalithkumar Seenivasan, Mathias Unberath_
  <details><summary>Abstract</summary>
  Reasoning segmentation (RS) aims to identify and segment objects of interestbased on implicit text queries. As such, RS is a catalyst for embodied AIagents, enabling them to interpret high-level commands without requiringexplicit step-by-step guidance. However, current RS approaches rely heavily onthe visual perception capabilities of multimodal large language models (LLMs),leading to several major limitations. First, they struggle with queries thatrequire multiple steps of reasoning or those that involve complexspatial/temporal relationships. Second, they necessitate LLM fine-tuning, whichmay require frequent updates to maintain compatibility with contemporary LLMsand may increase risks of catastrophic forgetting during fine-tuning. Finally,being primarily designed for static images or offline video processing, theyscale poorly to online video data. To address these limitations, we propose anagent framework that disentangles perception and reasoning for online video RSwithout LLM fine-tuning. Our innovation is the introduction of a just-in-timedigital twin concept, where -- given an implicit query -- a LLM plans theconstruction of a low-level scene representation from high-level video usingspecialist vision models. We refer to this approach to creating a digital twinas "just-in-time" because the LLM planner will anticipate the need for specificinformation and only request this limited subset instead of always evaluatingevery specialist model. The LLM then performs reasoning on this digital twinrepresentation to identify target objects. To evaluate our approach, weintroduce a new comprehensive video reasoning segmentation benchmark comprising200 videos with 895 implicit text queries. The benchmark spans three reasoningcategories (semantic, spatial, and temporal) with three different reasoningchain complexity.
  </details>

- **[What Changed and What Could Have Changed? State-Change Counterfactuals for Procedure-Aware Video Representation Learning](http://arxiv.org/abs/2503.21055v1)**  `arXiv:2503.21055`  
  _Chi-Hsi Kung, Frangil Ramirez, Juhyung Ha, Yi-Ting Chen, David Crandall, Yi-Hsuan Tsai_
  <details><summary>Abstract</summary>
  Understanding a procedural activity requires modeling both how action stepstransform the scene, and how evolving scene transformations can influence thesequence of action steps, even those that are accidental or erroneous. Existingwork has studied procedure-aware video representations by proposing novelapproaches such as modeling the temporal order of actions and has notexplicitly learned the state changes (scene transformations). In this work, westudy procedure-aware video representation learning by incorporatingstate-change descriptions generated by Large Language Models (LLMs) assupervision signals for video encoders. Moreover, we generate state-changecounterfactuals that simulate hypothesized failure outcomes, allowing models tolearn by imagining the unseen ``What if'' scenarios. This counterfactualreasoning facilitates the model's ability to understand the cause and effect ofeach step in an activity. To verify the procedure awareness of our model, weconduct extensive experiments on procedure-aware tasks, including temporalaction segmentation and error detection. Our results demonstrate theeffectiveness of the proposed state-change descriptions and theircounterfactuals and achieve significant improvements on multiple tasks. We willmake our source code and data publicly available soon.
  </details>

- **[Reason-RFT: Reinforcement Fine-Tuning for Visual Reasoning](http://arxiv.org/abs/2503.20752v2)**  `arXiv:2503.20752`  
  _Huajie Tan, Yuheng Ji, Xiaoshuai Hao, Minglan Lin, Pengwei Wang, Zhongyuan Wang, et al._
  <details><summary>Abstract</summary>
  Visual reasoning abilities play a crucial role in understanding complexmultimodal data, advancing both domain-specific applications and artificialgeneral intelligence (AGI). Existing methods improve VLM reasoning viaChain-of-Thought (CoT) supervised fine-tuning, using meticulously annotatedtraining data to enhance visual reasoning capabilities. However, this trainingparadigm may lead to overfitting and cognitive rigidity, restricting themodel's ability to transfer visual reasoning skills across domains and limitingits real-world applicability. To address these limitations, we proposeReason-RFT, a novel reinforcement fine-tuning framework that significantlyenhances generalization capabilities in visual reasoning tasks. Reason-RFTintroduces a two-phase training framework for visual reasoning: (1) SupervisedFine-Tuning (SFT) with curated Chain-of-Thought (CoT) data activates thereasoning potential of Vision-Language Models (VLMs), followed by (2) GroupRelative Policy Optimization (GRPO)-based reinforcement learning that generatesmultiple reasoning-response pairs, significantly enhancing generalization invisual reasoning tasks. To evaluate Reason-RFT's visual reasoning capabilities,we reconstructed a comprehensive dataset spanning visual counting, structureperception, and spatial transformation. Experimental results demonstrateReasoning-RFT's three key advantages: (1) Performance Enhancement: achievingstate-of-the-art results across multiple tasks, outperforming most mainstreamopen-source and proprietary models; (2) Generalization Superiority:consistently maintaining robust performance across diverse tasks and domains,outperforming alternative training paradigms; (3) Data Efficiency: excelling infew-shot learning scenarios while surpassing full-dataset SFT baselines.Project website: https://tanhuajie.github.io/ReasonRFT
  </details>

- **[Flip Learning: Weakly Supervised Erase to Segment Nodules in Breast Ultrasound](http://arxiv.org/abs/2503.20685v2)**  `arXiv:2503.20685`  
  _Yuhao Huang, Ao Chang, Haoran Dou, Xing Tao, Xinrui Zhou, Yan Cao, et al._
  <details><summary>Abstract</summary>
  Accurate segmentation of nodules in both 2D breast ultrasound (BUS) and 3Dautomated breast ultrasound (ABUS) is crucial for clinical diagnosis andtreatment planning. Therefore, developing an automated system for nodulesegmentation can enhance user independence and expedite clinical analysis.Unlike fully-supervised learning, weakly-supervised segmentation (WSS) canstreamline the laborious and intricate annotation process. However, current WSSmethods face challenges in achieving precise nodule segmentation, as many ofthem depend on inaccurate activation maps or inefficient pseudo-mask generationalgorithms. In this study, we introduce a novel multi-agent reinforcementlearning-based WSS framework called Flip Learning, which relies solely on 2D/3Dboxes for accurate segmentation. Specifically, multiple agents are employed toerase the target from the box to facilitate classification tag flipping, withthe erased region serving as the predicted segmentation mask. The keycontributions of this research are as follows: (1) Adoption of asuperpixel/supervoxel-based approach to encode the standardized environment,capturing boundary priors and expediting the learning process. (2) Introductionof three meticulously designed rewards, comprising a classification scorereward and two intensity distribution rewards, to steer the agents' erasingprocess precisely, thereby avoiding both under- and over-segmentation. (3)Implementation of a progressive curriculum learning strategy to enable agentsto interact with the environment in a progressively challenging manner, therebyenhancing learning efficiency. Extensively validated on the large in-house BUSand ABUS datasets, our Flip Learning method outperforms state-of-the-art WSSmethods and foundation models, and achieves comparable performance asfully-supervised learning algorithms.
  </details>

- **[Mitigating Low-Level Visual Hallucinations Requires Self-Awareness: Database, Model and Training Strategy](http://arxiv.org/abs/2503.20673v2)**  `arXiv:2503.20673`  
  _Yinan Sun, Xiongkuo Min, Zicheng Zhang, Yixuan Gao, Yuqin Cao, Guangtao Zhai_
  <details><summary>Abstract</summary>
  The rapid development of multimodal large language models has resulted inremarkable advancements in visual perception and understanding, consolidatingseveral tasks into a single visual question-answering framework. However, thesemodels are prone to hallucinations, which limit their reliability as artificialintelligence systems. While this issue is extensively researched in naturallanguage processing and image captioning, there remains a lack of investigationof hallucinations in Low-level Visual Perception and Understanding (HLPU),especially in the context of image quality assessment tasks. We consider thatthese hallucinations arise from an absence of clear self-awareness within themodels. To address this issue, we first introduce the HLPU instructiondatabase, the first instruction database specifically focused on hallucinationsin low-level vision tasks. This database contains approximately 200Kquestion-answer pairs and comprises four subsets, each covering different typesof instructions. Subsequently, we propose the Self-Awareness FailureElimination (SAFEQA) model, which utilizes image features, salient regionfeatures and quality features to improve the perception and comprehensionabilities of the model in low-level vision tasks. Furthermore, we propose theEnhancing Self-Awareness Preference Optimization (ESA-PO) framework to increasethe model's awareness of knowledge boundaries, thereby mitigating the incidenceof hallucination. Finally, we conduct comprehensive experiments on low-levelvision tasks, with the results demonstrating that our proposed methodsignificantly enhances self-awareness of the model in these tasks and reduceshallucinations. Notably, our proposed method improves both accuracy andself-awareness of the proposed model and outperforms close-source models interms of various evaluation metrics.
  </details>

- **[Imitating Radiological Scrolling: A Global-Local Attention Model for 3D Chest CT Volumes Multi-Label Anomaly Classification](http://arxiv.org/abs/2503.20652v2)**  `arXiv:2503.20652`  
  _Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel_
  <details><summary>Abstract</summary>
  The rapid increase in the number of Computed Tomography (CT) scanexaminations has created an urgent need for automated tools, such as organsegmentation, anomaly classification, and report generation, to assistradiologists with their growing workload. Multi-label classification ofThree-Dimensional (3D) CT scans is a challenging task due to the volumetricnature of the data and the variety of anomalies to be detected. Existing deeplearning methods based on Convolutional Neural Networks (CNNs) struggle tocapture long-range dependencies effectively, while Vision Transformers requireextensive pre-training, posing challenges for practical use. Additionally,these existing methods do not explicitly model the radiologist's navigationalbehavior while scrolling through CT scan slices, which requires both globalcontext understanding and local detail awareness. In this study, we presentCT-Scroll, a novel global-local attention model specifically designed toemulate the scrolling behavior of radiologists during the analysis of 3D CTscans. Our approach is evaluated on two public datasets, demonstrating itsefficacy through comprehensive experiments and an ablation study thathighlights the contribution of each model component.
  </details>

- **[MAR-3D: Progressive Masked Auto-regressor for High-Resolution 3D Generation](http://arxiv.org/abs/2503.20519v2)**  `arXiv:2503.20519`  
  _Jinnan Chen, Lingting Zhu, Zeyu Hu, Shengju Qian, Yugang Chen, Xin Wang, et al._
  <details><summary>Abstract</summary>
  Recent advances in auto-regressive transformers have revolutionizedgenerative modeling across different domains, from language processing tovisual generation, demonstrating remarkable capabilities. However, applyingthese advances to 3D generation presents three key challenges: the unorderednature of 3D data conflicts with sequential next-token prediction paradigm,conventional vector quantization approaches incur substantial compression losswhen applied to 3D meshes, and the lack of efficient scaling strategies forhigher resolution latent prediction. To address these challenges, we introduceMAR-3D, which integrates a pyramid variational autoencoder with a cascadedmasked auto-regressive transformer (Cascaded MAR) for progressive latentupscaling in the continuous space. Our architecture employs random maskingduring training and auto-regressive denoising in random order during inference,naturally accommodating the unordered property of 3D latent tokens.Additionally, we propose a cascaded training strategy with conditionaugmentation that enables efficiently up-scale the latent token resolution withfast convergence. Extensive experiments demonstrate that MAR-3D not onlyachieves superior performance and generalization capabilities compared toexisting methods but also exhibits enhanced scaling capabilities compared tojoint distribution modeling approaches (e.g., diffusion transformers).
  </details>

- **[Consistency Trajectory Matching for One-Step Generative Super-Resolution](http://arxiv.org/abs/2503.20349v2)**  `arXiv:2503.20349`  
  _Weiyi You, Mingyang Zhang, Leheng Zhang, Xingyu Zhou, Kexuan Shi, Shuhang Gu_
  <details><summary>Abstract</summary>
  Current diffusion-based super-resolution (SR) approaches achieve commendableperformance at the cost of high inference overhead. Therefore, distillationtechniques are utilized to accelerate the multi-step teacher model intoone-step student model. Nevertheless, these methods significantly raisetraining costs and constrain the performance of the student model by theteacher model. To overcome these tough challenges, we propose ConsistencyTrajectory Matching for Super-Resolution (CTMSR), a distillation-free strategythat is able to generate photo-realistic SR results in one step. Concretely, wefirst formulate a Probability Flow Ordinary Differential Equation (PF-ODE)trajectory to establish a deterministic mapping from low-resolution (LR) imageswith noise to high-resolution (HR) images. Then we apply the ConsistencyTraining (CT) strategy to directly learn the mapping in one step, eliminatingthe necessity of pre-trained diffusion model. To further enhance theperformance and better leverage the ground-truth during the training process,we aim to align the distribution of SR results more closely with that of thenatural images. To this end, we propose to minimize the discrepancy betweentheir respective PF-ODE trajectories from the LR image distribution by ourmeticulously designed Distribution Trajectory Matching (DTM) loss, resulting inimproved realism of our recovered HR images. Comprehensive experimental resultsdemonstrate that the proposed methods can attain comparable or even superiorcapabilities on both synthetic and real datasets while maintaining minimalinference latency.
  </details>

- **[Recovering Dynamic 3D Sketches from Videos](http://arxiv.org/abs/2503.20321v2)**  `arXiv:2503.20321`  
  _Jaeah Lee, Changwoon Choi, Young Min Kim, Jaesik Park_
  <details><summary>Abstract</summary>
  Understanding 3D motion from videos presents inherent challenges due to thediverse types of movement, ranging from rigid and deformable objects toarticulated structures. To overcome this, we propose Liv3Stroke, a novelapproach for abstracting objects in motion with deformable 3D strokes. Thedetailed movements of an object may be represented by unstructured motionvectors or a set of motion primitives using a pre-defined articulation from atemplate model. Just as a free-hand sketch can intuitively visualize scenes orintentions with a sparse set of lines, we utilize a set of parametric 3D curvesto capture a set of spatially smooth motion elements for general objects withunknown structures. We first extract noisy, 3D point cloud motion guidance fromvideo frames using semantic features, and our approach deforms a set of curvesto abstract essential motion features as a set of explicit 3D representations.Such abstraction enables an understanding of prominent components of motionswhile maintaining robustness to environmental factors. Our approach allowsdirect analysis of 3D object movements from video, tackling the uncertaintythat typically occurs when translating real-world motion into recorded footage.The project page is accessible via: https://jaeah.me/liv3stroke_web
  </details>

- **[Leveraging 3D Geometric Priors in 2D Rotation Symmetry Detection](http://arxiv.org/abs/2503.20235v2)**  `arXiv:2503.20235`  
  _Ahyun Seo, Minsu Cho_
  <details><summary>Abstract</summary>
  Symmetry plays a vital role in understanding structural patterns, aidingobject recognition and scene interpretation. This paper focuses on rotationsymmetry, where objects remain unchanged when rotated around a central axis,requiring detection of rotation centers and supporting vertices. Traditionalmethods relied on hand-crafted feature matching, while recent segmentationmodels based on convolutional neural networks detect rotation centers butstruggle with 3D geometric consistency due to viewpoint distortions. Toovercome this, we propose a model that directly predicts rotation centers andvertices in 3D space and projects the results back to 2D while preservingstructural integrity. By incorporating a vertex reconstruction stage enforcing3D geometric priors -- such as equal side lengths and interior angles -- ourmodel enhances robustness and accuracy. Experiments on the DENDI dataset showsuperior performance in rotation axis detection and validate the impact of 3Dpriors through ablation studies.
  </details>

- **[EventMamba: Enhancing Spatio-Temporal Locality with State Space Models for Event-Based Video Reconstruction](http://arxiv.org/abs/2503.19721v2)**  `arXiv:2503.19721`  
  _Chengjie Ge, Xueyang Fu, Peng He, Kunyu Wang, Chengzhi Cao, Zheng-Jun Zha_
  <details><summary>Abstract</summary>
  Leveraging its robust linear global modeling capability, Mamba has notablyexcelled in computer vision. Despite its success, existing Mamba-based visionmodels have overlooked the nuances of event-driven tasks, especially in videoreconstruction. Event-based video reconstruction (EBVR) demands spatialtranslation invariance and close attention to local event relationships in thespatio-temporal domain. Unfortunately, conventional Mamba algorithms applystatic window partitions and standard reshape scanning methods, leading tosignificant losses in local connectivity. To overcome these limitations, weintroduce EventMamba--a specialized model designed for EBVR tasks. EventMambainnovates by incorporating random window offset (RWO) in the spatial domain,moving away from the restrictive fixed partitioning. Additionally, it featuresa new consistent traversal serialization approach in the spatio-temporaldomain, which maintains the proximity of adjacent events both spatially andtemporally. These enhancements enable EventMamba to retain Mamba's robustmodeling capabilities while significantly preserving the spatio-temporallocality of event data. Comprehensive testing on multiple datasets shows thatEventMamba markedly enhances video reconstruction, drastically improvingcomputation speed while delivering superior visual quality compared toTransformer-based methods.
  </details>

- **[RGB-Th-Bench: A Dense benchmark for Visual-Thermal Understanding of Vision Language Models](http://arxiv.org/abs/2503.19654v2)**  `arXiv:2503.19654`  
  _Mehdi Moshtaghi, Siavash H. Khajavi, Joni Pajarinen_
  <details><summary>Abstract</summary>
  We introduce RGB-Th-Bench, the first benchmark designed to evaluate theability of Vision-Language Models (VLMs) to comprehend RGB-Thermal image pairs.While VLMs have demonstrated remarkable progress in visual reasoning andmultimodal understanding, their evaluation has been predominantly limited toRGB-based benchmarks, leaving a critical gap in assessing their capabilities ininfrared vision tasks. Existing visible-infrared datasets are eithertask-specific or lack high-quality annotations necessary for rigorous modelevaluation. To address these limitations, RGB-Th-Bench provides a comprehensiveevaluation framework covering 14 distinct skill dimensions, with a total of1,600+ expert-annotated Yes/No questions. The benchmark employs two accuracymetrics: a standard question-level accuracy and a stricter skill-levelaccuracy, which evaluates model robustness across multiple questions withineach skill dimension. This design ensures a thorough assessment of modelperformance, including resilience to adversarial and hallucinated responses. Weconduct extensive evaluations on 19 state-of-the-art VLMs, revealingsignificant performance gaps in RGB-Thermal understanding. Our results showthat even the strongest models struggle with thermal image comprehension, withperformance heavily constrained by their RGB-based capabilities. Additionally,the lack of large-scale application-specific and expert-annotatedthermal-caption-pair datasets in pre-training is an important reason of theobserved performance gap. RGB-Th-Bench highlights the urgent need for furtheradvancements in multimodal learning to bridge the gap between visible andthermal image understanding. The dataset is available through this link, andthe evaluation code will also be made publicly available.
  </details>

- **[SlowFast-LLaVA-1.5: A Family of Token-Efficient Video Large Language Models for Long-Form Video Understanding](http://arxiv.org/abs/2503.18943v2)**  `arXiv:2503.18943`  
  _Mingze Xu, Mingfei Gao, Shiyu Li, Jiasen Lu, Zhe Gan, Zhengfeng Lai, et al._
  <details><summary>Abstract</summary>
  We introduce SlowFast-LLaVA-1.5 (abbreviated as SF-LLaVA-1.5), a family ofvideo large language models (LLMs) offering a token-efficient solution forlong-form video understanding. We incorporate the two-stream SlowFast mechanisminto a streamlined training pipeline, and perform joint video-image training ona carefully curated data mixture of only publicly available datasets. Ourprimary focus is on highly efficient model scales (1B and 3B), demonstratingthat even relatively small Video LLMs can achieve state-of-the-art performanceon video understanding, meeting the demand for mobile-friendly models.Experimental results demonstrate that SF-LLaVA-1.5 achieves superiorperformance on a wide range of video and image tasks, with robust results atall model sizes (ranging from 1B to 7B). Notably, SF-LLaVA-1.5 achievesstate-of-the-art results in long-form video understanding (e.g., LongVideoBenchand MLVU) and excels at small scales across various video benchmarks.
  </details>

- **[Training-free Diffusion Acceleration with Bottleneck Sampling](http://arxiv.org/abs/2503.18940v2)**  `arXiv:2503.18940`  
  _Ye Tian, Xin Xia, Yuxi Ren, Shanchuan Lin, Xing Wang, Xuefeng Xiao, et al._
  <details><summary>Abstract</summary>
  Diffusion models have demonstrated remarkable capabilities in visual contentgeneration but remain challenging to deploy due to their high computationalcost during inference. This computational burden primarily arises from thequadratic complexity of self-attention with respect to image or videoresolution. While existing acceleration methods often compromise output qualityor necessitate costly retraining, we observe that most diffusion models arepre-trained at lower resolutions, presenting an opportunity to exploit theselow-resolution priors for more efficient inference without degradingperformance. In this work, we introduce Bottleneck Sampling, a training-freeframework that leverages low-resolution priors to reduce computational overheadwhile preserving output fidelity. Bottleneck Sampling follows a high-low-highdenoising workflow: it performs high-resolution denoising in the initial andfinal stages while operating at lower resolutions in intermediate steps. Tomitigate aliasing and blurring artifacts, we further refine the resolutiontransition points and adaptively shift the denoising timesteps at each stage.We evaluate Bottleneck Sampling on both image and video generation tasks, whereextensive experiments demonstrate that it accelerates inference by up to3$\times$ for image generation and 2.5$\times$ for video generation, all whilemaintaining output quality comparable to the standard full-resolution samplingprocess across multiple evaluation metrics.
  </details>

- **[Image-to-Text for Medical Reports Using Adaptive Co-Attention and Triple-LSTM Module](http://arxiv.org/abs/2503.18297v2)**  `arXiv:2503.18297`  
  _Yishen Liu, Shengda Liu, Hudan Pan_
  <details><summary>Abstract</summary>
  Medical report generation requires specialized expertise that general largemodels often fail to accurately capture. Moreover, the inherent repetition andsimilarity in medical data make it difficult for models to extract meaningfulfeatures, resulting in a tendency to overfit. So in this paper, we propose amultimodal model, Co-Attention Triple-LSTM Network (CA-TriNet), a deep learningmodel that combines transformer architectures with a Multi-LSTM network. ItsCo-Attention module synergistically links a vision transformer with a texttransformer to better differentiate medical images with similarities, augmentedby an adaptive weight operator to catch and amplify image labels with minorsimilarities. Furthermore, its Triple-LSTM module refines generated sentencesusing targeted image objects. Extensive evaluations over three public datasetshave demonstrated that CA-TriNet outperforms state-of-the-art models in termsof comprehensive ability, even pre-trained large language models on somemetrics.
  </details>

- **[MotionDiff: Training-free Zero-shot Interactive Motion Editing via Flow-assisted Multi-view Diffusion](http://arxiv.org/abs/2503.17695v2)**  `arXiv:2503.17695`  
  _Yikun Ma, Yiqing Li, Jiawei Wu, Xing Luo, Zhi Jin_
  <details><summary>Abstract</summary>
  Generative models have made remarkable advancements and are capable ofproducing high-quality content. However, performing controllable editing withgenerative models remains challenging, due to their inherent uncertainty inoutputs. This challenge is praticularly pronounced in motion editing, whichinvolves the processing of spatial information. While some physics-basedgenerative methods have attempted to implement motion editing, they typicallyoperate on single-view images with simple motions, such as translation anddragging. These methods struggle to handle complex rotation and stretchingmotions and ensure multi-view consistency, often necessitatingresource-intensive retraining. To address these challenges, we proposeMotionDiff, a training-free zero-shot diffusion method that leverages opticalflow for complex multi-view motion editing. Specifically, given a static scene,users can interactively select objects of interest to add motion priors. Theproposed Point Kinematic Model (PKM) then estimates corresponding multi-viewoptical flows during the Multi-view Flow Estimation Stage (MFES). Subsequently,these optical flows are utilized to generate multi-view motion results throughdecoupled motion representation in the Multi-view Motion Diffusion Stage(MMDS). Extensive experiments demonstrate that MotionDiff outperforms otherphysics-based generative motion editing methods in achieving high-qualitymulti-view consistent motion results. Notably, MotionDiff does not requireretraining, enabling users to conveniently adapt it for various down-streamtasks.
  </details>

- **[Temporal-Guided Spiking Neural Networks for Event-Based Human Action Recognition](http://arxiv.org/abs/2503.17132v2)**  `arXiv:2503.17132`  
  _Siyuan Yang, Shilin Lu, Shizheng Wang, Meng Hwa Er, Zengwei Zheng, Alex C. Kot_
  <details><summary>Abstract</summary>
  This paper explores the promising interplay between spiking neural networks(SNNs) and event-based cameras for privacy-preserving human action recognition(HAR). The unique feature of event cameras in capturing only the outlines ofmotion, combined with SNNs' proficiency in processing spatiotemporal datathrough spikes, establishes a highly synergistic compatibility for event-basedHAR. Previous studies, however, have been limited by SNNs' ability to processlong-term temporal information, essential for precise HAR. In this paper, weintroduce two novel frameworks to address this: temporal segment-based SNN(\textit{TS-SNN}) and 3D convolutional SNN (\textit{3D-SNN}). The\textit{TS-SNN} extracts long-term temporal information by dividing actionsinto shorter segments, while the \textit{3D-SNN} replaces 2D spatial elementswith 3D components to facilitate the transmission of temporal information. Topromote further research in event-based HAR, we create a dataset,\textit{FallingDetection-CeleX}, collected using the high-resolution CeleX-Vevent camera $(1280 \times 800)$, comprising 7 distinct actions. Extensiveexperimental results show that our proposed frameworks surpass state-of-the-artSNN methods on our newly collected dataset and three other neuromorphicdatasets, showcasing their effectiveness in handling long-range temporalinformation for event-based HAR.
  </details>

- **[Multimodal Feature-Driven Deep Learning for the Prediction of Duck Body Dimensions and Weight](http://arxiv.org/abs/2503.14001v3)**  `arXiv:2503.14001`  
  _Yi Xiao, Qiannan Han, Gang Shu, Guiping Liang, Hongyan Zhang, Song Wang, et al._
  <details><summary>Abstract</summary>
  Accurate body dimension and weight measurements are critical for optimizingpoultry management, health assessment, and economic efficiency. This studyintroduces an innovative deep learning-based model leveraging multimodaldata-2D RGB images from different views, depth images, and 3D point clouds-forthe non-invasive estimation of duck body dimensions and weight. A dataset of1,023 Linwu ducks, comprising over 5,000 samples with diverse postures andconditions, was collected to support model training. The proposed methodinnovatively employs PointNet++ to extract key feature points from pointclouds, extracts and computes corresponding 3D geometric features, and fusesthem with multi-view convolutional 2D features. A Transformer encoder is thenutilized to capture long-range dependencies and refine feature interactions,thereby enhancing prediction robustness. The model achieved a mean absolutepercentage error (MAPE) of 6.33% and an R2 of 0.953 across eight morphometricparameters, demonstrating strong predictive capability. Unlike conventionalmanual measurements, the proposed model enables high-precision estimation whileeliminating the necessity for physical handling, thereby reducing animal stressand broadening its application scope. This study marks the first application ofdeep learning techniques to poultry body dimension and weight estimation,providing a valuable reference for the intelligent and precise management ofthe livestock industry with far-reaching practical significance.
  </details>

- **[DefectFill: Realistic Defect Generation with Inpainting Diffusion Model for Visual Inspection](http://arxiv.org/abs/2503.13985v2)**  `arXiv:2503.13985`  
  _Jaewoo Song, Daemin Park, Kanghyun Baek, Sangyub Lee, Jooyoung Choi, Eunji Kim, et al._
  <details><summary>Abstract</summary>
  Developing effective visual inspection models remains challenging due to thescarcity of defect data. While image generation models have been used tosynthesize defect images, producing highly realistic defects remains difficult.We propose DefectFill, a novel method for realistic defect generation thatrequires only a few reference defect images. It leverages a fine-tunedinpainting diffusion model, optimized with our custom loss functionsincorporating defect, object, and attention terms. It enables precise captureof detailed, localized defect features and their seamless integration intodefect-free objects. Additionally, our Low-Fidelity Selection method furtherenhances the defect sample quality. Experiments show that DefectFill generateshigh-quality defect images, enabling visual inspection models to achievestate-of-the-art performance on the MVTec AD dataset.
  </details>

- **[Towards Better Alignment: Training Diffusion Models with Reinforcement Learning Against Sparse Rewards](http://arxiv.org/abs/2503.11240v2)**  `arXiv:2503.11240`  
  _Zijing Hu, Fengda Zhang, Long Chen, Kun Kuang, Jiahui Li, Kaifeng Gao, et al._
  <details><summary>Abstract</summary>
  Diffusion models have achieved remarkable success in text-to-imagegeneration. However, their practical applications are hindered by themisalignment between generated images and corresponding text prompts. To tacklethis issue, reinforcement learning (RL) has been considered for diffusion modelfine-tuning. Yet, RL's effectiveness is limited by the challenge of sparsereward, where feedback is only available at the end of the generation process.This makes it difficult to identify which actions during the denoising processcontribute positively to the final generated image, potentially leading toineffective or unnecessary denoising policies. To this end, this paper presentsa novel RL-based framework that addresses the sparse reward problem whentraining diffusion models. Our framework, named $\text{B}^2\text{-DiffuRL}$,employs two strategies: \textbf{B}ackward progressive training and\textbf{B}ranch-based sampling. For one thing, backward progressive trainingfocuses initially on the final timesteps of denoising process and graduallyextends the training interval to earlier timesteps, easing the learningdifficulty from sparse rewards. For another, we perform branch-based samplingfor each training interval. By comparing the samples within the same branch, wecan identify how much the policies of the current training interval contributeto the final image, which helps to learn effective policies instead ofunnecessary ones. $\text{B}^2\text{-DiffuRL}$ is compatible with existingoptimization algorithms. Extensive experiments demonstrate the effectiveness of$\text{B}^2\text{-DiffuRL}$ in improving prompt-image alignment and maintainingdiversity in generated images. The code for this work is available.
  </details>

- **[MouseGPT: A Large-scale Vision-Language Model for Mouse Behavior Analysis](http://arxiv.org/abs/2503.10212v2)**  `arXiv:2503.10212`  
  _Teng Xu, Taotao Zhou, Youjia Wang, Peng Yang, Simin Tang, Kuixiang Shao, et al._
  <details><summary>Abstract</summary>
  Analyzing animal behavior is crucial in advancing neuroscience, yetquantifying and deciphering its intricate dynamics remains a significantchallenge. Traditional machine vision approaches, despite their ability todetect spontaneous behaviors, fall short due to limited interpretability andreliance on manual labeling, which restricts the exploration of the fullbehavioral spectrum. Here, we introduce MouseGPT, a Vision-Language Model (VLM)that integrates visual cues with natural language to revolutionize mousebehavior analysis. Built upon our first-of-its-kind dataset - incorporatingpose dynamics and open-vocabulary behavioral annotations across over 42 millionframes of diverse psychiatric conditions - MouseGPT provides a novel,context-rich method for comprehensive behavior interpretation. Our holisticanalysis framework enables detailed behavior profiling, clustering, and novelbehavior discovery, offering deep insights without the need for labor -intensive manual annotation. Evaluations reveal that MouseGPT surpassesexisting models in precision, adaptability, and descriptive richness,positioning it as a transformative tool for ethology and for unraveling complexbehavioral dynamics in animal models.
  </details>

- **[SimROD: A Simple Baseline for Raw Object Detection with Global and Local Enhancements](http://arxiv.org/abs/2503.07101v2)**  `arXiv:2503.07101`  
  _Haiyang Xie, Xi Shen, Shihua Huang, Qirui Wang, Zheng Wang_
  <details><summary>Abstract</summary>
  Most visual models are designed for sRGB images, yet RAW data offerssignificant advantages for object detection by preserving sensor informationbefore ISP processing. This enables improved detection accuracy and moreefficient hardware designs by bypassing the ISP. However, RAW object detectionis challenging due to limited training data, unbalanced pixel distributions,and sensor noise. To address this, we propose SimROD, a lightweight andeffective approach for RAW object detection. We introduce a Global GammaEnhancement (GGE) module, which applies a learnable global gamma transformationwith only four parameters, improving feature representation while keeping themodel efficient. Additionally, we leverage the green channel's richer signal toenhance local details, aligning with the human eye's sensitivity and Bayerfilter design. Extensive experiments on multiple RAW object detection datasetsand detectors demonstrate that SimROD outperforms state-of-the-art methods likeRAW-Adapter and DIAP while maintaining efficiency. Our work highlights thepotential of RAW data for real-world object detection. Code is available athttps://ocean146.github.io/SimROD2025/.
  </details>

- **[FaceID-6M: A Large-Scale, Open-Source FaceID Customization Dataset](http://arxiv.org/abs/2503.07091v3)**  `arXiv:2503.07091`  
  _Shuhe Wang, Xiaoya Li, Jiwei Li, Guoyin Wang, Xiaofei Sun, Bob Zhu, et al._
  <details><summary>Abstract</summary>
  Due to the data-driven nature of current face identity (FaceID) customizationmethods, all state-of-the-art models rely on large-scale datasets containingmillions of high-quality text-image pairs for training. However, none of thesedatasets are publicly available, which restricts transparency and hindersfurther advancements in the field.  To address this issue, in this paper, we collect and release FaceID-6M, thefirst large-scale, open-source FaceID dataset containing 6 million high-qualitytext-image pairs. Filtered from LAION-5B \cite{schuhmann2022laion}, FaceID-6Mundergoes a rigorous image and text filtering steps to ensure dataset quality,including resolution filtering to maintain high-quality images and faces, facefiltering to remove images that lack human faces, and keyword-based strategy toretain descriptions containing human-related terms (e.g., nationality,professions and names). Through these cleaning processes, FaceID-6M provides ahigh-quality dataset optimized for training powerful FaceID customizationmodels, facilitating advancements in the field by offering an open resource forresearch and development.  We conduct extensive experiments to show the effectiveness of our FaceID-6M,demonstrating that models trained on our FaceID-6M dataset achieve performancethat is comparable to, and slightly better than currently available industrialmodels. Additionally, to support and advance research in the FaceIDcustomization community, we make our code, datasets, and models fully publiclyavailable. Our codes, models, and datasets are available at:https://github.com/ShuheSH/FaceID-6M.
  </details>

- **[Rethinking Video Tokenization: A Conditioned Diffusion-based Approach](http://arxiv.org/abs/2503.03708v3)**  `arXiv:2503.03708`  
  _Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, et al._
  <details><summary>Abstract</summary>
  Existing video tokenizers typically use the traditional VariationalAutoencoder (VAE) architecture for video compression and reconstruction.However, to achieve good performance, its training process often relies oncomplex multi-stage training tricks that go beyond basic reconstruction lossand KL regularization. Among these tricks, the most challenging is the precisetuning of adversarial training with additional Generative Adversarial Networks(GANs) in the final stage, which can hinder stable convergence. In contrast toGANs, diffusion models offer more stable training processes and can generatehigher-quality results. Inspired by these advantages, we propose CDT, a novelConditioned Diffusion-based video Tokenizer, that replaces the GAN-baseddecoder with a conditional causal diffusion model. The encoder compressesspatio-temporal information into compact latents, while the decoderreconstructs videos through a reverse diffusion process conditioned on theselatents. During inference, we incorporate a feature cache mechanism to generatevideos of arbitrary length while maintaining temporal continuity and adoptsampling acceleration technique to enhance efficiency. Trained using only abasic MSE diffusion loss for reconstruction, along with KL term and LPIPSperceptual loss from scratch, extensive experiments demonstrate that CDTachieves state-of-the-art performance in video reconstruction tasks with just asingle-step sampling. Even a scaled-down version of CDT (3$\times$ inferencespeedup) still performs comparably with top baselines. Moreover, the latentvideo generation model trained with CDT also exhibits superior performance. Thesource code and pretrained weights are available athttps://github.com/ali-vilab/CDT.
  </details>

- **[Generalizable Prompt Learning of CLIP: A Brief Overview](http://arxiv.org/abs/2503.01263v2)**  `arXiv:2503.01263`  
  _Fangming Cui, Yonggang Zhang, Xuan Wang, Xule Wang, Liang Xiao_
  <details><summary>Abstract</summary>
  Existing vision-language models (VLMs) such as CLIP have showcased animpressive capability to generalize well across various downstream tasks. Thesemodels leverage the synergy between visual and textual information, enablingthem to understand and reason about the content present in images and text in aunified manner. This article provides a brief overview of CLIP based onfew-shot prompt learning, including experimental data and technicalcharacteristics of some methods. The purpose of this review is to provide areference for researchers who have just started their research in generalizableprompting of CLIP through few-shot training for classification across 15datasets and also to facilitate the integration of this field by researchers inother downstream tasks.
  </details>

- **[VideoHandles: Editing 3D Object Compositions in Videos Using Video Generative Priors](http://arxiv.org/abs/2503.01107v2)**  `arXiv:2503.01107`  
  _Juil Koo, Paul Guerrero, Chun-Hao Paul Huang, Duygu Ceylan, Minhyuk Sung_
  <details><summary>Abstract</summary>
  Generative methods for image and video editing use generative models aspriors to perform edits despite incomplete information, such as changing thecomposition of 3D objects shown in a single image. Recent methods have shownpromising composition editing results in the image setting, but in the videosetting, editing methods have focused on editing object's appearance andmotion, or camera motion, and as a result, methods to edit object compositionin videos are still missing. We propose \name as a method for editing 3D objectcompositions in videos of static scenes with camera motion. Our approach allowsediting the 3D position of a 3D object across all frames of a video in atemporally consistent manner. This is achieved by lifting intermediate featuresof a generative model to a 3D reconstruction that is shared between all frames,editing the reconstruction, and projecting the features on the editedreconstruction back to each frame. To the best of our knowledge, this is thefirst generative approach to edit object compositions in videos. Our approachis simple and training-free, while outperforming state-of-the-art image editingbaselines.
  </details>

- **[ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval](http://arxiv.org/abs/2502.15682v2)**  `arXiv:2502.15682`  
  _Guanqi Zhan, Yuanpei Liu, Kai Han, Weidi Xie, Andrew Zisserman_
  <details><summary>Abstract</summary>
  The objective in this paper is to improve the performance of text-to-imageretrieval. To this end, we introduce a new framework that can boost theperformance of large-scale pre-trained vision-language models, so that they canbe used for text-to-image re-ranking. The approach, Enhanced Language-ImagePre-training (ELIP), uses the text query, via a simple MLP mapping network, topredict a set of visual prompts to condition the ViT image encoding. ELIP caneasily be applied to the commonly used CLIP, SigLIP and BLIP-2 networks. Totrain the architecture with limited computing resources, we develop a 'studentfriendly' best practice, involving global hard sample mining, and curation of alarge-scale dataset. On the evaluation side, we set up two newout-of-distribution (OOD) benchmarks, Occluded COCO and ImageNet-R, to assessthe zero-shot generalisation of the models to different domains. The resultsdemonstrate that ELIP significantly boosts CLIP/SigLIP/SigLIP-2 text-to-imageretrieval performance and outperforms BLIP-2 on several benchmarks, as well asproviding an easy means to adapt to OOD datasets.
  </details>

- **[TripoSG: High-Fidelity 3D Shape Synthesis using Large-Scale Rectified Flow Models](http://arxiv.org/abs/2502.06608v3)**  `arXiv:2502.06608`  
  _Yangguang Li, Zi-Xin Zou, Zexiang Liu, Dehu Wang, Yuan Liang, Zhipeng Yu, et al._
  <details><summary>Abstract</summary>
  Recent advancements in diffusion techniques have propelled image and videogeneration to unprecedented levels of quality, significantly accelerating thedeployment and application of generative AI. However, 3D shape generationtechnology has so far lagged behind, constrained by limitations in 3D datascale, complexity of 3D data processing, and insufficient exploration ofadvanced techniques in the 3D domain. Current approaches to 3D shape generationface substantial challenges in terms of output quality, generalizationcapability, and alignment with input conditions. We present TripoSG, a newstreamlined shape diffusion paradigm capable of generating high-fidelity 3Dmeshes with precise correspondence to input images. Specifically, we propose:1) A large-scale rectified flow transformer for 3D shape generation, achievingstate-of-the-art fidelity through training on extensive, high-quality data. 2)A hybrid supervised training strategy combining SDF, normal, and eikonal lossesfor 3D VAE, achieving high-quality 3D reconstruction performance. 3) A dataprocessing pipeline to generate 2 million high-quality 3D samples, highlightingthe crucial rules for data quality and quantity in training 3D generativemodels. Through comprehensive experiments, we have validated the effectivenessof each component in our new framework. The seamless integration of these partshas enabled TripoSG to achieve state-of-the-art performance in 3D shapegeneration. The resulting 3D shapes exhibit enhanced detail due tohigh-resolution capabilities and demonstrate exceptional fidelity to inputimages. Moreover, TripoSG demonstrates improved versatility in generating 3Dmodels from diverse image styles and contents, showcasing strong generalizationcapabilities. To foster progress and innovation in the field of 3D generation,we will make our model publicly available.
  </details>

- **[LANTERN++: Enhancing Relaxed Speculative Decoding with Static Tree Drafting for Visual Auto-regressive Models](http://arxiv.org/abs/2502.06352v2)**  `arXiv:2502.06352`  
  _Sihwan Park, Doohyuk Jang, Sungyub Kim, Souvik Kundu, Eunho Yang_
  <details><summary>Abstract</summary>
  Speculative decoding has been widely used to accelerate auto-regressive (AR)text generation. However, its effectiveness for visual AR models remainslimited due to token selection ambiguity, where multiple tokens share similarlylow probabilities and thus reduce acceptance rates. Recently, relaxedspeculative decoding with dynamic tree drafting was proposed to mitigate thisambiguity, demonstrating promising results in accelerating visual AR models.However, we observe that token selection ambiguity still negatively affectsdynamic tree drafting, resulting in shallow draft trees and limitedacceleration. To overcome this issue, we introduce LANTERN++, a refinedframework that integrates static tree drafting with a tailored relaxedacceptance condition, allowing drafts to be selected independently oflow-confidence predictions. This enables the acceptance of deeper sequences,improving decoding efficiency while preserving image quality. Extensiveexperiments on state-of-the-art visual AR models demonstrate that LANTERN++significantly accelerates inference, achieving up to $\mathbf{\times 2.56}$speedup over standard AR decoding while maintaining high image quality. Thecode is publicly available at https://github.com/jadohu/LANTERN.
  </details>

- **[Leveraging Textual Anatomical Knowledge for Class-Imbalanced Semi-Supervised Multi-Organ Segmentation](http://arxiv.org/abs/2501.13470v2)**  `arXiv:2501.13470`  
  _Yuliang Gu, Weilun Tsao, Bo Du, Thierry G√©raud, Yongchao Xu_
  <details><summary>Abstract</summary>
  Annotating 3D medical images demands substantial time and expertise, drivingthe adoption of semi-supervised learning (SSL) for segmentation tasks. However,the complex anatomical structures of organs often lead to significant classimbalances, posing major challenges for deploying SSL in real-world scenarios.Despite the availability of valuable prior information, such as inter-organrelative positions and organ shape priors, existing SSL methods have yet tofully leverage these insights. To address this gap, we propose a novel approachthat integrates textual anatomical knowledge (TAK) into the segmentation model.Specifically, we use GPT-4o to generate textual descriptions of anatomicalpriors, which are then encoded using a CLIP-based model. These encoded priorsare injected into the segmentation model as parameters of the segmentationhead. Additionally, contrastive learning is employed to enhance the alignmentbetween textual priors and visual features. Extensive experiments demonstratethe superior performance of our method, significantly surpassingstate-of-the-art approaches. The source code will be available at:https://github.com/Lunn88/TAK-Semi.
  </details>

- **[Survey on Monocular Metric Depth Estimation](http://arxiv.org/abs/2501.11841v2)**  `arXiv:2501.11841`  
  _Jiuling Zhang_
  <details><summary>Abstract</summary>
  Monocular Depth Estimation (MDE) is fundamental to computer vision, enablingspatial understanding, 3D reconstruction, and autonomous driving. Deeplearning-based MDE predicts relative depth from a single image, but the lack ofmetric scale introduces inconsistencies, limiting applicability in tasks suchas visual SLAM, 3D reconstruction, and novel view synthesis. Monocular MetricDepth Estimation (MMDE) overcomes this limitation by enabling precisescene-scale inference, improving depth consistency, enhancing stability insequential tasks, and streamlining integration into practical systems. Thispaper systematically reviews the evolution of depth estimation, fromtraditional geometric methods to deep learning breakthroughs, emphasizingscale-agnostic approaches in zero-shot generalization which is crucial foradvancing MMDE. Recent progress in zero-shot MMDE is examined, focusing onchallenges such as model generalization and boundary detail loss. To addressthese issues, researchers have explored unlabeled data augmentation, imagepatching, architectural optimization, and generative techniques. This reviewanalyzes these developments, assessing their impact and limitations. Keyfindings are synthesized, unresolved challenges outlined, and future researchdirection proposal. By providing a clear technical roadmap and insight intoemerging trends, this work aims to drive innovation and expand the real-worldapplications of MMDE.
  </details>

- **[OVO-Bench: How Far is Your Video-LLMs from Real-World Online Video Understanding?](http://arxiv.org/abs/2501.05510v2)**  `arXiv:2501.05510`  
  _Yifei Li, Junbo Niu, Ziyang Miao, Chunjiang Ge, Yuanhang Zhou, Qihao He, et al._
  <details><summary>Abstract</summary>
  Temporal Awareness, the ability to reason dynamically based on the timestampwhen a question is raised, is the key distinction between offline and onlinevideo LLMs. Unlike offline models, which rely on complete videos for static,post hoc analysis, online models process video streams incrementally anddynamically adapt their responses based on the timestamp at which the questionis posed. Despite its significance, temporal awareness has not been adequatelyevaluated in existing benchmarks. To fill this gap, we present OVO-Bench(Online-VideO-Benchmark), a novel video benchmark that emphasizes theimportance of timestamps for advanced online video understanding capabilitybenchmarking. OVO-Bench evaluates the ability of video LLMs to reason andrespond to events occurring at specific timestamps under three distinctscenarios: (1) Backward tracing: trace back to past events to answer thequestion. (2) Real-time understanding: understand and respond to events as theyunfold at the current timestamp. (3) Forward active responding: delay theresponse until sufficient future information becomes available to answer thequestion accurately. OVO-Bench comprises 12 tasks, featuring 644 unique videosand approximately human-curated 2,800 fine-grained meta-annotations withprecise timestamps. We combine automated generation pipelines with humancuration. With these high-quality samples, we further developed an evaluationpipeline to systematically query video LLMs along the video timeline.Evaluations of nine Video-LLMs reveal that, despite advancements on traditionalbenchmarks, current models struggle with online video understanding, showing asignificant gap compared to human agents. We hope OVO-Bench will drive progressin video LLMs and inspire future research in online video reasoning. Ourbenchmark and code can be accessed at https://github.com/JoeLeelyf/OVO-Bench.
  </details>

- **[LongViTU: Instruction Tuning for Long-Form Video Understanding](http://arxiv.org/abs/2501.05037v2)**  `arXiv:2501.05037`  
  _Rujie Wu, Xiaojian Ma, Hai Ci, Yue Fan, Yuxuan Wang, Haozhe Zhao, et al._
  <details><summary>Abstract</summary>
  This paper introduces LongViTU, a large-scale (~121k QA pairs, ~900h videos),automatically generated dataset for long-form video understanding. We propose asystematic approach that organizes videos into a hierarchical tree structurefor QA generation and incorporates self-revision mechanisms to ensurehigh-quality QA pairs. Each QA pair in LongViTU features: 1) long-term context(average certificate length of 4.6 minutes); 2) rich knowledge and condensedreasoning (commonsense, causality, planning, etc.)). We also offer explicittimestamp annotations of relevant events for each QA pair. We have conductedextensive human studies on LongViTU, and the results prove the quality of ourdataset. To better evaluate the challenges posed by LongViTU's emphasis onlong-term context and condensed reasoning, we manually curate a subset ofLongViTU into a benchmark. Evaluations using a state-of-the-art open-sourcemodel (LongVU), a proprietary model (Gemini-1.5-Pro), and human annotatorsyield GPT-4 scores of 49.9, 52.3, and 81.0, respectively, underscoring thesubstantial difficulty presented by LongViTU questions. Performing supervisedfine-tuning (SFT) of LongVU and LLaVA-Video on LongViTU data results in averageperformance gains of 2.5% and 3.7%, respectively, across a suite of long videounderstanding benchmarks (EgoSchema, VideoMME-Long, MLVU, LVBench).
  </details>

- **[TREAD: Token Routing for Efficient Architecture-agnostic Diffusion Training](http://arxiv.org/abs/2501.04765v2)**  `arXiv:2501.04765`  
  _Felix Krause, Timy Phan, Ming Gui, Stefan Andreas Baumann, Vincent Tao Hu, Bj√∂rn Ommer_
  <details><summary>Abstract</summary>
  Diffusion models have emerged as the mainstream approach for visualgeneration. However, these models typically suffer from sample inefficiency andhigh training costs. Consequently, methods for efficient finetuning, inferenceand personalization were quickly adopted by the community. However, trainingthese models in the first place remains very costly. While several recentapproaches - including masking, distillation, and architectural modifications -have been proposed to improve training efficiency, each of these methods comeswith a tradeoff: they achieve enhanced performance at the expense of increasedcomputational cost or vice versa. In contrast, this work aims to improvetraining efficiency as well as generative performance at the same time throughroutes that act as a transport mechanism for randomly selected tokens fromearly layers to deeper layers of the model. Our method is not limited to thecommon transformer-based model - it can also be applied to state-space modelsand achieves this without architectural modifications or additional parameters.Finally, we show that TREAD reduces computational cost and simultaneouslyboosts model performance on the standard ImageNet-256 benchmark inclass-conditional synthesis. Both of these benefits multiply to a convergencespeedup of 14x at 400K training iterations compared to DiT and 37x compared tothe best benchmark performance of DiT at 7M training iterations. Furthermore,we achieve a competitive FID of 2.09 in a guided and 3.93 in an unguidedsetting, which improves upon the DiT, without architectural changes.
  </details>

- **[UAV-DETR: Efficient End-to-End Object Detection for Unmanned Aerial Vehicle Imagery](http://arxiv.org/abs/2501.01855v2)**  `arXiv:2501.01855`  
  _Huaxiang Zhang, Kai Liu, Zhongxue Gan, Guo-Niu Zhu_
  <details><summary>Abstract</summary>
  Unmanned aerial vehicle object detection (UAV-OD) has been widely used invarious scenarios. However, most existing UAV-OD algorithms rely on manuallydesigned components, which require extensive tuning. End-to-end models that donot depend on such manually designed components are mainly designed for naturalimages, which are less effective for UAV imagery. To address such challenges,this paper proposes an efficient detection transformer (DETR) frameworktailored for UAV imagery, i.e., UAV-DETR. The framework includes a multi-scalefeature fusion with frequency enhancement module, which captures both spatialand frequency information at different scales. In addition, a frequency-focuseddown-sampling module is presented to retain critical spatial details duringdown-sampling. A semantic alignment and calibration module is developed toalign and fuse features from different fusion paths. Experimental resultsdemonstrate the effectiveness and generalization of our approach across variousUAV imagery datasets. On the VisDrone dataset, our method improves AP by 3.1\%and $\text{AP}_{50}$ by 4.2\% over the baseline. Similar enhancements areobserved on the UAVVaste dataset. The project page:https://github.com/ValiantDiligent/UAV-DETR
  </details>

- **[SyncDiff: Synchronized Motion Diffusion for Multi-Body Human-Object Interaction Synthesis](http://arxiv.org/abs/2412.20104v4)**  `arXiv:2412.20104`  
  _Wenkun He, Yun Liu, Ruitao Liu, Li Yi_
  <details><summary>Abstract</summary>
  Synthesizing realistic human-object interaction motions is a critical problemin VR/AR and human animation. Unlike the commonly studied scenarios involving asingle human or hand interacting with one object, we address a more genericmulti-body setting with arbitrary numbers of humans, hands, and objects. Thiscomplexity introduces significant challenges in synchronizing motions due tothe high correlations and mutual influences among bodies. To address thesechallenges, we introduce SyncDiff, a novel method for multi-body interactionsynthesis using a synchronized motion diffusion strategy. SyncDiff employs asingle diffusion model to capture the joint distribution of multi-body motions.To enhance motion fidelity, we propose a frequency-domain motion decompositionscheme. Additionally, we introduce a new set of alignment scores to emphasizethe synchronization of different body motions. SyncDiff jointly optimizes bothdata sample likelihood and alignment likelihood through an explicitsynchronization strategy. Extensive experiments across four datasets withvarious multi-body configurations demonstrate the superiority of SyncDiff overexisting state-of-the-art motion synthesis methods.
  </details>

- **[Video-Panda: Parameter-efficient Alignment for Encoder-free Video-Language Models](http://arxiv.org/abs/2412.18609v2)**  `arXiv:2412.18609`  
  _Jinhui Yi, Syed Talal Wasim, Yanan Luo, Muzammal Naseer, Juergen Gall_
  <details><summary>Abstract</summary>
  We present an efficient encoder-free approach for video-languageunderstanding that achieves competitive performance while significantlyreducing computational overhead. Current video-language models typically relyon heavyweight image encoders (300M-1.1B parameters) or video encoders (1B-1.4Bparameters), creating a substantial computational burden when processingmulti-frame videos. Our method introduces a novel Spatio-Temporal AlignmentBlock (STAB) that directly processes video inputs without requiring pre-trainedencoders while using only 45M parameters for visual processing - at least a6.5$\times$ reduction compared to traditional approaches. The STAB architecturecombines Local Spatio-Temporal Encoding for fine-grained feature extraction,efficient spatial downsampling through learned attention and separatemechanisms for modeling frame-level and video-level relationships. Our modelachieves comparable or superior performance to encoder-based approaches foropen-ended video question answering on standard benchmarks. The fine-grainedvideo question-answering evaluation demonstrates our model's effectiveness,outperforming the encoder-based approaches Video-ChatGPT and Video-LLaVA in keyaspects like correctness and temporal understanding. Extensive ablation studiesvalidate our architectural choices and demonstrate the effectiveness of ourspatio-temporal modeling approach while achieving 3-4$\times$ faster processingspeeds than previous methods. Code is available athttps://jh-yi.github.io/Video-Panda.
  </details>

- **[Layer- and Timestep-Adaptive Differentiable Token Compression Ratios for Efficient Diffusion Transformers](http://arxiv.org/abs/2412.16822v2)**  `arXiv:2412.16822`  
  _Haoran You, Connelly Barnes, Yuqian Zhou, Yan Kang, Zhenbang Du, Wei Zhou, et al._
  <details><summary>Abstract</summary>
  Diffusion Transformers (DiTs) have achieved state-of-the-art (SOTA) imagegeneration quality but suffer from high latency and memory inefficiency, makingthem difficult to deploy on resource-constrained devices. One major efficiencybottleneck is that existing DiTs apply equal computation across all regions ofan image. However, not all image tokens are equally important, and certainlocalized areas require more computation, such as objects. To address this, wepropose DiffCR, a dynamic DiT inference framework with differentiablecompression ratios, which automatically learns to dynamically route computationacross layers and timesteps for each image token, resulting in efficient DiTs.Specifically, DiffCR integrates three features: (1) A token-level routingscheme where each DiT layer includes a router that is fine-tuned jointly withmodel weights to predict token importance scores. In this way, unimportanttokens bypass the entire layer's computation; (2) A layer-wise differentiableratio mechanism where different DiT layers automatically learn varyingcompression ratios from a zero initialization, resulting in large compressionratios in redundant layers while others remain less compressed or evenuncompressed; (3) A timestep-wise differentiable ratio mechanism where eachdenoising timestep learns its own compression ratio. The resulting patternshows higher ratios for noisier timesteps and lower ratios as the image becomesclearer. Extensive experiments on text-to-image and inpainting tasks show thatDiffCR effectively captures dynamism across token, layer, and timestep axes,achieving superior trade-offs between generation quality and efficiencycompared to prior works. The project website is available athttps://www.haoranyou.com/diffcr.
  </details>

- **[OmniSplat: Taming Feed-Forward 3D Gaussian Splatting for Omnidirectional Images with Editable Capabilities](http://arxiv.org/abs/2412.16604v2)**  `arXiv:2412.16604`  
  _Suyoung Lee, Jaeyoung Chung, Kihoon Kim, Jaeyoo Huh, Gunhee Lee, Minsoo Lee, et al._
  <details><summary>Abstract</summary>
  Feed-forward 3D Gaussian splatting (3DGS) models have gained significantpopularity due to their ability to generate scenes immediately without needingper-scene optimization. Although omnidirectional images are becoming morepopular since they reduce the computation required for image stitching tocomposite a holistic scene, existing feed-forward models are only designed forperspective images. The unique optical properties of omnidirectional imagesmake it difficult for feature encoders to correctly understand the context ofthe image and make the Gaussian non-uniform in space, which hinders the imagequality synthesized from novel views. We propose OmniSplat, a training-freefast feed-forward 3DGS generation framework for omnidirectional images. Weadopt a Yin-Yang grid and decompose images based on it to reduce the domain gapbetween omnidirectional and perspective images. The Yin-Yang grid can use theexisting CNN structure as it is, but its quasi-uniform characteristic allowsthe decomposed image to be similar to a perspective image, so it can exploitthe strong prior knowledge of the learned feed-forward network. OmniSplatdemonstrates higher reconstruction accuracy than existing feed-forward networkstrained on perspective images. Our project page is available on:https://robot0321.github.io/omnisplat/index.html.
  </details>

- **[EnvGS: Modeling View-Dependent Appearance with Environment Gaussian](http://arxiv.org/abs/2412.15215v2)**  `arXiv:2412.15215`  
  _Tao Xie, Xi Chen, Zhen Xu, Yiman Xie, Yudong Jin, Yujun Shen, et al._
  <details><summary>Abstract</summary>
  Reconstructing complex reflections in real-world scenes from 2D images isessential for achieving photorealistic novel view synthesis. Existing methodsthat utilize environment maps to model reflections from distant lighting oftenstruggle with high-frequency reflection details and fail to account fornear-field reflections. In this work, we introduce EnvGS, a novel approach thatemploys a set of Gaussian primitives as an explicit 3D representation forcapturing reflections of environments. These environment Gaussian primitivesare incorporated with base Gaussian primitives to model the appearance of thewhole scene. To efficiently render these environment Gaussian primitives, wedeveloped a ray-tracing-based renderer that leverages the GPU's RT core forfast rendering. This allows us to jointly optimize our model for high-qualityreconstruction while maintaining real-time rendering speeds. Results frommultiple real-world and synthetic datasets demonstrate that our method producessignificantly more detailed reflections, achieving the best rendering qualityin real-time novel view synthesis. The code is available athttps://zju3dv.github.io/envgs.
  </details>

- **[SegMAN: Omni-scale Context Modeling with State Space Models and Local Attention for Semantic Segmentation](http://arxiv.org/abs/2412.11890v2)**  `arXiv:2412.11890`  
  _Yunxiang Fu, Meng Lou, Yizhou Yu_
  <details><summary>Abstract</summary>
  High-quality semantic segmentation relies on three key capabilities: globalcontext modeling, local detail encoding, and multi-scale feature extraction.However, recent methods struggle to possess all these capabilitiessimultaneously. Hence, we aim to empower segmentation networks tosimultaneously carry out efficient global context modeling, high-quality localdetail encoding, and rich multi-scale feature representation for varying inputresolutions. In this paper, we introduce SegMAN, a novel linear-time modelcomprising a hybrid feature encoder dubbed SegMAN Encoder, and a decoder basedon state space models. Specifically, the SegMAN Encoder synergisticallyintegrates sliding local attention with dynamic state space models, enablinghighly efficient global context modeling while preserving fine-grained localdetails. Meanwhile, the MMSCopE module in our decoder enhances multi-scalecontext feature extraction and adaptively scales with the input resolution. OurSegMAN-B Encoder achieves 85.1% ImageNet-1k accuracy (+1.5% over VMamba-S withfewer parameters). When paired with our decoder, the full SegMAN-B modelachieves 52.6% mIoU on ADE20K (+1.6% over SegNeXt-L with 15% fewer GFLOPs),83.8% mIoU on Cityscapes (+2.1% over SegFormer-B3 with half the GFLOPs), and1.6% higher mIoU than VWFormer-B3 on COCO-Stuff with lower GFLOPs. Our code isavailable at https://github.com/yunxiangfu2001/SegMAN.
  </details>

- **[Do Multimodal Large Language Models See Like Humans?](http://arxiv.org/abs/2412.09603v2)**  `arXiv:2412.09603`  
  _Jiaying Lin, Shuquan Ye, Rynson W. H. Lau_
  <details><summary>Abstract</summary>
  Multimodal Large Language Models (MLLMs) have achieved impressive results onvarious vision tasks, leveraging recent advancements in large language models.However, a critical question remains unaddressed: do MLLMs perceive visualinformation similarly to humans? Current benchmarks lack the ability toevaluate MLLMs from this perspective. To address this challenge, we introduceHVSBench, a large-scale benchmark designed to assess the alignment betweenMLLMs and the human visual system (HVS) on fundamental vision tasks that mirrorhuman vision. HVSBench curated over 85K multimodal samples, spanning 13categories and 5 fields in HVS, including Prominence, Subitizing, Prioritizing,Free-Viewing, and Searching. Extensive experiments demonstrate theeffectiveness of our benchmark in providing a comprehensive evaluation ofMLLMs. Specifically, we evaluate 13 MLLMs, revealing that even the best modelsshow significant room for improvement, with most achieving only moderateresults. Our experiments reveal that HVSBench presents a new and significantchallenge for cutting-edge MLLMs. Diverse human participants attained strongperformance, significantly outperforming MLLMs, which further underscores thebenchmark's high quality. We believe that HVSBench will facilitate research onhuman-aligned and explainable MLLMs, marking a key step in understanding howMLLMs perceive and process visual information.
  </details>

- **[RatBodyFormer: Rat Body Surface from Keypoints](http://arxiv.org/abs/2412.09599v3)**  `arXiv:2412.09599`  
  _Ayaka Higami, Karin Oshima, Tomoyo Isoguchi Shiramatsu, Hirokazu Takahashi, Shohei Nobuhara, Ko Nishino_
  <details><summary>Abstract</summary>
  Analyzing rat behavior lies at the heart of many scientific studies. Pastmethods for automated rodent modeling have focused on 3D pose estimation fromkeypoints, e.g., face and appendages. The pose, however, does not capture therich body surface movement encoding the subtle rat behaviors like curling andstretching. The body surface lacks features that can be visually defined,evading these established keypoint-based methods. In this paper, we introducethe first method for reconstructing the rat body surface as a dense set ofpoints by learning to predict it from the sparse keypoints that can be detectedwith past methods. Our method consists of two key contributions. The first isRatDome, a novel multi-camera system for rat behavior capture, and alarge-scale dataset captured with it that consists of pairs of 3D keypoints and3D body surface points. The second is RatBodyFormer, a novel network totransform detected keypoints to 3D body surface points. RatBodyFormer isagnostic to the exact locations of the 3D body surface points in the trainingdata and is trained with masked-learning. We experimentally validate ourframework with a number of real-world experiments. Our results collectivelyserve as a novel foundation for automated rat behavior analysis.
  </details>

- **[StyleStudio: Text-Driven Style Transfer with Selective Control of Style Elements](http://arxiv.org/abs/2412.08503v2)**  `arXiv:2412.08503`  
  _Mingkun Lei, Xue Song, Beier Zhu, Hao Wang, Chi Zhang_
  <details><summary>Abstract</summary>
  Text-driven style transfer aims to merge the style of a reference image withcontent described by a text prompt. Recent advancements in text-to-image modelshave improved the nuance of style transformations, yet significant challengesremain, particularly with overfitting to reference styles, limiting stylisticcontrol, and misaligning with textual content. In this paper, we propose threecomplementary strategies to address these issues. First, we introduce across-modal Adaptive Instance Normalization (AdaIN) mechanism for betterintegration of style and text features, enhancing alignment. Second, we developa Style-based Classifier-Free Guidance (SCFG) approach that enables selectivecontrol over stylistic elements, reducing irrelevant influences. Finally, weincorporate a teacher model during early generation stages to stabilize spatiallayouts and mitigate artifacts. Our extensive evaluations demonstratesignificant improvements in style transfer quality and alignment with textualprompts. Furthermore, our approach can be integrated into existing styletransfer frameworks without fine-tuning.
  </details>

- **[Video Motion Transfer with Diffusion Transformers](http://arxiv.org/abs/2412.07776v2)**  `arXiv:2412.07776`  
  _Alexander Pondaven, Aliaksandr Siarohin, Sergey Tulyakov, Philip Torr, Fabio Pizzati_
  <details><summary>Abstract</summary>
  We propose DiTFlow, a method for transferring the motion of a reference videoto a newly synthesized one, designed specifically for Diffusion Transformers(DiT). We first process the reference video with a pre-trained DiT to analyzecross-frame attention maps and extract a patch-wise motion signal called theAttention Motion Flow (AMF). We guide the latent denoising process in anoptimization-based, training-free, manner by optimizing latents with our AMFloss to generate videos reproducing the motion of the reference one. We alsoapply our optimization strategy to transformer positional embeddings, grantingus a boost in zero-shot motion transfer capabilities. We evaluate DiTFlowagainst recently published methods, outperforming all across multiple metricsand human evaluation.
  </details>

- **[ReCap: Better Gaussian Relighting with Cross-Environment Captures](http://arxiv.org/abs/2412.07534v3)**  `arXiv:2412.07534`  
  _Jingzhi Li, Zongwei Wu, Eduard Zamfir, Radu Timofte_
  <details><summary>Abstract</summary>
  Accurate 3D objects relighting in diverse unseen environments is crucial forrealistic virtual object placement. Due to the albedo-lighting ambiguity,existing methods often fall short in producing faithful relights. Withoutproper constraints, observed training views can be explained by numerouscombinations of lighting and material attributes, lacking physicalcorrespondence with the actual environment maps used for relighting. In thiswork, we present ReCap, treating cross-environment captures as multi-tasktarget to provide the missing supervision that cuts through the entanglement.Specifically, ReCap jointly optimizes multiple lighting representations thatshare a common set of material attributes. This naturally harmonizes a coherentset of lighting representations around the mutual material attributes,exploiting commonalities and differences across varied object appearances. Suchcoherence enables physically sound lighting reconstruction and robust materialestimation - both essential for accurate relighting. Together with astreamlined shading function and effective post-processing, ReCap outperformsall leading competitors on an expanded relighting benchmark.
  </details>

- **[Beyond [cls]: Exploring the true potential of Masked Image Modeling representations](http://arxiv.org/abs/2412.03215v2)**  `arXiv:2412.03215`  
  _Marcin Przewiƒô≈∫likowski, Randall Balestriero, Wojciech Jasi≈Ñski, Marek ≈ömieja, Bartosz Zieli≈Ñski_
  <details><summary>Abstract</summary>
  Masked Image Modeling (MIM) has emerged as a promising approach forSelf-Supervised Learning (SSL) of visual representations. However, theout-of-the-box performance of MIMs is typically inferior to competingapproaches. Most users cannot afford fine-tuning due to the need for largeamounts of data, high GPU consumption, and specialized user knowledge.Therefore, the practical use of MIM representations is limited. In this paperwe ask what is the reason for the poor out-of-the-box performance of MIMs. Isit due to weaker features produced by MIM models, or is it due to suboptimalusage? Through detailed analysis, we show that attention in MIMs is spreadalmost uniformly over many patches, leading to ineffective aggregation by the[cls] token. Based on this insight, we propose Selective Aggregation to bettercapture the rich semantic information retained in patch tokens, whichsignificantly improves the out-of-the-box performance of MIM.
  </details>

- **[Frequency-Guided Diffusion Model with Perturbation Training for Skeleton-Based Video Anomaly Detection](http://arxiv.org/abs/2412.03044v2)**  `arXiv:2412.03044`  
  _Xiaofeng Tan, Hongsong Wang, Xin Geng, Liang Wang_
  <details><summary>Abstract</summary>
  Video anomaly detection (VAD) is a vital yet complex open-set task incomputer vision, commonly tackled through reconstruction-based methods.However, these methods struggle with two key limitations: (1) insufficientrobustness in open-set scenarios, where unseen normal motions are frequentlymisclassified as anomalies, and (2) an overemphasis on, but restricted capacityfor, local motion reconstruction, which are inherently difficult to captureaccurately due to their diversity. To overcome these challenges, we introduce anovel frequency-guided diffusion model with perturbation training. First, weenhance robustness by training a generator to produce perturbed samples, whichare similar to normal samples and target the weakness of the reconstructionmodel. This training paradigm expands the reconstruction domain of the model,improving its generalization to unseen normal motions. Second, to address theoveremphasis on motion details, we employ the 2D Discrete Cosine Transform(DCT) to separate high-frequency (local) and low-frequency (global) motioncomponents. By guiding the diffusion model with observed high-frequencyinformation, we prioritize the reconstruction of low-frequency components,enabling more accurate and robust anomaly detection. Extensive experiments onfive widely used VAD datasets demonstrate that our approach surpassesstate-of-the-art methods, underscoring its effectiveness in open-set scenariosand diverse motion contexts. Our project website ishttps://xiaofeng-tan.github.io/projects/FG-Diff/index.html.
  </details>

- **[OODFace: Benchmarking Robustness of Face Recognition under Common Corruptions and Appearance Variations](http://arxiv.org/abs/2412.02479v2)**  `arXiv:2412.02479`  
  _Caixin Kang, Yubo Chen, Shouwei Ruan, Shiji Zhao, Ruochen Zhang, Jiayi Wang, et al._
  <details><summary>Abstract</summary>
  With the rise of deep learning, facial recognition technology has seenextensive research and rapid development. Although facial recognition isconsidered a mature technology, we find that existing open-source models andcommercial algorithms lack robustness in certain complex Out-of-Distribution(OOD) scenarios, raising concerns about the reliability of these systems. Inthis paper, we introduce OODFace, which explores the OOD challenges faced byfacial recognition models from two perspectives: common corruptions andappearance variations. We systematically design 30 OOD scenarios across 9 majorcategories tailored for facial recognition. By simulating these challenges onpublic datasets, we establish three robustness benchmarks: LFW-C/V, CFP-FP-C/V,and YTF-C/V. We then conduct extensive experiments on 19 facial recognitionmodels and 3 commercial APIs, along with extended physical experiments on facemasks to assess their robustness. Next, we explore potential solutions from twoperspectives: defense strategies and Vision-Language Models (VLMs). Based onthe results, we draw several key insights, highlighting the vulnerability offacial recognition systems to OOD data and suggesting possible solutions.Additionally, we offer a unified toolkit that includes all corruption andvariation types, easily extendable to other datasets. We hope that ourbenchmarks and findings can provide guidance for future improvements in facialrecognition model robustness.
  </details>

- **[Improving Object Detection by Modifying Synthetic Data with Explainable AI](http://arxiv.org/abs/2412.01477v2)**  `arXiv:2412.01477`  
  _Nitish Mital, Simon Malzard, Richard Walters, Celso M. De Melo, Raghuveer Rao, Victoria Nockles_
  <details><summary>Abstract</summary>
  Limited real-world data severely impacts model performance in many computervision domains, particularly for samples that are underrepresented in training.Synthetically generated images are a promising solution, but 1) it remainsunclear how to design synthetic training data to optimally improve modelperformance (e.g, whether and where to introduce more realism or moreabstraction) and 2) the domain expertise, time and effort required from humanoperators for this design and optimisation process represents a major practicalchallenge. Here we propose a novel conceptual approach to improve theefficiency of designing synthetic images, by using robust Explainable AI (XAI)techniques to guide a human-in-the-loop process of modifying 3D mesh modelsused to generate these images. Importantly, this framework allows bothmodifications that increase and decrease realism in synthetic data, which canboth improve model performance. We illustrate this concept using a real-worldexample where data are sparse; detection of vehicles in infrared imagery. Wefine-tune an initial YOLOv8 model on the ATR DSIAC infrared dataset andsynthetic images generated from 3D mesh models in the Unity gaming engine, andthen use XAI saliency maps to guide modification of our Unity models. We showthat synthetic data can improve detection of vehicles in orientations unseen intraining by 4.6% (to mAP50 = 94.6%). We further improve performance by anadditional 1.5% (to 96.1%) through our new XAI-guided approach, which reducesmisclassifications through both increasing and decreasing the realism ofdifferent parts of the synthetic data. Our proof-of-concept results pave theway for fine, XAI-controlled curation of synthetic datasets tailored to improveobject detection performance, whilst simultaneously reducing the burden onhuman operators in designing and optimising these datasets.
  </details>

- **[Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding](http://arxiv.org/abs/2412.00493v2)**  `arXiv:2412.00493`  
  _Duo Zheng, Shijia Huang, Liwei Wang_
  <details><summary>Abstract</summary>
  The rapid advancement of Multimodal Large Language Models (MLLMs) hassignificantly impacted various multimodal tasks. However, these models facechallenges in tasks that require spatial understanding within 3D environments.Efforts to enhance MLLMs, such as incorporating point cloud features, have beenmade, yet a considerable gap remains between the models' learnedrepresentations and the inherent complexity of 3D scenes. This discrepancylargely stems from the training of MLLMs on predominantly 2D data, whichrestricts their effectiveness in comprehending 3D spaces. To address thisissue, in this paper, we propose a novel generalist model, i.e., Video-3D LLM,for 3D scene understanding. By treating 3D scenes as dynamic videos andincorporating 3D position encoding into these representations, our Video-3D LLMaligns video representations with real-world spatial contexts more accurately.In addition, we have implemented a maximum coverage sampling technique tooptimize the trade-off between computational cost and performance. Extensiveexperiments demonstrate that our model achieves state-of-the-art performance onseveral 3D scene understanding benchmarks, including ScanRefer, Multi3DRefer,Scan2Cap, ScanQA, and SQA3D.
  </details>

- **[Feedback-driven object detection and iterative model improvement](http://arxiv.org/abs/2411.19835v3)**  `arXiv:2411.19835`  
  _S√∂nke Tenckhoff, Mario Koddenbrock, Erik Rodner_
  <details><summary>Abstract</summary>
  Automated object detection has become increasingly valuable across diverseapplications, yet efficient, high-quality annotation remains a persistentchallenge. In this paper, we present the development and evaluation of aplatform designed to interactively improve object detection models. Theplatform allows uploading and annotating images as well as fine-tuning objectdetection models. Users can then manually review and refine annotations,further creating improved snapshots that are used for automatic objectdetection on subsequent image uploads - a process we refer to as semi-automaticannotation resulting in a significant gain in annotation efficiency.  Whereas iterative refinement of model results to speed up annotation hasbecome common practice, we are the first to quantitatively evaluate itsbenefits with respect to time, effort, and interaction savings. Ourexperimental results show clear evidence for a significant time reduction of upto 53% for semi-automatic compared to manual annotation. Importantly, theseefficiency gains did not compromise annotation quality, while matching oroccasionally even exceeding the accuracy of manual annotations. These findingsdemonstrate the potential of our lightweight annotation platform for creatinghigh-quality object detection datasets and provide best practices to guidefuture development of annotation platforms.  The platform is open-source, with the frontend and backend repositoriesavailable on GitHub. To support the understanding of our labeling process, wehave created an explanatory video demonstrating the methodology usingmicroscopy images of E. coli bacteria as an example.
  </details>

- **[VIRES: Video Instance Repainting via Sketch and Text Guided Generation](http://arxiv.org/abs/2411.16199v5)**  `arXiv:2411.16199`  
  _Shuchen Weng, Haojie Zheng, Peixuan Zhan, Yuchen Hong, Han Jiang, Si Li, et al._
  <details><summary>Abstract</summary>
  We introduce VIRES, a video instance repainting method with sketch and textguidance, enabling video instance repainting, replacement, generation, andremoval. Existing approaches struggle with temporal consistency and accuratealignment with the provided sketch sequence. VIRES leverages the generativepriors of text-to-video models to maintain temporal consistency and producevisually pleasing results. We propose the Sequential ControlNet with thestandardized self-scaling, which effectively extracts structure layouts andadaptively captures high-contrast sketch details. We further augment thediffusion transformer backbone with the sketch attention to interpret andinject fine-grained sketch semantics. A sketch-aware encoder ensures thatrepainted results are aligned with the provided sketch sequence. Additionally,we contribute the VireSet, a dataset with detailed annotations tailored fortraining and evaluating video instance editing methods. Experimental resultsdemonstrate the effectiveness of VIRES, which outperforms state-of-the-artmethods in visual quality, temporal consistency, condition alignment, and humanratings. Project page: https://hjzheng.net/projects/VIRES/
  </details>

- **[Event-boosted Deformable 3D Gaussians for Dynamic Scene Reconstruction](http://arxiv.org/abs/2411.16180v2)**  `arXiv:2411.16180`  
  _Wenhao Xu, Wenming Weng, Yueyi Zhang, Ruikang Xu, Zhiwei Xiong_
  <details><summary>Abstract</summary>
  Deformable 3D Gaussian Splatting (3D-GS) is limited by missing intermediatemotion information due to the low temporal resolution of RGB cameras. Toaddress this, we introduce the first approach combining event cameras, whichcapture high-temporal-resolution, continuous motion data, with deformable 3D-GSfor dynamic scene reconstruction. We observe that threshold modeling for eventsplays a crucial role in achieving high-quality reconstruction. Therefore, wepropose a GS-Threshold Joint Modeling strategy, creating a mutually reinforcingprocess that greatly improves both 3D reconstruction and threshold modeling.Moreover, we introduce a Dynamic-Static Decomposition strategy that firstidentifies dynamic areas by exploiting the inability of static Gaussians torepresent motions, then applies a buffer-based soft decomposition to separatedynamic and static areas. This strategy accelerates rendering by avoidingunnecessary deformation in static areas, and focuses on dynamic areas toenhance fidelity. Additionally, we contribute the first event-inclusive 4Dbenchmark with synthetic and real-world dynamic scenes, on which our methodachieves state-of-the-art performance.
  </details>

- **[SplatFlow: Self-Supervised Dynamic Gaussian Splatting in Neural Motion Flow Field for Autonomous Driving](http://arxiv.org/abs/2411.15482v2)**  `arXiv:2411.15482`  
  _Su Sun, Cheng Zhao, Zhuoyang Sun, Yingjie Victor Chen, Mei Chen_
  <details><summary>Abstract</summary>
  Most existing Dynamic Gaussian Splatting methods for complex dynamic urbanscenarios rely on accurate object-level supervision from expensive manuallabeling, limiting their scalability in real-world applications. In this paper,we introduce SplatFlow, a Self-Supervised Dynamic Gaussian Splatting withinNeural Motion Flow Fields (NMFF) to learn 4D space-time representations withoutrequiring tracked 3D bounding boxes, enabling accurate dynamic scenereconstruction and novel view RGB/depth/flow synthesis. SplatFlow designs aunified framework to seamlessly integrate time-dependent 4D Gaussianrepresentation within NMFF, where NMFF is a set of implicit functions to modeltemporal motions of both LiDAR points and Gaussians as continuous motion flowfields. Leveraging NMFF, SplatFlow effectively decomposes static background anddynamic objects, representing them with 3D and 4D Gaussian primitives,respectively. NMFF also models the correspondences of each 4D Gaussian acrosstime, which aggregates temporal features to enhance cross-view consistency ofdynamic components. SplatFlow further improves dynamic object identification bydistilling features from 2D foundation models into 4D space-timerepresentation. Comprehensive evaluations conducted on the Waymo and KITTIDatasets validate SplatFlow's state-of-the-art (SOTA) performance for bothimage reconstruction and novel view synthesis in dynamic urban scenarios.
  </details>

- **[Dynamics-Aware Gaussian Splatting Streaming Towards Fast On-the-Fly 4D Reconstruction](http://arxiv.org/abs/2411.14847v2)**  `arXiv:2411.14847`  
  _Zhening Liu, Yingdong Hu, Xinjie Zhang, Rui Song, Jiawei Shao, Zehong Lin, et al._
  <details><summary>Abstract</summary>
  The recent development of 3D Gaussian Splatting (3DGS) has led to greatinterest in 4D dynamic spatial reconstruction. Existing approaches mainly relyon full-length multi-view videos, while there has been limited exploration ofonline reconstruction methods that enable on-the-fly training and per-timestepstreaming. Current 3DGS-based streaming methods treat the Gaussian primitivesuniformly and constantly renew the densified Gaussians, thereby overlooking thedifference between dynamic and static features as well as neglecting thetemporal continuity in the scene. To address these limitations, we propose anovel three-stage pipeline for iterative streamable 4D dynamic spatialreconstruction. Our pipeline comprises a selective inheritance stage topreserve temporal continuity, a dynamics-aware shift stage to distinguishdynamic and static primitives and optimize their movements, and an error-guideddensification stage to accommodate emerging objects. Our method achievesstate-of-the-art performance in online 4D reconstruction, demonstrating thefastest on-the-fly training, superior representation quality, and real-timerendering capability. Project page: https://www.liuzhening.top/DASS
  </details>

- **[GMAI-VL & GMAI-VL-5.5M: A Large Vision-Language Model and A Comprehensive Multimodal Dataset Towards General Medical AI](http://arxiv.org/abs/2411.14522v2)**  `arXiv:2411.14522`  
  _Tianbin Li, Yanzhou Su, Wei Li, Bin Fu, Zhe Chen, Ziyan Huang, et al._
  <details><summary>Abstract</summary>
  Despite significant advancements in general AI, its effectiveness in themedical domain is limited by the lack of specialized medical knowledge. Toaddress this, we formulate GMAI-VL-5.5M, a multimodal medical dataset createdby converting hundreds of specialized medical datasets with various annotationsinto high-quality image-text pairs. This dataset offers comprehensive taskcoverage, diverse modalities, and rich image-text data. Building upon thisdataset, we develop GMAI-VL, a general medical vision-language model, with athree-stage training strategy that enhances the integration of visual andtextual information. This approach significantly improves the model's abilityto process multimodal data, supporting accurate diagnoses and clinicaldecision-making. Experiments show that GMAI-VL achieves state-of-the-artperformance across various multimodal medical tasks, including visual questionanswering and medical image diagnosis.
  </details>

- **[Not Just Object, But State: Compositional Incremental Learning without Forgetting](http://arxiv.org/abs/2411.01739v3)**  `arXiv:2411.01739`  
  _Yanyi Zhang, Binglin Qiu, Qi Jia, Yu Liu, Ran He_
  <details><summary>Abstract</summary>
  Most incremental learners excessively prioritize coarse classes of objectswhile neglecting various kinds of states (e.g. color and material) attached tothe objects. As a result, they are limited in the ability to reasonfine-grained compositionality of state-object pairs. To remedy this limitation,we propose a novel task called Compositional Incremental Learning(composition-IL), enabling the model to recognize state-object compositions asa whole in an incremental learning fashion. Since the lack of suitablebenchmarks, we re-organize two existing datasets and make them tailored forcomposition-IL. Then, we propose a prompt-based Composition Incremental Learner(CompILer), to overcome the ambiguous composition boundary problem whichchallenges composition-IL largely. Specifically, we exploit multi-pool promptlearning, which is regularized by inter-pool prompt discrepancy and intra-poolprompt diversity. Besides, we devise object-injected state prompting by usingobject prompts to guide the selection of state prompts. Furthermore, we fusethe selected prompts by a generalized-mean strategy, to eliminate irrelevantinformation learned in the prompts. Extensive experiments on two datasetsexhibit state-of-the-art performance achieved by CompILer.
  </details>

- **[A Survey on Computational Solutions for Reconstructing Complete Objects by Reassembling Their Fractured Parts](http://arxiv.org/abs/2410.14770v2)**  `arXiv:2410.14770`  
  _Jiaxin Lu, Yongqing Liang, Huijun Han, Jiacheng Hua, Junfeng Jiang, Xin Li, et al._
  <details><summary>Abstract</summary>
  Reconstructing a complete object from its parts is a fundamental problem inmany scientific domains. The purpose of this article is to provide a systematicsurvey on this topic. The reassembly problem requires understanding theattributes of individual pieces and establishing matches between differentpieces. Many approaches also model priors of the underlying complete object.Existing approaches are tightly connected problems of shape segmentation, shapematching, and learning shape priors. We provide existing algorithms in thiscontext and emphasize their similarities and differences to general-purposeapproaches. We also survey the trends from early non-deep learning approachesto more recent deep learning approaches. In addition to algorithms, this surveywill also describe existing datasets, open-source software packages, andapplications. To the best of our knowledge, this is the first comprehensivesurvey on this topic in computer graphics.
  </details>

- **[AnomalyNCD: Towards Novel Anomaly Class Discovery in Industrial Scenarios](http://arxiv.org/abs/2410.14379v2)**  `arXiv:2410.14379`  
  _Ziming Huang, Xurui Li, Haotian Liu, Feng Xue, Yuzhe Wang, Yu Zhou_
  <details><summary>Abstract</summary>
  Recently, multi-class anomaly classification has garnered increasingattention. Previous methods directly cluster anomalies but often struggle dueto the lack of anomaly-prior knowledge. Acquiring this knowledge faces twoissues: the non-prominent and weak-semantics anomalies. In this paper, wepropose AnomalyNCD, a multi-class anomaly classification network compatiblewith different anomaly detection methods. To address the non-prominence ofanomalies, we design main element binarization (MEBin) to obtainanomaly-centered images, ensuring anomalies are learned while avoiding theimpact of incorrect detections. Next, to learn anomalies with weak semantics,we design mask-guided representation learning, which focuses on isolatedanomalies guided by masks and reduces confusion from erroneous inputs throughcorrected pseudo labels. Finally, to enable flexible classification at bothregion and image levels, we develop a region merging strategy that determinesthe overall image category based on the classified anomaly regions. Our methodoutperforms the state-of-the-art works on the MVTec AD and MTD datasets.Compared with the current methods, AnomalyNCD combined with zero-shot anomalydetection method achieves a 10.8% $F_1$ gain, 8.8% NMI gain, and 9.5% ARI gainon MVTec AD, and 12.8% $F_1$ gain, 5.7% NMI gain, and 10.8% ARI gain on MTD.Code is available at https://github.com/HUST-SLOW/AnomalyNCD.
  </details>

- **[ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and Wisdom](http://arxiv.org/abs/2410.14138v2)**  `arXiv:2410.14138`  
  _Jingqi Zhou, Sheng Wang, Jingwei Dong, Lei Li, Jiahui Gao, Jiyue Jiang, et al._
  <details><summary>Abstract</summary>
  Large vision-language models (LVLMs) have witnessed significant progress onvisual understanding tasks. However, they often prioritize language knowledgeover image information on visual reasoning tasks, incurring performancedegradation. To tackle this issue, we first identify the drawbacks of existingsolutions (i.e., insufficient and irrelevant visual descriptions, and limitedmulti-modal capacities). We then decompose visual reasoning process into twostages: visual perception (i.e., eyesight) and textual reasoning (i.e.,wisdom), and introduce a novel visual reasoning framework named ProReason. Thisframework features multi-run proactive perception and decoupledvision-reasoning capabilities. Briefly, given a multi-modal question, ProReasoniterates proactive information collection and reasoning until the answer can beconcluded with necessary and sufficient visual descriptions. Notably, thedisassociation of capabilities allows seamless integration of existing largelanguage models (LLMs) to compensate for the reasoning deficits of LVLMs. Ourextensive experiments demonstrate that ProReason outperforms both existingmulti-step reasoning frameworks and passive peer methods on a wide range ofbenchmarks for both open-source and closed-source models. In addition, with theassistance of LLMs, ProReason achieves a performance improvement of up to 15%on MMMU benchmark. Our insights into existing solutions and the decoupledperspective for feasible integration of LLMs illuminate future research onvisual reasoning techniques, especially LLM-assisted ones.
  </details>

- **[GI-GS: Global Illumination Decomposition on Gaussian Splatting for Inverse Rendering](http://arxiv.org/abs/2410.02619v2)**  `arXiv:2410.02619`  
  _Hongze Chen, Zehong Lin, Jun Zhang_
  <details><summary>Abstract</summary>
  We present GI-GS, a novel inverse rendering framework that leverages 3DGaussian Splatting (3DGS) and deferred shading to achieve photo-realistic novelview synthesis and relighting. In inverse rendering, accurately modeling theshading processes of objects is essential for achieving high-fidelity results.Therefore, it is critical to incorporate global illumination to account forindirect lighting that reaches an object after multiple bounces across thescene. Previous 3DGS-based methods have attempted to model indirect lighting bycharacterizing indirect illumination as learnable lighting volumes oradditional attributes of each Gaussian, while using baked occlusion torepresent shadow effects. These methods, however, fail to accurately model thecomplex physical interactions between light and objects, making it impossibleto construct realistic indirect illumination during relighting. To address thislimitation, we propose to calculate indirect lighting using efficient pathtracing with deferred shading. In our framework, we first render a G-buffer tocapture the detailed geometry and material properties of the scene. Then, weperform physically-based rendering (PBR) only for direct lighting. With theG-buffer and previous rendering results, the indirect lighting can becalculated through a lightweight path tracing. Our method effectively modelsindirect lighting under any given lighting conditions, thereby achieving betternovel view synthesis and competitive relighting. Quantitative and qualitativeresults show that our GI-GS outperforms existing baselines in both renderingquality and efficiency.
  </details>

- **[Multi-View and Multi-Scale Alignment for Contrastive Language-Image Pre-training in Mammography](http://arxiv.org/abs/2409.18119v2)**  `arXiv:2409.18119`  
  _Yuexi Du, John Onofrey, Nicha C. Dvornek_
  <details><summary>Abstract</summary>
  Contrastive Language-Image Pre-training (CLIP) demonstrates strong potentialin medical image analysis but requires substantial data and computationalresources. Due to these restrictions, existing CLIP applications in medicalimaging focus mainly on modalities like chest X-rays that have abundantimage-report data available, leaving many other important modalitiesunderexplored. Here, we propose one of the first adaptations of the full CLIPmodel to mammography, which presents significant challenges due to labeled datascarcity, high-resolution images with small regions of interest, and class-wiseimbalance. We first develop a specialized supervision framework for mammographythat leverages its multi-view nature. Furthermore, we design a symmetric localalignment module to better focus on detailed features in high-resolutionimages. Lastly, we incorporate a parameter-efficient fine-tuning approach forlarge language models pre-trained with medical knowledge to address datalimitations. Our multi-view and multi-scale alignment (MaMA) method outperformsstate-of-the-art baselines for three different tasks on two large real-worldmammography datasets, EMBED and RSNA-Mammo, with only 52% model size comparedwith the largest baseline. The code is available athttps://github.com/XYPB/MaMA
  </details>

- **[GCA-SUNet: A Gated Context-Aware Swin-UNet for Exemplar-Free Counting](http://arxiv.org/abs/2409.12249v2)**  `arXiv:2409.12249`  
  _Yuzhe Wu, Yipeng Xu, Tianyu Xu, Jialu Zhang, Jianfeng Ren, Xudong Jiang_
  <details><summary>Abstract</summary>
  Exemplar-Free Counting aims to count objects of interest without intensiveannotations of objects or exemplars. To achieve this, we propose a GatedContext-Aware Swin-UNet (GCA-SUNet) to directly map an input image to thedensity map of countable objects. Specifically, a set of Swin transformers forman encoder to derive a robust feature representation, and a Gated Context-AwareModulation block is designed to suppress irrelevant objects or backgroundthrough a gate mechanism and exploit the attentive support of objects ofinterest through a self-similarity matrix. The gate strategy is alsoincorporated into the bottleneck network and the decoder of the Swin-UNet tohighlight the features most relevant to objects of interest. By explicitlyexploiting the attentive support among countable objects and eliminatingirrelevant features through the gate mechanisms, the proposed GCA-SUNet focuseson and counts objects of interest without relying on predefined categories orexemplars. Experimental results on the real-world datasets such as FSC-147 andCARPK demonstrate that GCA-SUNet significantly and consistently outperformsstate-of-the-art methods. The code is available athttps://github.com/Amordia/GCA-SUNet.
  </details>

- **[StableMamba: Distillation-free Scaling of Large SSMs for Images and Videos](http://arxiv.org/abs/2409.11867v2)**  `arXiv:2409.11867`  
  _Hamid Suleman, Syed Talal Wasim, Muzammal Naseer, Juergen Gall_
  <details><summary>Abstract</summary>
  State-space models (SSMs), exemplified by S4, have introduced a novel contextmodeling method by integrating state-space techniques into deep learning.However, they struggle with global context modeling due to theirdata-independent matrices. The Mamba model addressed this with data-dependentvariants via the S6 selective-scan algorithm, enhancing context modeling,especially for long sequences. However, Mamba-based architectures are difficultto scale with respect to the number of parameters, which is a major limitationfor vision applications. This paper addresses the scalability issue of largeSSMs for image classification and action recognition without requiringadditional techniques like knowledge distillation. We analyze the distinctcharacteristics of Mamba-based and Attention-based models, proposing aMamba-Attention interleaved architecture that enhances scalability, robustness,and performance. We demonstrate that the stable and efficient interleavedarchitecture resolves the scalability issue of Mamba-based architectures forimages and videos and increases robustness to common artifacts like JPEGcompression. Our thorough evaluation on the ImageNet-1K, Kinetics-400 andSomething-Something-v2 benchmarks demonstrates that our approach improves theaccuracy of state-of-the-art Mamba-based architectures by up to $+1.7$.
  </details>

- **[Structure Modeling Activation Free Fourier Network for Spacecraft Image Denoising](http://arxiv.org/abs/2409.07067v2)**  `arXiv:2409.07067`  
  _Jingfan Yang, Hu Gao, Ying Zhang, Bowen Ma, Depeng Dang_
  <details><summary>Abstract</summary>
  Spacecraft image denoising is a crucial fundamental technology closelyrelated to aerospace research. However, the existing deep learning-based imagedenoising methods are primarily designed for natural image and fail toadequately consider the characteristics of spacecraft image(e.g. low-lightconditions, repetitive periodic structures), resulting in suboptimalperformance in the spacecraft image denoising task. To address theaforementioned problems, we propose a Structure modeling Activation FreeFourier Network (SAFFN), which is an efficient spacecraft image denoisingmethod including Structure Modeling Block (SMB) and Activation Free FourierBlock (AFFB). We present SMB to effectively extract edge information and modelthe structure for better identification of spacecraft components from darkregions in spacecraft noise image. We present AFFB and utilize an improved FastFourier block to extract repetitive periodic features and long-rangeinformation in noisy spacecraft image. Extensive experimental resultsdemonstrate that our SAFFN performs competitively compared to thestate-of-the-art methods on spacecraft noise image datasets. The codes areavailable at: https://github.com/shenduke/SAFFN.
  </details>

- **[Volumetric Surfaces: Representing Fuzzy Geometries with Layered Meshes](http://arxiv.org/abs/2409.02482v2)**  `arXiv:2409.02482`  
  _Stefano Esposito, Anpei Chen, Christian Reiser, Samuel Rota Bul√≤, Lorenzo Porzi, Katja Schwarz, et al._
  <details><summary>Abstract</summary>
  High-quality view synthesis relies on volume rendering, splatting, or surfacerendering. While surface rendering is typically the fastest, it struggles toaccurately model fuzzy geometry like hair. In turn, alpha-blending techniquesexcel at representing fuzzy materials but require an unbounded number ofsamples per ray (P1). Further overheads are induced by empty space skipping involume rendering (P2) and sorting input primitives in splatting (P3). Wepresent a novel representation for real-time view synthesis where the (P1)number of sampling locations is small and bounded, (P2) sampling locations areefficiently found via rasterization, and (P3) rendering is sorting-free. Weachieve this by representing objects as semi-transparent multi-layer meshesrendered in a fixed order. First, we model surface layers as signed distancefunction (SDF) shells with optimal spacing learned during training. Then, webake them as meshes and fit UV textures. Unlike single-surface methods, ourmulti-layer representation effectively models fuzzy objects. In contrast tovolume and splatting-based methods, our approach enables real-time rendering onlow-power laptops and smartphones.
  </details>

- **[SMAFormer: Synergistic Multi-Attention Transformer for Medical Image Segmentation](http://arxiv.org/abs/2409.00346v3)**  `arXiv:2409.00346`  
  _Fuchen Zheng, Xuhang Chen, Weihuang Liu, Haolun Li, Yingtie Lei, Jiahui He, et al._
  <details><summary>Abstract</summary>
  In medical image segmentation, specialized computer vision techniques,notably transformers grounded in attention mechanisms and residual networksemploying skip connections, have been instrumental in advancing performance.Nonetheless, previous models often falter when segmenting small, irregularlyshaped tumors. To this end, we introduce SMAFormer, an efficient,Transformer-based architecture that fuses multiple attention mechanisms forenhanced segmentation of small tumors and organs. SMAFormer can capture bothlocal and global features for medical image segmentation. The architecturecomprises two pivotal components. First, a Synergistic Multi-Attention (SMA)Transformer block is proposed, which has the benefits of Pixel Attention,Channel Attention, and Spatial Attention for feature enrichment. Second,addressing the challenge of information loss incurred during attentionmechanism transitions and feature fusion, we design a Feature Fusion Modulator.This module bolsters the integration between the channel and spatial attentionby mitigating reshaping-induced information attrition. To evaluate our method,we conduct extensive experiments on various medical image segmentation tasks,including multi-organ, liver tumor, and bladder tumor segmentation, achievingstate-of-the-art results. Code and models are available at:https://github.com/CXH-Research/SMAFormer.
  </details>

- **[MESA: Effective Matching Redundancy Reduction by Semantic Area Segmentation](http://arxiv.org/abs/2408.00279v2)**  `arXiv:2408.00279`  
  _Yesheng Zhang, Shuhan Shen, Xu Zhao_
  <details><summary>Abstract</summary>
  We propose MESA and DMESA as novel feature matching methods, which utilizeSegment Anything Model (SAM) to effectively mitigate matching redundancy. Thekey insight of our methods is to establish implicit-semantic area matchingprior to point matching, based on advanced image understanding of SAM. Then,informative area matches with consistent internal semantic are able to undergodense feature comparison, facilitating precise inside-area point matching.Specifically, MESA adopts a sparse matching framework and first obtainscandidate areas from SAM results through a novel Area Graph (AG). Then, areamatching among the candidates is formulated as graph energy minimization andsolved by graphical models derived from AG. To address the efficiency issue ofMESA, we further propose DMESA as its dense counterpart, applying a densematching framework. After candidate areas are identified by AG, DMESAestablishes area matches through generating dense matching distributions. Thedistributions are produced from off-the-shelf patch matching utilizing theGaussian Mixture Model and refined via the Expectation Maximization. With lessrepetitive computation, DMESA showcases a speed improvement of nearly fivetimes compared to MESA, while maintaining competitive accuracy. Our methods areextensively evaluated on five datasets encompassing indoor and outdoor scenes.The results illustrate consistent performance improvements from our methods forfive distinct point matching baselines across all datasets. Furthermore, ourmethods exhibit promise generalization and improved robustness against imageresolution variations. The code is publicly available athttps://github.com/Easonyesheng/A2PM-MESA.
  </details>

- **[On the Viability of Semi-Supervised Segmentation Methods for Statistical Shape Modeling](http://arxiv.org/abs/2407.15260v2)**  `arXiv:2407.15260`  
  _Asma Khan, Tushar Kataria, Janmesh Ukey, Shireen Y. Elhabian_
  <details><summary>Abstract</summary>
  Statistical Shape Models (SSMs) excel at identifying population levelanatomical variations, which is at the core of various clinical and biomedicalapplications, including morphology-based diagnostics and surgical planning.However, the effectiveness of SSM is often constrained by the necessity forexpert-driven manual segmentation, a process that is both time-intensive andexpensive, thereby restricting their broader application and utility. Recentdeep learning approaches enable the direct estimation of Statistical ShapeModels (SSMs) from unsegmented images. While these models can predict SSMswithout segmentation during deployment, they do not address the challenge ofacquiring the manual annotations needed for training, particularly inresource-limited settings. Semi-supervised models for anatomy segmentation canmitigate the annotation burden. Yet, despite the abundance of availableapproaches, there are no established guidelines to inform end-users on theireffectiveness for the downstream task of constructing SSMs. In this study, wesystematically evaluate the potential of semi-supervised methods as viablealternatives to manual segmentations for building SSMs. We establish a newperformance benchmark by employing various semi-supervised methods for anatomysegmentation under low annotation settings, utilizing the predictedsegmentations for the task of SSM. Our results indicate that some methodsproduce noisy segmentation, which is very unfavorable for SSM tasks, whileothers can capture the correct modes of variations in the population cohortwith 60-80% reduction in required manual annotation
  </details>

- **[Gaussian Splatting Lucas-Kanade](http://arxiv.org/abs/2407.11309v2)**  `arXiv:2407.11309`  
  _Liuyue Xie, Joel Julin, Koichiro Niinuma, Laszlo A. Jeni_
  <details><summary>Abstract</summary>
  Gaussian Splatting and its dynamic extensions are effective forreconstructing 3D scenes from 2D images when there is significant cameramovement to facilitate motion parallax and when scene objects remain relativelystatic. However, in many real-world scenarios, these conditions are not met. Asa consequence, data-driven semantic and geometric priors have been favored asregularizers, despite their bias toward training data and their neglect ofbroader movement dynamics.  Departing from this practice, we propose a novel analytical approach thatadapts the classical Lucas-Kanade method to dynamic Gaussian splatting. Byleveraging the intrinsic properties of the forward warp field network, wederive an analytical velocity field that, through time integration, facilitatesaccurate scene flow computation. This enables the precise enforcement of motionconstraints on warp fields, thus constraining both 2D motion and 3D positionsof the Gaussians. Our method excels in reconstructing highly dynamic sceneswith minimal camera movement, as demonstrated through experiments on bothsynthetic and real-world scenes.
  </details>

- **[BACON: Improving Clarity of Image Captions via Bag-of-Concept Graphs](http://arxiv.org/abs/2407.03314v2)**  `arXiv:2407.03314`  
  _Zhantao Yang, Ruili Feng, Keyu Yan, Huangji Wang, Zhicai Wang, Shangwen Zhu, et al._
  <details><summary>Abstract</summary>
  Advancements in large Vision-Language Models have brought precise, accurateimage captioning, vital for advancing multi-modal image understanding andprocessing. Yet these captions often carry lengthy, intertwined contexts thatare difficult to parse and frequently overlook essential cues, posing a greatbarrier for models like GroundingDINO and SDXL, which lack the strong textencoding and syntax analysis needed to fully leverage dense captions. Toaddress this, we propose BACON, a prompting method that breaks downVLM-generated captions into disentangled, structured elements such as objects,relationships, styles, and themes. This approach not only minimizes confusionfrom handling complex contexts but also allows for efficient transfer into aJSON dictionary, enabling models without linguistic processing capabilities toeasily access key information. We annotated 100,000 image-caption pairs usingBACON with GPT-4V and trained an LLaVA captioner on this dataset, enabling itto produce BACON-style captions without relying on costly GPT-4V. Evaluationsof overall quality, precision, and recall-as well as user studies-demonstratethat the resulting caption model consistently outperforms other SOTA VLM modelsin generating high-quality captions. Besides, we show that BACON-style captionsexhibit better clarity when applied to various models, enabling them toaccomplish previously unattainable tasks or surpass existing SOTA solutionswithout training. For example, BACON-style captions help GroundingDINO achieve1.51x higher recall scores on open-vocabulary object detection tasks comparedto leading methods.
  </details>

- **[Frequency-Controlled Diffusion Model for Versatile Text-Guided Image-to-Image Translation](http://arxiv.org/abs/2407.03006v2)**  `arXiv:2407.03006`  
  _Xiang Gao, Zhengbo Xu, Junhan Zhao, Jiaying Liu_
  <details><summary>Abstract</summary>
  Recently, large-scale text-to-image (T2I) diffusion models have emerged as apowerful tool for image-to-image translation (I2I), allowing open-domain imagetranslation via user-provided text prompts. This paper proposesfrequency-controlled diffusion model (FCDiffusion), an end-to-enddiffusion-based framework that contributes a novel solution to text-guided I2Ifrom a frequency-domain perspective. At the heart of our framework is afeature-space frequency-domain filtering module based on Discrete CosineTransform, which filters the latent features of the source image in the DCTdomain, yielding filtered image features bearing different DCT spectral bandsas different control signals to the pre-trained Latent Diffusion Model. Wereveal that control signals of different DCT spectral bands bridge the sourceimage and the T2I generated image in different correlations (e.g., style,structure, layout, contour, etc.), and thus enable versatile I2I applicationsemphasizing different I2I correlations, including style-guided contentcreation, image semantic manipulation, image scene translation, and image styletranslation. Different from related approaches, FCDiffusion establishes aunified text-guided I2I framework suitable for diverse image translation taskssimply by switching among different frequency control branches at inferencetime. The effectiveness and superiority of our method for text-guided I2I aredemonstrated with extensive experiments both qualitatively and quantitatively.Our project is publicly available at:https://xianggao1102.github.io/FCDiffusion/.
  </details>

- **[VIA: Unified Spatiotemporal Video Adaptation Framework for Global and Local Video Editing](http://arxiv.org/abs/2406.12831v3)**  `arXiv:2406.12831`  
  _Jing Gu, Yuwei Fang, Ivan Skorokhodov, Peter Wonka, Xinya Du, Sergey Tulyakov, et al._
  <details><summary>Abstract</summary>
  Video editing serves as a fundamental pillar of digital media, spanningapplications in entertainment, education, and professional communication.However, previous methods often overlook the necessity of comprehensivelyunderstanding both global and local contexts, leading to inaccurate andinconsistent edits in the spatiotemporal dimension, especially for long videos.In this paper, we introduce VIA, a unified spatiotemporal Video Adaptationframework for global and local video editing, pushing the limits ofconsistently editing minute-long videos. First, to ensure local consistencywithin individual frames, we designed test-time editing adaptation to adapt apre-trained image editing model for improving consistency between potentialediting directions and the text instruction, and adapts masked latent variablesfor precise local control. Furthermore, to maintain global consistency over thevideo sequence, we introduce spatiotemporal adaptation that recursively gatherconsistent attention variables in key frames and strategically applies themacross the whole sequence to realize the editing effects. Extensive experimentsdemonstrate that, compared to baseline methods, our VIA approach produces editsthat are more faithful to the source videos, more coherent in thespatiotemporal context, and more precise in local control. More importantly, weshow that VIA can achieve consistent long video editing in minutes, unlockingthe potential for advanced video editing tasks over long video sequences.
  </details>

- **[What Do You See? Enhancing Zero-Shot Image Classification with Multimodal Large Language Models](http://arxiv.org/abs/2405.15668v4)**  `arXiv:2405.15668`  
  _Abdelrahman Abdelhamed, Mahmoud Afifi, Alec Go_
  <details><summary>Abstract</summary>
  Large language models (LLMs) have been effectively used for many computervision tasks, including image classification. In this paper, we present asimple yet effective approach for zero-shot image classification usingmultimodal LLMs. Using multimodal LLMs, we generate comprehensive textualrepresentations from input images. These textual representations are thenutilized to generate fixed-dimensional features in a cross-modal embeddingspace. Subsequently, these features are fused together to perform zero-shotclassification using a linear classifier. Our method does not require promptengineering for each dataset; instead, we use a single, straightforward set ofprompts across all datasets. We evaluated our method on several datasets andour results demonstrate its remarkable effectiveness, surpassing benchmarkaccuracy on multiple datasets. On average, for ten benchmarks, our methodachieved an accuracy gain of 6.2 percentage points, with an increase of 6.8percentage points on the ImageNet dataset, compared to prior methodsre-evaluated with the same setup. Our findings highlight the potential ofmultimodal LLMs to enhance computer vision tasks such as zero-shot imageclassification, offering a significant improvement over traditional methods.
  </details>

- **[Image segmentation of treated and untreated tumor spheroids by Fully Convolutional Networks](http://arxiv.org/abs/2405.01105v3)**  `arXiv:2405.01105`  
  _Matthias Streller, So≈àa Michl√≠kov√°, Willy Ciecior, Katharina L√∂nnecke, Leoni A. Kunz-Schughart, Steffen Lange, et al._
  <details><summary>Abstract</summary>
  Multicellular tumor spheroids (MCTS) are advanced cell culture systems forassessing the impact of combinatorial radio(chemo)therapy. They exhibittherapeutically relevant in-vivo-like characteristics from 3D cell-cell andcell-matrix interactions to radial pathophysiological gradients related toproliferative activity and nutrient/oxygen supply, altering cellularradioresponse. State-of-the-art assays quantify long-term curative endpointsbased on collected brightfield image time series from large treated spheroidpopulations per irradiation dose and treatment arm. Here, spheroid controlprobabilities are documented analogous to in-vivo tumor control probabilitiesbased on Kaplan-Meier curves. This analyses require laborious spheroidsegmentation of up to 100.000 images per treatment arm to extract relevantstructural information from the images, e.g., diameter, area, volume andcircularity. While several image analysis algorithms are available for spheroidsegmentation, they all focus on compact MCTS with clearly distinguishable outerrim throughout growth. However, treated MCTS may partly be detached anddestroyed and are usually obscured by dead cell debris. We successfully traintwo Fully Convolutional Networks, UNet and HRNet, and optimize theirhyperparameters to develop an automatic segmentation for both untreated andtreated MCTS. We systematically validate the automatic segmentation on larger,independent data sets of spheroids derived from two human head-and-neck cancercell lines. We find an excellent overlap between manual and automaticsegmentation for most images, quantified by Jaccard indices at around 90%. Forimages with smaller overlap of the segmentations, we demonstrate that thiserror is comparable to the variations across segmentations from differentbiological experts, suggesting that these images represent biologically unclearor ambiguous cases.
  </details>

- **[Gaga: Group Any Gaussians via 3D-aware Memory Bank](http://arxiv.org/abs/2404.07977v2)**  `arXiv:2404.07977`  
  _Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, Ming-Hsuan Yang_
  <details><summary>Abstract</summary>
  We introduce Gaga, a framework that reconstructs and segments open-world 3Dscenes by leveraging inconsistent 2D masks predicted by zero-shotclass-agnostic segmentation models. Contrasted to prior 3D scene segmentationapproaches that rely on video object tracking or contrastive learning methods,Gaga utilizes spatial information and effectively associates object masksacross diverse camera poses through a novel 3D-aware memory bank. Byeliminating the assumption of continuous view changes in training images, Gagademonstrates robustness to variations in camera poses, particularly beneficialfor sparsely sampled images, ensuring precise mask label consistency.Furthermore, Gaga accommodates 2D segmentation masks from diverse sources anddemonstrates robust performance with different open-world zero-shotclass-agnostic segmentation models, significantly enhancing its versatility.Extensive qualitative and quantitative evaluations demonstrate that Gagaperforms favorably against state-of-the-art methods, emphasizing its potentialfor real-world applications such as 3D scene understanding and manipulation.
  </details>

- **[MoReVQA: Exploring Modular Reasoning Models for Video Question Answering](http://arxiv.org/abs/2404.06511v2)**  `arXiv:2404.06511`  
  _Juhong Min, Shyamal Buch, Arsha Nagrani, Minsu Cho, Cordelia Schmid_
  <details><summary>Abstract</summary>
  This paper addresses the task of video question answering (videoQA) via adecomposed multi-stage, modular reasoning framework. Previous modular methodshave shown promise with a single planning stage ungrounded in visual content.However, through a simple and effective baseline, we find that such systems canlead to brittle behavior in practice for challenging videoQA settings. Thus,unlike traditional single-stage planning methods, we propose a multi-stagesystem consisting of an event parser, a grounding stage, and a final reasoningstage in conjunction with an external memory. All stages are training-free, andperformed using few-shot prompting of large models, creating interpretableintermediate outputs at each stage. By decomposing the underlying planning andtask complexity, our method, MoReVQA, improves over prior work on standardvideoQA benchmarks (NExT-QA, iVQA, EgoSchema, ActivityNet-QA) withstate-of-the-art results, and extensions to related tasks (grounded videoQA,paragraph captioning).
  </details>

- **[Contextual AD Narration with Interleaved Multimodal Sequence](http://arxiv.org/abs/2403.12922v2)**  `arXiv:2403.12922`  
  _Hanlin Wang, Zhan Tong, Kecheng Zheng, Yujun Shen, Limin Wang_
  <details><summary>Abstract</summary>
  The Audio Description (AD) task aims to generate descriptions of visualelements for visually impaired individuals to help them access long-form videocontent, like movies. With video feature, text, character bank and contextinformation as inputs, the generated ADs are able to correspond to thecharacters by name and provide reasonable, contextual descriptions to helpaudience understand the storyline of movie. To achieve this goal, we propose toleverage pre-trained foundation models through a simple and unified frameworkto generate ADs with interleaved multimodal sequence as input, termed asUni-AD. To enhance the alignment of features across various modalities withfiner granularity, we introduce a simple and lightweight module that maps videofeatures into the textual feature space. Moreover, we also propose acharacter-refinement module to provide more precise information by identifyingthe main characters who play more significant roles in the video context. Withthese unique designs, we further incorporate contextual information and acontrastive loss into our architecture to generate smoother and morecontextually appropriate ADs. Experiments on multiple AD datasets show thatUni-AD performs well on AD generation, which demonstrates the effectiveness ofour approach. Our code is available at: https://github.com/ant-research/UniAD.
  </details>

- **[How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey](http://arxiv.org/abs/2402.13255v3)**  `arXiv:2402.13255`  
  _Fabio Tosi, Youmin Zhang, Ziren Gong, Erik Sandstr√∂m, Stefano Mattoccia, Martin R. Oswald, et al._
  <details><summary>Abstract</summary>
  Over the past two decades, research in the field of Simultaneous Localizationand Mapping (SLAM) has undergone a significant evolution, highlighting itscritical role in enabling autonomous exploration of unknown environments. Thisevolution ranges from hand-crafted methods, through the era of deep learning,to more recent developments focused on Neural Radiance Fields (NeRFs) and 3DGaussian Splatting (3DGS) representations. Recognizing the growing body ofresearch and the absence of a comprehensive survey on the topic, this paperaims to provide the first comprehensive overview of SLAM progress through thelens of the latest advancements in radiance fields. It sheds light on thebackground, evolutionary path, inherent strengths and limitations, and servesas a fundamental reference to highlight the dynamic progress and specificchallenges.
  </details>

- **[Towards Complementary Knowledge Distillation for Efficient Dense Image Prediction](http://arxiv.org/abs/2401.13174v3)**  `arXiv:2401.13174`  
  _Dong Zhang, Pingcheng Dong, Long Chen, Kwang-Ting Cheng_
  <details><summary>Abstract</summary>
  It has been revealed that small efficient dense image prediction (EDIP)models, trained using the knowledge distillation (KD) framework, encounter twokey challenges, including maintaining boundary region completeness andpreserving target region connectivity, despite their favorable capacity torecognize main object regions. In this work, we propose a complementaryboundary and context distillation (BCD) method within the KD framework forEDIPs, which facilitates the targeted knowledge transfer from large accurateteacher models to compact efficient student models. Specifically, the boundarydistillation component focuses on extracting explicit object-level semanticboundaries from the hierarchical feature maps of the backbone network toenhance the student model's mask quality in boundary regions. Concurrently, thecontext distillation component leverages self-relations as a bridge to transferimplicit pixel-level contexts from the teacher model to the student model,ensuring strong connectivity in target regions. Our proposed BCD method isspecifically designed for EDIP tasks and is characterized by its simplicity andefficiency. Extensive experimental results across semantic segmentation, objectdetection, and instance segmentation on various representative datasetsdemonstrate that our method can outperform existing methods without requiringextra supervisions or incurring increased inference costs, resulting inwell-defined object boundaries and smooth connecting regions.
  </details>

- **[GMTalker: Gaussian Mixture-based Audio-Driven Emotional Talking Video Portraits](http://arxiv.org/abs/2312.07669v3)**  `arXiv:2312.07669`  
  _Yibo Xia, Lizhen Wang, Xiang Deng, Xiaoyan Luo, Yunhong Wang, Yebin Liu_
  <details><summary>Abstract</summary>
  Synthesizing high-fidelity and emotion-controllable talking video portraits,with audio-lip sync, vivid expressions, realistic head poses, and eye blinks,has been an important and challenging task in recent years. Most existingmethods suffer in achieving personalized and precise emotion control, smoothtransitions between different emotion states, and the generation of diversemotions. To tackle these challenges, we present GMTalker, a Gaussianmixture-based emotional talking portraits generation framework. Specifically,we propose a Gaussian mixture-based expression generator that can construct acontinuous and disentangled latent space, achieving more flexible emotionmanipulation. Furthermore, we introduce a normalizing flow-based motiongenerator pretrained on a large dataset with a wide-range motion to generatediverse head poses, blinks, and eyeball movements. Finally, we propose apersonalized emotion-guided head generator with an emotion mapping network thatcan synthesize high-fidelity and faithful emotional video portraits. Bothquantitative and qualitative experiments demonstrate our method outperformsprevious methods in image quality, photo-realism, emotion accuracy, and motiondiversity.
  </details>

- **[Learning Multi-modal Representations by Watching Hundreds of Surgical Video Lectures](http://arxiv.org/abs/2307.15220v4)**  `arXiv:2307.15220`  
  _Kun Yuan, Vinkle Srivastav, Tong Yu, Joel L. Lavanchy, Jacques Marescaux, Pietro Mascagni, et al._
  <details><summary>Abstract</summary>
  Recent advancements in surgical computer vision applications have been drivenby vision-only models, which do not explicitly integrate the rich semantics oflanguage into their design. These methods rely on manually annotated surgicalvideos to predict a fixed set of object categories, limiting theirgeneralizability to unseen surgical procedures and downstream tasks. In thiswork, we put forward the idea that the surgical video lectures availablethrough open surgical e-learning platforms can provide effective vision andlanguage supervisory signals for multi-modal representation learning withoutrelying on manual annotations. We address the surgery-specific linguisticchallenges present in surgical video lectures by employing multiplecomplementary automatic speech recognition systems to generate texttranscriptions. We then present a novel method, SurgVLP - Surgical VisionLanguage Pre-training, for multi-modal representation learning. Extensiveexperiments across diverse surgical procedures and tasks demonstrate that themulti-modal representations learned by SurgVLP exhibit strong transferabilityand adaptability in surgical video analysis. Furthermore, our zero-shotevaluations highlight SurgVLP's potential as a general-purpose foundation modelfor surgical workflow analysis, reducing the reliance on extensive manualannotations for downstream tasks, and facilitating adaptation methods such asfew-shot learning to build a scalable and data-efficient solution for variousdownstream surgical applications. The [trainingcode](https://github.com/CAMMA-public/SurgVLP) and[weights](https://github.com/CAMMA-public/PeskaVLP) are public.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Machine Learning üìä

<details open><summary>Click to Collapse</summary>

- **[A Unified Framework for Diffusion Bridge Problems: Flow Matching and Schr√∂dinger Matching into One](http://arxiv.org/abs/2503.21756v1)**  `arXiv:2503.21756`  
  _Minyoung Kim_
  <details><summary>Abstract</summary>
  The bridge problem is to find an SDE (or sometimes an ODE) that bridges twogiven distributions. The application areas of the bridge problem are enormous,among which the recent generative modeling (e.g., conditional or unconditionalimage generation) is the most popular. Also the famous Schr\"{o}dinger bridgeproblem, a widely known problem for a century, is a special instance of thebridge problem. Two most popular algorithms to tackle the bridge problems inthe deep learning era are: (conditional) flow matching and iterative fittingalgorithms, where the former confined to ODE solutions, and the latterspecifically for the Schr\"{o}dinger bridge problem. The main contribution ofthis article is in two folds: i) We provide concise reviews of these algorithmswith technical details to some extent; ii) We propose a novel unifiedperspective and framework that subsumes these seemingly unrelated algorithms(and their variants) into one. In particular, we show that our unifiedframework can instantiate the Flow Matching (FM) algorithm, the (mini-batch)optimal transport FM algorithm, the (mini-batch) Schr\"{o}dinger bridge FMalgorithm, and the deep Schr\"{o}dinger bridge matching (DSBM) algorithm as itsspecial cases. We believe that this unified framework will be useful forviewing the bridge problems in a more general and flexible perspective, and inturn can help researchers and practitioners to develop new bridge algorithms intheir fields.
  </details>

- **[Energy Minimization for Participatory Federated Learning in IoT Analyzed via Game Theory](http://arxiv.org/abs/2503.21722v1)**  `arXiv:2503.21722`  
  _Alessandro Buratto, Elia Guerra, Marco Miozzo, Paolo Dini, Leonardo Badia_
  <details><summary>Abstract</summary>
  The Internet of Things requires intelligent decision making in manyscenarios. To this end, resources available at the individual nodes for sensingor computing, or both, can be leveraged. This results in approaches known asparticipatory sensing and federated learning, respectively. We investigate thesimultaneous implementation of both, through a distributed approach based onempowering local nodes with game theoretic decision making. A global objectiveof energy minimization is combined with the individual node's optimization oflocal expenditure for sensing and transmitting data over multiple learningrounds. We present extensive evaluations of this technique, based on both atheoretical framework and experiments in a simulated network scenario with realdata. Such a distributed approach can reach a desired level of accuracy forfederated learning without a centralized supervision of the data collector.However, depending on the weight attributed to the local costs of the singlenode, it may also result in a significantly high Price of Anarchy (from 1.28onwards). Thus, we argue for the need of incentive mechanisms, possibly basedon Age of Information of the single nodes.
  </details>

- **[Elementwise Layer Normalization](http://arxiv.org/abs/2503.21708v1)**  `arXiv:2503.21708`  
  _Felix Stollenwerk_
  <details><summary>Abstract</summary>
  A recent paper proposed Dynamic Tanh (DyT) as a drop-in replacement for LayerNormalization. Although the method is empirically well-motivated and appealingfrom a practical point of view, it lacks a theoretical foundation. In thiswork, we derive DyT mathematically and show that a well-defined approximationis needed to do so. By dropping said approximation, an alternative element-wisetransformation is obtained, which we call Elementwise Layer Normalization(ELN). We demonstrate that ELN resembles Layer Normalization more accuratelythan DyT does.
  </details>

- **[Learning to Represent Individual Differences for Choice Decision Making](http://arxiv.org/abs/2503.21704v1)**  `arXiv:2503.21704`  
  _Yan-Ying Chen, Yue Weng, Alexandre Filipowicz, Rumen Iliev, Francine Chen, Shabnam Hakimi, et al._
  <details><summary>Abstract</summary>
  Human decision making can be challenging to predict because decisions areaffected by a number of complex factors. Adding to this complexity,decision-making processes can differ considerably between individuals, andmethods aimed at predicting human decisions need to take individual differencesinto account. Behavioral science offers methods by which to measure individualdifferences (e.g., questionnaires, behavioral models), but these are oftennarrowed down to low dimensions and not tailored to specific prediction tasks.This paper investigates the use of representation learning to measureindividual differences from behavioral experiment data. Representation learningoffers a flexible approach to create individual embeddings from data that areboth structured (e.g., demographic information) and unstructured (e.g., freetext), where the flexibility provides more options for individual differencemeasures for personalization, e.g., free text responses may allow foropen-ended questions that are less privacy-sensitive. In the current paper weuse representation learning to characterize individual differences in humanperformance on an economic decision-making task. We demonstrate that modelsusing representation learning to capture individual differences consistentlyimprove decision predictions over models without representation learning, andeven outperform well-known theory-based behavioral models used in theseenvironments. Our results propose that representation learning offers a usefuland flexible tool to capture individual differences.
  </details>

- **[A tale of two goals: leveraging sequentiality in multi-goal scenarios](http://arxiv.org/abs/2503.21677v1)**  `arXiv:2503.21677`  
  _Olivier Serris, St√©phane Doncieux, Olivier Sigaud_
  <details><summary>Abstract</summary>
  Several hierarchical reinforcement learning methods leverage planning tocreate a graph or sequences of intermediate goals, guiding a lower-levelgoal-conditioned (GC) policy to reach some final goals. The low-level policy istypically conditioned on the current goal, with the aim of reaching it asquickly as possible. However, this approach can fail when an intermediate goalcan be reached in multiple ways, some of which may make it impossible tocontinue toward subsequent goals. To address this issue, we introduce twoinstances of Markov Decision Process (MDP) where the optimization objectivefavors policies that not only reach the current goal but also subsequent ones.In the first, the agent is conditioned on both the current and final goals,while in the second, it is conditioned on the next two goals in the sequence.We conduct a series of experiments on navigation and pole-balancing tasks inwhich sequences of intermediate goals are given. By evaluating policies trainedwith TD3+HER on both the standard GC-MDP and our proposed MDPs, we show that,in most cases, conditioning on the next two goals improves stability and sampleefficiency over other approaches.
  </details>

- **[Model Assembly Learning with Heterogeneous Layer Weight Merging](http://arxiv.org/abs/2503.21657v1)**  `arXiv:2503.21657`  
  _Yi-Kai Zhang, Jin Wang, Xu-Xiang Zhong, De-Chuan Zhan, Han-Jia Ye_
  <details><summary>Abstract</summary>
  Model merging acquires general capabilities without extra data or training bycombining multiple models' parameters. Previous approaches achieve linear modeconnectivity by aligning parameters into the same loss basin using permutationinvariance. In this paper, we introduce Model Assembly Learning (MAL), a novelparadigm for model merging that iteratively integrates parameters from diversemodels in an open-ended model zoo to enhance the base model's capabilities.Unlike previous works that require identical architectures, MAL allows themerging of heterogeneous architectures and selective parameters across layers.Specifically, the base model can incorporate parameters from different layersof multiple pre-trained models. We systematically investigate the conditionsand fundamental settings of heterogeneous parameter merging, addressing allpossible mismatches in layer widths between the base and target models.Furthermore, we establish key laws and provide practical guidelines foreffectively implementing MAL.
  </details>

- **[Data-Driven Extreme Response Estimation](http://arxiv.org/abs/2503.21638v1)**  `arXiv:2503.21638`  
  _Samuel J. Edwards, Michael D. Levine_
  <details><summary>Abstract</summary>
  A method to rapidly estimate extreme ship response events is developed inthis paper. The method involves training by a Long Short-Term Memory (LSTM)neural network to correct a lower-fidelity hydrodynamic model to the level of ahigher-fidelity simulation. More focus is placed on larger responses byisolating the time-series near peak events identified in the lower-fidelitysimulations and training on only the shorter time-series around the largeevent. The method is tested on the estimation of pitch time-series maxima inSea State 5 (significant wave height of 4.0 meters and modal period of 15.0seconds,) generated by a lower-fidelity hydrodynamic solver known as SimpleCodeand a higher-fidelity tool known as the Large Amplitude Motion Program (LAMP).The results are also compared with an LSTM trained without specialconsiderations for large events.
  </details>

- **[When Astronomy Meets AI: Manazel For Crescent Visibility Prediction in Morocco](http://arxiv.org/abs/2503.21634v1)**  `arXiv:2503.21634`  
  _Yassir Lairgi_
  <details><summary>Abstract</summary>
  The accurate determination of the beginning of each Hijri month is essentialfor religious, cultural, and administrative purposes. Manazel (The code anddatasets are available at https://github.com/lairgiyassir/manazel) addressesthis challenge in Morocco by leveraging 13 years of crescent visibility data torefine the ODEH criterion, a widely used standard for lunar crescent visibilityprediction. The study integrates two key features, the Arc of Vision (ARCV) andthe total width of the crescent (W), to enhance the accuracy of lunarvisibility assessments. A machine learning approach utilizing the LogisticRegression algorithm is employed to classify crescent visibility conditions,achieving a predictive accuracy of 98.83%. This data-driven methodology offersa robust and reliable framework for determining the start of the Hijri month,comparing different data classification tools, and improving the consistency oflunar calendar calculations in Morocco. The findings demonstrate theeffectiveness of machine learning in astronomical applications and highlightthe potential for further enhancements in the modeling of crescent visibility.
  </details>

- **[ClusterSC: Advancing Synthetic Control with Donor Selection](http://arxiv.org/abs/2503.21629v1)**  `arXiv:2503.21629`  
  _Saeyoung Rho, Andrew Tang, Noah Bergam, Rachel Cummings, Vishal Misra_
  <details><summary>Abstract</summary>
  In causal inference with observational studies, synthetic control (SC) hasemerged as a prominent tool. SC has traditionally been applied toaggregate-level datasets, but more recent work has extended its use toindividual-level data. As they contain a greater number of observed units, thisshift introduces the curse of dimensionality to SC. To address this, we proposeCluster Synthetic Control (ClusterSC), based on the idea that groups ofindividuals may exist where behavior aligns internally but diverges betweengroups. ClusterSC incorporates a clustering step to select only the relevantdonors for the target. We provide theoretical guarantees on the improvementsinduced by ClusterSC, supported by empirical demonstrations on synthetic andreal-world datasets. The results indicate that ClusterSC consistentlyoutperforms classical SC approaches.
  </details>

- **[Provable Reduction in Communication Rounds for Non-Smooth Convex Federated Learning](http://arxiv.org/abs/2503.21627v1)**  `arXiv:2503.21627`  
  _Karlo Palenzuela, Ali Dadras, Alp Yurtsever, Tommy L√∂fstedt_
  <details><summary>Abstract</summary>
  Multiple local steps are key to communication-efficient federated learning.However, theoretical guarantees for such algorithms, without dataheterogeneity-bounding assumptions, have been lacking in general non-smoothconvex problems. Leveraging projection-efficient optimization methods, wepropose FedMLS, a federated learning algorithm with provable improvements frommultiple local steps. FedMLS attains an $\epsilon$-suboptimal solution in$\mathcal{O}(1/\epsilon)$ communication rounds, requiring a total of$\mathcal{O}(1/\epsilon^2)$ stochastic subgradient oracle calls.
  </details>

- **[Leveraging Language Models for Analyzing Longitudinal Experiential Data in Education](http://arxiv.org/abs/2503.21617v1)**  `arXiv:2503.21617`  
  _Ahatsham Hayat, Bilal Khan, Mohammad Rashedul Hasan_
  <details><summary>Abstract</summary>
  We propose a novel approach to leveraging pre-trained language models (LMs)for early forecasting of academic trajectories in STEM students usinghigh-dimensional longitudinal experiential data. This data, which capturesstudents' study-related activities, behaviors, and psychological states, offersvaluable insights for forecasting-based interventions. Key challenges inhandling such data include high rates of missing values, limited dataset sizedue to costly data collection, and complex temporal variability acrossmodalities. Our approach addresses these issues through a comprehensive dataenrichment process, integrating strategies for managing missing values,augmenting data, and embedding task-specific instructions and contextual cuesto enhance the models' capacity for learning temporal patterns. Throughextensive experiments on a curated student learning dataset, we evaluate bothencoder-decoder and decoder-only LMs. While our findings show that LMseffectively integrate data across modalities and exhibit resilience to missingdata, they primarily rely on high-level statistical patterns rather thandemonstrating a deeper understanding of temporal dynamics. Furthermore, theirability to interpret explicit temporal information remains limited. This workadvances educational data science by highlighting both the potential andlimitations of LMs in modeling student trajectories for early interventionbased on longitudinal experiential data.
  </details>

- **[Critical Iterative Denoising: A Discrete Generative Model Applied to Graphs](http://arxiv.org/abs/2503.21592v1)**  `arXiv:2503.21592`  
  _Yoann Boget, Alexandros Kalousis_
  <details><summary>Abstract</summary>
  Discrete Diffusion and Flow Matching models have significantly advancedgenerative modeling for discrete structures, including graphs. However, thetime dependencies in the noising process of these models lead to erroraccumulation and propagation during the backward process. This issue,particularly pronounced in mask diffusion, is a known limitation in sequencemodeling and, as we demonstrate, also impacts discrete diffusion models forgraphs.  To address this problem, we propose a novel framework called IterativeDenoising, which simplifies discrete diffusion and circumvents the issue byassuming conditional independence across time. Additionally, we enhance ourmodel by incorporating a Critic, which during generation selectively retains orcorrupts elements in an instance based on their likelihood under the datadistribution. Our empirical evaluations demonstrate that the proposed methodsignificantly outperforms existing discrete diffusion baselines in graphgeneration tasks.
  </details>

- **[Generalizable Implicit Neural Representations via Parameterized Latent Dynamics for Baroclinic Ocean Forecasting](http://arxiv.org/abs/2503.21588v1)**  `arXiv:2503.21588`  
  _Guang Zhao, Xihaier Luo, Seungjun Lee, Yihui Ren, Shinjae Yoo, Luke Van Roekel, et al._
  <details><summary>Abstract</summary>
  Mesoscale ocean dynamics play a critical role in climate systems, governingheat transport, hurricane genesis, and drought patterns. However, simulatingthese processes at high resolution remains computationally prohibitive due totheir nonlinear, multiscale nature and vast spatiotemporal domains. Implicitneural representations (INRs) reduce the computational costs asresolution-independent surrogates but fail in many-query scenarios (inversemodeling) requiring rapid evaluations across diverse parameters. We presentPINROD, a novel framework combining dynamics-aware implicit neuralrepresentations with parameterized neural ordinary differential equations toaddress these limitations. By integrating parametric dependencies into latentdynamics, our method efficiently captures nonlinear oceanic behavior acrossvarying boundary conditions and physical parameters. Experiments on oceanmesoscale activity data show superior accuracy over existing baselines andimproved computational efficiency compared to standard numerical simulations.
  </details>

- **[Fusion of Graph Neural Networks via Optimal Transport](http://arxiv.org/abs/2503.21579v1)**  `arXiv:2503.21579`  
  _Weronika Ormaniec, Michael Vollenweider, Elisa Hoskovec_
  <details><summary>Abstract</summary>
  In this paper, we explore the idea of combining GCNs into one model. To thatend, we align the weights of different models layer-wise using optimaltransport (OT). We present and evaluate three types of transportation costs andshow that the studied fusion method consistently outperforms the performance ofvanilla averaging. Finally, we present results suggesting that model fusionusing OT is harder in the case of GCNs than MLPs and that incorporating thegraph structure into the process does not improve the performance of themethod.
  </details>

- **[Consistent Multigroup Low-Rank Approximation](http://arxiv.org/abs/2503.21563v1)**  `arXiv:2503.21563`  
  _Antonis Matakos, Martino Ciaperoni, Heikki Mannila_
  <details><summary>Abstract</summary>
  We consider the problem of consistent low-rank approximation for multigroupdata: we ask for a sequence of $k$ basis vectors such that projecting the dataonto their spanned subspace treats all groups as equally as possible, byminimizing the maximum error among the groups. Additionally, we require thatthe sequence of basis vectors satisfies the natural consistency property: whenlooking for the best $k$ vectors, the first $d<k$ vectors are the best possiblesolution to the problem of finding $d$ basis vectors. Thus, this multigrouplow-rank approximation method naturally generalizes \svd and reduces to \svdfor data with a single group. We give an iterative algorithm for this task thatsequentially adds to the basis the vector that gives the best rank$-1$projection according to the min-max criterion, and then projects the data ontothe orthogonal complement of that vector. For finding the best rank$-1$projection, we use primal-dual approaches or semidefinite programming. Weanalyze the theoretical properties of the algorithms and demonstrateempirically that the proposed methods compare favorably to existing methods formultigroup (or fair) PCA.
  </details>

- **[SyncSDE: A Probabilistic Framework for Diffusion Synchronization](http://arxiv.org/abs/2503.21555v1)**  `arXiv:2503.21555`  
  _Hyunjun Lee, Hyunsoo Lee, Sookwan Han_
  <details><summary>Abstract</summary>
  There have been many attempts to leverage multiple diffusion models forcollaborative generation, extending beyond the original domain. A prominentapproach involves synchronizing multiple diffusion trajectories by mixing theestimated scores to artificially correlate the generation processes. However,existing methods rely on naive heuristics, such as averaging, withoutconsidering task specificity. These approaches do not clarify why such methodswork and often fail when a heuristic suitable for one task is blindly appliedto others. In this paper, we present a probabilistic framework for analyzingwhy diffusion synchronization works and reveal where heuristics should befocused - modeling correlations between multiple trajectories and adapting themto each specific task. We further identify optimal correlation models per task,achieving better results than previous approaches that apply a single heuristicacross all tasks without justification.
  </details>

- **[Exploring the Energy Landscape of RBMs: Reciprocal Space Insights into Bosons, Hierarchical Learning and Symmetry Breaking](http://arxiv.org/abs/2503.21536v1)**  `arXiv:2503.21536`  
  _J. Quetzalc√≥atl Toledo-Marin, Anindita Maiti, Geoffrey C. Fox, Roger G. Melko_
  <details><summary>Abstract</summary>
  Deep generative models have become ubiquitous due to their ability to learnand sample from complex distributions. Despite the proliferation of variousframeworks, the relationships among these models remain largely unexplored, agap that hinders the development of a unified theory of AI learning. We addresstwo central challenges: clarifying the connections between different deepgenerative models and deepening our understanding of their learning mechanisms.We focus on Restricted Boltzmann Machines (RBMs), known for their universalapproximation capabilities for discrete distributions. By introducing areciprocal space formulation, we reveal a connection between RBMs, diffusionprocesses, and coupled Bosons. We show that at initialization, the RBM operatesat a saddle point, where the local curvature is determined by the singularvalues, whose distribution follows the Marcenko-Pastur law and exhibitsrotational symmetry. During training, this rotational symmetry is broken due tohierarchical learning, where different degrees of freedom progressively capturefeatures at multiple levels of abstraction. This leads to a symmetry breakingin the energy landscape, reminiscent of Landau theory. This symmetry breakingin the energy landscape is characterized by the singular values and the weightmatrix eigenvector matrix. We derive the corresponding free energy in amean-field approximation. We show that in the limit of infinite size RBM, thereciprocal variables are Gaussian distributed. Our findings indicate that inthis regime, there will be some modes for which the diffusion process will notconverge to the Boltzmann distribution. To illustrate our results, we trainedreplicas of RBMs with different hidden layer sizes using the MNIST dataset. Ourfindings bridge the gap between disparate generative frameworks and also shedlight on the processes underpinning learning in generative models.
  </details>

- **[Uncertainty-aware Bayesian machine learning modelling of land cover classification](http://arxiv.org/abs/2503.21510v1)**  `arXiv:2503.21510`  
  _Samuel Bilson, Anna Pustogvar_
  <details><summary>Abstract</summary>
  Land cover classification involves the production of land cover maps, whichdetermine the type of land through remote sensing imagery. Over recent years,such classification is being performed by machine learning classificationmodels, which can give highly accurate predictions on land cover per pixelusing large quantities of input training data. However, such models do notcurrently take account of input measurement uncertainty, which is vital fortraceability in metrology. In this work we propose a Bayesian classificationframework using generative modelling to take account of input measurementuncertainty. We take the specific case of Bayesian quadratic discriminantanalysis, and apply it to land cover datasets from Copernicus Sentinel-2 in2020 and 2021. We benchmark the performance of the model against more popularclassification models used in land cover maps such as random forests and neuralnetworks. We find that such Bayesian models are more trustworthy, in the sensethat they are more interpretable, explicitly model the input measurementuncertainty, and maintain predictive performance of class probability outputsacross datasets of different years and sizes, whilst also being computationallyefficient.
  </details>

- **[F-INR: Functional Tensor Decomposition for Implicit Neural Representations](http://arxiv.org/abs/2503.21507v1)**  `arXiv:2503.21507`  
  _Sai Karthikeya Vemuri, Tim B√ºchner, Joachim Denzler_
  <details><summary>Abstract</summary>
  Implicit Neural Representation (INR) has emerged as a powerful tool forencoding discrete signals into continuous, differentiable functions usingneural networks. However, these models often have an unfortunate reliance onmonolithic architectures to represent high-dimensional data, leading toprohibitive computational costs as dimensionality grows. We propose F-INR, aframework that reformulates INR learning through functional tensordecomposition, breaking down high-dimensional tasks into lightweight,axis-specific sub-networks. Each sub-network learns a low-dimensional datacomponent (e.g., spatial or temporal). Then, we combine these components viatensor operations, reducing forward pass complexity while improving accuracythrough specialized learning. F-INR is modular and, therefore,architecture-agnostic, compatible with MLPs, SIREN, WIRE, or otherstate-of-the-art INR architecture. It is also decomposition-agnostic,supporting CP, TT, and Tucker modes with user-defined rank for speed-accuracycontrol. In our experiments, F-INR trains $100\times$ faster than existingapproaches on video tasks while achieving higher fidelity (+3.4 dB PSNR).Similar gains hold for image compression, physics simulations, and 3D geometryreconstruction. Through this, F-INR offers a new scalable, flexible solutionfor high-dimensional signal modeling.
  </details>

- **[Adaptive Resampling with Bootstrap for Noisy Multi-Objective Optimization Problems](http://arxiv.org/abs/2503.21495v1)**  `arXiv:2503.21495`  
  _Timo Budszuhn, Mark Joachim Krallmann, Daniel Horn_
  <details><summary>Abstract</summary>
  The challenge of noisy multi-objective optimization lies in the constanttrade-off between exploring new decision points and improving the precision ofknown points through resampling. This decision should take into account boththe variability of the objective functions and the current estimate of a pointin relation to the Pareto front. Since the amount and distribution of noise aregenerally unknown, it is desirable for a decision function to be highlyadaptive to the properties of the optimization problem. This paper presents aresampling decision function that incorporates the stochastic nature of theoptimization problem by using bootstrapping and the probability of dominance.The distribution-free estimation of the probability of dominance is achievedusing bootstrap estimates of the means. To make the procedure applicable evenwith very few observations, we transfer the distribution observed at otherdecision points. The efficiency of this resampling approach is demonstrated byapplying it in the NSGA-II algorithm with a sequential resampling procedureunder multiple noise variations.
  </details>

- **[DATA-WA: Demand-based Adaptive Task Assignment with Dynamic Worker Availability Windows](http://arxiv.org/abs/2503.21458v1)**  `arXiv:2503.21458`  
  _Jinwen Chen, Jiannan Guo, Dazhuo Qiu, Yawen Li, Guanhua Ye, Yan Zhao, et al._
  <details><summary>Abstract</summary>
  With the rapid advancement of mobile networks and the widespread use ofmobile devices, spatial crowdsourcing, which involves assigning location-basedtasks to mobile workers, has gained significant attention. However, mostexisting research focuses on task assignment at the current moment, overlookingthe fluctuating demand and supply between tasks and workers over time. Toaddress this issue, we introduce an adaptive task assignment problem, whichaims to maximize the number of assigned tasks by dynamically adjusting taskassignments in response to changing demand and supply. We develop a spatialcrowdsourcing framework, namely demand-based adaptive task assignment withdynamic worker availability windows, which consists of two components includingtask demand prediction and task assignment. In the first component, weconstruct a graph adjacency matrix representing the demand dependencyrelationships in different regions and employ a multivariate time serieslearning approach to predict future task demands. In the task assignmentcomponent, we adjust tasks to workers based on these predictions, workeravailability windows, and the current task assignments, where each worker hasan availability window that indicates the time periods they are available fortask assignments. To reduce the search space of task assignments and beefficient, we propose a worker dependency separation approach based on graphpartition and a task value function with reinforcement learning. Experiments onreal data demonstrate that our proposals are both effective and efficient.
  </details>

- **[Stochastic Engrams for Efficient Continual Learning with Binarized Neural Networks](http://arxiv.org/abs/2503.21436v1)**  `arXiv:2503.21436`  
  _Isabelle Aguilar, Luis Fernando Herbozo Contreras, Omid Kavehei_
  <details><summary>Abstract</summary>
  The ability to learn continuously in artificial neural networks (ANNs) isoften limited by catastrophic forgetting, a phenomenon in which new knowledgebecomes dominant. By taking mechanisms of memory encoding in neuroscience (aka.engrams) as inspiration, we propose a novel approach that integratesstochastically-activated engrams as a gating mechanism for metaplasticbinarized neural networks (mBNNs). This method leverages the computationalefficiency of mBNNs combined with the robustness of probabilistic memory tracesto mitigate forgetting and maintain the model's reliability. Previouslyvalidated metaplastic optimization techniques have been incorporated to enhancesynaptic stability further. Compared to baseline binarized models and benchmarkfully connected continual learning approaches, our method is the only strategycapable of reaching average accuracies over 20% in class-incremental scenariosand achieving comparable domain-incremental results to full precisionstate-of-the-art methods. Furthermore, we achieve a significant reduction inpeak GPU and RAM usage, under 5% and 20%, respectively. Our findingsdemonstrate (A) an improved stability vs. plasticity trade-off, (B) a reducedmemory intensiveness, and (C) an enhanced performance in binarizedarchitectures. By uniting principles of neuroscience and efficient computing,we offer new insights into the design of scalable and robust deep learningsystems.
  </details>

- **[Nearest Neighbour Equilibrium Clustering](http://arxiv.org/abs/2503.21431v1)**  `arXiv:2503.21431`  
  _David P. Hofmeyr_
  <details><summary>Abstract</summary>
  A novel and intuitive nearest neighbours based clustering algorithm isintroduced, in which a cluster is defined in terms of an equilibrium conditionwhich balances its size and cohesiveness. The formulation of the equilibriumcondition allows for a quantification of the strength of alignment of eachpoint to a cluster, with these cluster alignment strengths leading naturally toa model selection criterion which renders the proposed approach fullyautomatable. The algorithm is simple to implement and computationallyefficient, and produces clustering solutions of extremely high quality incomparison with relevant benchmarks from the literature. R code to implementthe approach is available from https://github.com/DavidHofmeyr/NNEC.
  </details>

- **[AdvSGM: Differentially Private Graph Learning via Adversarial Skip-gram Model](http://arxiv.org/abs/2503.21426v1)**  `arXiv:2503.21426`  
  _Sen Zhang, Qingqing Ye, Haibo Hu, Jianliang Xu_
  <details><summary>Abstract</summary>
  The skip-gram model (SGM), which employs a neural network to generate nodevectors, serves as the basis for numerous popular graph embedding techniques.However, since the training datasets contain sensitive linkage information, theparameters of a released SGM may encode private information and posesignificant privacy risks. Differential privacy (DP) is a rigorous standard forprotecting individual privacy in data analysis. Nevertheless, when applyingdifferential privacy to skip-gram in graphs, it becomes highly challenging dueto the complex link relationships, which potentially result in high sensitivityand necessitate substantial noise injection. To tackle this challenge, wepresent AdvSGM, a differentially private skip-gram for graphs via adversarialtraining. Our core idea is to leverage adversarial training to privatizeskip-gram while improving its utility. Towards this end, we develop a noveladversarial training module by devising two optimizable noise terms thatcorrespond to the parameters of a skip-gram. By fine-tuning the weights betweenmodules within AdvSGM, we can achieve differentially private gradient updateswithout additional noise injection. Extensive experimental results on sixreal-world graph datasets show that AdvSGM preserves high data utility acrossdifferent downstream tasks.
  </details>

- **[ProHOC: Probabilistic Hierarchical Out-of-Distribution Classification via Multi-Depth Networks](http://arxiv.org/abs/2503.21397v1)**  `arXiv:2503.21397`  
  _Erik Wallin, Fredrik Kahl, Lars Hammarstrand_
  <details><summary>Abstract</summary>
  Out-of-distribution (OOD) detection in deep learning has traditionally beenframed as a binary task, where samples are either classified as belonging tothe known classes or marked as OOD, with little attention given to the semanticrelationships between OOD samples and the in-distribution (ID) classes. Wepropose a framework for detecting and classifying OOD samples in a given classhierarchy. Specifically, we aim to predict OOD data to their correct internalnodes of the class hierarchy, whereas the known ID classes should be predictedas their corresponding leaf nodes. Our approach leverages the class hierarchyto create a probabilistic model and we implement this model by using networkstrained for ID classification at multiple hierarchy depths. We conductexperiments on three datasets with predefined class hierarchies and show theeffectiveness of our method. Our code is available athttps://github.com/walline/prohoc.
  </details>

- **[Investigating the Duality of Interpretability and Explainability in Machine Learning](http://arxiv.org/abs/2503.21356v1)**  `arXiv:2503.21356`  
  _Moncef Garouani, Josiane Mothe, Ayah Barhrhouj, Julien Aligon_
  <details><summary>Abstract</summary>
  The rapid evolution of machine learning (ML) has led to the widespreadadoption of complex "black box" models, such as deep neural networks andensemble methods. These models exhibit exceptional predictive performance,making them invaluable for critical decision-making across diverse domainswithin society. However, their inherently opaque nature raises concerns abouttransparency and interpretability, making them untrustworthy decision supportsystems. To alleviate such a barrier to high-stakes adoption, researchcommunity focus has been on developing methods to explain black box models as ameans to address the challenges they pose. Efforts are focused on explainingthese models instead of developing ones that are inherently interpretable.Designing inherently interpretable models from the outset, however, can pavethe path towards responsible and beneficial applications in the field of ML. Inthis position paper, we clarify the chasm between explaining black boxes andadopting inherently interpretable models. We emphasize the imperative need formodel interpretability and, following the purpose of attaining better (i.e.,more effective or efficient w.r.t. predictive performance) and trustworthypredictors, provide an experimental evaluation of latest hybrid learningmethods that integrates symbolic knowledge into neural network predictors. Wedemonstrate how interpretable hybrid models could potentially supplant blackbox ones in different domains.
  </details>

- **[Scalable Expectation Estimation with Subtractive Mixture Models](http://arxiv.org/abs/2503.21346v1)**  `arXiv:2503.21346`  
  _Lena Zellinger, Nicola Branchini, V√≠ctor Elvira, Antonio Vergari_
  <details><summary>Abstract</summary>
  Many Monte Carlo (MC) and importance sampling (IS) methods use mixture models(MMs) for their simplicity and ability to capture multimodal distributions.Recently, subtractive mixture models (SMMs), i.e. MMs with negativecoefficients, have shown greater expressiveness and success in generativemodeling. However, their negative parameters complicate sampling, requiringcostly auto-regressive techniques or accept-reject algorithms that do not scalein high dimensions. In this work, we use the difference representation of SMMsto construct an unbiased IS estimator ($\Delta\text{Ex}$) that removes the needto sample from the SMM, enabling high-dimensional expectation estimation withSMMs. In our experiments, we show that $\Delta\text{Ex}$ can achieve comparableestimation quality to auto-regressive sampling while being considerably fasterin MC estimation. Moreover, we conduct initial experiments with$\Delta\text{Ex}$ using hand-crafted proposals, gaining first insights into howto construct safe proposals for $\Delta\text{Ex}$.
  </details>

- **[Tricking Retrievers with Influential Tokens: An Efficient Black-Box Corpus Poisoning Attack](http://arxiv.org/abs/2503.21315v1)**  `arXiv:2503.21315`  
  _Cheng Wang, Yiwei Wang, Yujun Cai, Bryan Hooi_
  <details><summary>Abstract</summary>
  Retrieval-augmented generation (RAG) systems enhance large language models byincorporating external knowledge, addressing issues like outdated internalknowledge and hallucination. However, their reliance on external knowledgebases makes them vulnerable to corpus poisoning attacks, where adversarialpassages can be injected to manipulate retrieval results. Existing methods forcrafting such passages, such as random token replacement or training inversionmodels, are often slow and computationally expensive, requiring either accessto retriever's gradients or large computational resources. To address theselimitations, we propose Dynamic Importance-Guided Genetic Algorithm (DIGA), anefficient black-box method that leverages two key properties of retrievers:insensitivity to token order and bias towards influential tokens. By focusingon these characteristics, DIGA dynamically adjusts its genetic operations togenerate effective adversarial passages with significantly reduced time andmemory usage. Our experimental evaluation shows that DIGA achieves superiorefficiency and scalability compared to existing methods, while maintainingcomparable or better attack success rates across multiple datasets.
  </details>

- **[HOT: Hadamard-based Optimized Training](http://arxiv.org/abs/2503.21261v1)**  `arXiv:2503.21261`  
  _Seonggon Kim, Juncheol Shin, Seung-taek Woo, Eunhyeok Park_
  <details><summary>Abstract</summary>
  It has become increasingly important to optimize backpropagation to reducememory usage and computational overhead. Achieving this goal is highlychallenging, as multiple objectives must be considered jointly whilemaintaining training quality. In this paper, we focus on matrix multiplication,which accounts for the largest portion of training costs, and analyze itsbackpropagation in detail to identify lightweight techniques that offer thebest benefits. Based on this analysis, we introduce a novel method,Hadamard-based Optimized Training (HOT). In this approach, we applyHadamard-based optimizations, such as Hadamard quantization and Hadamardlow-rank approximation, selectively and with awareness of the suitability ofeach optimization for different backward paths. Additionally, we introduce twoenhancements: activation buffer compression and layer-wise quantizer selection.Our extensive analysis shows that HOT achieves up to 75% memory savings and a2.6 times acceleration on real GPUs, with negligible accuracy loss compared toFP32 precision.
  </details>

- **[Dual-Splitting Conformal Prediction for Multi-Step Time Series Forecasting](http://arxiv.org/abs/2503.21251v1)**  `arXiv:2503.21251`  
  _Qingdi Yu, Zhiwei Cao, Ruihang Wang, Zhen Yang, Lijun Deng, Min Hu, et al._
  <details><summary>Abstract</summary>
  Time series forecasting is crucial for applications like resource schedulingand risk management, where multi-step predictions provide a comprehensive viewof future trends. Uncertainty Quantification (UQ) is a mainstream approach foraddressing forecasting uncertainties, with Conformal Prediction (CP) gainingattention due to its model-agnostic nature and statistical guarantees. However,most variants of CP are designed for single-step predictions and facechallenges in multi-step scenarios, such as reliance on real-time data andlimited scalability. This highlights the need for CP methods specificallytailored to multi-step forecasting. We propose the Dual-Splitting ConformalPrediction (DSCP) method, a novel CP approach designed to capture inherentdependencies within time-series data for multi-step forecasting. Experimentalresults on real-world datasets from four different domains demonstrate that theproposed DSCP significantly outperforms existing CP variants in terms of theWinkler Score, achieving a performance improvement of up to 23.59% compared tostate-of-the-art methods. Furthermore, we deployed the DSCP approach forrenewable energy generation and IT load forecasting in power management of areal-world trajectory-based application, achieving an 11.25% reduction incarbon emissions through predictive optimization of data center operations andcontrols.
  </details>

- **[Improving $(Œ±, f)$-Byzantine Resilience in Federated Learning via layerwise aggregation and cosine distance](http://arxiv.org/abs/2503.21244v1)**  `arXiv:2503.21244`  
  _Mario Garc√≠a-M√°rquez, Nuria Rodr√≠guez-Barroso, M. Victoria Luz√≥n, Francisco Herrera_
  <details><summary>Abstract</summary>
  The rapid development of artificial intelligence systems has amplifiedsocietal concerns regarding their usage, necessitating regulatory frameworksthat encompass data privacy. Federated Learning (FL) is posed as potentialsolution to data privacy challenges in distributed machine learning by enablingcollaborative model training {without data sharing}. However, FL systems remainvulnerable to Byzantine attacks, where malicious nodes contribute corruptedmodel updates. While Byzantine Resilient operators have emerged as a widelyadopted robust aggregation algorithm to mitigate these attacks, its efficacydiminishes significantly in high-dimensional parameter spaces, sometimesleading to poor performing models. This paper introduces Layerwise CosineAggregation, a novel aggregation scheme designed to enhance robustness of theserules in such high-dimensional settings while preserving computationalefficiency. A theoretical analysis is presented, demonstrating the superiorrobustness of the proposed Layerwise Cosine Aggregation compared to originalrobust aggregation operators. Empirical evaluation across diverse imageclassification datasets, under varying data distributions and Byzantine attackscenarios, consistently demonstrates the improved performance of LayerwiseCosine Aggregation, achieving up to a 16% increase in model accuracy.
  </details>

- **[Feature-Enhanced Machine Learning for All-Cause Mortality Prediction in Healthcare Data](http://arxiv.org/abs/2503.21241v1)**  `arXiv:2503.21241`  
  _HyeYoung Lee, Pavel Tsoi_
  <details><summary>Abstract</summary>
  Accurate patient mortality prediction enables effective risk stratification,leading to personalized treatment plans and improved patient outcomes. However,predicting mortality in healthcare remains a significant challenge, withexisting studies often focusing on specific diseases or limited predictor sets.This study evaluates machine learning models for all-cause in-hospitalmortality prediction using the MIMIC-III database, employing a comprehensivefeature engineering approach. Guided by clinical expertise and literature, weextracted key features such as vital signs (e.g., heart rate, blood pressure),laboratory results (e.g., creatinine, glucose), and demographic information.The Random Forest model achieved the highest performance with an AUC of 0.94,significantly outperforming other machine learning and deep learningapproaches. This demonstrates Random Forest's robustness in handlinghigh-dimensional, noisy clinical data and its potential for developingeffective clinical decision support tools. Our findings highlight theimportance of careful feature engineering for accurate mortality prediction. Weconclude by discussing implications for clinical adoption and propose futuredirections, including enhancing model robustness and tailoring predictionmodels for specific diseases.
  </details>

- **[Efficient Learning for Entropy-regularized Markov Decision Processes via Multilevel Monte Carlo](http://arxiv.org/abs/2503.21224v1)**  `arXiv:2503.21224`  
  _Matthieu Meunier, Christoph Reisinger, Yufei Zhang_
  <details><summary>Abstract</summary>
  Designing efficient learning algorithms with complexity guarantees for Markovdecision processes (MDPs) with large or continuous state and action spacesremains a fundamental challenge. We address this challenge forentropy-regularized MDPs with Polish state and action spaces, assuming accessto a generative model of the environment. We propose a novel family ofmultilevel Monte Carlo (MLMC) algorithms that integrate fixed-point iterationwith MLMC techniques and a generic stochastic approximation of the Bellmanoperator. We quantify the precise impact of the chosen approximate Bellmanoperator on the accuracy of the resulting MLMC estimator. Leveraging this erroranalysis, we show that using a biased plain MC estimate for the Bellmanoperator results in quasi-polynomial sample complexity, whereas an unbiasedrandomized multilevel approximation of the Bellman operator achieves polynomialsample complexity in expectation. Notably, these complexity bounds areindependent of the dimensions or cardinalities of the state and action spaces,distinguishing our approach from existing algorithms whose complexities scalewith the sizes of these spaces. We validate these theoretical performanceguarantees through numerical experiments.
  </details>

- **[Rethinking Graph Structure Learning in the Era of LLMs](http://arxiv.org/abs/2503.21223v1)**  `arXiv:2503.21223`  
  _Zhihan Zhang, Xunkai Li, Guang Zeng, Hongchao Qin, Ronghua Li, Guoren Wang_
  <details><summary>Abstract</summary>
  Recently, the emergence of large language models (LLMs) has promptedresearchers to explore the integration of language descriptions into graphs,aiming to enhance model encoding capabilities from a data-centric perspective.This graph representation is called text-attributed graphs (TAGs). A review ofprior advancements highlights that graph structure learning (GSL) is a pivotaltechnique for improving data utility, making it highly relevant to efficientTAG learning. However, most GSL methods are tailored for traditional graphswithout textual information, underscoring the necessity of developing a new GSLparadigm. Despite clear motivations, it remains challenging: (1) How can wedefine a reasonable optimization objective for GSL in the era of LLMs,considering the massive parameters in LLM? (2) How can we design an efficientmodel architecture that enables seamless integration of LLM for thisoptimization objective? For Question 1, we reformulate existing GSLoptimization objectives as a tree optimization framework, shifting the focusfrom obtaining a well-trained edge predictor to a language-aware tree sampler.For Question 2, we propose decoupled and training-free model design principlesfor LLM integration, shifting the focus from computation-intensive fine-tuningto more efficient inference. Based on this, we propose Large Language and TreeAssistant (LLaTA), which leverages tree-based LLM in-context learning toenhance the understanding of topology and text, enabling reliable inference andgenerating improved graph structure. Extensive experiments on 10 TAG datasetsdemonstrate that LLaTA enjoys flexibility - incorporated with any backbone;scalability - outperforms other LLM-based GSL methods in terms of runningefficiency; effectiveness - achieves SOTA performance.
  </details>

- **[Resource-Efficient Federated Fine-Tuning Large Language Models for Heterogeneous Data](http://arxiv.org/abs/2503.21213v1)**  `arXiv:2503.21213`  
  _Jun Liu, Yunming Liao, Hongli Xu, Yang Xu_
  <details><summary>Abstract</summary>
  Fine-tuning large language models (LLMs) via federated learning, i.e.,FedLLM, has been proposed to adapt LLMs for various downstream applications ina privacy-preserving way. To reduce the fine-tuning costs onresource-constrained devices, FedLoRA is proposed to fine-tune only a smallsubset of model parameters by integrating low-rank adaptation (LoRA) intoFedLLM. However, apart from resource constraints, there is still anothercritical challenge, i.e., data heterogeneity, severely hindering theimplementation of FedLoRA in practical applications. Herein, inspired by theprevious group-based federated learning paradigm, we propose a hierarchicalFedLoRA framework, termed HierFedLoRA, to address these challenges.Specifically, HierFedLoRA partitions all devices into multiple near-IID groupsand adjusts the intra-group aggregation frequency for each group to eliminatethe negative effects of non-IID data. Meanwhile, to reduce the computation andcommunication cost, HierFedLoRA dynamically assigns diverse and suitablefine-tuning depth (i.e., the number of continuous fine-tuning layers from theoutput) for each group. HierFedLoRA explores jointly optimizing aggregationfrequency and depth upon their coupled relationship to better enhance theperformance of FedLoRA. Extensive experiments are conducted on a physicalplatform with 80 commercial devices. The results show that HierFedLoRA improvesthe final model accuracy by 1.6% to 4.2%, speeding up the fine-tuning processby at least 2.1$\times$, compared to the strong baselines.
  </details>

- **[Learning Generalizable Skills from Offline Multi-Task Data for Multi-Agent Cooperation](http://arxiv.org/abs/2503.21200v1)**  `arXiv:2503.21200`  
  _Sicong Liu, Yang Shu, Chenjuan Guo, Bin Yang_
  <details><summary>Abstract</summary>
  Learning cooperative multi-agent policy from offline multi-task data that cangeneralize to unseen tasks with varying numbers of agents and targets is anattractive problem in many scenarios. Although aggregating general behaviorpatterns among multiple tasks as skills to improve policy transfer is apromising approach, two primary challenges hinder the further advancement ofskill learning in offline multi-task MARL. Firstly, extracting generalcooperative behaviors from various action sequences as common skills lacksbringing cooperative temporal knowledge into them. Secondly, existing worksonly involve common skills and can not adaptively choose independent knowledgeas task-specific skills in each task for fine-grained action execution. Totackle these challenges, we propose Hierarchical and Separate Skill Discovery(HiSSD), a novel approach for generalizable offline multi-task MARL throughskill learning. HiSSD leverages a hierarchical framework that jointly learnscommon and task-specific skills. The common skills learn cooperative temporalknowledge and enable in-sample exploitation for offline multi-task MARL. Thetask-specific skills represent the priors of each task and achieve atask-guided fine-grained action execution. To verify the advancement of ourmethod, we conduct experiments on multi-agent MuJoCo and SMAC benchmarks. Aftertraining the policy using HiSSD on offline multi-task data, the empiricalresults show that HiSSD assigns effective cooperative behaviors and obtainssuperior performance in unseen tasks.
  </details>

- **[Unveiling the Potential of Superexpressive Networks in Implicit Neural Representations](http://arxiv.org/abs/2503.21166v1)**  `arXiv:2503.21166`  
  _Uvini Balasuriya Mudiyanselage, Woojin Cho, Minju Jo, Noseong Park, Kookjin Lee_
  <details><summary>Abstract</summary>
  In this study, we examine the potential of one of the ``superexpressive''networks in the context of learning neural functions for representing complexsignals and performing machine learning downstream tasks. Our focus is onevaluating their performance on computer vision and scientific machine learningtasks including signal representation/inverse problems and solutions of partialdifferential equations. Through an empirical investigation in various benchmarktasks, we demonstrate that superexpressive networks, as proposed by [Zhang etal. NeurIPS, 2022], which employ a specialized network structure characterizedby having an additional dimension, namely width, depth, and ``height'', cansurpass recent implicit neural representations that use highly-specializednonlinear activation functions.
  </details>

- **[A Data Balancing and Ensemble Learning Approach for Credit Card Fraud Detection](http://arxiv.org/abs/2503.21160v1)**  `arXiv:2503.21160`  
  _Yuhan Wang_
  <details><summary>Abstract</summary>
  This research introduces an innovative method for identifying credit cardfraud by combining the SMOTE-KMEANS technique with an ensemble machine learningmodel. The proposed model was benchmarked against traditional models such aslogistic regression, decision trees, random forests, and support vectormachines. Performance was evaluated using metrics, including accuracy, recall,and area under the curve (AUC). The results demonstrated that the proposedmodel achieved superior performance, with an AUC of 0.96 when combined with theSMOTE-KMEANS algorithm. This indicates a significant improvement in detectingfraudulent transactions while maintaining high precision and recall. The studyalso explores the application of different oversampling techniques to enhancethe performance of various classifiers. The findings suggest that the proposedmethod is robust and effective for classification tasks on balanced datasets.Future research directions include further optimization of the SMOTE-KMEANSapproach and its integration into existing fraud detection systems to enhancefinancial security and consumer protection.
  </details>

- **[Multi-Objective Optimization for Privacy-Utility Balance in Differentially Private Federated Learning](http://arxiv.org/abs/2503.21159v1)**  `arXiv:2503.21159`  
  _Kanishka Ranaweera, David Smith, Pubudu N. Pathirana, Ming Ding, Thierry Rakotoarivelo, Aruna Seneviratne_
  <details><summary>Abstract</summary>
  Federated learning (FL) enables collaborative model training acrossdistributed clients without sharing raw data, making it a promising approachfor privacy-preserving machine learning. However, ensuring differential privacy(DP) in FL presents challenges due to the trade-off between model utility andprivacy protection. Clipping gradients before aggregation is a common strategyto limit privacy loss, but selecting an optimal clipping norm is non-trivial,as excessively high values compromise privacy, while overly restrictiveclipping degrades model performance. In this work, we propose an adaptiveclipping mechanism that dynamically adjusts the clipping norm using amulti-objective optimization framework. By integrating privacy and utilityconsiderations into the optimization objective, our approach balances privacypreservation with model accuracy. We theoretically analyze the convergenceproperties of our method and demonstrate its effectiveness through extensiveexperiments on MNIST, Fashion-MNIST, and CIFAR-10 datasets. Our results showthat adaptive clipping consistently outperforms fixed-clipping baselines,achieving improved accuracy under the same privacy constraints. This workhighlights the potential of dynamic clipping strategies to enhanceprivacy-utility trade-offs in differentially private federated learning.
  </details>

- **[Real-Time Evaluation Models for RAG: Who Detects Hallucinations Best?](http://arxiv.org/abs/2503.21157v1)**  `arXiv:2503.21157`  
  _Ashish Sardana_
  <details><summary>Abstract</summary>
  This article surveys Evaluation models to automatically detect hallucinationsin Retrieval-Augmented Generation (RAG), and presents a comprehensive benchmarkof their performance across six RAG applications. Methods included in our studyinclude: LLM-as-a-Judge, Prometheus, Lynx, the Hughes Hallucination EvaluationModel (HHEM), and the Trustworthy Language Model (TLM). These approaches areall reference-free, requiring no ground-truth answers/labels to catch incorrectLLM responses. Our study reveals that, across diverse RAG applications, some ofthese approaches consistently detect incorrect RAG responses with highprecision/recall.
  </details>

- **[Embedding Domain-Specific Knowledge from LLMs into the Feature Engineering Pipeline](http://arxiv.org/abs/2503.21155v1)**  `arXiv:2503.21155`  
  _Jo√£o Eduardo Batista_
  <details><summary>Abstract</summary>
  Feature engineering is mandatory in the machine learning pipeline to obtainrobust models. While evolutionary computation is well-known for its greatresults both in feature selection and feature construction, its methods arecomputationally expensive due to the large number of evaluations required toinduce the final model. Part of the reason why these algorithms require a largenumber of evaluations is their lack of domain-specific knowledge, resulting ina lot of random guessing during evolution. In this work, we propose using LargeLanguage Models (LLMs) as an initial feature construction step to add knowledgeto the dataset. By doing so, our results show that the evolution can convergefaster, saving us computational resources. The proposed approach only providesthe names of the features in the dataset and the target objective to the LLM,making it usable even when working with datasets containing private data. Whileconsistent improvements to test performance were only observed for one-third ofthe datasets (CSS, PM, and IM10), possibly due to problems being easilyexplored by LLMs, this approach only decreased the model performance in 1/77test cases. Additionally, this work introduces the M6GP feature engineeringalgorithm to symbolic regression, showing it can improve the results of therandom forest regressor and produce competitive results with its predecessor,M3GP.
  </details>

- **[Federated Learning with Differential Privacy: An Utility-Enhanced Approach](http://arxiv.org/abs/2503.21154v1)**  `arXiv:2503.21154`  
  _Kanishka Ranaweera, Dinh C. Nguyen, Pubudu N. Pathirana, David Smith, Ming Ding, Thierry Rakotoarivelo, et al._
  <details><summary>Abstract</summary>
  Federated learning has emerged as an attractive approach to protect dataprivacy by eliminating the need for sharing clients' data while reducingcommunication costs compared with centralized machine learning algorithms.However, recent studies have shown that federated learning alone does notguarantee privacy, as private data may still be inferred from the uploadedparameters to the central server. In order to successfully avoid data leakage,adopting differential privacy (DP) in the local optimization process or in thelocal update aggregation process has emerged as two feasible ways for achievingsample-level or user-level privacy guarantees respectively, in federatedlearning models. However, compared to their non-private equivalents, theseapproaches suffer from a poor utility. To improve the privacy-utilitytrade-off, we present a modification to these vanilla differentially privatealgorithms based on a Haar wavelet transformation step and a novel noiseinjection scheme that significantly lowers the asymptotic bound of the noisevariance. We also present a holistic convergence analysis of our proposedalgorithm, showing that our method yields better convergence performance thanthe vanilla DP algorithms. Numerical experiments on real-world datasetsdemonstrate that our method outperforms existing approaches in model utilitywhile maintaining the same privacy guarantees.
  </details>

- **[MoQa: Rethinking MoE Quantization with Multi-stage Data-model Distribution Awareness](http://arxiv.org/abs/2503.21135v1)**  `arXiv:2503.21135`  
  _Zihao Zheng, Xiuping Cui, Size Zheng, Maoliang Li, Jiayu Chen, Yun, et al._
  <details><summary>Abstract</summary>
  With the advances in artificial intelligence, Mix-of-Experts (MoE) has becomethe main form of Large Language Models (LLMs), and its demand for modelcompression is increasing. Quantization is an effective method that not onlycompresses the models but also significantly accelerates their performance.Existing quantization methods have gradually shifted the focus from parameterscaling to the analysis of data distributions. However, their analysis isdesigned for dense LLMs and relies on the simple one-model-all-data mapping,which is unsuitable for MoEs. This paper proposes a new quantization frameworkcalled MoQa. MoQa decouples the data-model distribution complexity of MoEs inmultiple analysis stages, quantitively revealing the dynamics during sparsedata activation, data-parameter mapping, and inter-expert correlations. Basedon these, MoQa identifies particular experts' and parameters' significance withoptimal data-model distribution awareness and proposes a series of fine-grainedmix-quantization strategies adaptive to various data activation and expertcombination scenarios. Moreover, MoQa discusses the limitations of existingquantization and analyzes the impact of each stage analysis, showing novelinsights for MoE quantization. Experiments show that MoQa achieves a 1.69~2.18perplexity decrease in language modeling tasks and a 1.58%~8.91% accuracyimprovement in zero-shot inference tasks. We believe MoQa will play a role infuture MoE construction, optimization, and compression.
  </details>

- **[AugWard: Augmentation-Aware Representation Learning for Accurate Graph Classification](http://arxiv.org/abs/2503.21105v1)**  `arXiv:2503.21105`  
  _Minjun Kim, Jaehyeon Choi, SeungJoo Lee, Jinhong Jung, U Kang_
  <details><summary>Abstract</summary>
  How can we accurately classify graphs? Graph classification is a pivotal taskin data mining with applications in social network analysis, web analysis, drugdiscovery, molecular property prediction, etc. Graph neural networks haveachieved the state-of-the-art performance in graph classification, but theyconsistently struggle with overfitting. To mitigate overfitting, researchershave introduced various representation learning methods utilizing graphaugmentation. However, existing methods rely on simplistic use of graphaugmentation, which loses augmentation-induced differences and limits theexpressiveness of representations.  In this paper, we propose AugWard (Augmentation-Aware Training with GraphDistance and Consistency Regularization), a novel graph representation learningframework that carefully considers the diversity introduced by graphaugmentation. AugWard applies augmentation-aware training to predict the graphdistance between the augmented graph and its original one, aligning therepresentation difference directly with graph distance at both feature andstructure levels. Furthermore, AugWard employs consistency regularization toencourage the classifier to handle richer representations. Experimental resultsshow that AugWard gives the state-of-the-art performance in supervised,semi-supervised graph classification, and transfer learning.
  </details>

- **[Low Stein Discrepancy via Message-Passing Monte Carlo](http://arxiv.org/abs/2503.21103v1)**  `arXiv:2503.21103`  
  _Nathan Kirk, T. Konstantin Rusch, Jakob Zech, Daniela Rus_
  <details><summary>Abstract</summary>
  Message-Passing Monte Carlo (MPMC) was recently introduced as a novellow-discrepancy sampling approach leveraging tools from geometric deeplearning. While originally designed for generating uniform point sets, weextend this framework to sample from general multivariate probabilitydistributions with known probability density function. Our proposed method,Stein-Message-Passing Monte Carlo (Stein-MPMC), minimizes a kernelized Steindiscrepancy, ensuring improved sample quality. Finally, we show that Stein-MPMCoutperforms competing methods, such as Stein Variational Gradient Descent and(greedy) Stein Points, by achieving a lower Stein discrepancy.
  </details>

- **[Confidence Adjusted Surprise Measure for Active Resourceful Trials (CA-SMART): A Data-driven Active Learning Framework for Accelerating Material Discovery under Resource Constraints](http://arxiv.org/abs/2503.21095v1)**  `arXiv:2503.21095`  
  _Ahmed Shoyeb Raihan, Zhichao Liu, Tanveer Hossain Bhuiyan, Imtiaz Ahmed_
  <details><summary>Abstract</summary>
  Accelerating the discovery and manufacturing of advanced materials withspecific properties is a critical yet formidable challenge due to vast searchspace, high costs of experiments, and time-intensive nature of materialcharacterization. In recent years, active learning, where a surrogate machinelearning (ML) model mimics the scientific discovery process of a humanscientist, has emerged as a promising approach to address these challenges byguiding experimentation toward high-value outcomes with a limited budget. Amongthe diverse active learning philosophies, the concept of surprise (capturingthe divergence between expected and observed outcomes) has demonstratedsignificant potential to drive experimental trials and refine predictivemodels. Scientific discovery often stems from surprise thereby making it anatural driver to guide the search process. Despite its promise, prior studiesleveraging surprise metrics such as Shannon and Bayesian surprise lackmechanisms to account for prior confidence, leading to excessive exploration ofuncertain regions that may not yield useful information. To address this, wepropose the Confidence-Adjusted Surprise Measure for Active Resourceful Trials(CA-SMART), a novel Bayesian active learning framework tailored for optimizingdata-driven experimentation. On a high level, CA-SMART incorporatesConfidence-Adjusted Surprise (CAS) to dynamically balance exploration andexploitation by amplifying surprises in regions where the model is more certainwhile discounting them in highly uncertain areas. We evaluated CA-SMART on twobenchmark functions (Six-Hump Camelback and Griewank) and in predicting thefatigue strength of steel. The results demonstrate superior accuracy andefficiency compared to traditional surprise metrics, standard BayesianOptimization (BO) acquisition functions and conventional ML methods.
  </details>

- **[Geographical hotspot prediction based on point cloud-voxel-community partition clustering](http://arxiv.org/abs/2503.21084v1)**  `arXiv:2503.21084`  
  _Yan Tang_
  <details><summary>Abstract</summary>
  Existing solutions to the hotspot prediction problem in the field ofgeographic information remain at a relatively preliminary stage. This studypresents a novel approach for detecting and predicting geographical hotspots,utilizing point cloud-voxel-community partition clustering. By analyzinghigh-dimensional data, we represent spatial information through point clouds,which are then subdivided into multiple voxels to enhance analyticalefficiency. Our method identifies spatial voxels with similar characteristicsthrough community partitioning, thereby revealing underlying patterns inhotspot distributions. Experimental results indicate that when applied to adataset of archaeological sites in Turkey, our approach achieves a 19.31%increase in processing speed, with an accuracy loss of merely 6%, outperformingtraditional clustering methods. This method not only provides a freshperspective for hotspot prediction but also serves as an effective tool forhigh-dimensional data analysis.
  </details>

- **[Uncertainty propagation in feed-forward neural network models](http://arxiv.org/abs/2503.21059v1)**  `arXiv:2503.21059`  
  _Jeremy Diamzon, Daniele Venturi_
  <details><summary>Abstract</summary>
  We develop new uncertainty propagation methods for feed-forward neuralnetwork architectures with leaky ReLu activation functions subject to randomperturbations in the input vectors. In particular, we derive analyticalexpressions for the probability density function (PDF) of the neural networkoutput and its statistical moments as a function of the input uncertainty andthe parameters of the network, i.e., weights and biases. A key finding is thatan appropriate linearization of the leaky ReLu activation function yieldsaccurate statistical results even for large perturbations in the input vectors.This can be attributed to the way information propagates through the network.We also propose new analytically tractable Gaussian copula surrogate models toapproximate the full joint PDF of the neural network output. To validate ourtheorical results, we conduct Monte Carlo simulations and a thorough erroranalysis on a multi-layer neural network representing a nonlinearintegro-differential operator between two polynomial function spaces. Ourfindings demonstrate excellent agreement between the theoretical predictionsand Monte Carlo simulations.
  </details>

- **[An Empirical Study of the Impact of Federated Learning on Machine Learning Model Accuracy](http://arxiv.org/abs/2503.20768v2)**  `arXiv:2503.20768`  
  _Haotian Yang, Zhuoran Wang, Benson Chou, Sophie Xu, Hao Wang, Jingxian Wang, et al._
  <details><summary>Abstract</summary>
  Federated Learning (FL) enables distributed ML model training on private userdata at the global scale. Despite the potential of FL demonstrated in manydomains, an in-depth view of its impact on model accuracy remains unclear. Inthis paper, we investigate, systematically, how this learning paradigm canaffect the accuracy of state-of-the-art ML models for a variety of ML tasks. Wepresent an empirical study that involves various data types: text, image,audio, and video, and FL configuration knobs: data distribution, FL scale,client sampling, and local and global computations. Our experiments areconducted in a unified FL framework to achieve high fidelity, with substantialhuman efforts and resource investments. Based on the results, we perform aquantitative analysis of the impact of FL, and highlight challenging scenarioswhere applying FL degrades the accuracy of the model drastically and identifycases where the impact is negligible. The detailed and extensive findings canbenefit practical deployments and future development of FL.
  </details>

- **[DR-PETS: Learning-Based Control With Planning in Adversarial Environments](http://arxiv.org/abs/2503.20660v2)**  `arXiv:2503.20660`  
  _Hozefa Jesawada, Antonio Acernese, Giovanni Russo, Carmen Del Vecchio_
  <details><summary>Abstract</summary>
  Ensuring robustness against epistemic, possibly adversarial, perturbations isessential for reliable real-world decision-making. While the ProbabilisticEnsembles with Trajectory Sampling (PETS) algorithm inherently handlesuncertainty via ensemble-based probabilistic models, it lacks guaranteesagainst structured adversarial or worst-case uncertainty distributions. Toaddress this, we propose DR-PETS, a distributionally robust extension of PETSthat certifies robustness against adversarial perturbations. We formalizeuncertainty via a p-Wasserstein ambiguity set, enabling worst-case-awareplanning through a min-max optimization framework. While PETS passivelyaccounts for stochasticity, DR-PETS actively optimizes robustness via atractable convex approximation integrated into PETS planning loop. Experimentson pendulum stabilization and cart-pole balancing show that DR-PETS certifiesrobustness against adversarial parameter perturbations, achieving consistentperformance in worst-case scenarios where PETS deteriorates.
  </details>

- **[ScalingNoise: Scaling Inference-Time Search for Generating Infinite Videos](http://arxiv.org/abs/2503.16400v2)**  `arXiv:2503.16400`  
  _Haolin Yang, Feilong Tang, Ming Hu, Yulong Li, Yexin Liu, Zelin Peng, et al._
  <details><summary>Abstract</summary>
  Video diffusion models (VDMs) facilitate the generation of high-qualityvideos, with current research predominantly concentrated on scaling effortsduring training through improvements in data quality, computational resources,and model complexity. However, inference-time scaling has received lessattention, with most approaches restricting models to a single generationattempt. Recent studies have uncovered the existence of "golden noises" thatcan enhance video quality during generation. Building on this, we find thatguiding the scaling inference-time search of VDMs to identify better noisecandidates not only evaluates the quality of the frames generated in thecurrent step but also preserves the high-level object features by referencingthe anchor frame from previous multi-chunks, thereby delivering long-termvalue. Our analysis reveals that diffusion models inherently possess flexibleadjustments of computation by varying denoising steps, and even a one-stepdenoising approach, when guided by a reward signal, yields significantlong-term benefits. Based on the observation, we proposeScalingNoise, aplug-and-play inference-time search strategy that identifies golden initialnoises for the diffusion sampling process to improve global content consistencyand visual diversity. Specifically, we perform one-step denoising to convertinitial noises into a clip and subsequently evaluate its long-term value,leveraging a reward model anchored by previously generated content. Moreover,to preserve diversity, we sample candidates from a tilted noise distributionthat up-weights promising noises. In this way, ScalingNoise significantlyreduces noise-induced errors, ensuring more coherent and spatiotemporallyconsistent video generation. Extensive experiments on benchmark datasetsdemonstrate that the proposed ScalingNoise effectively improves long videogeneration.
  </details>

- **[Time and Memory Trade-off of KV-Cache Compression in Tensor Transformer Decoding](http://arxiv.org/abs/2503.11108v2)**  `arXiv:2503.11108`  
  _Yifang Chen, Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Yu Tian_
  <details><summary>Abstract</summary>
  The key-value (KV) cache in the tensor version of transformers presents asignificant bottleneck during inference. While previous work analyzes thefundamental space complexity barriers in standard attention mechanisms [Harisand Onak, 2025], our work generalizes the space complexity barriers result totensor attention version. Our theoretical contributions rely on a reductionfrom communication complexity and deduce the memory lower bound fortensor-structured attention mechanisms when $d = \Omega(\log n)$. Furthermore,we introduce two types of tensor attention cache and present a trade-offbetween time and memory for two scenarios. Overall, our work provides atheoretical foundation for us to understand the time-memory tradeoff ofKV-Cache compression in tensor attention decoding and offers more perspectivesin developing more memory-efficient tensor attention Transformer architectures.
  </details>

- **[Deep Cut-informed Graph Embedding and Clustering](http://arxiv.org/abs/2503.06635v2)**  `arXiv:2503.06635`  
  _Zhiyuan Ning, Zaitian Wang, Ran Zhang, Ping Xu, Kunpeng Liu, Pengyang Wang, et al._
  <details><summary>Abstract</summary>
  Graph clustering aims to divide the graph into different clusters. Therecently emerging deep graph clustering approaches are largely built on graphneural networks (GNN). However, GNN is designed for general graph encoding andthere is a common issue of representation collapse in existing GNN-based deepgraph clustering algorithms. We attribute two main reasons for such issues: (i)the inductive bias of GNN models: GNNs tend to generate similar representationsfor proximal nodes. Since graphs often contain a non-negligible amount ofinter-cluster links, the bias results in error message passing and leads tobiased clustering; (ii) the clustering guided loss function: most traditionalapproaches strive to make all samples closer to pre-learned cluster centers,which causes a degenerate solution assigning all data points to a single labelthus make all samples and less discriminative. To address these challenges, weinvestigate graph clustering from a graph cut perspective and propose aninnovative and non-GNN-based Deep Cut-informed Graph embedding and Clusteringframework, namely DCGC. This framework includes two modules: (i) cut-informedgraph encoding; (ii) self-supervised graph clustering via optimal transport.For the encoding module, we derive a cut-informed graph embedding objective tofuse graph structure and attributes by minimizing their joint normalized cut.For the clustering module, we utilize the optimal transport theory to obtainthe clustering assignments, which can balance the guidance of "proximity to thepre-learned cluster center". With the above two tailored designs, DCGC is moresuitable for the graph clustering task, which can effectively alleviate theproblem of representation collapse and achieve better performance. We conductextensive experiments to demonstrate that our method is simple but effectivecompared with benchmarks.
  </details>

- **[GNNMerge: Merging of GNN Models Without Accessing Training Data](http://arxiv.org/abs/2503.03384v2)**  `arXiv:2503.03384`  
  _Vipul Garg, Ishita Thakre, Sayan Ranu_
  <details><summary>Abstract</summary>
  Model merging has gained prominence in machine learning as a method tointegrate multiple trained models into a single model without accessing theoriginal training data. While existing approaches have demonstrated success indomains such as computer vision and NLP, their application to Graph NeuralNetworks (GNNs) remains unexplored. These methods often rely on the assumptionof shared initialization, which is seldom applicable to GNNs. In this work, weundertake the first benchmarking study of model merging algorithms for GNNs,revealing their limited effectiveness in this context. To address thesechallenges, we propose GNNMerge, which utilizes a task-agnostic node embeddingalignment strategy to merge GNNs. Furthermore, we establish that under a mildrelaxation, the proposed optimization objective admits direct analyticalsolutions for widely used GNN architectures, significantly enhancing itscomputational efficiency. Empirical evaluations across diverse datasets, tasks,and architectures establish GNNMerge to be up to 24% more accurate thanexisting methods while delivering over 2 orders of magnitude speed-up comparedto training from scratch.
  </details>

- **[Improving clustering quality evaluation in noisy Gaussian mixtures](http://arxiv.org/abs/2503.00379v2)**  `arXiv:2503.00379`  
  _Renato Cordeiro de Amorim, Vladimir Makarenkov_
  <details><summary>Abstract</summary>
  Clustering is a well-established technique in machine learning and dataanalysis, widely used across various domains. Cluster validity indices, such asthe Average Silhouette Width, Calinski-Harabasz, and Davies-Bouldin indices,play a crucial role in assessing clustering quality when external ground truthlabels are unavailable. However, these measures can be affected by the featurerelevance issue, potentially leading to unreliable evaluations inhigh-dimensional or noisy data sets.  We introduce a theoretically grounded Feature Importance Rescaling (FIR)method that enhances the quality of clustering validation by adjusting featurecontributions based on their dispersion. It attenuates noise features,clarifies clustering compactness and separation, and thereby aligns clusteringvalidation more closely with the ground truth. Through extensive experiments onsynthetic data sets under different configurations, we demonstrate that FIRconsistently improves the correlation between the values of cluster validityindices and the ground truth, particularly in settings with noisy or irrelevantfeatures.  The results show that FIR increases the robustness of clustering evaluation,reduces variability in performance across different data sets, and remainseffective even when clusters exhibit significant overlap. These findingshighlight the potential of FIR as a valuable enhancement of clusteringvalidation, making it a practical tool for unsupervised learning tasks wherelabelled data is unavailable.
  </details>

- **[Starjob: Dataset for LLM-Driven Job Shop Scheduling](http://arxiv.org/abs/2503.01877v2)**  `arXiv:2503.01877`  
  _Henrik Abgaryan, Tristan Cazenave, Ararat Harutyunyan_
  <details><summary>Abstract</summary>
  Large Language Models (LLMs) have shown remarkable capabilities acrossvarious domains, but their potential for solving combinatorial optimizationproblems remains largely unexplored. In this paper, we investigate theapplicability of LLMs to the Job Shop Scheduling Problem (JSSP), a classicchallenge in combinatorial optimization that requires efficient job allocationto machines to minimize makespan. To this end, we introduce Starjob, the firstsupervised dataset for JSSP, comprising 130k instances specifically designedfor training LLMs. Leveraging this dataset, we fine-tune the LLaMA 8B 4-bitquantized model with the LoRA method to develop an end-to-end schedulingapproach. Our evaluation on standard benchmarks demonstrates that the proposedLLM-based method not only surpasses traditional Priority Dispatching Rules(PDRs) but also achieves notable improvements over state-of-the-art neuralapproaches like L2D, with an average improvement of 15.36% on DMU and 7.85% onTaillard benchmarks. These results highlight the untapped potential of LLMs intackling combinatorial optimization problems, paving the way for futureadvancements in this area.
  </details>

- **[TSKANMixer: Kolmogorov-Arnold Networks with MLP-Mixer Model for Time Series Forecasting](http://arxiv.org/abs/2502.18410v2)**  `arXiv:2502.18410`  
  _Young-Chae Hong, Bei Xiao, Yangho Chen_
  <details><summary>Abstract</summary>
  Time series forecasting has long been a focus of research across diversefields, including economics, energy, healthcare, and traffic management. Recentworks have introduced innovative architectures for time series models, such asthe Time-Series Mixer (TSMixer), which leverages multi-layer perceptrons (MLPs)to enhance prediction accuracy by effectively capturing both spatial andtemporal dependencies within the data. In this paper, we investigate thecapabilities of the Kolmogorov-Arnold Networks (KANs) for time-seriesforecasting by modifying TSMixer with a KAN layer (TSKANMixer). Experimentalresults demonstrate that TSKANMixer tends to improve prediction accuracy overthe original TSMixer across multiple datasets, ranking among the top-performingmodels compared to other time series approaches. Our results show that the KANsare promising alternatives to improve the performance of time seriesforecasting by replacing or extending traditional MLPs.
  </details>

- **[Probabilistic neural operators for functional uncertainty quantification](http://arxiv.org/abs/2502.12902v2)**  `arXiv:2502.12902`  
  _Christopher B√ºlte, Philipp Scholl, Gitta Kutyniok_
  <details><summary>Abstract</summary>
  Neural operators aim to approximate the solution operator of a system ofdifferential equations purely from data. They have shown immense success inmodeling complex dynamical systems across various domains. However, theoccurrence of uncertainties inherent in both model and data has so far rarelybeen taken into account\textemdash{}a critical limitation in complex, chaoticsystems such as weather forecasting. In this paper, we introduce theprobabilistic neural operator (PNO), a framework for learning probabilitydistributions over the output function space of neural operators. PNO extendsneural operators with generative modeling based on strictly proper scoringrules, integrating uncertainty information directly into the training process.We provide a theoretical justification for the approach and demonstrateimproved performance in quantifying uncertainty across different domains andwith respect to different baselines. Furthermore, PNO requires minimaladjustment to existing architectures, shows improved performance for mostprobabilistic prediction tasks, and leads to well-calibrated predictivedistributions and adequate uncertainty representations even for long dynamicaltrajectories. Implementing our approach into large-scale models for physicalapplications can lead to improvements in corresponding uncertaintyquantification and extreme event identification, ultimately leading to a deeperunderstanding of the prediction of such surrogate models.
  </details>

- **[Rethinking the Global Knowledge of CLIP in Training-Free Open-Vocabulary Semantic Segmentation](http://arxiv.org/abs/2502.06818v2)**  `arXiv:2502.06818`  
  _Jingyun Wang, Cilin Yan, Guoliang Kang_
  <details><summary>Abstract</summary>
  Recent works modify CLIP to perform open-vocabulary semantic segmentation ina training-free manner (TF-OVSS). In vanilla CLIP, patch-wise imagerepresentations mainly encode homogeneous image-level properties, which hindersthe application of CLIP to the dense prediction task. Previous TF-OVSS workssacrifice globality to enhance the locality of CLIP features, by making eachpatch mainly attend to itself or its neighboring patches within a narrow localwindow. With their modifications,the ability of CLIP to aggregate globalcontext information is largely weakened. Differently, in this paper, we rethinkthe global knowledge encoded by CLIP and propose GCLIP to answer how to extractand utilize beneficial global knowledge of CLIP for TF-OVSS. As therepresentation of each patch is finally determined by the attention weights andthe Value embeddings, we propose to reshape the last-block attention and Valueembeddings to aggregate useful global context into final features. Firstly, weaim to equip the last-block attention with image-level properties while notintroducing homogeneous attention patterns across patches. To realize the goal,we fuse the attention from the global-token emerging blocks with theQuery-Query attention. Secondly, we aim to make Value embeddings of thelast-block attention module more semantically correlated. To realize this, wedesign a novel channel suppression strategy.Extensive experiments on fivestandard benchmarks demonstrate that our method consistently outperformsprevious state-of-the-arts.
  </details>

- **[Inductive-Associative Meta-learning Pipeline with Human Cognitive Patterns for Unseen Drug-Target Interaction Prediction](http://arxiv.org/abs/2501.16391v2)**  `arXiv:2501.16391`  
  _Xiaoqing Lian, Jie Zhu, Tianxu Lv, Shiyun Nie, Hang Fan, Guosheng Wu, et al._
  <details><summary>Abstract</summary>
  Significant differences in protein structures hinder the generalization ofexisting drug-target interaction (DTI) models, which often rely heavily onpre-learned binding principles or detailed annotations. In contrast, BioBridgedesigns an Inductive-Associative pipeline inspired by the workflow ofscientists who base their accumulated expertise on drawing insights into noveldrug-target pairs from weakly related references. BioBridge predicts noveldrug-target interactions using limited sequence data, incorporating multi-levelencoders with adversarial training to accumulate transferable bindingprinciples. On these principles basis, BioBridge employs a dynamic prototypemeta-learning framework to associate insights from weakly related annotations,enabling robust predictions for previously unseen drug-target pairs. Extensiveexperiments demonstrate that BioBridge surpasses existing models, especiallyfor unseen proteins. Notably, when only homologous protein binding data isavailable, BioBridge proves effective for virtual screening of the epidermalgrowth factor receptor and adenosine receptor, underscoring its potential indrug discovery.
  </details>

- **[Pretraining with random noise for uncertainty calibration](http://arxiv.org/abs/2412.17411v2)**  `arXiv:2412.17411`  
  _Jeonghwan Cheon, Se-Bum Paik_
  <details><summary>Abstract</summary>
  Uncertainty calibration is crucial for various machine learning applications,yet it remains challenging. Many models exhibit hallucinations - confident yetinaccurate responses - due to miscalibrated confidence. Here, we show that thecommon practice of random initialization in deep learning, often considered astandard technique, is an underlying cause of this miscalibration, leading toexcessively high confidence in untrained networks. Our method, inspired bydevelopmental neuroscience, addresses this issue by simply pretraining networkswith random noise and labels, reducing overconfidence and bringing initialconfidence levels closer to chance. This ensures optimal calibration, aligningconfidence with accuracy during subsequent data training, without the need foradditional pre- or post-processing. Pre-calibrated networks excel atidentifying "unknown data," showing low confidence for out-of-distributioninputs, thereby resolving confidence miscalibration.
  </details>

- **[GNN-Transformer Cooperative Architecture for Trustworthy Graph Contrastive Learning](http://arxiv.org/abs/2412.16218v4)**  `arXiv:2412.16218`  
  _Jianqing Liang, Xinkai Wei, Min Chen, Zhiqiang Wang, Jiye Liang_
  <details><summary>Abstract</summary>
  Graph contrastive learning (GCL) has become a hot topic in the field of graphrepresentation learning. In contrast to traditional supervised learning relyingon a large number of labels, GCL exploits augmentation strategies to generatemultiple views and positive/negative pairs, both of which greatly influence theperformance. Unfortunately, commonly used random augmentations may disturb theunderlying semantics of graphs. Moreover, traditional GNNs, a type of widelyemployed encoders in GCL, are inevitably confronted with over-smoothing andover-squashing problems. To address these issues, we propose GNN-TransformerCooperative Architecture for Trustworthy Graph Contrastive Learning (GTCA),which inherits the advantages of both GNN and Transformer, incorporating graphtopology to obtain comprehensive graph representations. Theoretical analysisverifies the trustworthiness of the proposed method. Extensive experiments onbenchmark datasets demonstrate state-of-the-art empirical performance.
  </details>

- **[PAPAYA Federated Analytics Stack: Engineering Privacy, Scalability and Practicality](http://arxiv.org/abs/2412.02340v2)**  `arXiv:2412.02340`  
  _Harish Srinivas, Graham Cormode, Mehrdad Honarkhah, Samuel Lurye, Jonathan Hehir, Lunwen He, et al._
  <details><summary>Abstract</summary>
  Cross-device Federated Analytics (FA) is a distributed computation paradigmdesigned to answer analytics queries about and derive insights from data heldlocally on users' devices. On-device computations combined with other privacyand security measures ensure that only minimal data is transmitted off-device,achieving a high standard of data protection. Despite FA's broad relevance, theapplicability of existing FA systems is limited by compromised accuracy; lackof flexibility for data analytics; and an inability to scale effectively. Inthis paper, we describe our approach to combine privacy, scalability, andpracticality to build and deploy a system that overcomes these limitations. OurFA system leverages trusted execution environments (TEEs) and optimizes the useof on-device computing resources to facilitate federated data processing acrosslarge fleets of devices, while ensuring robust, defensible, and verifiableprivacy safeguards. We focus on federated analytics (statistics andmonitoring), in contrast to systems for federated learning (ML workloads), andwe flag the key differences.
  </details>

- **[ATM: Improving Model Merging by Alternating Tuning and Merging](http://arxiv.org/abs/2411.03055v3)**  `arXiv:2411.03055`  
  _Luca Zhou, Daniele Solombrino, Donato Crisostomi, Maria Sofia Bucarelli, Fabrizio Silvestri, Emanuele Rodol√†_
  <details><summary>Abstract</summary>
  Model merging has recently emerged as a cost-efficient paradigm formulti-task learning. Among current approaches, task arithmetic stands out forits simplicity and effectiveness. In this paper, we motivate the effectivenessof task vectors by linking them to multi-task gradients. We show that in asingle-epoch scenario, if the optimization is performed via gradient descent,task vectors are after one step mathematically equivalent to the gradientsobtained via gradient descent in a multi-task setting, and still approximatethese gradients in subsequent epochs. Furthermore, we show that theeffectiveness of task vectors is largely driven by the first epoch's gradient.Given this parallel between task vectors and gradients, we propose viewingmodel merging as a single step in an iterative process that alternates betweentuning and merging (ATM). We then propose two ways to utilize ATM. The first isto replace multi-task learning with ATM in scenarios where data sharing isprohibited, such as federated learning. The second is to improve the outcome ofany model merging algorithm by applying a few post-hoc iterations of ATM on asmall validation dataset, which is commonly available for hyperparametertuning. Finally, we provide both empirical and theoretical support for theeffectiveness of ATM, demonstrating that it minimizes an upper bound on theloss obtained by jointly finetuning all tasks.
  </details>

- **[LSEAttention is All You Need for Time Series Forecasting](http://arxiv.org/abs/2410.23749v4)**  `arXiv:2410.23749`  
  _Dizhen Liang_
  <details><summary>Abstract</summary>
  Transformer-based architectures have achieved remarkable success in naturallanguage processing and computer vision. However, their performance inmultivariate long-term forecasting often falls short compared to simpler linearbaselines. Previous research has identified the traditional attention mechanismas a key factor limiting their effectiveness in this domain. To bridge thisgap, we introduce LATST, a novel approach designed to mitigate entropy collapseand training instability common challenges in Transformer-based time seriesforecasting. We rigorously evaluate LATST across multiple real-worldmultivariate time series datasets, demonstrating its ability to outperformexisting state-of-the-art Transformer models. Notably, LATST manages to achievecompetitive performance with fewer parameters than some linear models oncertain datasets, highlighting its efficiency and effectiveness.
  </details>

- **[Evaluating the effects of Data Sparsity on the Link-level Bicycling Volume Estimation: A Graph Convolutional Neural Network Approach](http://arxiv.org/abs/2410.08522v2)**  `arXiv:2410.08522`  
  _Mohit Gupta, Debjit Bhowmick, Meead Saberi, Shirui Pan, Ben Beck_
  <details><summary>Abstract</summary>
  Accurate bicycling volume estimation is crucial for making informed decisionsand planning about future investments in bicycling infrastructure. However,traditional link-level volume estimation models are effective for motorizedtraffic but face significant challenges when applied to the bicycling contextbecause of sparse data and the intricate nature of bicycling mobility patterns.To the best of our knowledge, we present the first study to utilize a GraphConvolutional Network (GCN) architecture to model link-level bicycling volumesand systematically investigate the impact of varying levels of data sparsity(0%--99%) on model performance, simulating real-world scenarios. We haveleveraged Strava Metro data as the primary source of bicycling counts across15,933 road segments/links in the City of Melbourne, Australia. To evaluate theeffectiveness of the GCN model, we benchmark it against traditional machinelearning models, such as linear regression, support vector machines, and randomforest. Our results show that the GCN model outperforms these traditionalmodels in predicting Annual Average Daily Bicycle (AADB) counts, demonstratingits ability to capture the spatial dependencies inherent in bicycle trafficnetworks. While GCN remains robust up to 80% sparsity, its performance declinessharply beyond this threshold, highlighting the challenges of extreme datasparsity. These findings underscore the potential of GCNs in enhancingbicycling volume estimation, while also emphasizing the need for furtherresearch on methods to improve model resilience under high-sparsity conditions.Our findings offer valuable insights for city planners aiming to improvebicycling infrastructure and promote sustainable transportation.
  </details>

- **[Self-Contrastive Forward-Forward Algorithm](http://arxiv.org/abs/2409.11593v2)**  `arXiv:2409.11593`  
  _Xing Chen, Dongshu Liu, Jeremie Laydevant, Julie Grollier_
  <details><summary>Abstract</summary>
  Agents that operate autonomously benefit from lifelong learning capabilities.However, compatible training algorithms must comply with the decentralizednature of these systems, which imposes constraints on both the parameter countsand the computational resources. The Forward-Forward (FF) algorithm is one ofthese. FF relies only on feedforward operations, the same used for inference,for optimizing layer-wise objectives. This purely forward approach eliminatesthe need for transpose operations required in traditional backpropagation.Despite its potential, FF has failed to reach state-of-the-art performance onmost standard benchmark tasks, in part due to unreliable negative datageneration methods for unsupervised learning.  In this work, we propose the Self-Contrastive Forward-Forward (SCFF)algorithm, a competitive training method aimed at closing this performance gap.Inspired by standard self-supervised contrastive learning for vision tasks,SCFF generates positive and negative inputs applicable across various datasets.The method demonstrates superior performance compared to existing unsupervisedlocal learning algorithms on several benchmark datasets, including MNIST,CIFAR-10, STL-10, and Tiny ImageNet. We extend FF's application to trainingrecurrent neural networks, expanding its utility to sequential data tasks.These findings pave the way for high-accuracy, real-time learning onresource-constrained edge devices.
  </details>

- **[Joint Estimation and Prediction of City-wide Delivery Demand: A Large Language Model Empowered Graph-based Learning Approach](http://arxiv.org/abs/2408.17258v3)**  `arXiv:2408.17258`  
  _Tong Nie, Junlin He, Yuewen Mei, Guoyang Qin, Guilong Li, Jian Sun, et al._
  <details><summary>Abstract</summary>
  The proliferation of e-commerce and urbanization has significantlyintensified delivery operations in urban areas, boosting the volume andcomplexity of delivery demand. Data-driven predictive methods, especially thoseutilizing machine learning techniques, have emerged to handle thesecomplexities in urban delivery demand management problems. One particularlypressing issue that has yet to be sufficiently addressed is the jointestimation and prediction of city-wide delivery demand, as well as thegeneralization of the model to new cities. To this end, we formulate thisproblem as a transferable graph-based spatiotemporal learning task. First, anindividual-collective message-passing neural network model is formalized tocapture the interaction between demand patterns of associated regions. Second,by exploiting recent advances in large language models (LLMs), we extractgeneral geospatial knowledge encodings from the unstructured locational datausing the embedding generated by LLMs. Last, to encourage the cross-citygeneralization of the model, we integrate the encoding into the demandpredictor in a transferable way. Comprehensive empirical evaluation results ontwo real-world delivery datasets, including eight cities in China and the US,demonstrate that our model significantly outperforms state-of-the-art baselinesin accuracy, efficiency, and transferability.
  </details>

- **[Neural Exploratory Landscape Analysis for Meta-Black-Box-Optimization](http://arxiv.org/abs/2408.10672v3)**  `arXiv:2408.10672`  
  _Zeyuan Ma, Jiacheng Chen, Hongshu Guo, Yue-Jiao Gong_
  <details><summary>Abstract</summary>
  Recent research in Meta-Black-Box Optimization (MetaBBO) have shown thatmeta-trained neural networks can effectively guide the design of black-boxoptimizers, significantly reducing the need for expert tuning and deliveringrobust performance across complex problem distributions. Despite their success,a paradox remains: MetaBBO still rely on human-crafted Exploratory LandscapeAnalysis features to inform the meta-level agent about the low-leveloptimization progress. To address the gap, this paper proposes NeuralExploratory Landscape Analysis (NeurELA), a novel framework that dynamicallyprofiles landscape features through a two-stage, attention-based neuralnetwork, executed in an entirely end-to-end fashion. NeurELA is pre-trainedover a variety of MetaBBO algorithms using a multi-task neuroevolutionstrategy. Extensive experiments show that NeurELA achieves consistentlysuperior performance when integrated into different and even unseen MetaBBOtasks and can be efficiently fine-tuned for further performance boost. Thisadvancement marks a pivotal step in making MetaBBO algorithms more autonomousand broadly applicable. The source code of NeurELA can be accessed athttps://github.com/GMC-DRL/Neur-ELA.
  </details>

- **[Automatically Adaptive Conformal Risk Control](http://arxiv.org/abs/2406.17819v4)**  `arXiv:2406.17819`  
  _Vincent Blot, Anastasios N Angelopoulos, Michael I Jordan, Nicolas J-B Brunel_
  <details><summary>Abstract</summary>
  Science and technology have a growing need for effective mechanisms thatensure reliable, controlled performance from black-box machine learningalgorithms. These performance guarantees should ideally hold conditionally onthe input-that is the performance guarantees should hold, at leastapproximately, no matter what the input. However, beyond stylized discretegroupings such as ethnicity and gender, the right notion of conditioning can bedifficult to define. For example, in problems such as image segmentation, wewant the uncertainty to reflect the intrinsic difficulty of the test sample,but this may be difficult to capture via a conditioning event. Building on therecent work of Gibbs et al. [2023], we propose a methodology for achievingapproximate conditional control of statistical risks-the expected value of lossfunctions-by adapting to the difficulty of test samples. Our framework goesbeyond traditional conditional risk control based on user-provided conditioningevents to the algorithmic, data-driven determination of appropriate functionclasses for conditioning. We apply this framework to various regression andsegmentation tasks, enabling finer-grained control over model performance anddemonstrating that by continuously monitoring and adjusting these parameters,we can achieve superior precision compared to conventional risk-controlmethods.
  </details>

- **[GenoTEX: A Benchmark for Automated Gene Expression Data Analysis in Alignment with Bioinformaticians](http://arxiv.org/abs/2406.15341v2)**  `arXiv:2406.15341`  
  _Haoyang Liu, Shuyu Chen, Ye Zhang, Haohan Wang_
  <details><summary>Abstract</summary>
  Recent advancements in machine learning have significantly improved theidentification of disease-associated genes from gene expression datasets.However, these processes often require extensive expertise and manual effort,limiting their scalability. Large Language Model (LLM)-based agents have shownpromise in automating these tasks due to their increasing problem-solvingabilities. To support the evaluation and development of such methods, weintroduce GenoTEX, a benchmark dataset for the automated analysis of geneexpression data. GenoTEX provides annotated code and results for solving a widerange of gene identification problems, encompassing dataset selection,preprocessing, and statistical analysis, in a pipeline that followscomputational genomics standards. The benchmark includes expert-curatedannotations from bioinformaticians to ensure accuracy and reliability. Toprovide baselines for these tasks, we present GenoAgent, a team of LLM-basedagents that adopt a multi-step programming workflow with flexibleself-correction, to collaboratively analyze gene expression datasets. Ourexperiments demonstrate the potential of LLM-based methods in analyzing genomicdata, while error analysis highlights the challenges and areas for futureimprovement. We propose GenoTEX as a promising resource for benchmarking andenhancing automated methods for gene expression data analysis. The benchmark isavailable at https://github.com/Liu-Hy/GenoTex.
  </details>

- **[Unlearning during Learning: An Efficient Federated Machine Unlearning Method](http://arxiv.org/abs/2405.15474v2)**  `arXiv:2405.15474`  
  _Hanlin Gu, Gongxi Zhu, Jie Zhang, Xinyuan Zhao, Yuxing Han, Lixin Fan, et al._
  <details><summary>Abstract</summary>
  In recent years, Federated Learning (FL) has garnered significant attentionas a distributed machine learning paradigm. To facilitate the implementation ofthe right to be forgotten, the concept of federated machine unlearning (FMU)has also emerged. However, current FMU approaches often involve additionaltime-consuming steps and may not offer comprehensive unlearning capabilities,which renders them less practical in real FL scenarios. In this paper, weintroduce FedAU, an innovative and efficient FMU framework aimed at overcomingthese limitations. Specifically, FedAU incorporates a lightweight auxiliaryunlearning module into the learning process and employs a straightforwardlinear operation to facilitate unlearning. This approach eliminates therequirement for extra time-consuming steps, rendering it well-suited for FL.Furthermore, FedAU exhibits remarkable versatility. It not only enablesmultiple clients to carry out unlearning tasks concurrently but also supportsunlearning at various levels of granularity, including individual data samples,specific classes, and even at the client level. We conducted extensiveexperiments on MNIST, CIFAR10, and CIFAR100 datasets to evaluate theperformance of FedAU. The results demonstrate that FedAU effectively achievesthe desired unlearning effect while maintaining model accuracy. Our code isavailiable at https://github.com/Liar-Mask/FedAU.
  </details>

- **[Stochastic Inference of Plate Bending from Heterogeneous Data: Physics-informed Gaussian Processes via Kirchhoff-Love Theory](http://arxiv.org/abs/2405.12802v2)**  `arXiv:2405.12802`  
  _Igor Kavrakov, Gledson Rodrigo Tondo, Guido Morgenthal_
  <details><summary>Abstract</summary>
  Advancements in machine learning and an abundance of structural monitoringdata have inspired the integration of mechanical models with probabilisticmodels to identify a structure's state and quantify the uncertainty of itsphysical parameters and response. In this paper, we propose an inferencemethodology for classical Kirchhoff-Love plates via physics-informed GaussianProcesses (GP). A probabilistic model is formulated as a multi-output GP byplacing a GP prior on the deflection and deriving the covariance function usingthe linear differential operators of the plate governing equations. Theposteriors of the flexural rigidity, hyperparameters, and plate response areinferred in a Bayesian manner using Markov chain Monte Carlo (MCMC) samplingfrom noisy measurements. We demonstrate the applicability with two examples: asimply supported plate subjected to a sinusoidal load and a fixed platesubjected to a uniform load. The results illustrate how the proposedmethodology can be employed to perform stochastic inference for plate rigidityand physical quantities by integrating measurements from various sensor typesand qualities. Potential applications of the presented methodology are instructural health monitoring and uncertainty quantification of plate-likestructures.
  </details>

- **[Combining Relevance and Magnitude for Resource-Aware DNN Pruning](http://arxiv.org/abs/2405.13088v2)**  `arXiv:2405.13088`  
  _Carla Fabiana Chiasserini, Francesco Malandrino, Nuria Molner, Zhiqiang Zhao_
  <details><summary>Abstract</summary>
  Pruning neural networks, i.e., removing some of their parameters whilstretaining their accuracy, is one of the main ways to reduce the latency of amachine learning pipeline, especially in resource- and/or bandwidth-constrainedscenarios. In this context, the pruning technique, i.e., how to choose theparameters to remove, is critical to the system performance. In this paper, wepropose a novel pruning approach, called FlexRel and predicated upon combiningtraining-time and inference-time information, namely, parameter magnitude andrelevance, in order to improve the resulting accuracy whilst saving bothcomputational resources and bandwidth. Our performance evaluation shows thatFlexRel is able to achieve higher pruning factors, saving over 35% bandwidthfor typical accuracy targets.
  </details>

- **[Sparse Feature Circuits: Discovering and Editing Interpretable Causal Graphs in Language Models](http://arxiv.org/abs/2403.19647v3)**  `arXiv:2403.19647`  
  _Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, Aaron Mueller_
  <details><summary>Abstract</summary>
  We introduce methods for discovering and applying sparse feature circuits.These are causally implicated subnetworks of human-interpretable features forexplaining language model behaviors. Circuits identified in prior work consistof polysemantic and difficult-to-interpret units like attention heads orneurons, rendering them unsuitable for many downstream applications. Incontrast, sparse feature circuits enable detailed understanding ofunanticipated mechanisms. Because they are based on fine-grained units, sparsefeature circuits are useful for downstream tasks: We introduce SHIFT, where weimprove the generalization of a classifier by ablating features that a humanjudges to be task-irrelevant. Finally, we demonstrate an entirely unsupervisedand scalable interpretability pipeline by discovering thousands of sparsefeature circuits for automatically discovered model behaviors.
  </details>

- **[Self-Expansion of Pre-trained Models with Mixture of Adapters for Continual Learning](http://arxiv.org/abs/2403.18886v3)**  `arXiv:2403.18886`  
  _Huiyi Wang, Haodong Lu, Lina Yao, Dong Gong_
  <details><summary>Abstract</summary>
  Continual learning (CL) aims to continually accumulate knowledge from anon-stationary data stream without catastrophic forgetting of learnedknowledge, requiring a balance between stability and adaptability. Relying onthe generalizable representation in pre-trained models (PTMs), PTM-based CLmethods perform effective continual adaptation on downstream tasks by addinglearnable adapters or prompts upon the frozen PTMs. However, many existingPTM-based CL methods use restricted adaptation on a fixed set of these modulesto avoid forgetting, suffering from limited CL ability. Periodically addingtask-specific modules results in linear model growth rate and impairedknowledge reuse. We propose Self-Expansion of pre-trained models withModularized Adaptation (SEMA), a novel approach to enhance the control ofstability-plasticity balance in PTM-based CL. SEMA automatically decides toreuse or add adapter modules on demand in CL, depending on whether significantdistribution shift that cannot be handled is detected at differentrepresentation levels. We design modular adapter consisting of a functionaladapter and a representation descriptor. The representation descriptors aretrained as a distribution shift indicator and used to trigger self-expansionsignals. For better composing the adapters, an expandable weighting router islearned jointly for mixture of adapter outputs. SEMA enables better knowledgereuse and sub-linear expansion rate. Extensive experiments demonstrate theeffectiveness of the proposed self-expansion method, achieving state-of-the-artperformance compared to PTM-based CL methods without memory rehearsal. Code isavailable at https://github.com/huiyiwang01/SEMA-CL.
  </details>

- **[Debiased Offline Representation Learning for Fast Online Adaptation in Non-stationary Dynamics](http://arxiv.org/abs/2402.11317v2)**  `arXiv:2402.11317`  
  _Xinyu Zhang, Wenjie Qiu, Yi-Chen Li, Lei Yuan, Chengxing Jia, Zongzhang Zhang, et al._
  <details><summary>Abstract</summary>
  Developing policies that can adjust to non-stationary environments isessential for real-world reinforcement learning applications. However, learningsuch adaptable policies in offline settings, with only a limited set ofpre-collected trajectories, presents significant challenges. A key difficultyarises because the limited offline data makes it hard for the context encoderto differentiate between changes in the environment dynamics and shifts in thebehavior policy, often leading to context misassociations. To address thisissue, we introduce a novel approach called Debiased Offline Representation forfast online Adaptation (DORA). DORA incorporates an information bottleneckprinciple that maximizes mutual information between the dynamics encoding andthe environmental data, while minimizing mutual information between thedynamics encoding and the actions of the behavior policy. We present apractical implementation of DORA, leveraging tractable bounds of theinformation bottleneck principle. Our experimental evaluation across sixbenchmark MuJoCo tasks with variable parameters demonstrates that DORA not onlyachieves a more precise dynamics encoding but also significantly outperformsexisting baselines in terms of performance.
  </details>

- **[FedMIA: An Effective Membership Inference Attack Exploiting "All for One" Principle in Federated Learning](http://arxiv.org/abs/2402.06289v3)**  `arXiv:2402.06289`  
  _Gongxi Zhu, Donghao Li, Hanlin Gu, Yuan Yao, Lixin Fan, Yuxing Han_
  <details><summary>Abstract</summary>
  Federated Learning (FL) is a promising approach for training machine learningmodels on decentralized data while preserving privacy. However, privacy risks,particularly Membership Inference Attacks (MIAs), which aim to determinewhether a specific data point belongs to a target client's training set, remaina significant concern. Existing methods for implementing MIAs in FL primarilyanalyze updates from the target client, focusing on metrics such as loss,gradient norm, and gradient difference. However, these methods fail to leverageupdates from non-target clients, potentially underutilizing availableinformation. In this paper, we first formulate a one-tailed likelihood-ratiohypothesis test based on the likelihood of updates from non-target clients.Building upon this formulation, we introduce a three-step Membership InferenceAttack (MIA) method, called FedMIA, which follows the "all for one"--leveragingupdates from all clients across multiple communication rounds to enhance MIAeffectiveness. Both theoretical analysis and extensive experimental resultsdemonstrate that FedMIA outperforms existing MIAs in both classification andgenerative tasks. Additionally, it can be integrated as an extension toexisting methods and is robust against various defense strategies, Non-IIDdata, and different federated structures. Our code is available inhttps://github.com/Liar-Mask/FedMIA.
  </details>

- **[Partial Gromov-Wasserstein Metric](http://arxiv.org/abs/2402.03664v5)**  `arXiv:2402.03664`  
  _Yikun Bai, Rocio Diaz Martin, Abihith Kothapalli, Hengrong Du, Xinran Liu, Soheil Kolouri_
  <details><summary>Abstract</summary>
  The Gromov-Wasserstein (GW) distance has gained increasing interest in themachine learning community in recent years, as it allows for the comparison ofmeasures in different metric spaces. To overcome the limitations imposed by theequal mass requirements of the classical GW problem, researchers have begunexploring its application in unbalanced settings. However, Unbalanced GW (UGW)can only be regarded as a discrepancy rather than a rigorous metric/distancebetween two metric measure spaces (mm-spaces). In this paper, we propose aparticular case of the UGW problem, termed Partial Gromov-Wasserstein (PGW). Weestablish that PGW is a well-defined metric between mm-spaces and discuss itstheoretical properties, including the existence of a minimizer for the PGWproblem and the relationship between PGW and GW, among others. We then proposetwo variants of the Frank-Wolfe algorithm for solving the PGW problem and showthat they are mathematically and computationally equivalent. Moreover, based onour PGW metric, we introduce the analogous concept of barycenters formm-spaces. Finally, we validate the effectiveness of our PGW metric and relatedsolvers in applications such as shape matching, shape retrieval, and shapeinterpolation, comparing them against existing baselines. Our code is availableat https://github.com/mint-vu/PGW_Metric.
  </details>

- **[Graph Anomaly Detection in Time Series: A Survey](http://arxiv.org/abs/2302.00058v5)**  `arXiv:2302.00058`  
  _Thi Kieu Khanh Ho, Ali Karami, Narges Armanfard_
  <details><summary>Abstract</summary>
  With the recent advances in technology, a wide range of systems continue tocollect a large amount of data over time and thus generate time series.Time-Series Anomaly Detection (TSAD) is an important task in varioustime-series applications such as e-commerce, cybersecurity, vehiclemaintenance, and healthcare monitoring. However, this task is very challengingas it requires considering both the intra-variable dependency (relationshipswithin a variable over time) and the inter-variable dependency (relationshipsbetween multiple variables) existing in time-series data. Recent graph-basedapproaches have made impressive progress in tackling the challenges of thisfield. In this survey, we conduct a comprehensive and up-to-date review of TSADusing graphs, referred to as G-TSAD. First, we explore the significantpotential of graph representation for time-series data and and itscontributions to facilitating anomaly detection. Then, we reviewstate-of-the-art graph anomaly detection techniques, mostly leveraging deeplearning architectures, in the context of time series. For each method, wediscuss its strengths, limitations, and the specific applications where itexcels. Finally, we address both the technical and application challengescurrently facing the field, and suggest potential future directions foradvancing research and improving practical outcomes.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Multiagent Systems üåê

<details open><summary>Click to Collapse</summary>

- **[Multi-agent Uncertainty-Aware Pessimistic Model-Based Reinforcement Learning for Connected Autonomous Vehicles](http://arxiv.org/abs/2503.20462v1)**  `arXiv:2503.20462`  
  _Ruoqi Wen, Rongpeng Li, Xing Xu, Zhifeng Zhao_
  <details><summary>Abstract</summary>
  Deep Reinforcement Learning (DRL) holds significant promise for achievinghuman-like Autonomous Vehicle (AV) capabilities, but suffers from low sampleefficiency and challenges in reward design. Model-Based Reinforcement Learning(MBRL) offers improved sample efficiency and generalizability compared toModel-Free Reinforcement Learning (MFRL) in various multi-agent decision-makingscenarios. Nevertheless, MBRL faces critical difficulties in estimatinguncertainty during the model learning phase, thereby limiting its scalabilityand applicability in real-world scenarios. Additionally, most ConnectedAutonomous Vehicle (CAV) studies focus on single-agent decision-making, whileexisting multi-agent MBRL solutions lack computationally tractable algorithmswith Probably Approximately Correct (PAC) guarantees, an essential factor forensuring policy reliability with limited training data. To address thesechallenges, we propose MA-PMBRL, a novel Multi-Agent Pessimistic Model-BasedReinforcement Learning framework for CAVs, incorporating a max-min optimizationapproach to enhance robustness and decision-making. To mitigate the inherentsubjectivity of uncertainty estimation in MBRL and avoid incurring catastrophicfailures in AV, MA-PMBRL employs a pessimistic optimization framework combinedwith Projected Gradient Descent (PGD) for both model and policy learning.MA-PMBRL also employs general function approximations under partial datasetcoverage to enhance learning efficiency and system-level performance. Bybounding the suboptimality of the resulting policy under mild theoreticalassumptions, we successfully establish PAC guarantees for MA-PMBRL,demonstrating that the proposed framework represents a significant step towardscalable, efficient, and reliable multi-agent decision-making for CAVs.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

### Robotics ü§ñ

<details open><summary>Click to Collapse</summary>

- **[Enhancing Underwater Navigation through Cross-Correlation-Aware Deep INS/DVL Fusion](http://arxiv.org/abs/2503.21727v1)**  `arXiv:2503.21727`  
  _Nadav Cohen, Itzik Klein_
  <details><summary>Abstract</summary>
  The accurate navigation of autonomous underwater vehicles critically dependson the precision of Doppler velocity log (DVL) velocity measurements. Recentadvancements in deep learning have demonstrated significant potential inimproving DVL outputs by leveraging spatiotemporal dependencies across multiplesensor modalities. However, integrating these estimates into model-basedfilters, such as the extended Kalman filter, introduces statisticalinconsistencies, most notably, cross-correlations between process andmeasurement noise. This paper addresses this challenge by proposing across-correlation-aware deep INS/DVL fusion framework. Building upon BeamsNet,a convolutional neural network designed to estimate AUV velocity using DVL andinertial data, we integrate its output into a navigation filter that explicitlyaccounts for the cross-correlation induced between the noise sources. Thisapproach improves filter consistency and better reflects the underlying sensorerror structure. Evaluated on two real-world underwater trajectories, theproposed method outperforms both least squares and cross-correlation-neglectingapproaches in terms of state uncertainty. Notably, improvements exceed 10% invelocity and misalignment angle confidence metrics. Beyond demonstratingempirical performance, this framework provides a theoretically principledmechanism for embedding deep learning outputs within stochastic filters.
  </details>

- **[Dataset and Analysis of Long-Term Skill Acquisition in Robot-Assisted Minimally Invasive Surgery](http://arxiv.org/abs/2503.21591v1)**  `arXiv:2503.21591`  
  _Yarden Sharon, Alex Geftler, Hanna Kossowsky Lev, Ilana Nisky_
  <details><summary>Abstract</summary>
  Objective: We aim to investigate long-term robotic surgical skill acquisitionamong surgical residents and the effects of training intervals and fatigue onperformance. Methods: For six months, surgical residents participated in threetraining sessions once a month, surrounding a single 26-hour hospital shift. Ineach shift, they participated in training sessions scheduled before, during,and after the shift. In each training session, they performed three dry-labtraining tasks: Ring Tower Transfer, Knot-Tying, and Suturing. We collected acomprehensive dataset, including videos synchronized with kinematic data,activity tracking, and scans of the suturing pads. Results: We collected adataset of 972 trials performed by 18 residents of different surgicalspecializations. Participants demonstrated consistent performance improvementacross all tasks. In addition, we found variations in between-shift learningand forgetting across metrics and tasks, and hints for possible effects offatigue. Conclusion: The findings from our first analysis shed light on thelong-term learning processes of robotic surgical skills with extended intervalsand varying levels of fatigue. Significance: This study lays the groundwork forfuture research aimed at optimizing training protocols and enhancing AIapplications in surgery, ultimately contributing to improved patient outcomes.The dataset will be made available upon acceptance of our journal submission.
  </details>

- **[Cooking Task Planning using LLM and Verified by Graph Network](http://arxiv.org/abs/2503.21564v1)**  `arXiv:2503.21564`  
  _Ryunosuke Takebayashi, Vitor Hideyo Isume, Takuya Kiyokawa, Weiwei Wan, Kensuke Harada_
  <details><summary>Abstract</summary>
  Cooking tasks remain a challenging problem for robotics due to theircomplexity. Videos of people cooking are a valuable source of information forsuch task, but introduces a lot of variability in terms of how to translatethis data to a robotic environment. This research aims to streamline thisprocess, focusing on the task plan generation step, by using a Large LanguageModel (LLM)-based Task and Motion Planning (TAMP) framework to autonomouslygenerate cooking task plans from videos with subtitles, and execute them.Conventional LLM-based task planning methods are not well-suited forinterpreting the cooking video data due to uncertainty in the videos, and therisk of hallucination in its output. To address both of these problems, weexplore using LLMs in combination with Functional Object-Oriented Networks(FOON), to validate the plan and provide feedback in case of failure. Thiscombination can generate task sequences with manipulation motions that arelogically correct and executable by a robot. We compare the execution of thegenerated plans for 5 cooking recipes from our approach against the plansgenerated by a few-shot LLM-only approach for a dual-arm robot setup. It couldsuccessfully execute 4 of the plans generated by our approach, whereas only 1of the plans generated by solely using the LLM could be executed.
  </details>

- **[Data-Driven Contact-Aware Control Method for Real-Time Deformable Tool Manipulation: A Case Study in the Environmental Swabbing](http://arxiv.org/abs/2503.21491v1)**  `arXiv:2503.21491`  
  _Siavash Mahmoudi, Amirreza Davar, Dongyi Wang_
  <details><summary>Abstract</summary>
  Deformable Object Manipulation (DOM) remains a critical challenge in roboticsdue to the complexities of developing suitable model-based control strategies.Deformable Tool Manipulation (DTM) further complicates this task by introducingadditional uncertainties between the robot and its environment. While humanseffortlessly manipulate deformable tools using touch and experience, roboticsystems struggle to maintain stability and precision. To address thesechallenges, we present a novel State-Adaptive Koopman LQR (SA-KLQR) controlframework for real-time deformable tool manipulation, demonstrated through acase study in environmental swab sampling for food safety. This methodleverages Koopman operator-based control to linearize nonlinear dynamics whileadapting to state-dependent variations in tool deformation and contact forces.A tactile-based feedback system dynamically estimates and regulates the swabtool's angle, contact pressure, and surface coverage, ensuring compliance withfood safety standards. Additionally, a sensor-embedded contact pad monitorsforce distribution to mitigate tool pivoting and deformation, improvingstability during dynamic interactions. Experimental results validate theSA-KLQR approach, demonstrating accurate contact angle estimation, robusttrajectory tracking, and reliable force regulation. The proposed frameworkenhances precision, adaptability, and real-time control in deformable toolmanipulation, bridging the gap between data-driven learning and optimal controlin robotic interaction tasks.
  </details>

- **[STAMICS: Splat, Track And Map with Integrated Consistency and Semantics for Dense RGB-D SLAM](http://arxiv.org/abs/2503.21425v1)**  `arXiv:2503.21425`  
  _Yongxu Wang, Xu Cao, Weiyun Yi, Zhaoxin Fan_
  <details><summary>Abstract</summary>
  Simultaneous Localization and Mapping (SLAM) is a critical task in robotics,enabling systems to autonomously navigate and understand complex environments.Current SLAM approaches predominantly rely on geometric cues for mapping andlocalization, but they often fail to ensure semantic consistency, particularlyin dynamic or densely populated scenes. To address this limitation, weintroduce STAMICS, a novel method that integrates semantic information with 3DGaussian representations to enhance both localization and mapping accuracy.STAMICS consists of three key components: a 3D Gaussian-based scenerepresentation for high-fidelity reconstruction, a graph-based clusteringtechnique that enforces temporal semantic consistency, and an open-vocabularysystem that allows for the classification of unseen objects. Extensiveexperiments show that STAMICS significantly improves camera pose estimation andmap quality, outperforming state-of-the-art methods while reducingreconstruction errors. Code will be public available.
  </details>

- **[AcL: Action Learner for Fault-Tolerant Quadruped Locomotion Control](http://arxiv.org/abs/2503.21401v1)**  `arXiv:2503.21401`  
  _Tianyu Xu, Yaoyu Cheng, Pinxi Shen, Lin Zhao, Electrical, Computer Engineering, et al._
  <details><summary>Abstract</summary>
  Quadrupedal robots can learn versatile locomotion skills but remainvulnerable when one or more joints lose power. In contrast, dogs and cats canadopt limping gaits when injured, demonstrating their remarkable ability toadapt to physical conditions. Inspired by such adaptability, this paperpresents Action Learner (AcL), a novel teacher-student reinforcement learningframework that enables quadrupeds to autonomously adapt their gait for stablewalking under multiple joint faults. Unlike conventional teacher-studentapproaches that enforce strict imitation, AcL leverages teacher policies togenerate style rewards, guiding the student policy without requiring precisereplication. We train multiple teacher policies, each corresponding to adifferent fault condition, and subsequently distill them into a single studentpolicy with an encoder-decoder architecture. While prior works primarilyaddress single-joint faults, AcL enables quadrupeds to walk with up to fourfaulty joints across one or two legs, autonomously switching between differentlimping gaits when faults occur. We validate AcL on a real Go2 quadruped robotunder single- and double-joint faults, demonstrating fault-tolerant, stablewalking, smooth gait transitions between normal and lamb gaits, and robustnessagainst external disturbances.
  </details>

- **[A Data-Driven Method for INS/DVL Alignment](http://arxiv.org/abs/2503.21350v1)**  `arXiv:2503.21350`  
  _Guy Damari, Itzik Klein_
  <details><summary>Abstract</summary>
  Autonomous underwater vehicles (AUVs) are sophisticated robotic platformscrucial for a wide range of applications. The accuracy of AUV navigationsystems is critical to their success. Inertial sensors and Doppler velocitylogs (DVL) fusion is a promising solution for long-range underwater navigation.However, the effectiveness of this fusion depends heavily on an accuratealignment between the inertial sensors and the DVL. While current alignmentmethods show promise, there remains significant room for improvement in termsof accuracy, convergence time, and alignment trajectory efficiency. In thisresearch we propose an end-to-end deep learning framework for the alignmentprocess. By leveraging deep-learning capabilities, such as noise reduction andcapture of nonlinearities in the data, we show using simulative data, that ourproposed approach enhances both alignment accuracy and reduces convergence timebeyond current model-based methods.
  </details>

- **[Lidar-only Odometry based on Multiple Scan-to-Scan Alignments over a Moving Window](http://arxiv.org/abs/2503.21293v1)**  `arXiv:2503.21293`  
  _Aaron Kurda, Simon Steuernagel, Marcus Baum_
  <details><summary>Abstract</summary>
  Lidar-only odometry considers the pose estimation of a mobile robot based onthe accumulation of motion increments extracted from consecutive lidar scans.Many existing approaches to the problem use a scan-to-map registration, whichneglects the accumulation of errors within the maintained map due to drift.Other methods use a refinement step that jointly optimizes the local map on afeature basis. We propose a solution that avoids this by using multipleindependent scan-to-scan Iterative Closest Points (ICP) registrations toprevious scans in order to derive constraints for a pose graph. Theoptimization of the pose graph then not only yields an accurate estimate forthe latest pose, but also enables the refinement of previous scans in theoptimization window. By avoiding the need to recompute the scan-to-scanalignments, the computational load is minimized. Extensive evaluation on thepublic KITTI and MulRan datasets as well as on a custom automotive lidardataset is carried out. Results show that the proposed approach achievesstate-of-the-art estimation accuracy, while alleviating the mentioned issues.
  </details>

- **[An analysis of higher-order kinematics formalisms for an innovative surgical parallel robot](http://arxiv.org/abs/2503.21291v1)**  `arXiv:2503.21291`  
  _Calin Vaida, Iosif Birlescu, Bogdan Gherman, Daniel Condurache, Damien Chablat, Doina Pisla_
  <details><summary>Abstract</summary>
  The paper presents a novel modular hybrid parallel robot for pancreaticsurgery and its higher-order kinematics derived based on various formalisms.The classical vector, homogeneous transformation matrices and dual quaternionapproaches are studied for the kinematic functions using both classicaldifferentiation and multidual algebra. The algorithms for inverse kinematicsfor all three studied formalisms are presented for both differentiation andmultidual algebra approaches. Furthermore, these algorithms are compared basedon numerical stability, execution times and number and type of mathematicalfunctions and operators contained in each algorithm. A statistical analysisshows that there is significant improvement in execution time for thealgorithms implemented using multidual algebra, while the numerical stabilityis appropriate for all algorithms derived based on differentiation andmultidual algebra. While the implementation of the kinematic algorithms usingmultidual algebra shows positive results when benchmarked on a standard PC,further work is required to evaluate the multidual algorithms onhardware/software used for the modular parallel robot command and control.
  </details>

- **[Haptic bilateral teleoperation system for free-hand dental procedures](http://arxiv.org/abs/2503.21288v1)**  `arXiv:2503.21288`  
  _Lorenzo Pagliara, Enrico Ferrentino, Andrea Chiacchio, Giovanni Russo_
  <details><summary>Abstract</summary>
  Free-hand dental procedures are typically repetitive, time-consuming andrequire high precision and manual dexterity. Dental robots can play a key rolein improving procedural accuracy and safety, enhancing patient comfort, andreducing operator workload. However, robotic solutions for free-hand proceduresremain limited or completely lacking, and their acceptance is still low. Toaddress this gap, we develop a haptic bilateral teleoperation system (HBTS) forfree-hand dental procedures. The system includes a dedicated mechanicalend-effector, compatible with standard clinical tools, and equipped with anendoscopic camera for improved visibility of the intervention site. By ensuringmotion and force correspondence between the operator's actions and the robot'smovements, monitored through visual feedback, we enhance the operator's sensoryawareness and motor accuracy. Furthermore, recognizing the need to ensureprocedural safety, we limit interaction forces by scaling the motion referencesprovided to the admittance controller based solely on measured contact forces.This ensures effective force limitation in all contact states without requiringprior knowledge of the environment. The proposed HBTS is validated in a dentalscaling procedure using a dental phantom. The results show that the systemimproves the naturalness, safety, and accuracy of teleoperation, highlightingits potential to enhance free-hand dental procedures.
  </details>

- **[Output-Feedback Boundary Control of Thermally and Flow-Induced Vibrations in Slender Timoshenko Beams](http://arxiv.org/abs/2503.21281v1)**  `arXiv:2503.21281`  
  _Chengyi Wang, Ji Wang_
  <details><summary>Abstract</summary>
  This work is motivated by the engineering challenge of suppressing vibrationsin turbine blades of aero engines, which often operate under extreme thermalconditions and high-Mach aerodynamic environments that give rise to complexvibration phenomena, commonly referred to as thermally-induced and flow-inducedvibrations. Using Hamilton's variational principle, the system is modeled as arotating slender Timoshenko beam under thermal and aerodynamic loads, describedby a mixed hyperbolic-parabolic PDE system where instabilities occur bothwithin the PDE domain and at the uncontrolled boundary, and the two types ofPDEs are cascaded in the domain. For such a system, we present thestate-feedback control design based on the PDE backstepping method. Recognizingthat the distributed temperature gradients and structural vibrations in theTimoshenko beam are typically unmeasurable in practice, we design a stateobserver for the mixed hyperbolic-parabolic PDE system. Based on this observer,an output-feedback controller is then built to regulate the overall systemusing only available boundary measurements. In the closed-loop system, thestate of the uncontrolled boundary, i.e., the furthest state from the controlinput, is proved to be exponentially convergent to zero, and all signals areproved as uniformly ultimately bounded. The proposed control design isvalidated on an aero-engine flexible blade under extreme thermal andaerodynamic conditions.
  </details>

- **[OminiAdapt: Learning Cross-Task Invariance for Robust and Environment-Aware Robotic Manipulation](http://arxiv.org/abs/2503.21257v1)**  `arXiv:2503.21257`  
  _Yongxu Wang, Weiyun Yi, Xinhao Kong, Wanting Li_
  <details><summary>Abstract</summary>
  With the rapid development of embodied intelligence, leveraging large-scalehuman data for high-level imitation learning on humanoid robots has become afocal point of interest in both academia and industry. However, applyinghumanoid robots to precision operation domains remains challenging due to thecomplexities they face in perception and control processes, the long-standingphysical differences in morphology and actuation mechanisms between humanoidrobots and humans, and the lack of task-relevant features obtained fromegocentric vision. To address the issue of covariate shift in imitationlearning, this paper proposes an imitation learning algorithm tailored forhumanoid robots. By focusing on the primary task objectives, filtering outbackground information, and incorporating channel feature fusion with spatialattention mechanisms, the proposed algorithm suppresses environmentaldisturbances and utilizes a dynamic weight update strategy to significantlyimprove the success rate of humanoid robots in accomplishing target tasks.Experimental results demonstrate that the proposed method exhibits robustnessand scalability across various typical task scenarios, providing new ideas andapproaches for autonomous learning and control in humanoid robots. The projectwill be open-sourced on GitHub.
  </details>

- **[Dimensional optimization of single-DOF planar rigid link-flapping mechanisms for high lift and low power](http://arxiv.org/abs/2503.21204v1)**  `arXiv:2503.21204`  
  _Shyam Sunder Nishad, Anupam Saxena_
  <details><summary>Abstract</summary>
  Rigid link flapping mechanisms remain the most practical choice for flappingwing micro-aerial vehicles (MAVs) to carry useful payloads and onboardbatteries for free flight due to their long-term durability and reliability.However, to achieve high agility and maneuverability-like insects-MAVs withthese mechanisms require significant weight reduction. One approach involvesusing single-DOF planar rigid linkages, which are rarely optimizeddimensionally for high lift and low power so that smaller motors and batteriescould be used. We integrated a mechanism simulator based on a quasistaticnonlinear finite element method with an unsteady vortex lattice method-basedaerodynamic analysis tool within an optimization routine. We optimized threedifferent mechanism topologies from the literature. As a result, significantpower savings were observed up to 42% in some cases, due to increased amplitudeand higher lift coefficients resulting from optimized asymmetric sweepingvelocity profiles. We also conducted an uncertainty analysis that revealed theneed for high manufacturing tolerances to ensure reliable mechanismperformance. The presented unified computational tool also facilitates theoptimal selection of MAV components based on the payload and flight timerequirements.
  </details>

- **[TAGA: A Tangent-Based Reactive Approach for Socially Compliant Robot Navigation Around Human Groups](http://arxiv.org/abs/2503.21168v1)**  `arXiv:2503.21168`  
  _Utsha Kumar Roy, Sejuti Rahman_
  <details><summary>Abstract</summary>
  Robot navigation in densely populated environments presents significantchallenges, particularly regarding the interplay between individual and groupdynamics. Current navigation models predominantly address interactions withindividual pedestrians while failing to account for human groups that naturallyform in real-world settings. Conversely, the limited models implementinggroup-aware navigation typically prioritize group dynamics at the expense ofindividual interactions, both of which are essential for socially appropriatenavigation. This research extends an existing simulation framework toincorporate both individual pedestrians and human groups. We present TangentAction for Group Avoidance (TAGA), a modular reactive mechanism that can beintegrated with existing navigation frameworks to enhance their group-awarenesscapabilities. TAGA dynamically modifies robot trajectories using tangentaction-based avoidance strategies while preserving the underlying model'scapacity to navigate around individuals. Additionally, we introduce GroupCollision Rate (GCR), a novel metric to quantitatively assess how effectivelyrobots maintain group integrity during navigation. Through comprehensivesimulation-based benchmarking, we demonstrate that integrating TAGA withstate-of-the-art navigation models (ORCA, Social Force, DS-RNN, and AG-RL)reduces group intrusions by 45.7-78.6% while maintaining comparable successrates and navigation efficiency. Future work will focus on real-worldimplementation and validation of this approach.
  </details>

- **[Safe Human Robot Navigation in Warehouse Scenario](http://arxiv.org/abs/2503.21141v1)**  `arXiv:2503.21141`  
  _Seth Farrell, Chenghao Li, Hongzhan Yu, Ryo Yoshimitsu, Sicun Gao, Henrik I. Christensen_
  <details><summary>Abstract</summary>
  The integration of autonomous mobile robots (AMRs) in industrialenvironments, particularly warehouses, has revolutionized logistics andoperational efficiency. However, ensuring the safety of human workers indynamic, shared spaces remains a critical challenge. This work proposes a novelmethodology that leverages control barrier functions (CBFs) to enhance safetyin warehouse navigation. By integrating learning-based CBFs with the OpenRobotics Middleware Framework (OpenRMF), the system achieves adaptive andsafety-enhanced controls in multi-robot, multi-agent scenarios. Experimentsconducted using various robot platforms demonstrate the efficacy of theproposed approach in avoiding static and dynamic obstacles, including humanpedestrians. Our experiments evaluate different scenarios in which the numberof robots, robot platforms, speed, and number of obstacles are varied, fromwhich we achieve promising performance.
  </details>

- **[Fuzzy-Logic-based model predictive control: A paradigm integrating optimal and common-sense decision making](http://arxiv.org/abs/2503.21065v1)**  `arXiv:2503.21065`  
  _Filip Surma, Anahita Jamshidnejad_
  <details><summary>Abstract</summary>
  This paper introduces a novel concept, fuzzy-logic-based model predictivecontrol (FLMPC), along with a multi-robot control approach for exploringunknown environments and locating targets. Traditional model predictive control(MPC) methods rely on Bayesian theory to represent environmental knowledge andoptimize a stochastic cost function, often leading to high computational costsand lack of effectiveness in locating all the targets. Our approach insteadleverages FLMPC and extends it to a bi-level parent-child architecture forenhanced coordination and extended decision making horizon. Extractinghigh-level information from probability distributions and local observations,FLMPC simplifies the optimization problem and significantly extends itsoperational horizon compared to other MPC methods. We conducted extensivesimulations in unknown 2-dimensional environments with randomly placedobstacles and humans. We compared the performance and computation time of FLMPCagainst MPC with a stochastic cost function, then evaluated the impact ofintegrating the high-level parent FLMPC layer. The results indicate that ourapproaches significantly improve both performance and computation time,enhancing coordination of robots and reducing the impact of uncertainty inlarge-scale search and rescue environments.
  </details>

- **[Risk-Aware Reinforcement Learning for Autonomous Driving: Improving Safety When Driving through Intersection](http://arxiv.org/abs/2503.19690v2)**  `arXiv:2503.19690`  
  _Bo Leng, Ran Yu, Wei Han, Lu Xiong, Zhuoren Li, Hailong Huang_
  <details><summary>Abstract</summary>
  Applying reinforcement learning to autonomous driving has garnered widespreadattention. However, classical reinforcement learning methods optimize policiesby maximizing expected rewards but lack sufficient safety considerations, oftenputting agents in hazardous situations. This paper proposes a risk-awarereinforcement learning approach for autonomous driving to improve the safetyperformance when crossing the intersection. Safe critics are constructed toevaluate driving risk and work in conjunction with the reward critic to updatethe actor. Based on this, a Lagrangian relaxation method and cyclic gradientiteration are combined to project actions into a feasible safe region.Furthermore, a Multi-hop and Multi-layer perception (MLP) mixed AttentionMechanism (MMAM) is incorporated into the actor-critic network, enabling thepolicy to adapt to dynamic traffic and overcome permutation sensitivitychallenges. This allows the policy to focus more effectively on surroundingpotential risks while enhancing the identification of passing opportunities.Simulation tests are conducted on different tasks at unsignalizedintersections. The results show that the proposed approach effectively reducescollision rates and improves crossing efficiency in comparison to baselinealgorithms. Additionally, our ablation experiments demonstrate the benefits ofincorporating risk-awareness and MMAM into RL.
  </details>

- **[Efficient Continual Adaptation of Pretrained Robotic Policy with Online Meta-Learned Adapters](http://arxiv.org/abs/2503.18684v2)**  `arXiv:2503.18684`  
  _Ruiqi Zhu, Endong Sun, Guanhe Huang, Oya Celiktutan_
  <details><summary>Abstract</summary>
  Continual adaptation is essential for general autonomous agents. For example,a household robot pretrained with a repertoire of skills must still adapt tounseen tasks specific to each household. Motivated by this, building uponparameter-efficient fine-tuning in language models, prior works have exploredlightweight adapters to adapt pretrained policies, which can preserve learnedfeatures from the pretraining phase and demonstrate good adaptationperformances. However, these approaches treat task learning separately,limiting knowledge transfer between tasks. In this paper, we propose OnlineMeta-Learned adapters (OMLA). Instead of applying adapters directly, OMLA canfacilitate knowledge transfer from previously learned tasks to current learningtasks through a novel meta-learning objective. Extensive experiments in bothsimulated and real-world environments demonstrate that OMLA can lead to betteradaptation performances compared to the baseline methods. The project link:https://ricky-zhu.github.io/OMLA/.
  </details>

- **[LaMOuR: Leveraging Language Models for Out-of-Distribution Recovery in Reinforcement Learning](http://arxiv.org/abs/2503.17125v4)**  `arXiv:2503.17125`  
  _Chan Kim, Seung-Woo Seo, Seong-Woo Kim_
  <details><summary>Abstract</summary>
  Deep Reinforcement Learning (DRL) has demonstrated strong performance inrobotic control but remains susceptible to out-of-distribution (OOD) states,often resulting in unreliable actions and task failure. While previous methodshave focused on minimizing or preventing OOD occurrences, they largely neglectrecovery once an agent encounters such states. Although the latest research hasattempted to address this by guiding agents back to in-distribution states,their reliance on uncertainty estimation hinders scalability in complexenvironments. To overcome this limitation, we introduce Language Models forOut-of-Distribution Recovery (LaMOuR), which enables recovery learning withoutrelying on uncertainty estimation. LaMOuR generates dense reward codes thatguide the agent back to a state where it can successfully perform its originaltask, leveraging the capabilities of LVLMs in image description, logicalreasoning, and code generation. Experimental results show that LaMOuRsubstantially enhances recovery efficiency across diverse locomotion tasks andeven generalizes effectively to complex environments, including humanoidlocomotion and mobile manipulation, where existing methods struggle. The codeand supplementary materials are available at https://lamour-rl.github.io/.
  </details>

- **[GR00T N1: An Open Foundation Model for Generalist Humanoid Robots](http://arxiv.org/abs/2503.14734v2)**  `arXiv:2503.14734`  
  _NVIDIA, :, Johan Bjorck, Fernando Casta√±eda, Nikita Cherniadev, Xingye Da, et al._
  <details><summary>Abstract</summary>
  General-purpose robots need a versatile body and an intelligent mind. Recentadvancements in humanoid robots have shown great promise as a hardware platformfor building generalist autonomy in the human world. A robot foundation model,trained on massive and diverse data sources, is essential for enabling therobots to reason about novel situations, robustly handle real-worldvariability, and rapidly learn new tasks. To this end, we introduce GR00T N1,an open foundation model for humanoid robots. GR00T N1 is aVision-Language-Action (VLA) model with a dual-system architecture. Thevision-language module (System 2) interprets the environment through vision andlanguage instructions. The subsequent diffusion transformer module (System 1)generates fluid motor actions in real time. Both modules are tightly coupledand jointly trained end-to-end. We train GR00T N1 with a heterogeneous mixtureof real-robot trajectories, human videos, and synthetically generated datasets.We show that our generalist robot model GR00T N1 outperforms thestate-of-the-art imitation learning baselines on standard simulation benchmarksacross multiple robot embodiments. Furthermore, we deploy our model on theFourier GR-1 humanoid robot for language-conditioned bimanual manipulationtasks, achieving strong performance with high data efficiency.
  </details>

- **[MUSE: A Real-Time Multi-Sensor State Estimator for Quadruped Robots](http://arxiv.org/abs/2503.12101v2)**  `arXiv:2503.12101`  
  _Ylenia Nistic√≤, Jo√£o Carlos Virgolino Soares, Lorenzo Amatucci, Geoff Fink, Claudio Semini_
  <details><summary>Abstract</summary>
  This paper introduces an innovative state estimator, MUSE (MUlti-sensor StateEstimator), designed to enhance state estimation's accuracy and real-timeperformance in quadruped robot navigation. The proposed state estimator buildsupon our previous work presented in [1]. It integrates data from a range ofonboard sensors, including IMUs, encoders, cameras, and LiDARs, to deliver acomprehensive and reliable estimation of the robot's pose and motion, even inslippery scenarios. We tested MUSE on a Unitree Aliengo robot, successfullyclosing the locomotion control loop in difficult scenarios, including slipperyand uneven terrain. Benchmarking against Pronto [2] and VILENS [3] showed 67.6%and 26.7% reductions in translational errors, respectively. Additionally, MUSEoutperformed DLIO [4], a LiDAR-inertial odometry system in rotational errorsand frequency, while the proprioceptive version of MUSE (P-MUSE) outperformedTSIF [5], with a 45.9% reduction in absolute trajectory error (ATE).
  </details>

- **[DexForce: Extracting Force-informed Actions from Kinesthetic Demonstrations for Dexterous Manipulation](http://arxiv.org/abs/2501.10356v2)**  `arXiv:2501.10356`  
  _Claire Chen, Zhongchun Yu, Hojung Choi, Mark Cutkosky, Jeannette Bohg_
  <details><summary>Abstract</summary>
  Imitation learning requires high-quality demonstrations consisting ofsequences of state-action pairs. For contact-rich dexterous manipulation tasksthat require dexterity, the actions in these state-action pairs must producethe right forces. Current widely-used methods for collecting dexterousmanipulation demonstrations are difficult to use for demonstrating contact-richtasks due to unintuitive human-to-robot motion retargeting and the lack ofdirect haptic feedback. Motivated by these concerns, we propose DexForce.DexForce leverages contact forces, measured during kinesthetic demonstrations,to compute force-informed actions for policy learning. We collectdemonstrations for six tasks and show that policies trained on ourforce-informed actions achieve an average success rate of 76% across all tasks.In contrast, policies trained directly on actions that do not account forcontact forces have near-zero success rates. We also conduct a study ablatingthe inclusion of force data in policy observations. We find that while usingforce data never hurts policy performance, it helps most for tasks that requireadvanced levels of precision and coordination, like opening an AirPods case andunscrewing a nut.
  </details>

- **[AnyBimanual: Transferring Unimanual Policy for General Bimanual Manipulation](http://arxiv.org/abs/2412.06779v2)**  `arXiv:2412.06779`  
  _Guanxing Lu, Tengbo Yu, Haoyuan Deng, Season Si Chen, Yansong Tang, Ziwei Wang_
  <details><summary>Abstract</summary>
  Performing general language-conditioned bimanual manipulation tasks is ofgreat importance for many applications ranging from household service toindustrial assembly. However, collecting bimanual manipulation data isexpensive due to the high-dimensional action space, which poses challenges forconventional methods to handle general bimanual manipulation tasks. Incontrast, unimanual policy has recently demonstrated impressivegeneralizability across a wide range of tasks because of scaled modelparameters and training data, which can provide sharable manipulation knowledgefor bimanual systems. To this end, we propose a plug-and-play method namedAnyBimanual, which transfers pre-trained unimanual policy to general bimanualmanipulation policy with few bimanual demonstrations. Specifically, we firstintroduce a skill manager to dynamically schedule the skill representationsdiscovered from pre-trained unimanual policy for bimanual manipulation tasks,which linearly combines skill primitives with task-oriented compensation torepresent the bimanual manipulation instruction. To mitigate the observationdiscrepancy between unimanual and bimanual systems, we present a visual alignerto generate soft masks for visual embedding of the workspace, which aims toalign visual input of unimanual policy model for each arm with those duringpretraining stage. AnyBimanual shows superiority on 12 simulated tasks fromRLBench2 with a sizable 12.67% improvement in success rate over previousmethods. Experiments on 9 real-world tasks further verify its practicality withan average success rate of 84.62%.
  </details>

- **[Constrained Nonlinear Kaczmarz Projection on Intersections of Manifolds for Coordinated Multi-Robot Mobile Manipulation](http://arxiv.org/abs/2410.21630v2)**  `arXiv:2410.21630`  
  _Akshaya Agrawal, Parker Mayer, Zachary Kingston, Geoffrey A. Hollinger_
  <details><summary>Abstract</summary>
  Cooperative manipulation tasks impose various structure-, task-, androbot-specific constraints on mobile manipulators. However, current methodsstruggle to model and solve these myriad constraints simultaneously. We proposea twofold solution: first, we model constraints as a family of manifoldsamenable to simultaneous solving. Second, we introduce the constrainednonlinear Kaczmarz (cNKZ) projection technique to produce constraint-satisfyingsolutions. Experiments show that cNKZ dramatically outperforms baselineapproaches, which cannot find solutions at all. We integrate cNKZ with asampling-based motion planning algorithm to generate complex, coordinatedmotions for 3 to 6 mobile manipulators (18--36 DoF), with cNKZ solving up to 80nonlinear constraints simultaneously and achieving up to a 92% success rate incluttered environments. We also demonstrate our approach on hardware usingthree Turtlebot3 Waffle Pi robots with OpenMANIPULATOR-X arms.
  </details>

- **[Integrating Naturalistic Insights in Objective Multi-Vehicle Safety Framework](http://arxiv.org/abs/2408.09769v2)**  `arXiv:2408.09769`  
  _Enrico Del Re, Amirhesam Aghanouri, Cristina Olaverri-Monreal_
  <details><summary>Abstract</summary>
  As autonomous vehicle technology advances, the precise assessment of safetyin complex traffic scenarios becomes crucial, especially in mixed-vehicleenvironments where human perception of safety must be taken into account. Thispaper presents a framework designed for assessing traffic safety inmulti-vehicle situations, facilitating the simultaneous utilization of diverseobjective safety metrics. Additionally, it allows the integration of subjectiveperception of safety by adjusting model parameters. The framework was appliedto evaluate various model configurations in car-following scenarios on ahighway, utilizing naturalistic driving datasets. The evaluation of the modelshowed an outstanding performance, particularly when integrating multipleobjective safety measures. Furthermore, the performance was significantlyenhanced when considering all surrounding vehicles.
  </details>

- **[Towards Optimizing a Convex Cover of Collision-Free Space for Trajectory Generation](http://arxiv.org/abs/2406.09631v3)**  `arXiv:2406.09631`  
  _Yuwei Wu, Igor Spasojevic, Pratik Chaudhari, Vijay Kumar_
  <details><summary>Abstract</summary>
  We propose an online iterative algorithm to optimize a convex cover tounder-approximate the free space for autonomous navigation to delineate SafeFlight Corridors (SFC). The convex cover consists of a set of polytopes suchthat the union of the polytopes represents obstacle-free space, allowing us tofind trajectories for robots that lie within the convex cover. In order to findthe SFC that facilitates trajectory optimization, we iteratively findoverlapping polytopes of maximum volumes that include specified waypointsinitialized by a geometric or kinematic planner. Constraints at waypointsappear in two alternating stages of a joint optimization problem, which issolved by a novel heuristic-based iterative algorithm with partiallydistributed variables. We validate the effectiveness of our proposed algorithmusing a range of parameterized environments and show its applications fortwo-stage motion planning.
  </details>

- **[Safety-Aware Human-Lead Vehicle Platooning by Proactively Reacting to Uncertain Human Behaving](http://arxiv.org/abs/2405.07556v3)**  `arXiv:2405.07556`  
  _Jia Hu, Shuhan Wang, Yiming Zhang, Haoran Wang, Zhilong Liu, Guangzhi Cao_
  <details><summary>Abstract</summary>
  Human-Lead Cooperative Adaptive Cruise Control (HL-CACC) is regarded as apromising vehicle platooning technology in real-world implementation. Byutilizing a Human-driven Vehicle (HV) as the platoon leader, HL-CACC reducesthe cost and enhances the reliability of perception and decision-making.However, state-of-the-art HL-CACC technology still has a great limitation ondriving safety due to the lack of considering the leading human driver'suncertain behavior. In this study, a HL-CACC controller is designed based onStochastic Model Predictive Control (SMPC). It is enabled to predict thedriving intention of the leading Connected Human-Driven Vehicle (CHV). Theproposed controller has the following features: i) enhanced perceived safety inoscillating traffic; ii) guaranteed safety against hard brakes; iii)computational efficiency for real-time implementation. The proposed controlleris evaluated on a PreScan&Simulink simulation platform. Real vehicle trajectorydata is collected for the calibration of the simulation. Results reveal thatthe proposed controller: i) improves perceived safety by 19.17% in oscillatingtraffic; ii) enhances actual safety by 7.76% against hard brakes; iii) isconfirmed with string stability. The computation time is approximately 3.2milliseconds when running on a laptop equipped with an Intel i5-13500H CPU.This indicates the proposed controller is ready for real-time implementation.
  </details>

- **[Mirroring the Parking Target: An Optimal-Control-Based Parking Motion Planner with Strengthened Parking Reliability and Faster Parking Completion](http://arxiv.org/abs/2405.07538v2)**  `arXiv:2405.07538`  
  _Jia Hu, Yongwei Feng, Shuoyuan Li, Haoran Wang, Jaehyun So, Junnian Zheng_
  <details><summary>Abstract</summary>
  Automated Parking Assist (APA) systems are now facing great challenges of lowadoption in applications, due to users' concerns about parking capability,reliability, and completion efficiency. To upgrade the conventional APAplanners and enhance user's acceptance, this research proposes anoptimal-control-based parking motion planner. Its highlight lies in its controllogic: planning trajectories by mirroring the parking target. This methodenables: i) parking capability in narrow spaces; ii) better parking reliabilityby expanding Operation Design Domain (ODD); iii) faster completion of parkingprocess; iv) enhanced computational efficiency; v) universal to all types ofparking. A comprehensive evaluation is conducted. Results demonstrate theproposed planner does enhance parking success rate by 40.6%, improve parkingcompletion efficiency by 18.0%, and expand ODD by 86.1%. It shows itssuperiority in difficult parking cases, such as the parallel parking scenarioand narrow spaces. Moreover, the average computation time of the proposedplanner is 74 milliseconds. Results indicate that the proposed planner is readyfor real-time commercial applications.
  </details>

- **[Model-Predictive Trajectory Generation for Aerial Search and Coverage](http://arxiv.org/abs/2403.05944v2)**  `arXiv:2403.05944`  
  _Hugo Matias, Daniel Silvestre_
  <details><summary>Abstract</summary>
  This paper introduces a trajectory planning algorithm for search and coveragemissions with an Unmanned Aerial Vehicle (UAV) based on an uncertainty map thatrepresents prior knowledge of the target region, modeled by a Gaussian MixtureModel (GMM). The trajectory planning problem is formulated as an OptimalControl Problem (OCP), which aims to maximize the uncertainty reduction withina specified mission duration. However, this results in an intractable OCP whoseobjective functional cannot be expressed in closed form. To address this, wepropose a Model Predictive Control (MPC) algorithm based on a relaxedformulation of the objective function to approximate the optimal solutions.This relaxation promotes efficient map exploration by penalizing overlaps inthe UAV's visibility regions along the trajectory. The algorithm can produceefficient and smooth trajectories, and it can be efficiently implemented usingstandard Nonlinear Programming solvers, being suitable for real-time planning.Unlike traditional methods, which often rely on discretizing the mission spaceand using complex mixed-integer formulations, our approach is computationallyefficient and easier to implement. The MPC algorithm is initially assessed inMATLAB, followed by Gazebo simulations and actual experimental tests conductedin an outdoor environment. The results demonstrate that the proposed strategycan generate efficient and smooth trajectories for search and coveragemissions.
  </details>

[‚Üë Back to Top](#-full-archive)

</details>

